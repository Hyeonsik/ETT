{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ae6088-a06e-4a63-9777-3a23984cab27",
   "metadata": {},
   "source": [
    "# Input loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "538a52c0-d59e-4bc3-ac25-272113147f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-04T17:26:14.985602Z",
     "iopub.status.busy": "2023-06-04T17:26:14.985090Z",
     "iopub.status.idle": "2023-06-04T17:26:15.006796Z",
     "shell.execute_reply": "2023-06-04T17:26:15.005844Z",
     "shell.execute_reply.started": "2023-06-04T17:26:14.985541Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (24764, 5), x_test: (6190, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import random, os, datetime, pickle\n",
    "import scipy, csv, math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "dat = np.load(f'dataset/ETT_size-outliers3.npz')\n",
    "#df = pickle.load(open('dataset/df-outliers3', 'rb'))\n",
    "x, y = dat['x'], dat['y']\n",
    "y_old  = dat['y_old']\n",
    "\n",
    "# training set의 뒤쪽 20%를 test set 으로 사용 (이미 shuffle된 상태)\n",
    "nsamp = len(y)\n",
    "ntest = int(nsamp * 0.2)\n",
    "ntrain = nsamp - ntest\n",
    "x_test = x[-ntest:, :]\n",
    "y_test = y[-ntest:]\n",
    "y_test_old = y_old[-ntest:]\n",
    "x_train = x[:ntrain, :]\n",
    "y_train = y[:ntrain]\n",
    "\n",
    "print(f'x_train: {(x_train).shape}, x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfcafa6-0b23-4656-85ad-814e4ca6d49a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-04T17:26:16.009972Z",
     "iopub.status.busy": "2023-06-04T17:26:16.009612Z",
     "iopub.status.idle": "2023-06-04T17:26:16.018127Z",
     "shell.execute_reply": "2023-06-04T17:26:16.017015Z",
     "shell.execute_reply.started": "2023-06-04T17:26:16.009923Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (24764, 4), x_test: (6190, 4)\n"
     ]
    }
   ],
   "source": [
    "# input: age, weight, height, cuff\n",
    "x_train = np.concatenate((x_train[:,0:1], x_train[:,2:5]),axis=-1)\n",
    "x_test = np.concatenate((x_test[:,0:1], x_test[:,2:5]),axis=-1)\n",
    "\n",
    "print(f'x_train: {x_train.shape}, x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b892d6d-8e1e-49b3-bbdf-40fff76bd63a",
   "metadata": {},
   "source": [
    "# Traditional age-based formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bf436e8-48b7-4043-8129-9fb95410c29f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T16:36:34.307236Z",
     "iopub.status.busy": "2023-03-09T16:36:34.306671Z",
     "iopub.status.idle": "2023-03-09T16:36:34.322175Z",
     "shell.execute_reply": "2023-03-09T16:36:34.321309Z",
     "shell.execute_reply.started": "2023-03-09T16:36:34.307182Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model(traditional) = age/4+4 (uncuffed) or age/4+3.5 (cuffeD)\n",
      "--------------\n",
      "explained_variance_score: 0.847\n",
      "mean_squared_errors: 0.297\n",
      "mean_absolute_errors: 0.437\n",
      "r2_score: 0.688\n",
      "acc: 0.279\n",
      "acc(+-0.5mm): 0.853\n"
     ]
    }
   ],
   "source": [
    "print('--------------')\n",
    "print('old model(traditional) = age/4+4 (uncuffed) or age/4+3.5 (cuffeD)')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_test_old):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_test_old):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_test_old):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_test_old):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_test_old==y_test)\n",
    "acc3 = np.mean((y_test_old >= y_test-0.5) & (y_test_old <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f5c8fff-c8e6-4f14-8859-574a58097e74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T16:38:09.005775Z",
     "iopub.status.busy": "2023-03-09T16:38:09.005186Z",
     "iopub.status.idle": "2023-03-09T16:38:09.021925Z",
     "shell.execute_reply": "2023-03-09T16:38:09.021025Z",
     "shell.execute_reply.started": "2023-03-09T16:38:09.005721Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model(Duracher) =  age/4+3.5 (cuffed)\n",
      "--------------\n",
      "explained_variance_score: 0.901\n",
      "mean_squared_errors: 0.285\n",
      "mean_absolute_errors: 0.420\n",
      "r2_score: 0.742\n",
      "acc: 0.294\n",
      "acc(+-0.5mm): 0.883\n"
     ]
    }
   ],
   "source": [
    "cuff_mask = x_test[:,3]==1\n",
    "\n",
    "print('--------------')\n",
    "print('old model(Duracher) =  age/4+3.5 (cuffed)')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test[cuff_mask], y_test_old[cuff_mask]):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test[cuff_mask], y_test_old[cuff_mask]):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test[cuff_mask], y_test_old[cuff_mask]):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test[cuff_mask], y_test_old[cuff_mask]):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_test_old[cuff_mask]==y_test[cuff_mask])\n",
    "acc3 = np.mean((y_test_old[cuff_mask] >= y_test[cuff_mask]-0.5) & (y_test_old[cuff_mask] <= y_test[cuff_mask]+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c04d6e3f-8434-4f65-837f-45134c560f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-21T02:08:38.010307Z",
     "iopub.status.busy": "2023-04-21T02:08:38.009865Z",
     "iopub.status.idle": "2023-04-21T02:08:38.026651Z",
     "shell.execute_reply": "2023-04-21T02:08:38.025906Z",
     "shell.execute_reply.started": "2023-04-21T02:08:38.010253Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model(Cole) = age/4+4 (cuffed)\n",
      "--------------\n",
      "explained_variance_score: 0.901\n",
      "mean_squared_errors: 0.115\n",
      "mean_absolute_errors: 0.214\n",
      "r2_score: 0.896\n",
      "acc: 0.589\n",
      "acc(+-0.5mm): 0.983\n"
     ]
    }
   ],
   "source": [
    "y_test_old1 = y_test_old + 0.5\n",
    "cuff_mask = x_test[:,3]==1\n",
    "\n",
    "print('--------------')\n",
    "print('old model(Cole) = age/4+4 (cuffed)')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test[cuff_mask], y_test_old1[cuff_mask]):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test[cuff_mask], y_test_old1[cuff_mask]):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test[cuff_mask], y_test_old1[cuff_mask]):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test[cuff_mask], y_test_old1[cuff_mask]):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_test_old1[cuff_mask]==y_test[cuff_mask])\n",
    "acc3 = np.mean((y_test_old1[cuff_mask] >= y_test[cuff_mask]-0.5) & (y_test_old1[cuff_mask] <= y_test[cuff_mask]+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc2b1dce-9124-4d88-9919-186cbc06176f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T16:38:15.166464Z",
     "iopub.status.busy": "2023-03-09T16:38:15.165941Z",
     "iopub.status.idle": "2023-03-09T16:38:15.182296Z",
     "shell.execute_reply": "2023-03-09T16:38:15.181442Z",
     "shell.execute_reply.started": "2023-03-09T16:38:15.166412Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model(Cole) = age/4+4 (uncuffed)\n",
      "--------------\n",
      "explained_variance_score: 0.819\n",
      "mean_squared_errors: 0.302\n",
      "mean_absolute_errors: 0.444\n",
      "r2_score: 0.660\n",
      "acc: 0.273\n",
      "acc(+-0.5mm): 0.840\n"
     ]
    }
   ],
   "source": [
    "cuff_mask = x_test[:,3]==0\n",
    "\n",
    "print('--------------')\n",
    "print('old model(Cole) = age/4+4 (uncuffed)')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test[cuff_mask], y_test_old[cuff_mask]):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test[cuff_mask], y_test_old[cuff_mask]):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test[cuff_mask], y_test_old[cuff_mask]):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test[cuff_mask], y_test_old[cuff_mask]):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_test_old[cuff_mask]==y_test[cuff_mask])\n",
    "acc3 = np.mean((y_test_old[cuff_mask] >= y_test[cuff_mask]-0.5) & (y_test_old[cuff_mask] <= y_test[cuff_mask]+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3368cc12-c9cc-4160-b649-961a76f9c932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T07:43:12.475966Z",
     "iopub.status.busy": "2023-05-01T07:43:12.475470Z",
     "iopub.status.idle": "2023-05-01T07:43:12.495406Z",
     "shell.execute_reply": "2023-05-01T07:43:12.494369Z",
     "shell.execute_reply.started": "2023-05-01T07:43:12.475907Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model(Penlington) = age/4+4.5 (uncuffed)\n",
      "--------------\n",
      "explained_variance_score: 0.819\n",
      "mean_squared_errors: 0.175\n",
      "mean_absolute_errors: 0.283\n",
      "r2_score: 0.802\n",
      "acc: 0.500\n",
      "acc(+-0.5mm): 0.933\n"
     ]
    }
   ],
   "source": [
    "y_test_old1 = y_test_old + 0.5\n",
    "cuff_mask = x_test[:,3]==0\n",
    "\n",
    "print('--------------')\n",
    "print('old model(Penlington) = age/4+4.5 (uncuffed)')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test[cuff_mask], y_test_old1[cuff_mask]):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test[cuff_mask], y_test_old1[cuff_mask]):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test[cuff_mask], y_test_old1[cuff_mask]):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test[cuff_mask], y_test_old1[cuff_mask]):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_test_old1[cuff_mask]==y_test[cuff_mask])\n",
    "acc3 = np.mean((y_test_old1[cuff_mask] >= y_test[cuff_mask]-0.5) & (y_test_old1[cuff_mask] <= y_test[cuff_mask]+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "117df34e-b50e-489b-a388-1799c27ddf1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T17:04:29.774352Z",
     "iopub.status.busy": "2023-04-12T17:04:29.773974Z",
     "iopub.status.idle": "2023-04-12T17:04:30.799429Z",
     "shell.execute_reply": "2023-04-12T17:04:30.798667Z",
     "shell.execute_reply.started": "2023-04-12T17:04:29.774313Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model(Cole) = age/4+4 (uncuffed)\n",
      "--------------\n",
      "acc: 0.273+-0.013 -> 0.260 ~ 0.285\n",
      "acc(+-0.5mm): 0.841+-0.011 -> 0.830 ~ 0.852 \n"
     ]
    }
   ],
   "source": [
    "cuff_mask = x_test[:,3]==0\n",
    "\n",
    "y1_trads, y2_trads = [], []\n",
    "\n",
    "print('--------------')\n",
    "print('old model(Cole) = age/4+4 (uncuffed)')\n",
    "print('--------------')\n",
    "\n",
    "for i in range(200):\n",
    "    mask = np.array([random.randint(0,1)==1 for j in range(len(x_test[cuff_mask]))])\n",
    "    X_test = (x_test[cuff_mask])[mask]\n",
    "    Y_test = (y_test[cuff_mask])[mask]\n",
    "    \n",
    "    y_trad = (y_test_old[cuff_mask])[mask]\n",
    "    y_trad = np.round(y_trad * 2) / 2\n",
    "    y1_trads.append(np.mean(y_trad == Y_test))\n",
    "    y2_trads.append(np.mean((y_trad >= Y_test-0.5) & (y_trad <= Y_test+0.5)))\n",
    "    \n",
    "    \n",
    "print(f'acc: {np.mean(y1_trads):.3f}+-{1.96*np.std(y1_trads):.3f} -> {np.mean(y1_trads)-1.96*np.std(y1_trads):.3f} ~ {np.mean(y1_trads)+1.96*np.std(y1_trads):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(y2_trads):.3f}+-{1.96*np.std(y2_trads):.3f} -> {np.mean(y2_trads)-1.96*np.std(y2_trads):.3f} ~ {np.mean(y2_trads)+1.96*np.std(y2_trads):.3f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98d37e70-893b-4c64-9b71-aef0dcad4bdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T07:10:58.621975Z",
     "iopub.status.busy": "2023-05-03T07:10:58.621663Z",
     "iopub.status.idle": "2023-05-03T07:10:59.228215Z",
     "shell.execute_reply": "2023-05-03T07:10:59.227550Z",
     "shell.execute_reply.started": "2023-05-03T07:10:58.621934Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model(Duracher) = age/4+3.5 (cuffed)\n",
      "--------------\n",
      "acc: 0.294+-0.022 -> 0.272 ~ 0.316\n",
      "acc(+-0.5mm): 0.883+-0.014 -> 0.869 ~ 0.898 \n"
     ]
    }
   ],
   "source": [
    "cuff_mask = x_test[:,3]==1\n",
    "\n",
    "y1_trads, y2_trads = [], []\n",
    "\n",
    "print('--------------')\n",
    "print('old model(Duracher) = age/4+3.5 (cuffed)')\n",
    "print('--------------')\n",
    "\n",
    "for i in range(200):\n",
    "    mask = np.array([random.randint(0,1)==1 for j in range(len(x_test[cuff_mask]))])\n",
    "    X_test = (x_test[cuff_mask])[mask]\n",
    "    Y_test = (y_test[cuff_mask])[mask]\n",
    "    \n",
    "    y_trad = (y_test_old[cuff_mask])[mask]\n",
    "    y_trad = np.round(y_trad * 2) / 2\n",
    "    y1_trads.append(np.mean(y_trad == Y_test))\n",
    "    y2_trads.append(np.mean((y_trad >= Y_test-0.5) & (y_trad <= Y_test+0.5)))\n",
    "    \n",
    "    \n",
    "print(f'acc: {np.mean(y1_trads):.3f}+-{1.96*np.std(y1_trads):.3f} -> {np.mean(y1_trads)-1.96*np.std(y1_trads):.3f} ~ {np.mean(y1_trads)+1.96*np.std(y1_trads):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(y2_trads):.3f}+-{1.96*np.std(y2_trads):.3f} -> {np.mean(y2_trads)-1.96*np.std(y2_trads):.3f} ~ {np.mean(y2_trads)+1.96*np.std(y2_trads):.3f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d884c2e-3138-4c32-8366-ca049910d178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T07:44:59.684756Z",
     "iopub.status.busy": "2023-05-01T07:44:59.684257Z",
     "iopub.status.idle": "2023-05-01T07:45:01.087853Z",
     "shell.execute_reply": "2023-05-01T07:45:01.087110Z",
     "shell.execute_reply.started": "2023-05-01T07:44:59.684696Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model(Penlington) = age/4+4.5 (uncuffed)\n",
      "--------------\n",
      "acc: 0.501+-0.015 -> 0.486 ~ 0.516\n",
      "acc(+-0.5mm): 0.933+-0.008 -> 0.925 ~ 0.940 \n"
     ]
    }
   ],
   "source": [
    "cuff_mask = x_test[:,3]==0\n",
    "y_test_old1 = y_test_old + 0.5\n",
    "\n",
    "y1_trads, y2_trads = [], []\n",
    "\n",
    "print('--------------')\n",
    "print('old model(Penlington) = age/4+4.5 (uncuffed)')\n",
    "print('--------------')\n",
    "\n",
    "for i in range(200):\n",
    "    mask = np.array([random.randint(0,1)==1 for j in range(len(x_test[cuff_mask]))])\n",
    "    X_test = (x_test[cuff_mask])[mask]\n",
    "    Y_test = (y_test[cuff_mask])[mask]\n",
    "    \n",
    "    y_trad = (y_test_old1[cuff_mask])[mask]\n",
    "    y_trad = np.round(y_trad * 2) / 2\n",
    "    y1_trads.append(np.mean(y_trad == Y_test))\n",
    "    y2_trads.append(np.mean((y_trad >= Y_test-0.5) & (y_trad <= Y_test+0.5)))\n",
    "    \n",
    "    \n",
    "print(f'acc: {np.mean(y1_trads):.3f}+-{1.96*np.std(y1_trads):.3f} -> {np.mean(y1_trads)-1.96*np.std(y1_trads):.3f} ~ {np.mean(y1_trads)+1.96*np.std(y1_trads):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(y2_trads):.3f}+-{1.96*np.std(y2_trads):.3f} -> {np.mean(y2_trads)-1.96*np.std(y2_trads):.3f} ~ {np.mean(y2_trads)+1.96*np.std(y2_trads):.3f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18853b53-e5f5-4b70-b041-211e068a1888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T07:12:16.611134Z",
     "iopub.status.busy": "2023-05-03T07:12:16.610633Z",
     "iopub.status.idle": "2023-05-03T07:12:17.236869Z",
     "shell.execute_reply": "2023-05-03T07:12:17.236190Z",
     "shell.execute_reply.started": "2023-05-03T07:12:16.611075Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model() = age/4+4 (cuffed)\n",
      "--------------\n",
      "acc: 0.590+-0.026 -> 0.565 ~ 0.616\n",
      "acc(+-0.5mm): 0.983+-0.006 -> 0.977 ~ 0.989 \n"
     ]
    }
   ],
   "source": [
    "cuff_mask = x_test[:,3]==1\n",
    "y_test_old1 = y_test_old + 0.5\n",
    "\n",
    "y1_trads, y2_trads = [], []\n",
    "\n",
    "print('--------------')\n",
    "print('old model() = age/4+4 (cuffed)')\n",
    "print('--------------')\n",
    "\n",
    "for i in range(200):\n",
    "    mask = np.array([random.randint(0,1)==1 for j in range(len(x_test[cuff_mask]))])\n",
    "    X_test = (x_test[cuff_mask])[mask]\n",
    "    Y_test = (y_test[cuff_mask])[mask]\n",
    "    \n",
    "    y_trad = (y_test_old1[cuff_mask])[mask]\n",
    "    y_trad = np.round(y_trad * 2) / 2\n",
    "    y1_trads.append(np.mean(y_trad == Y_test))\n",
    "    y2_trads.append(np.mean((y_trad >= Y_test-0.5) & (y_trad <= Y_test+0.5)))\n",
    "    \n",
    "    \n",
    "print(f'acc: {np.mean(y1_trads):.3f}+-{1.96*np.std(y1_trads):.3f} -> {np.mean(y1_trads)-1.96*np.std(y1_trads):.3f} ~ {np.mean(y1_trads)+1.96*np.std(y1_trads):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(y2_trads):.3f}+-{1.96*np.std(y2_trads):.3f} -> {np.mean(y2_trads)-1.96*np.std(y2_trads):.3f} ~ {np.mean(y2_trads)+1.96*np.std(y2_trads):.3f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "548c5b23-78ea-4166-81bb-c10c847eea34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T16:35:57.690958Z",
     "iopub.status.busy": "2023-03-09T16:35:57.690491Z",
     "iopub.status.idle": "2023-03-09T16:35:57.876493Z",
     "shell.execute_reply": "2023-03-09T16:35:57.875932Z",
     "shell.execute_reply.started": "2023-03-09T16:35:57.690908Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model(length-based) = height/30+2\n",
      "--------------\n",
      "explained_variance_score: 0.819\n",
      "mean_squared_errors: 0.292\n",
      "mean_absolute_errors: 0.416\n",
      "r2_score: 0.693\n",
      "acc: 0.333\n",
      "acc(+-0.5mm): 0.840\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "imp = IterativeImputer().fit(x_train)\n",
    "x_train_imputed = imp.transform(x_train)\n",
    "x_test_imputed = imp.transform(x_test)\n",
    "\n",
    "# Broselow tape : body length / 30 + 2\n",
    "y_bros = x_test_imputed[:,3] / 30 + 2\n",
    "y_bros = np.round(y_bros * 2) / 2\n",
    "\n",
    "print('--------------')\n",
    "print('old model(length-based) = height/30+2')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_bros):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_bros):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_bros):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_bros):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_bros==y_test)\n",
    "acc3 = np.mean((y_bros >= y_test-0.5) & (y_bros <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04ac22fb-b97f-4a5c-b668-7fb689dc19a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T14:38:45.849165Z",
     "iopub.status.busy": "2023-03-05T14:38:45.848588Z",
     "iopub.status.idle": "2023-03-05T14:38:46.055247Z",
     "shell.execute_reply": "2023-03-05T14:38:46.054661Z",
     "shell.execute_reply.started": "2023-03-05T14:38:45.849108Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model(weight) = weight/10 + 3.5\n",
      "--------------\n",
      "explained_variance_score: 0.704\n",
      "mean_squared_errors: 0.358\n",
      "mean_absolute_errors: 0.452\n",
      "r2_score: 0.624\n",
      "acc: 0.322\n",
      "acc(+-0.5mm): 0.805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "imp = IterativeImputer().fit(x_train)\n",
    "x_train_imputed = imp.transform(x_train)\n",
    "x_test_imputed = imp.transform(x_test)\n",
    "\n",
    "# Broselow tape : weight / 10 + 3.5\n",
    "y_bros = x_test_imputed[:,2] / 10 + 3.5\n",
    "y_bros = np.round(y_bros * 2) / 2\n",
    "\n",
    "print('--------------')\n",
    "print('old model(weight) = weight/10 + 3.5')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_bros):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_bros):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_bros):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_bros):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_bros==y_test)\n",
    "acc3 = np.mean((y_bros >= y_test-0.5) & (y_bros <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44308cf0-0366-49ae-9766-409e4b5c960b",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3da7767a-1bb1-4f15-b8ff-ea756a18f517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T16:21:38.569058Z",
     "iopub.status.busy": "2023-03-09T16:21:38.568516Z",
     "iopub.status.idle": "2023-03-09T16:21:38.798363Z",
     "shell.execute_reply": "2023-03-09T16:21:38.797503Z",
     "shell.execute_reply.started": "2023-03-09T16:21:38.569005Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (24764, 4), x_test: (6190, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "imp = IterativeImputer().fit(x_train)\n",
    "x_train_imputed = imp.transform(x_train)\n",
    "x_test_imputed = imp.transform(x_test)\n",
    "\n",
    "print(f'x_train: {x_train_imputed.shape}, x_test: {x_test_imputed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6641aca6-9795-40e2-93ae-4c0c227b1d2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T16:22:42.173511Z",
     "iopub.status.busy": "2023-03-09T16:22:42.173018Z",
     "iopub.status.idle": "2023-03-09T16:22:42.194240Z",
     "shell.execute_reply": "2023-03-09T16:22:42.193263Z",
     "shell.execute_reply.started": "2023-03-09T16:22:42.173459Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선형회귀 모델: 기울기 [ 0.07771479 -0.00735902  0.03077772 -0.47774297], 절편 1.872\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.871\n",
      "mean_squared_errors: 0.123\n",
      "mean_absolute_errors: 0.219\n",
      "r2_score: 0.871\n",
      "acc: 0.588\n",
      "acc(+-0.5mm): 0.974\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(x_train_imputed, y_train)\n",
    "y_pred = lin_model.predict(x_test_imputed)\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "print(f'선형회귀 모델: 기울기 {lin_model.coef_}, 절편 {lin_model.intercept_:.3f}')\n",
    "print('--------------')\n",
    "print('linear regression model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test)\n",
    "acc3 = np.mean((y_pred >= y_test-0.5) & (y_pred <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8e8cfbd-60b1-4127-a36d-bb82f36051d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T16:23:48.496245Z",
     "iopub.status.busy": "2023-03-09T16:23:48.495680Z",
     "iopub.status.idle": "2023-03-09T16:23:48.509948Z",
     "shell.execute_reply": "2023-03-09T16:23:48.509163Z",
     "shell.execute_reply.started": "2023-03-09T16:23:48.496191Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "odir_f = f'acc1-{acc1:.3f}_acc3-{acc3:.3f}_LR_4inputs_{nfold}fold'\n",
    "odir = f'result/outliers+w+h/size/{odir_f}'\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "pickle.dump(lin_model, open(f'{odir}/model', 'wb'))\n",
    "f = open(f'{odir}/result.txt', 'w')\n",
    "f.write(f'classification model')\n",
    "f.write(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "f.write(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d962d-ad92-4bbf-af99-1fde660981b0",
   "metadata": {},
   "source": [
    "### test validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d77b300-edd8-4587-aea9-34b610142ee1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T17:01:24.008041Z",
     "iopub.status.busy": "2023-04-12T17:01:24.007538Z",
     "iopub.status.idle": "2023-04-12T17:01:25.370612Z",
     "shell.execute_reply": "2023-04-12T17:01:25.369851Z",
     "shell.execute_reply.started": "2023-04-12T17:01:24.007989Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.587+-0.013 -> 0.575 ~ 0.600\n",
      "acc(+-0.5mm): 0.974+-0.004 -> 0.970 ~ 0.978 \n"
     ]
    }
   ],
   "source": [
    "# LR model\n",
    "lr = pickle.load(open(f'result/outliers+w+h/size/acc1-0.588_acc3-0.974_LR_4inputs/model','rb'))\n",
    "\n",
    "y1_lrs, y2_lrs = [], []\n",
    "for i in range(200):\n",
    "    \n",
    "    mask = np.array([random.randint(0,1)==1 for j in range(len(x_test_imputed))])\n",
    "    X_test = x_test_imputed[mask]\n",
    "    Y_test = y_test[mask]\n",
    "    \n",
    "    y_lr = lr.predict(X_test)\n",
    "    y_lr = np.round(y_lr * 2) / 2\n",
    "    y1_lrs.append(np.mean(y_lr == Y_test))\n",
    "    y2_lrs.append(np.mean((y_lr >= Y_test-0.5) & (y_lr <= Y_test+0.5)))\n",
    "    \n",
    "print(f'acc: {np.mean(y1_lrs):.3f}+-{1.96*np.std(y1_lrs):.3f} -> {np.mean(y1_lrs)-1.96*np.std(y1_lrs):.3f} ~ {np.mean(y1_lrs)+1.96*np.std(y1_lrs):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(y2_lrs):.3f}+-{1.96*np.std(y2_lrs):.3f} -> {np.mean(y2_lrs)-1.96*np.std(y2_lrs):.3f} ~ {np.mean(y2_lrs)+1.96*np.std(y2_lrs):.3f} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9576bbb-1ce7-4403-b0ec-eebcd71e8b5d",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b01b2c6-e72a-40ff-802b-e1d00406bdea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:49:13.011411Z",
     "iopub.status.busy": "2023-04-12T07:49:13.010863Z",
     "iopub.status.idle": "2023-04-12T07:49:13.019739Z",
     "shell.execute_reply": "2023-04-12T07:49:13.018724Z",
     "shell.execute_reply.started": "2023-04-12T07:49:13.011360Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (24764, 3), x_test: (6190, 3)\n"
     ]
    }
   ],
   "source": [
    "# input: age, weight, height, cuff\n",
    "x_train = np.concatenate((x_train[:,0:1], x_train[:,2:5]),axis=-1)\n",
    "x_test = np.concatenate((x_test[:,0:1], x_test[:,2:5]),axis=-1)\n",
    "\n",
    "print(f'x_train: {x_train.shape}, x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ad3a71c0-bf28-44dc-8cf4-0097fcb8f1c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-28T03:39:12.771161Z",
     "iopub.status.busy": "2023-05-28T03:39:12.770658Z",
     "iopub.status.idle": "2023-05-28T10:22:29.946519Z",
     "shell.execute_reply": "2023-05-28T10:22:29.945745Z",
     "shell.execute_reply.started": "2023-05-28T03:39:12.771101Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2880 candidates, totalling 28800 fits\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'colsample_bytree': 1, 'gamma': 0.7, 'max_depth': 7, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 1}\n",
      "0.9054475963493276\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.886\n",
      "mean_squared_errors: 0.109\n",
      "mean_absolute_errors: 0.199\n",
      "r2_score: 0.886\n",
      "acc: 0.620\n",
      "acc(+-0.5mm): 0.983\n"
     ]
    }
   ],
   "source": [
    "# age (일단위)\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.01, 0.03, 0.05, 0.07] #, #[0.01, 0.03, 0.05],\n",
    "                'max_depth': [3, 4, 5, 7],#[3,4,5],\n",
    "                'n_estimators': [25, 50, 75, 100, 300],\n",
    "                'subsample': [0.5, 0.8, 1], #[0.5, 0.8, 1],\n",
    "                'colsample_bytree': [0.5, 0.8, 1], #[0.8, 1],\n",
    "                'gamma': [0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [1, 10, 30, 100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=xgb.sklearn.XGBRegressor(),\n",
    "                n_jobs=-1,\n",
    "                verbose=3,\n",
    "                param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train, y_train)\n",
    "model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = model.predict(x_test).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test)\n",
    "acc3 = np.mean((y_pred >= y_test-0.5) & (y_pred <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65183d47-b138-4f0c-97b9-7a966a089bef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T17:57:24.499296Z",
     "iopub.status.busy": "2023-03-03T17:57:24.498710Z",
     "iopub.status.idle": "2023-03-03T17:58:52.290138Z",
     "shell.execute_reply": "2023-03-03T17:58:52.289388Z",
     "shell.execute_reply.started": "2023-03-03T17:57:24.499238Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 180 candidates, totalling 1800 fits\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'colsample_bytree': 1, 'max_depth': 5, 'n_estimators': 25, 'subsample': 1}\n",
      "0.905165370784901\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.886\n",
      "mean_squared_errors: 0.109\n",
      "mean_absolute_errors: 0.199\n",
      "r2_score: 0.886\n",
      "acc: 0.622\n",
      "acc(+-0.5mm): 0.981\n"
     ]
    }
   ],
   "source": [
    "# age (일단위)\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.01, 0.03, 0.05, 0.07] #, #[0.01, 0.03, 0.05],\n",
    "                'max_depth': [3, 4, 5, 7],#[3,4,5],\n",
    "                'n_estimators': [25, 50, 75, 100, 300],\n",
    "                'subsample': [0.5, 0.8, 1], #[0.5, 0.8, 1],\n",
    "                'colsample_bytree': [0.5, 0.8, 1], #[0.8, 1],\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=xgb.sklearn.XGBRegressor(),\n",
    "                n_jobs=-1,\n",
    "                verbose=3,\n",
    "                param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train, y_train)\n",
    "model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = model.predict(x_test).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test)\n",
    "acc3 = np.mean((y_pred >= y_test-0.5) & (y_pred <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e9e8d830-8b3d-466d-a845-0b6782a5f17a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-28T15:29:06.870822Z",
     "iopub.status.busy": "2023-05-28T15:29:06.870311Z",
     "iopub.status.idle": "2023-05-28T15:29:06.886589Z",
     "shell.execute_reply": "2023-05-28T15:29:06.885711Z",
     "shell.execute_reply.started": "2023-05-28T15:29:06.870762Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "odir_f = f'acc1-{acc1:.3f}_acc3-{acc3:.3f}_XGBR_4inputs_{nfold}fold'\n",
    "odir = f'result/outliers+w+h/size/{odir_f}'\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "model.save_model(f'{odir}/model.model')\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "pickle.dump(param_dict, open(f'{odir}/param_dict', 'wb'))\n",
    "f = open(f'{odir}/result.txt', 'w')\n",
    "f.write(f'classification model')\n",
    "f.write(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "f.write(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bf337-5ecd-486f-8463-e5443cb54a88",
   "metadata": {},
   "source": [
    "### test validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4647fac-2709-4f96-b383-7a3ad634c9a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:44:02.426751Z",
     "iopub.status.busy": "2023-04-12T14:44:02.426251Z",
     "iopub.status.idle": "2023-04-12T14:44:06.039973Z",
     "shell.execute_reply": "2023-04-12T14:44:06.039264Z",
     "shell.execute_reply.started": "2023-04-12T14:44:02.426699Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.622+-0.012 -> 0.610 ~ 0.634\n",
      "acc(+-0.5mm): 0.982+-0.003 -> 0.978 ~ 0.985 \n"
     ]
    }
   ],
   "source": [
    "# xgbr model\n",
    "xgbr = xgb.XGBRegressor()\n",
    "xgbr.load_model('result/outliers+w+h/size/acc1-0.622_acc3-0.981_XGBR_4inputs_10fold/model.model')\n",
    "\n",
    "y1_xgbrs, y2_xgbrs = [], []\n",
    "for i in range(200):\n",
    "    \n",
    "    mask = np.array([random.randint(0,1)==1 for j in range(len(x_test))])\n",
    "    X_test = x_test[mask]\n",
    "    Y_test = y_test[mask]\n",
    "    \n",
    "    y_xgbr = xgbr.predict(X_test)\n",
    "    y_xgbr = np.round(y_xgbr * 2) / 2\n",
    "    y1_xgbrs.append(np.mean(y_xgbr == Y_test))\n",
    "    y2_xgbrs.append(np.mean((y_xgbr >= Y_test-0.5) & (y_xgbr <= Y_test+0.5)))\n",
    "    \n",
    "print(f'acc: {np.mean(y1_xgbrs):.3f}+-{1.96*np.std(y1_xgbrs):.3f} -> {np.mean(y1_xgbrs)-1.96*np.std(y1_xgbrs):.3f} ~ {np.mean(y1_xgbrs)+1.96*np.std(y1_xgbrs):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(y2_xgbrs):.3f}+-{1.96*np.std(y2_xgbrs):.3f} -> {np.mean(y2_xgbrs)-1.96*np.std(y2_xgbrs):.3f} ~ {np.mean(y2_xgbrs)+1.96*np.std(y2_xgbrs):.3f} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4915a5c-9cc9-4551-abd3-ec8afe2512ef",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed090967-5060-400c-91ac-7fe0aa454856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:50:48.367624Z",
     "iopub.status.busy": "2023-04-12T07:50:48.367108Z",
     "iopub.status.idle": "2023-04-12T07:50:48.375516Z",
     "shell.execute_reply": "2023-04-12T07:50:48.374576Z",
     "shell.execute_reply.started": "2023-04-12T07:50:48.367572Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (24764, 3), x_test: (6190, 3)\n"
     ]
    }
   ],
   "source": [
    "# input: age, weight, height, cuff\n",
    "x_train = np.concatenate((x_train[:,0:1], x_train[:,2:5]),axis=-1)\n",
    "x_test = np.concatenate((x_test[:,0:1], x_test[:,2:5]),axis=-1)\n",
    "\n",
    "print(f'x_train: {x_train.shape}, x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85494e27-ee6d-45a6-b22e-6f55e06eaf3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:41:47.079964Z",
     "iopub.status.busy": "2023-04-12T14:41:47.079469Z",
     "iopub.status.idle": "2023-04-12T14:41:47.267044Z",
     "shell.execute_reply": "2023-04-12T14:41:47.266323Z",
     "shell.execute_reply.started": "2023-04-12T14:41:47.079912Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (24764, 4), x_test: (6190, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "imp = IterativeImputer().fit(x_train)\n",
    "x_train_imputed = imp.transform(x_train)\n",
    "x_test_imputed = imp.transform(x_test)\n",
    "\n",
    "print(f'x_train: {x_train_imputed.shape}, x_test: {x_test_imputed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e8fbf0f-d30d-45a3-8581-034162dbb281",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T06:40:19.836210Z",
     "iopub.status.busy": "2023-03-06T06:40:19.835703Z",
     "iopub.status.idle": "2023-03-06T06:50:01.038425Z",
     "shell.execute_reply": "2023-03-06T06:50:01.037685Z",
     "shell.execute_reply.started": "2023-03-06T06:40:19.836156Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 216 candidates, totalling 2160 fits\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.893 total time=  14.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.898 total time=   3.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.898 total time=   7.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.893 total time=   4.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.899 total time=   7.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.893 total time=   3.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.893 total time=   6.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.894 total time=  21.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.897 total time=  13.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.895 total time=   3.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.896 total time=   6.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.895 total time=  19.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.892 total time=   2.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.900 total time=   4.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.895 total time=   2.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.896 total time=   4.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.901 total time=   2.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.901 total time=   4.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.902 total time=   8.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.901 total time=   1.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.899 total time=   3.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.895 total time=   1.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.896 total time=   3.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.902 total time=  11.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.896 total time=   9.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.897 total time=   2.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.896 total time=   4.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.895 total time=   2.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.896 total time=   4.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.898 total time=   8.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.901 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.902 total time=   3.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.895 total time=   2.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.904 total time=   4.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.903 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.904 total time=   4.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.901 total time=   2.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.904 total time=   3.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.835 total time=   5.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.835 total time=  38.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.844 total time=   5.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.844 total time=  10.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.835 total time=  21.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.837 total time=   5.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.837 total time=  10.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.843 total time=  33.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.844 total time=  30.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.882 total time=  20.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.887 total time=   6.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.892 total time=   3.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.888 total time=   6.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.894 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.895 total time=   6.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.895 total time=  12.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.891 total time=   2.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.891 total time=   5.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.890 total time=  17.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.892 total time=  12.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.890 total time=   7.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.892 total time=   7.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.894 total time=  22.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.895 total time=  21.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.895 total time=  14.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.896 total time=   3.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.901 total time=   6.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.902 total time=  19.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.896 total time=  13.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.902 total time=   8.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.894 total time=  12.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.898 total time=   8.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.898 total time=   2.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.899 total time=   4.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.903 total time=   2.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.902 total time=   3.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.891 total time=   2.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.892 total time=   4.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.899 total time=   2.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.900 total time=   4.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.894 total time=  13.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.896 total time=   8.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.896 total time=   2.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.896 total time=   3.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.895 total time=  11.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.901 total time=   7.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.899 total time=  11.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.823 total time=   5.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.807 total time=  22.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.835 total time=  22.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.844 total time=  24.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.855 total time=   5.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.855 total time=  10.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.857 total time=   5.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.857 total time=  10.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.859 total time=   5.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.844 total time=  10.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.857 total time=  31.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.880 total time=  20.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.892 total time=   6.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.891 total time=   4.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.895 total time=   6.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.894 total time=   3.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.895 total time=   6.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.895 total time=   3.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.896 total time=   6.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.892 total time=   2.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.897 total time=   5.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.897 total time=   2.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.898 total time=   5.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.897 total time=  11.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.887 total time=   3.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.888 total time=   6.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.889 total time=   3.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.882 total time=   6.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.888 total time=  21.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.887 total time=   3.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.887 total time=   7.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.895 total time=   4.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.894 total time=   4.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.898 total time=  14.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.894 total time=  21.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.899 total time=   6.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.899 total time=   3.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.891 total time=   6.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.892 total time=   3.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.899 total time=   6.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.896 total time=  19.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.896 total time=  13.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.892 total time=   2.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.894 total time=   4.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.896 total time=   8.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.896 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.897 total time=   4.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.901 total time=  12.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.896 total time=   7.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.902 total time=  12.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.904 total time=   7.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.898 total time=  13.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.900 total time=   8.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.900 total time=   2.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.901 total time=   4.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.901 total time=  12.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.899 total time=   7.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.901 total time=   1.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.901 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.898 total time=  11.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.826 total time=   5.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.819 total time=  11.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.815 total time=  34.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.841 total time=  21.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.835 total time=   5.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.838 total time=  11.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.851 total time=   5.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.851 total time=  11.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.849 total time=   5.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.849 total time=  10.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.848 total time=  31.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.882 total time=  20.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.889 total time=   6.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.893 total time=   3.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.891 total time=   6.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.891 total time=   3.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.891 total time=   6.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.895 total time=  18.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.893 total time=  11.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.891 total time=   2.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.892 total time=   5.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.892 total time=  17.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.890 total time=  13.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.894 total time=   3.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.894 total time=   3.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.895 total time=   6.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.893 total time=   3.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.893 total time=   6.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.895 total time=   3.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.895 total time=   7.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.889 total time=  15.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.899 total time=  16.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.897 total time=   3.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.898 total time=   7.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.900 total time=  21.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.900 total time=  19.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.897 total time=  13.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.890 total time=   2.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.897 total time=   4.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.894 total time=  12.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.901 total time=   8.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.901 total time=  12.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.902 total time=   7.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.902 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.898 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.903 total time=  12.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.898 total time=   8.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.897 total time=   2.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.897 total time=   4.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.901 total time=  12.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.902 total time=   7.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.898 total time=   2.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.899 total time=   4.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.903 total time=   1.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.903 total time=   4.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.805 total time=   5.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.835 total time=  23.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.817 total time=   5.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.817 total time=  11.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.835 total time=   5.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.827 total time=  10.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.844 total time=  34.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.844 total time=  22.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.857 total time=   6.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.861 total time=  11.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.857 total time=   5.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.861 total time=  11.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.890 total time=   3.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.890 total time=   7.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.889 total time=  20.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.895 total time=   6.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.894 total time=   3.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.893 total time=   6.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.893 total time=   3.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.893 total time=   6.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.889 total time=   3.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.890 total time=   6.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.891 total time=  18.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.893 total time=  11.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.881 total time=   3.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.880 total time=   6.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.881 total time=   3.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.883 total time=   6.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.885 total time=  19.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.895 total time=  12.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.888 total time=   3.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.889 total time=   6.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.897 total time=  14.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.897 total time=   3.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.893 total time=   7.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.890 total time=   3.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.898 total time=   6.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.898 total time=  13.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.894 total time=   3.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.894 total time=   6.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.900 total time=   3.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.901 total time=   6.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.902 total time=  13.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.896 total time=   3.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.901 total time=   6.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.891 total time=   2.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.892 total time=   4.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.893 total time=  10.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.900 total time=   9.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.902 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.902 total time=   4.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.902 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.902 total time=   4.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.899 total time=   2.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.900 total time=   3.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.900 total time=  12.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.901 total time=   7.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.904 total time=  12.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.900 total time=   8.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.900 total time=   2.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.901 total time=   3.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.901 total time=  11.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.900 total time=   7.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.902 total time=   1.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.903 total time=   3.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.903 total time=   7.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.811 total time=   5.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.826 total time=  23.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.825 total time=   5.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.825 total time=  11.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.835 total time=   5.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.835 total time=  10.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.844 total time=  33.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.843 total time=  21.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.844 total time=   5.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.851 total time=  11.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.849 total time=   5.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.848 total time=  10.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.856 total time=  30.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.890 total time=  19.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.889 total time=  18.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.889 total time=  12.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.898 total time=  17.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.896 total time=  11.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.882 total time=  20.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.894 total time=  12.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.886 total time=  18.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.890 total time=  13.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.897 total time=   2.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.896 total time=   5.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.896 total time=   7.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.892 total time=  14.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.894 total time=  14.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.894 total time=  15.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.899 total time=   3.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.899 total time=   7.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.899 total time=   3.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.900 total time=   6.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.895 total time=   3.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.893 total time=   6.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.897 total time=  19.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.893 total time=  13.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.901 total time=   4.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.900 total time=   2.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.900 total time=   4.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.902 total time=  12.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.902 total time=   7.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.897 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.898 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.898 total time=  11.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.893 total time=   9.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.899 total time=   2.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.901 total time=   4.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.900 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.893 total time=   4.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.902 total time=  12.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.902 total time=   8.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.903 total time=   2.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.902 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.895 total time=   1.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.899 total time=   4.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.813 total time=   5.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.821 total time=  23.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.834 total time=   5.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.834 total time=  11.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.852 total time=   5.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.852 total time=  10.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.835 total time=   5.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.827 total time=  10.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.827 total time=  32.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.844 total time=  20.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.851 total time=  32.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.887 total time=  20.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.892 total time=  12.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.894 total time=  12.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.894 total time=   3.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.894 total time=   6.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.887 total time=  18.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.896 total time=  11.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.899 total time=  17.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.892 total time=  13.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.886 total time=   3.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.888 total time=   6.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.895 total time=  13.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.893 total time=   3.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.894 total time=   5.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.892 total time=   2.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.889 total time=   5.7s\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'bootstrap': True, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "0.9004637795018378\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.880\n",
      "mean_squared_errors: 0.114\n",
      "mean_absolute_errors: 0.207\n",
      "r2_score: 0.880\n",
      "acc: 0.606\n",
      "acc(+-0.5mm): 0.980\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.891 total time=  15.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.898 total time=   3.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.898 total time=   7.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.899 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.894 total time=   6.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.899 total time=  22.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.900 total time=  20.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.901 total time=  13.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.899 total time=   2.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.900 total time=   4.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.900 total time=   8.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.900 total time=   8.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.901 total time=  12.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.901 total time=   8.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.901 total time=  12.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.903 total time=   8.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.895 total time=   2.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.895 total time=   4.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.895 total time=   2.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.895 total time=   4.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.900 total time=   2.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.895 total time=   4.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.900 total time=  12.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.901 total time=   8.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.898 total time=   1.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.899 total time=   4.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.904 total time=  11.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.899 total time=   8.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.821 total time=  35.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.832 total time=  34.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.835 total time=  21.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.845 total time=  32.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.848 total time=  20.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.859 total time=  31.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.892 total time=   3.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.884 total time=  12.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.889 total time=  12.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.889 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.889 total time=   5.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.889 total time=  17.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.892 total time=  12.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.884 total time=   3.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.885 total time=   6.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.880 total time=   3.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.890 total time=   7.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.891 total time=   3.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.886 total time=   6.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.892 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.887 total time=   6.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.894 total time=  12.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.894 total time=   3.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.894 total time=   5.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.894 total time=  18.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.898 total time=  11.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.896 total time=  14.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.897 total time=  22.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.898 total time=  20.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.898 total time=  14.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.892 total time=   4.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.900 total time=   8.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.901 total time=   3.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.900 total time=   6.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.899 total time=   2.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.900 total time=   4.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.898 total time=  13.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.896 total time=   8.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.894 total time=   8.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.896 total time=   2.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.896 total time=   3.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.896 total time=  11.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.898 total time=   8.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.899 total time=   2.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.897 total time=   4.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.899 total time=   2.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.899 total time=   4.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.896 total time=   2.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.900 total time=   4.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.902 total time=  12.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.902 total time=   8.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.901 total time=   1.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.903 total time=   3.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.896 total time=  11.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.901 total time=   7.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.805 total time=  23.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.847 total time=   5.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.847 total time=  11.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.841 total time=   5.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.841 total time=  10.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.831 total time=   5.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.840 total time=  11.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.835 total time=  31.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.848 total time=  20.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.861 total time=  20.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.887 total time=   3.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.888 total time=   6.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.888 total time=  13.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.884 total time=  13.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.884 total time=  18.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.889 total time=  13.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.898 total time=   2.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.899 total time=   5.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.897 total time=   2.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.896 total time=   5.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.898 total time=  17.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.889 total time=  13.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.881 total time=  19.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.893 total time=  12.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.889 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.889 total time=   6.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.889 total time=  18.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.890 total time=  11.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.896 total time=  14.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.892 total time=   3.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.893 total time=   7.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.900 total time=  21.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.900 total time=  13.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.900 total time=  20.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.900 total time=  13.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.895 total time=   2.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.895 total time=   5.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.898 total time=  13.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.894 total time=   8.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.901 total time=   7.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.894 total time=   2.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.902 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.898 total time=  14.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.901 total time=   2.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.899 total time=   4.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.893 total time=   2.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.894 total time=   4.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.899 total time=   2.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.900 total time=   4.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.899 total time=  13.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.896 total time=  12.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.895 total time=   7.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.897 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.898 total time=   3.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.902 total time=   7.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.826 total time=   5.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.824 total time=  35.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.825 total time=  34.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.844 total time=  21.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.837 total time=  31.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.856 total time=  20.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.851 total time=  32.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.887 total time=   3.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.894 total time=  12.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.895 total time=  12.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.886 total time=   3.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.886 total time=   6.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.894 total time=  20.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.895 total time=   2.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.896 total time=   6.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.880 total time=   3.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.882 total time=   7.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.887 total time=   3.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.884 total time=   6.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.886 total time=   3.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.894 total time=   6.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.892 total time=  19.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.889 total time=  12.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.896 total time=  18.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.896 total time=  11.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.893 total time=  23.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.898 total time=  21.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.898 total time=  13.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.895 total time=  20.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.897 total time=  12.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.900 total time=  19.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.896 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.893 total time=   4.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.901 total time=   2.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.901 total time=   4.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.898 total time=  12.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.901 total time=   8.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.903 total time=   1.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.903 total time=   3.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.897 total time=   2.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.896 total time=   3.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.899 total time=  11.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.894 total time=   8.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.902 total time=   8.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.902 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.902 total time=   4.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.902 total time=  12.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.897 total time=   1.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.897 total time=   3.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.904 total time=   7.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.816 total time=   5.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.822 total time=  23.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.836 total time=   5.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.827 total time=  10.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.840 total time=   5.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.831 total time=  10.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.831 total time=  33.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.830 total time=  21.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.851 total time=   5.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.853 total time=  10.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.854 total time=  30.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.890 total time=  14.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.884 total time=  20.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.893 total time=  12.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.895 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.895 total time=   6.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.896 total time=  17.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.898 total time=  11.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.892 total time=  17.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.884 total time=  13.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.887 total time=  19.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.886 total time=  12.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.890 total time=  18.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.898 total time=  11.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.892 total time=  14.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.898 total time=  23.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.894 total time=   3.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.895 total time=   6.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.895 total time=  20.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.893 total time=  14.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.899 total time=   4.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.901 total time=   6.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.895 total time=   2.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.897 total time=   4.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.893 total time=  13.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.901 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.897 total time=   2.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.898 total time=   4.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.897 total time=   2.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.897 total time=   4.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.900 total time=  12.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.899 total time=   8.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.898 total time=   2.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.903 total time=   3.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.891 total time=   2.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.900 total time=   4.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.900 total time=  13.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.897 total time=  12.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.894 total time=   8.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.903 total time=   1.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.897 total time=   3.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.903 total time=  11.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.903 total time=   7.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.824 total time=  23.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.838 total time=  35.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.841 total time=   5.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.841 total time=  10.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.844 total time=  32.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.859 total time=  20.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.861 total time=  31.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.883 total time=  20.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.887 total time=  12.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.891 total time=  12.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.888 total time=   3.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.889 total time=   6.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.889 total time=  17.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.892 total time=  12.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.881 total time=   3.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.881 total time=   6.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.882 total time=   3.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.881 total time=   8.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.894 total time=   4.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.895 total time=   7.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.886 total time=   6.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.888 total time=  18.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.894 total time=  12.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.891 total time=   2.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.898 total time=   5.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.893 total time=  11.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.895 total time=   6.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.894 total time=  18.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.897 total time=  11.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.889 total time=  22.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.890 total time=  21.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.898 total time=  13.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.900 total time=  13.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.899 total time=   3.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.899 total time=   6.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.895 total time=  19.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.900 total time=   8.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.898 total time=   8.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.897 total time=   2.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.897 total time=   4.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.895 total time=   2.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.896 total time=   4.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.896 total time=  12.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.898 total time=   7.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.904 total time=  11.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.900 total time=   8.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.892 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.895 total time=   4.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.896 total time=  12.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.897 total time=   8.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.901 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.896 total time=   3.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.901 total time=  11.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.898 total time=   7.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.811 total time=  23.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.835 total time=   5.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.835 total time=  11.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.847 total time=  34.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.835 total time=  32.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.851 total time=  20.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.859 total time=  31.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.886 total time=  20.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.889 total time=  12.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.895 total time=  12.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.895 total time=   3.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.895 total time=   6.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.890 total time=  18.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.893 total time=  13.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.890 total time=   3.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.890 total time=   6.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.891 total time=   3.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.889 total time=   6.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.887 total time=   3.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.883 total time=   6.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.887 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.895 total time=   6.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.889 total time=  12.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.895 total time=   3.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.895 total time=   5.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.891 total time=   2.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.892 total time=   5.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.897 total time=  12.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.898 total time=   3.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.899 total time=   5.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.889 total time=  14.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.892 total time=   3.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.899 total time=   7.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.893 total time=   3.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.899 total time=   7.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.894 total time=  24.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.899 total time=   3.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.893 total time=   6.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.900 total time=  19.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.895 total time=  13.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.897 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.893 total time=   4.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.901 total time=  13.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.900 total time=   8.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.901 total time=   2.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.898 total time=   4.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.902 total time=  12.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.902 total time=   7.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.895 total time=   2.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.899 total time=   4.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.895 total time=   2.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.896 total time=   4.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.896 total time=  13.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.897 total time=  12.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.898 total time=   8.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.901 total time=   1.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.903 total time=   3.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.899 total time=  11.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.805 total time=  12.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.824 total time=  12.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.832 total time=   5.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.832 total time=  11.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.831 total time=   5.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.840 total time=  10.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.827 total time=  32.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.835 total time=  21.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.844 total time=  31.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.851 total time=  20.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.881 total time=   3.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.882 total time=   6.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.884 total time=  19.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.888 total time=  12.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.893 total time=  18.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.890 total time=  12.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.898 total time=   2.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.899 total time=   5.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.897 total time=  17.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.889 total time=  14.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.891 total time=   3.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.891 total time=   6.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.889 total time=  18.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.887 total time=  12.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.891 total time=   2.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.892 total time=   5.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.899 total time=  11.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.888 total time=  20.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.895 total time=  12.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.894 total time=   3.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.894 total time=   6.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.889 total time=  18.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.899 total time=  11.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.897 total time=   2.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.896 total time=   7.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.890 total time=   3.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.896 total time=   3.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.897 total time=   7.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.889 total time=   3.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.890 total time=   7.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.898 total time=   3.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.898 total time=   3.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.899 total time=   7.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.898 total time=   3.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.900 total time=  13.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.894 total time=  13.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.900 total time=   3.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.901 total time=   6.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.902 total time=  13.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.899 total time=   2.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.899 total time=   4.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.894 total time=   8.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.898 total time=   8.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.896 total time=  12.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.897 total time=   8.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.898 total time=   1.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.899 total time=   3.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.903 total time=  11.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.903 total time=   7.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.899 total time=  13.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.894 total time=   8.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.900 total time=   2.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.900 total time=   4.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.900 total time=  13.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.903 total time=  11.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.903 total time=   7.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.816 total time=  23.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.832 total time=   5.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.832 total time=  11.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.844 total time=   5.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.835 total time=  10.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.841 total time=  34.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.855 total time=  21.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.854 total time=   5.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.854 total time=  10.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.848 total time=   5.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.844 total time=  10.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.844 total time=  32.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.888 total time=   3.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.886 total time=  13.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.895 total time=  18.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.896 total time=  12.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.897 total time=  11.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.892 total time=   2.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.897 total time=   5.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.898 total time=  17.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.884 total time=  13.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.884 total time=  18.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.888 total time=  12.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.889 total time=  18.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.893 total time=  12.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.892 total time=   7.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.892 total time=  15.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.898 total time=  14.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.900 total time=  20.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.900 total time=  13.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.901 total time=  20.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.902 total time=  13.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.899 total time=   2.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.899 total time=   4.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.899 total time=  12.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.901 total time=  12.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.902 total time=  12.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.899 total time=   7.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.899 total time=   2.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.890 total time=   4.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.897 total time=   2.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.897 total time=   4.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.892 total time=  12.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.901 total time=   8.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.901 total time=   2.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.902 total time=   3.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.897 total time=  11.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.898 total time=   7.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.903 total time=  11.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.807 total time=   5.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.824 total time=  22.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.817 total time=  22.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.840 total time=  33.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.838 total time=  20.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.859 total time=  30.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.853 total time=  20.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.882 total time=   3.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.883 total time=   6.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.892 total time=  19.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.895 total time=  13.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.893 total time=   3.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.894 total time=   6.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.897 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.898 total time=   5.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.898 total time=  17.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.893 total time=  13.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.883 total time=   4.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.892 total time=   6.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.892 total time=   3.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.892 total time=   6.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.893 total time=   3.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.893 total time=   6.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.893 total time=  18.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.896 total time=  12.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.892 total time=   2.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.893 total time=   5.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.890 total time=  15.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.896 total time=   7.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.896 total time=  16.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.895 total time=  21.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.894 total time=  13.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.900 total time=  13.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.900 total time=   3.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.896 total time=   7.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.900 total time=   3.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.901 total time=   6.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.897 total time=  20.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.900 total time=   2.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.897 total time=   8.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.898 total time=   7.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.897 total time=   2.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.897 total time=   3.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.898 total time=  11.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.903 total time=   7.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.896 total time=  11.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.898 total time=   8.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.897 total time=   8.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.900 total time=   2.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.901 total time=   4.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.901 total time=  12.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.899 total time=   7.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.902 total time=   1.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.903 total time=   3.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.904 total time=   7.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.805 total time=  35.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.818 total time=  34.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.827 total time=  22.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.849 total time=  32.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.854 total time=  20.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.884 total time=   3.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.885 total time=   6.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.886 total time=  13.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.887 total time=  13.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.889 total time=  20.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.891 total time=  19.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.899 total time=  11.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.897 total time=   2.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.896 total time=   5.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.893 total time=  17.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.881 total time=  13.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.889 total time=  12.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.886 total time=   3.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.888 total time=   6.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.893 total time=  18.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.890 total time=  11.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.899 total time=  15.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.895 total time=   3.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.896 total time=   3.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.896 total time=   7.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.892 total time=  22.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.898 total time=  20.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.895 total time=  13.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.897 total time=  19.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.893 total time=  13.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.895 total time=   2.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.895 total time=   4.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.894 total time=  13.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.896 total time=  12.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.900 total time=   8.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.901 total time=   1.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.901 total time=   3.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.903 total time=  11.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.897 total time=   8.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.898 total time=   2.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.901 total time=   4.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.901 total time=  12.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.901 total time=   8.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.903 total time=   1.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.902 total time=   3.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.899 total time=  11.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.899 total time=   7.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.813 total time=  37.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.837 total time=  34.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.844 total time=  22.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.844 total time=   5.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.844 total time=  10.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.848 total time=  30.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.848 total time=  20.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.890 total time=   3.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.891 total time=   6.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.889 total time=  13.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.887 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.887 total time=   6.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.895 total time=  18.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.896 total time=  12.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.891 total time=   2.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.892 total time=   5.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.899 total time=  12.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.886 total time=   3.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.887 total time=   6.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.889 total time=   3.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.887 total time=   6.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.891 total time=   3.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.892 total time=   6.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.887 total time=   3.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.893 total time=   6.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.895 total time=  18.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.889 total time=  13.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.895 total time=   2.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.892 total time=   6.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.897 total time=  15.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.887 total time=   3.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.891 total time=   3.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.891 total time=   7.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.889 total time=  22.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.895 total time=  13.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.891 total time=   3.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.899 total time=   6.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.895 total time=   3.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.895 total time=   6.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.895 total time=  20.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.900 total time=  13.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.899 total time=   2.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.900 total time=   4.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.892 total time=  12.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.897 total time=  12.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.897 total time=   8.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.898 total time=  11.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.901 total time=   7.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.894 total time=  13.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.901 total time=   8.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.897 total time=   2.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.898 total time=   4.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.898 total time=  12.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.903 total time=   8.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.898 total time=   2.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.899 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.824 total time=   6.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.816 total time=  34.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.822 total time=  34.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.831 total time=  21.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.835 total time=  32.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.849 total time=  20.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.857 total time=  31.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.891 total time=   3.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.887 total time=  12.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.886 total time=  12.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.890 total time=   3.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.891 total time=   6.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.895 total time=  18.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.898 total time=  11.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.888 total time=   3.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.889 total time=   7.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.888 total time=   3.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.890 total time=   7.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.888 total time=   3.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.889 total time=   6.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.888 total time=   3.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.889 total time=   6.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.888 total time=  18.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.896 total time=  12.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.893 total time=  17.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.892 total time=   9.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.897 total time=  15.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.894 total time=   3.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.893 total time=   7.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.893 total time=   3.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.890 total time=   6.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.894 total time=  21.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.899 total time=  14.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.901 total time=   3.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.901 total time=   6.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.895 total time=   3.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.895 total time=   6.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.893 total time=  19.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.894 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.895 total time=   4.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.900 total time=  12.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.902 total time=  12.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.903 total time=   7.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.899 total time=  11.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.899 total time=   9.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.894 total time=   2.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.893 total time=   4.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.897 total time=   2.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.898 total time=   4.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.900 total time=  12.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.902 total time=   7.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.899 total time=  11.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.896 total time=   7.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.821 total time=  23.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.827 total time=   5.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.837 total time=  11.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.841 total time=   5.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.844 total time=  10.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.835 total time=   5.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.835 total time=  10.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.852 total time=  21.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.849 total time=   5.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.848 total time=  10.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.851 total time=  31.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.844 total time=  20.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.880 total time=   3.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.884 total time=   7.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.885 total time=  19.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.886 total time=  18.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.889 total time=  12.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.892 total time=  17.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.890 total time=  11.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.882 total time=  20.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.892 total time=  13.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.888 total time=   3.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.886 total time=   6.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.895 total time=  19.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.896 total time=  17.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.898 total time=   8.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.891 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.891 total time=   4.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.898 total time=  14.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.893 total time=  21.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.893 total time=  15.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.894 total time=  20.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.901 total time=  12.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.896 total time=  19.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.900 total time=   2.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.896 total time=   4.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.892 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.893 total time=   4.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.896 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.897 total time=   4.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.898 total time=   9.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.897 total time=   3.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.901 total time=  11.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.898 total time=   7.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.898 total time=  13.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.896 total time=  12.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.897 total time=   8.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.896 total time=  11.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.898 total time=   7.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.899 total time=  11.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.824 total time=   5.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.814 total time=  22.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.825 total time=  23.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.827 total time=   5.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.835 total time=  10.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.840 total time=  32.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.853 total time=  21.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.856 total time=   5.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.856 total time=  10.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.861 total time=  20.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.889 total time=   3.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.889 total time=   6.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.889 total time=  20.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.887 total time=   3.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.893 total time=   6.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.888 total time=  19.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.893 total time=  11.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.891 total time=   2.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.891 total time=   6.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.879 total time=   3.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.882 total time=   6.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.890 total time=   3.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.891 total time=   6.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.884 total time=  19.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.886 total time=  13.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.895 total time=   3.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.896 total time=   6.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.892 total time=   2.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.893 total time=   5.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.897 total time=  17.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.890 total time=   9.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.898 total time=  22.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.899 total time=  22.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.891 total time=  14.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.895 total time=  20.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.899 total time=  13.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.897 total time=  19.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.895 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.895 total time=   4.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.902 total time=  12.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.901 total time=   8.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.903 total time=   1.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.903 total time=   3.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.899 total time=  11.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.899 total time=   7.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.893 total time=  13.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.896 total time=   8.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.898 total time=  11.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.896 total time=   7.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.902 total time=  13.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.821 total time=   6.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.821 total time=  36.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.832 total time=  34.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.841 total time=  24.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.861 total time=   6.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.861 total time=  11.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.861 total time=   5.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.857 total time=  11.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.854 total time=   5.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.859 total time=  11.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.879 total time=   3.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.880 total time=   6.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.880 total time=  13.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.890 total time=  13.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.887 total time=  19.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.886 total time=  13.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.897 total time=   2.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.896 total time=   5.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.892 total time=   2.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.893 total time=   5.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.893 total time=  18.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.890 total time=  13.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.882 total time=   3.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.887 total time=   7.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.894 total time=   3.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.894 total time=   7.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.895 total time=  18.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.891 total time=  11.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.889 total time=   2.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.896 total time=   5.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.892 total time=  16.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.891 total time=  16.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.898 total time=   3.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.899 total time=   7.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.897 total time=   3.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.895 total time=   6.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.897 total time=   3.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.898 total time=   6.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.898 total time=  20.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.899 total time=  13.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.900 total time=  20.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.900 total time=  13.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.901 total time=   8.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.901 total time=   8.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.901 total time=   2.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.901 total time=   4.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.902 total time=   1.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.903 total time=   1.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.904 total time=   3.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.903 total time=   1.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.904 total time=   4.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.903 total time=   2.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.904 total time=   3.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.889 total time=   2.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.897 total time=   4.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.890 total time=   2.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.891 total time=   4.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.895 total time=  12.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.897 total time=   8.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.901 total time=   2.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.902 total time=   3.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.901 total time=  12.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.896 total time=  11.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.826 total time=  11.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.807 total time=  11.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.819 total time=  34.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.852 total time=  21.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.843 total time=   5.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.843 total time=  10.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.855 total time=  32.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.853 total time=  32.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.888 total time=  21.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.895 total time=  12.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.895 total time=  18.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.891 total time=  12.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.898 total time=   3.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.899 total time=   5.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.897 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.898 total time=   5.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.898 total time=  17.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.891 total time=  13.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.887 total time=  12.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.887 total time=   3.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.889 total time=   6.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.886 total time=  18.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.896 total time=  11.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.893 total time=  16.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.897 total time=   7.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.891 total time=   7.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.892 total time=   4.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.894 total time=   7.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.894 total time=   4.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.898 total time=   6.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.899 total time=   3.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.899 total time=   6.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.899 total time=  20.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.897 total time=  14.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.900 total time=   3.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.896 total time=   6.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.889 total time=   2.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.890 total time=   4.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.898 total time=   8.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.894 total time=   8.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.896 total time=  12.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.896 total time=   8.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.901 total time=   1.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.901 total time=   3.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.899 total time=  11.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.899 total time=   7.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.896 total time=  13.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.900 total time=  12.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.900 total time=   8.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.903 total time=  12.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.898 total time=  11.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.811 total time=  11.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.815 total time=  12.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.824 total time=  38.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.852 total time=   5.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.852 total time=  12.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.830 total time=   5.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.835 total time=  10.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.844 total time=  33.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.844 total time=   5.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.849 total time=  10.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.849 total time=  30.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.886 total time=   3.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.886 total time=   6.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.886 total time=   3.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.886 total time=   6.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.892 total time=   3.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.888 total time=   6.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.894 total time=  12.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.892 total time=   2.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.889 total time=   5.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.899 total time=  17.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.897 total time=  11.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.880 total time=  20.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.884 total time=  12.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.894 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.895 total time=   6.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.895 total time=  18.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.898 total time=  11.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.892 total time=  17.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.895 total time=   3.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.892 total time=   3.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.891 total time=  14.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.898 total time=  15.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.900 total time=  22.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.898 total time=  20.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.896 total time=  12.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.901 total time=  19.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.900 total time=   2.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.901 total time=   4.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.894 total time=  12.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.894 total time=   8.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.895 total time=   1.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.896 total time=   3.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.896 total time=  11.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.896 total time=   7.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.893 total time=  13.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.896 total time=   8.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.897 total time=   2.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.897 total time=   3.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.897 total time=  12.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.898 total time=   7.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.897 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.898 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.903 total time=  11.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.820 total time=   6.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.824 total time=  23.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.834 total time=  24.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.841 total time=   5.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.841 total time=  11.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.841 total time=  35.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.861 total time=  31.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.859 total time=  21.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.888 total time=   3.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.890 total time=   6.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.888 total time=  19.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.891 total time=  18.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.895 total time=  12.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.897 total time=   2.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.897 total time=   5.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.890 total time=  17.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.886 total time=  13.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.886 total time=   3.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.886 total time=   6.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.892 total time=  19.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.895 total time=  12.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.889 total time=   3.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.892 total time=   6.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.897 total time=   3.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.891 total time=   5.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.898 total time=  17.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.896 total time=   3.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.896 total time=   3.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.897 total time=   7.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.898 total time=  22.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.900 total time=  13.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.898 total time=   4.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.898 total time=   7.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.896 total time=   3.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.896 total time=   6.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.902 total time=  19.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.897 total time=  14.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.898 total time=   2.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.898 total time=   4.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.896 total time=  12.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.897 total time=  12.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.895 total time=   7.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.904 total time=  11.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.902 total time=   7.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.892 total time=   2.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.893 total time=   5.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.900 total time=   2.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.900 total time=   4.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.900 total time=   2.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.901 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.894 total time=  12.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.897 total time=   8.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.895 total time=   1.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.896 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.898 total time=  11.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.824 total time=  11.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.826 total time=  11.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.826 total time=  34.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.844 total time=  21.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.852 total time=  32.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.857 total time=  20.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.859 total time=   5.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.854 total time=  10.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.880 total time=   3.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.889 total time=   6.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.890 total time=  13.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.891 total time=  13.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.894 total time=  18.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.895 total time=  12.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.894 total time=  17.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.890 total time=  11.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.889 total time=   3.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.889 total time=   6.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.887 total time=  20.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.886 total time=  18.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.895 total time=  12.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.896 total time=  20.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.898 total time=  16.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.888 total time=   7.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.897 total time=   7.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.892 total time=  23.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.893 total time=   3.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.894 total time=   7.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.894 total time=  21.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.901 total time=  13.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.899 total time=   3.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.896 total time=   7.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.893 total time=   2.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.894 total time=   4.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.897 total time=   8.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.900 total time=   8.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.897 total time=  12.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.901 total time=   8.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.898 total time=   2.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.902 total time=   4.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.901 total time=   1.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.902 total time=   3.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.895 total time=   1.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.901 total time=   4.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.902 total time=  11.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.895 total time=   8.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.896 total time=  12.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.896 total time=   8.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.897 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.898 total time=   3.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.904 total time=   8.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.903 total time=   2.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.902 total time=   3.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.822 total time=   5.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.822 total time=  36.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.827 total time=  34.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.835 total time=  21.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.838 total time=  31.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.844 total time=  20.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.889 total time=   3.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.881 total time=   6.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.882 total time=  13.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.883 total time=  13.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.892 total time=  18.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.895 total time=  18.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.892 total time=  11.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.898 total time=  17.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.890 total time=  13.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.892 total time=  19.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.895 total time=  12.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.888 total time=  18.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.892 total time=  11.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.896 total time=  17.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.891 total time=  23.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.899 total time=  22.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.895 total time=  13.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.900 total time=  20.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.893 total time=  13.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.902 total time=  19.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.899 total time=   3.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.900 total time=   8.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.900 total time=  12.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.898 total time=   8.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.897 total time=   1.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.897 total time=   3.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.903 total time=  11.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.900 total time=   8.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.901 total time=  14.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.902 total time=  12.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.898 total time=   7.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.901 total time=   1.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.901 total time=   3.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.899 total time=   7.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.821 total time=   6.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.811 total time=  34.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.847 total time=  23.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.844 total time=   5.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.844 total time=  11.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.841 total time=  33.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.854 total time=  19.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.853 total time=   5.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.851 total time=  10.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.848 total time=  30.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.886 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.883 total time=   6.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.894 total time=   3.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.894 total time=   6.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.894 total time=   3.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.894 total time=   6.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.886 total time=  18.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.892 total time=  11.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.899 total time=  17.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.883 total time=  13.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.889 total time=  19.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.891 total time=  12.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.890 total time=   3.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.891 total time=   5.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.895 total time=   2.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.896 total time=   5.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.898 total time=  17.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.898 total time=  10.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.894 total time=   3.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.888 total time=   3.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.897 total time=   7.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.892 total time=  23.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.893 total time=   3.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.894 total time=   6.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.894 total time=  20.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.901 total time=  13.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.901 total time=  19.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.891 total time=  13.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.901 total time=   8.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.896 total time=   8.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.896 total time=   2.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.902 total time=   3.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.902 total time=   7.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.898 total time=   1.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.899 total time=   3.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.897 total time=   1.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.903 total time=   4.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.898 total time=  12.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.900 total time=   8.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.901 total time=   2.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.901 total time=   4.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.900 total time=   8.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.894 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.894 total time=   3.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.902 total time=  12.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.903 total time=  11.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.821 total time=  11.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.824 total time=  11.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.829 total time=  34.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.841 total time=  32.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.837 total time=  22.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.854 total time=  31.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.857 total time=  20.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.887 total time=   3.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.887 total time=   6.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.892 total time=  20.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.888 total time=   3.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.889 total time=   6.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.893 total time=  20.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.893 total time=  17.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.892 total time=  11.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.890 total time=  20.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.886 total time=  12.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.895 total time=  18.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.894 total time=  12.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.897 total time=   2.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.897 total time=   5.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.893 total time=  17.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.898 total time=  14.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.897 total time=  21.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.899 total time=  14.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.898 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.894 total time=   6.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.895 total time=   3.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.896 total time=   6.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.897 total time=  19.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.901 total time=  15.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.891 total time=   5.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.900 total time=  13.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.897 total time=   2.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.898 total time=   4.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.901 total time=  12.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.899 total time=   7.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.899 total time=  11.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.894 total time=   8.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.900 total time=  13.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.897 total time=  12.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.901 total time=   8.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.898 total time=   1.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.899 total time=   3.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.901 total time=  11.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.813 total time=  11.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.824 total time=  11.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.824 total time=  34.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.831 total time=  22.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.838 total time=   5.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.830 total time=  11.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.853 total time=   5.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.853 total time=  10.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.856 total time=  32.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.854 total time=  21.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.891 total time=   3.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.892 total time=   6.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.894 total time=   3.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.894 total time=   6.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.888 total time=  18.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.893 total time=  12.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.895 total time=   3.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.893 total time=   5.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.892 total time=   2.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.889 total time=   5.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.896 total time=  17.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.882 total time=  13.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.884 total time=  19.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.888 total time=  12.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.891 total time=  19.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.890 total time=  18.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.899 total time=   9.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.892 total time=  17.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.892 total time=  11.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.892 total time=  22.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.891 total time=  14.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.893 total time=  14.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.893 total time=   3.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.895 total time=   8.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.901 total time=   3.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.901 total time=   6.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.899 total time=   3.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.896 total time=   7.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.892 total time=   3.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.899 total time=   6.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.901 total time=  19.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.897 total time=   2.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.897 total time=   4.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.900 total time=   2.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.901 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.892 total time=   2.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.900 total time=   3.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.897 total time=  13.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.902 total time=   1.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.903 total time=   3.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.904 total time=   8.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.899 total time=   2.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.900 total time=   4.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.898 total time=   2.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.898 total time=   4.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.900 total time=  12.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.901 total time=   8.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.897 total time=   2.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.897 total time=   3.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.898 total time=  12.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.904 total time=  11.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.822 total time=  12.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.829 total time=  23.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.835 total time=  35.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.840 total time=  21.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.843 total time=  32.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.851 total time=  21.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.881 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.882 total time=   6.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.883 total time=  13.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.881 total time=  13.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.892 total time=  18.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.888 total time=  12.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.891 total time=   2.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.892 total time=   5.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.896 total time=  17.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.898 total time=  12.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.883 total time=  20.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.889 total time=  20.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.894 total time=  12.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.897 total time=   2.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.896 total time=   5.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.899 total time=  17.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.893 total time=  10.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.889 total time=   3.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.890 total time=   6.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.891 total time=   2.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.891 total time=   5.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.892 total time=  17.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.896 total time=  10.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.896 total time=  22.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.894 total time=  22.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.899 total time=  13.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.892 total time=  20.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.895 total time=  13.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.900 total time=  19.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.898 total time=   2.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.900 total time=   4.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.900 total time=   2.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.901 total time=   4.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.900 total time=   2.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.901 total time=   4.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.901 total time=   2.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.896 total time=   3.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.897 total time=  12.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.899 total time=   8.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.896 total time=   2.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.893 total time=   4.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.892 total time=   2.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.894 total time=   4.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.894 total time=  12.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.897 total time=   8.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.896 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.896 total time=   3.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.900 total time=  11.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.896 total time=   7.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.902 total time=  11.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.824 total time=   5.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.824 total time=  22.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.827 total time=  22.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.827 total time=  22.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.849 total time=   5.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.843 total time=  10.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.856 total time=   5.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.856 total time=  10.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.861 total time=  21.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.861 total time=   5.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.857 total time=  10.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.888 total time=   3.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.889 total time=   6.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.889 total time=  14.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.890 total time=  13.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.895 total time=  19.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.889 total time=  18.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.898 total time=  11.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.892 total time=   2.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.893 total time=   5.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.893 total time=  17.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.883 total time=  13.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.894 total time=  20.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.891 total time=  12.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.897 total time=   2.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.897 total time=   5.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.898 total time=  17.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.893 total time=  10.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.896 total time=   7.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.897 total time=  14.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.893 total time=  15.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.890 total time=   3.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.890 total time=   6.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.891 total time=  20.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.895 total time=  13.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.893 total time=  20.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.894 total time=  13.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.900 total time=   8.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.897 total time=   8.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.897 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.894 total time=   3.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.895 total time=  11.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.898 total time=   7.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.901 total time=  11.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.894 total time=   8.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.896 total time=   8.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.892 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.893 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.894 total time=  12.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.898 total time=   7.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.903 total time=  11.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.816 total time=  11.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.814 total time=  11.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.814 total time=  34.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.841 total time=  22.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.844 total time=   5.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.844 total time=  10.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.859 total time=   5.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.859 total time=  10.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.848 total time=   5.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.856 total time=  10.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.857 total time=  21.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.886 total time=   3.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.887 total time=   6.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.890 total time=  19.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.886 total time=   6.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.892 total time=  19.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.888 total time=  18.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.898 total time=  11.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.892 total time=  17.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.888 total time=  13.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.892 total time=  20.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.893 total time=  18.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.889 total time=  12.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.893 total time=  17.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.897 total time=  10.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.890 total time=   3.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.891 total time=   3.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.893 total time=   7.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.897 total time=  22.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.894 total time=  22.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.895 total time=  21.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.901 total time=  19.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.899 total time=  10.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.900 total time=  12.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.900 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.893 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.898 total time=  12.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.901 total time=   7.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.901 total time=  11.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.893 total time=   9.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.900 total time=   2.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.897 total time=   4.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.892 total time=   2.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.900 total time=   3.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.902 total time=   8.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.901 total time=   1.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.901 total time=   3.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.902 total time=   1.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.903 total time=   3.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.902 total time=   7.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.898 total time=   1.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.896 total time=   3.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.901 total time=  11.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.814 total time=   5.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.815 total time=  23.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.837 total time=  22.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.835 total time=  31.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.845 total time=  22.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.849 total time=  30.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.849 total time=  20.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.889 total time=   3.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.890 total time=   6.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.881 total time=  20.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.888 total time=  18.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.887 total time=  12.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.891 total time=   3.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.891 total time=   6.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.897 total time=  17.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.882 total time=  13.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.890 total time=  19.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.888 total time=  12.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.893 total time=  18.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.893 total time=  11.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.897 total time=   2.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.898 total time=   5.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.892 total time=  12.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.890 total time=   7.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.889 total time=   9.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.896 total time=   3.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.898 total time=   9.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.893 total time=   7.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.891 total time=  21.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.892 total time=  13.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.895 total time=   3.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.895 total time=   6.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.897 total time=  19.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.900 total time=  14.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.896 total time=   8.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.902 total time=   8.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.901 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.902 total time=   3.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.897 total time=   1.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.903 total time=   3.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.904 total time=   7.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.901 total time=   1.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.897 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.898 total time=  11.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.900 total time=   9.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.894 total time=  12.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.898 total time=   7.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.898 total time=   1.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.901 total time=   3.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.898 total time=  11.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.898 total time=   7.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.813 total time=  23.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.822 total time=   5.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.822 total time=  11.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.827 total time=   5.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.841 total time=  11.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.840 total time=   5.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.831 total time=  10.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.831 total time=  32.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.849 total time=  20.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.849 total time=  32.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.890 total time=  20.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.892 total time=  13.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.893 total time=  18.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.894 total time=  12.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.892 total time=  17.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.898 total time=  12.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.886 total time=  19.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.892 total time=  12.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.892 total time=   3.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.893 total time=   6.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.888 total time=  18.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.892 total time=  11.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.895 total time=   2.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.892 total time=   5.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.898 total time=  12.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.890 total time=   4.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.896 total time=   3.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.893 total time=  14.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.895 total time=  15.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.898 total time=   3.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.899 total time=   6.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.900 total time=  20.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.896 total time=  13.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.900 total time=  19.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.899 total time=  13.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.896 total time=   8.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.897 total time=   7.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.902 total time=  12.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.903 total time=   7.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.898 total time=  11.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.898 total time=   9.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.895 total time=   2.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.901 total time=   4.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.902 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.902 total time=   4.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.901 total time=   2.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.897 total time=   4.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.897 total time=  12.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.901 total time=   7.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.899 total time=  11.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.821 total time=  11.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.829 total time=  11.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.807 total time=  34.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.835 total time=  32.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.843 total time=  21.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.853 total time=  30.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.856 total time=  21.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.883 total time=   3.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.881 total time=   6.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.890 total time=  19.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.888 total time=  12.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.895 total time=  18.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.896 total time=  11.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.897 total time=   2.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.892 total time=   5.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.896 total time=  17.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.887 total time=  13.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.895 total time=  20.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.895 total time=  12.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.898 total time=   3.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.899 total time=   6.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.898 total time=   2.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.899 total time=   5.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.892 total time=   3.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.897 total time=   5.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.898 total time=  13.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.897 total time=  25.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.897 total time=   3.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.894 total time=   7.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.894 total time=  13.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.894 total time=   3.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.894 total time=   6.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.895 total time=   3.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.896 total time=   6.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.901 total time=  19.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.901 total time=  13.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.899 total time=   2.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.895 total time=   2.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.896 total time=   4.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.895 total time=  13.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.900 total time=   2.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.901 total time=   4.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.897 total time=  12.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.896 total time=   7.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.896 total time=  11.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.891 total time=   8.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.898 total time=  13.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.901 total time=   8.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.897 total time=   2.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.898 total time=   4.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.897 total time=   2.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.899 total time=   3.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.898 total time=  11.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.902 total time=   8.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.826 total time=  37.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.834 total time=  34.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.841 total time=  22.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.854 total time=   5.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.854 total time=  10.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.857 total time=  33.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.853 total time=  30.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.891 total time=   3.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.892 total time=   6.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.894 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.895 total time=   6.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.893 total time=  18.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.894 total time=  11.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.897 total time=  17.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.893 total time=  11.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.889 total time=  20.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.887 total time=  12.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.891 total time=  18.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.895 total time=  12.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.892 total time=   2.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.889 total time=   6.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.891 total time=   2.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.891 total time=   5.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.896 total time=  14.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.887 total time=  20.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.889 total time=   2.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.889 total time=   5.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.890 total time=  14.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.891 total time=   2.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.896 total time=   5.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.892 total time=  14.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.891 total time=   7.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.898 total time=  14.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.899 total time=  14.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.900 total time=  13.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.898 total time=  20.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.900 total time=  12.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.895 total time=   3.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.893 total time=   7.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.891 total time=   2.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.893 total time=   4.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.893 total time=   8.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.894 total time=   8.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.896 total time=  12.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.897 total time=   8.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.903 total time=  11.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.896 total time=   7.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.903 total time=  12.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.896 total time=   8.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.896 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.897 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.894 total time=   8.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.899 total time=   2.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.900 total time=   3.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.898 total time=  11.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.899 total time=   7.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.904 total time=  11.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.838 total time=   6.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.838 total time=  22.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.832 total time=  22.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.844 total time=   5.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.844 total time=  11.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.844 total time=  31.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.856 total time=  20.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.853 total time=  20.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.861 total time=  30.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.894 total time=   3.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.892 total time=  12.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.888 total time=  18.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.890 total time=  12.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.889 total time=   3.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.892 total time=   7.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.889 total time=   2.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.889 total time=   5.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.890 total time=  17.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.890 total time=  13.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.892 total time=  12.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.891 total time=   3.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.891 total time=   6.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.895 total time=  18.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.898 total time=  11.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.897 total time=   2.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.892 total time=   5.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.893 total time=  14.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.896 total time=  22.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.899 total time=  14.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.891 total time=  13.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.898 total time=   3.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.900 total time=   6.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.899 total time=  20.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.897 total time=  13.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.896 total time=   2.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.897 total time=   4.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.891 total time=  10.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.898 total time=  12.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.902 total time=  12.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.902 total time=   7.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.903 total time=  11.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.898 total time=   7.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.900 total time=  13.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.901 total time=   8.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.896 total time=   2.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.901 total time=   4.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.902 total time=  12.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.896 total time=   7.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.897 total time=   2.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.897 total time=   4.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.898 total time=  11.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.829 total time=   5.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.819 total time=  22.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.822 total time=  22.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.835 total time=  21.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.845 total time=   5.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.845 total time=  10.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.830 total time=  32.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.859 total time=  20.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.881 total time=   3.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.882 total time=   6.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.882 total time=  13.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.884 total time=  13.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.886 total time=  18.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.888 total time=  12.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.896 total time=  18.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.893 total time=  17.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.888 total time=  13.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.894 total time=   3.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.892 total time=   6.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.888 total time=  18.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.893 total time=  12.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.891 total time=  18.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.892 total time=  11.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.897 total time=  13.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.897 total time=  22.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.894 total time=  14.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.898 total time=  14.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.895 total time=   3.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.899 total time=   6.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.900 total time=   3.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.901 total time=   6.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.893 total time=  19.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.899 total time=  13.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.892 total time=   2.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.900 total time=   4.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.895 total time=   8.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.901 total time=  12.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.894 total time=  12.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.898 total time=   8.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.898 total time=   1.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.899 total time=   3.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.899 total time=  11.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.892 total time=   8.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.901 total time=  12.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.901 total time=   7.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.902 total time=  11.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.903 total time=   7.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.896 total time=  11.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.815 total time=   5.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.826 total time=  23.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.832 total time=  22.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.852 total time=  33.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.849 total time=  21.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.853 total time=   6.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.859 total time=  10.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.851 total time=   6.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.853 total time=  10.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.854 total time=  30.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.882 total time=   3.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.892 total time=   6.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.888 total time=   3.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.889 total time=   6.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.886 total time=   3.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.886 total time=   6.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.895 total time=  12.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.889 total time=   2.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.896 total time=   5.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.893 total time=  18.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.899 total time=  11.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.890 total time=  20.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.892 total time=  18.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.888 total time=  12.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.897 total time=   2.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.898 total time=   5.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.893 total time=  11.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.892 total time=   2.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.893 total time=   5.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.893 total time=  13.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.891 total time=  22.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.898 total time=  21.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.894 total time=  14.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.895 total time=  13.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.895 total time=   3.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.896 total time=   6.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.896 total time=  19.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.896 total time=   8.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.892 total time=   8.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.900 total time=  12.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.900 total time=   7.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.897 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.898 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.898 total time=  11.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.903 total time=   8.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.891 total time=  13.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.900 total time=   8.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.895 total time=   2.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.896 total time=   4.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.896 total time=  12.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.903 total time=   7.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.902 total time=  11.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.835 total time=  11.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.838 total time=  11.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.824 total time=  33.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.840 total time=  22.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.843 total time=   5.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.849 total time=  11.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.848 total time=   5.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.849 total time=  11.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.856 total time=   5.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.848 total time=  10.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.856 total time=  30.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.888 total time=  13.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.892 total time=  13.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.887 total time=   3.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.893 total time=   6.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.887 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.887 total time=   6.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.891 total time=  12.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.897 total time=   2.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.892 total time=   5.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.895 total time=   2.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.896 total time=   5.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.892 total time=  17.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.880 total time=  13.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.890 total time=  22.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.894 total time=   3.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.895 total time=   6.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.886 total time=   3.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.886 total time=   6.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.895 total time=  18.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.892 total time=  11.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.899 total time=  13.7s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "SEED = 98\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.05, 0.07, 0.1], #[0.01, 0.03, 0.05],\n",
    "                'n_estimators': [50, 100, 200, 300],\n",
    "                'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                'min_samples_split': [2,3,5],\n",
    "                'min_samples_leaf': [1,2,3],\n",
    "               'bootstrap': [True, False]\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=RandomForestRegressor(random_state = SEED),\n",
    "                    n_jobs=-1,\n",
    "                    verbose=3,\n",
    "                    param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train_imputed, y_train)\n",
    "#model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = gs.predict(x_test_imputed).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test)\n",
    "acc3 = np.mean((y_pred >= y_test-0.5) & (y_pred <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbc0a817-7b6f-4c8a-b371-44ba4a08c145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T07:35:04.293927Z",
     "iopub.status.busy": "2023-03-06T07:35:04.293347Z",
     "iopub.status.idle": "2023-03-06T07:35:04.550207Z",
     "shell.execute_reply": "2023-03-06T07:35:04.549524Z",
     "shell.execute_reply.started": "2023-03-06T07:35:04.293866Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= found hyperparameter =========\n",
      "{'bootstrap': True, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "0.9004637795018378\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.880\n",
      "mean_squared_errors: 0.114\n",
      "mean_absolute_errors: 0.207\n",
      "r2_score: 0.880\n",
      "acc: 0.606\n",
      "acc(+-0.5mm): 0.980\n"
     ]
    }
   ],
   "source": [
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test)\n",
    "acc3 = np.mean((y_pred >= y_test-0.5) & (y_pred <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')\n",
    "\n",
    "\n",
    "# save model\n",
    "odir_f = f'acc1-{acc1:.3f}_acc3-{acc3:.3f}_RF_4inputs_{nfold}fold'\n",
    "odir = f'result/outliers+w+h/size/{odir_f}'\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "#model.save_model(f'{odir}/model.model')\n",
    "pickle.dump(gs, open(f'{odir}/gridSearch','wb'))\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "pickle.dump(param_dict, open(f'{odir}/param_dict', 'wb'))\n",
    "f = open(f'{odir}/result.txt', 'w')\n",
    "f.write(f'classification model')\n",
    "f.write(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "f.write(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e009e5b-d380-42d7-bee0-c8933f8125e6",
   "metadata": {},
   "source": [
    "### test validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6043a4d-e744-4cff-9774-30860197721d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:41:52.591772Z",
     "iopub.status.busy": "2023-04-12T14:41:52.591259Z",
     "iopub.status.idle": "2023-04-12T14:42:39.038048Z",
     "shell.execute_reply": "2023-04-12T14:42:39.037294Z",
     "shell.execute_reply.started": "2023-04-12T14:41:52.591721Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.605+-0.012 -> 0.593 ~ 0.618\n",
      "acc(+-0.5mm): 0.980+-0.003 -> 0.977 ~ 0.984 \n"
     ]
    }
   ],
   "source": [
    "# rfr model\n",
    "rfr = pickle.load(open(f'result/outliers+w+h/size/acc1-0.606_acc3-0.980_RF_4inputs_10fold/gridSearch','rb'))\n",
    "\n",
    "y1_rfrs, y2_rfrs = [], []\n",
    "for i in range(200):\n",
    "    mask = np.array([random.randint(0,1)==1 for j in range(len(x_test_imputed))])\n",
    "    X_test = x_test_imputed[mask]\n",
    "    Y_test = y_test[mask]\n",
    "    \n",
    "    y_rfr = rfr.predict(X_test).flatten()\n",
    "    y_rfr = np.round(y_rfr * 2) / 2\n",
    "    y1_rfrs.append(np.mean(y_rfr == Y_test))\n",
    "    y2_rfrs.append(np.mean((y_rfr >= Y_test-0.5) & (y_rfr <= Y_test+0.5)))\n",
    "    \n",
    "    \n",
    "print(f'acc: {np.mean(y1_rfrs):.3f}+-{1.96*np.std(y1_rfrs):.3f} -> {np.mean(y1_rfrs)-1.96*np.std(y1_rfrs):.3f} ~ {np.mean(y1_rfrs)+1.96*np.std(y1_rfrs):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(y2_rfrs):.3f}+-{1.96*np.std(y2_rfrs):.3f} -> {np.mean(y2_rfrs)-1.96*np.std(y2_rfrs):.3f} ~ {np.mean(y2_rfrs)+1.96*np.std(y2_rfrs):.3f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5082f49b-9374-4217-8f10-64f4fca82042",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:14:51.168956Z",
     "iopub.status.busy": "2023-04-12T14:14:51.168457Z",
     "iopub.status.idle": "2023-04-12T14:14:51.177617Z",
     "shell.execute_reply": "2023-04-12T14:14:51.176824Z",
     "shell.execute_reply.started": "2023-04-12T14:14:51.168905Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rfr]  t-statistics: 0.04146207029951987   p-value: 0.8674149373727178\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "y1_rfrs = np.array(y1_rfrs)\n",
    "t_stat_x1, p_val_x1 = stats.kstest(y1_rfrs, 'norm', args=(y1_rfrs.mean(), y1_rfrs.var()**0.5))\n",
    "print('[rfr]  t-statistics:', t_stat_x1, '  p-value:', p_val_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf9b2860-e2fc-48bd-bf6c-aea0b517a8b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:28:15.258951Z",
     "iopub.status.busy": "2023-04-12T14:28:15.258180Z",
     "iopub.status.idle": "2023-04-12T14:28:15.269339Z",
     "shell.execute_reply": "2023-04-12T14:28:15.268331Z",
     "shell.execute_reply.started": "2023-04-12T14:28:15.258899Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[xgbr vs. rfr] statistic: 0.003252140858499127   p-value: 0.9545232533147272\n",
      "t-statistic: 25.77364827511988    p-value: 7.212965704872177e-87\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# 등분산 가정 검정\n",
    "var_test_stat_x1x2, var_test_p_val_x1x2 = stats.bartlett(y1_xgbrs, y1_rfrs)\n",
    "\n",
    "print('[xgbr vs. rfr]', 'statistic:', var_test_stat_x1x2, '  p-value:', var_test_p_val_x1x2)\n",
    "\n",
    "\n",
    "# two sample t-test\n",
    "t_stat, p_val = stats.ttest_ind(y1_xgbrs, y1_rfrs, \n",
    "                                #alternative='two-sided', #‘less’, ‘greater’\n",
    "                                equal_var=True)\n",
    "\n",
    "print('t-statistic:', t_stat, '   p-value:', p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f565995-f039-4680-8216-cd484f94f147",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7c88d25-43e2-4671-a41a-9e00d567a51b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:32:42.695726Z",
     "iopub.status.busy": "2023-04-12T14:32:42.694948Z",
     "iopub.status.idle": "2023-04-12T14:32:46.002188Z",
     "shell.execute_reply": "2023-04-12T14:32:46.001366Z",
     "shell.execute_reply.started": "2023-04-12T14:32:42.695674Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from keras import losses, metrics\n",
    "from keras.models import Sequential, Model, load_model, model_from_json\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization, Dropout, Activation, Input\n",
    "from keras.layers import GlobalAveragePooling1D, Flatten, SeparableConv1D, concatenate, Add\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats, interp\n",
    "import os, sys, pickle, shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, datetime, time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# tensorflow 사용 시 seed 고정\n",
    "def seed_everything(seed: int = 98):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "SEED = 98\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d5f67ee-b557-451f-9a3a-00ba0e09589f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:32:55.169504Z",
     "iopub.status.busy": "2023-04-12T14:32:55.169004Z",
     "iopub.status.idle": "2023-04-12T14:32:55.365750Z",
     "shell.execute_reply": "2023-04-12T14:32:55.364962Z",
     "shell.execute_reply.started": "2023-04-12T14:32:55.169453Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(pd.DataFrame(x_train))\n",
    "x_test = sc.transform(pd.DataFrame(x_test))\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "imp = IterativeImputer().fit(x_train)\n",
    "x_train_imputed = imp.transform(x_train)\n",
    "x_test_imputed = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66a5d5f1-7836-4a0d-8006-0304656707b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T08:08:55.166025Z",
     "iopub.status.busy": "2023-03-06T08:08:55.165554Z",
     "iopub.status.idle": "2023-03-06T08:08:55.193292Z",
     "shell.execute_reply": "2023-03-06T08:08:55.192387Z",
     "shell.execute_reply.started": "2023-03-06T08:08:55.165976Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start making test settings...done\n",
      "2023-03-06 17:08:55.189576\n"
     ]
    }
   ],
   "source": [
    "# folder\n",
    "nfold = 10  # 각각의 hyperparameter에 대해 k-fold 를 시행하고 평균을 구한다.\n",
    "ntest = 500\n",
    "rootdir = f\"result/outliers+w+h/size/DNN_size_4inputs\"\n",
    "\n",
    "if not os.path.exists(rootdir):\n",
    "    os.mkdir(rootdir)\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "f = open(f'{rootdir}/README.txt', 'w')\n",
    "f.write(f'model: DNN 2 layers, regression')\n",
    "f.write(f'input: age, height, weight, cuffed 유무  output: tube size')\n",
    "f.close()\n",
    "    \n",
    "\n",
    "# test_settings\n",
    "layer_settings, test_settings = [], []\n",
    "\n",
    "\n",
    "# hyperparamters pool\n",
    "dropout_opts  = [0, 0.1, 0.2, 0.3, 0.4, 0.5] # dropout rate\n",
    "dense_opts = [16, 32, 64, 128, 256, 512]\n",
    "BATCH_SIZE = [32, 64, 128, 256, 512]\n",
    "lr_opts = [0.001, 0.002, 0.0005]\n",
    "\n",
    "print('start making test settings...', end='', flush=True)\n",
    "# test settings\n",
    "dnodes, dropouts = [], []\n",
    "for i in range(2):\n",
    "    dnodes.append(0)\n",
    "    dropouts.append(0)\n",
    "\n",
    "\n",
    "for dnode1 in dense_opts:\n",
    "    for dropout1 in dropout_opts:\n",
    "        for dnode2 in dense_opts:\n",
    "            for dropout2 in dropout_opts:\n",
    "                for batch_size in BATCH_SIZE:\n",
    "                    for learning_rate in lr_opts:\n",
    "                        test_settings.append([dnode1, dropout1, dnode2, dropout2, batch_size, learning_rate])                                   \n",
    "\n",
    "                        \n",
    "print('done')\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "088ca283-4311-4ab3-b231-4756d3ee3b8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T08:08:56.600902Z",
     "iopub.status.busy": "2023-03-06T08:08:56.600471Z",
     "iopub.status.idle": "2023-03-06T11:02:04.450154Z",
     "shell.execute_reply": "2023-03-06T11:02:04.449423Z",
     "shell.execute_reply.started": "2023-03-06T08:08:56.600858Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random search 0/500\n",
      "Epoch 1/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.5654\n",
      "Epoch 00001: val_loss improved from inf to 0.12913, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 3s 4ms/step - loss: 0.5583 - val_loss: 0.1291\n",
      "Epoch 2/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00002: val_loss improved from 0.12913 to 0.10349, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2064 - val_loss: 0.1035\n",
      "Epoch 3/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.1834\n",
      "Epoch 00003: val_loss improved from 0.10349 to 0.10009, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1832 - val_loss: 0.1001\n",
      "Epoch 4/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.1695\n",
      "Epoch 00004: val_loss improved from 0.10009 to 0.09961, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1693 - val_loss: 0.0996\n",
      "Epoch 5/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.1542\n",
      "Epoch 00005: val_loss did not improve from 0.09961\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1544 - val_loss: 0.1067\n",
      "Epoch 6/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.1467\n",
      "Epoch 00006: val_loss improved from 0.09961 to 0.09815, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1463 - val_loss: 0.0981\n",
      "Epoch 7/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.1354\n",
      "Epoch 00007: val_loss did not improve from 0.09815\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1354 - val_loss: 0.0983\n",
      "Epoch 8/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.1358\n",
      "Epoch 00008: val_loss did not improve from 0.09815\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1356 - val_loss: 0.1020\n",
      " ###0 fold : val acc1 0.600, acc3 0.977, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556/558 [============================>.] - ETA: 0s - loss: 0.5714\n",
      "Epoch 00001: val_loss improved from inf to 0.13505, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 3s 4ms/step - loss: 0.5707 - val_loss: 0.1350\n",
      "Epoch 2/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.2032\n",
      "Epoch 00002: val_loss improved from 0.13505 to 0.10030, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2038 - val_loss: 0.1003\n",
      "Epoch 3/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.1861\n",
      "Epoch 00003: val_loss did not improve from 0.10030\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1862 - val_loss: 0.1008\n",
      "Epoch 4/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.1681\n",
      "Epoch 00004: val_loss improved from 0.10030 to 0.09740, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1679 - val_loss: 0.0974\n",
      "Epoch 5/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.1543\n",
      "Epoch 00005: val_loss did not improve from 0.09740\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1542 - val_loss: 0.1036\n",
      "Epoch 6/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.1474\n",
      "Epoch 00006: val_loss did not improve from 0.09740\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1472 - val_loss: 0.1021\n",
      " ###1 fold : val acc1 0.576, acc3 0.981, mae 0.222###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558/558 [==============================] - ETA: 0s - loss: 0.5759\n",
      "Epoch 00001: val_loss improved from inf to 0.14101, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 3s 4ms/step - loss: 0.5759 - val_loss: 0.1410\n",
      "Epoch 2/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.2050\n",
      "Epoch 00002: val_loss improved from 0.14101 to 0.10238, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2052 - val_loss: 0.1024\n",
      "Epoch 3/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00003: val_loss improved from 0.10238 to 0.10028, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1877 - val_loss: 0.1003\n",
      "Epoch 4/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.1654\n",
      "Epoch 00004: val_loss did not improve from 0.10028\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1652 - val_loss: 0.1100\n",
      "Epoch 5/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.1561\n",
      "Epoch 00005: val_loss did not improve from 0.10028\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1562 - val_loss: 0.1102\n",
      " ###2 fold : val acc1 0.604, acc3 0.975, mae 0.211###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555/558 [============================>.] - ETA: 0s - loss: 0.5667\n",
      "Epoch 00001: val_loss improved from inf to 0.11815, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 3s 4ms/step - loss: 0.5654 - val_loss: 0.1182\n",
      "Epoch 2/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00002: val_loss improved from 0.11815 to 0.10693, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2055 - val_loss: 0.1069\n",
      "Epoch 3/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.1833\n",
      "Epoch 00003: val_loss improved from 0.10693 to 0.10168, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1835 - val_loss: 0.1017\n",
      "Epoch 4/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.1662\n",
      "Epoch 00004: val_loss did not improve from 0.10168\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1662 - val_loss: 0.1124\n",
      "Epoch 5/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.1550\n",
      "Epoch 00005: val_loss did not improve from 0.10168\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1550 - val_loss: 0.1035\n",
      " ###3 fold : val acc1 0.596, acc3 0.968, mae 0.219###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/558 [============================>.] - ETA: 0s - loss: 0.5783\n",
      "Epoch 00001: val_loss improved from inf to 0.12856, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 3s 4ms/step - loss: 0.5713 - val_loss: 0.1286\n",
      "Epoch 2/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.2006\n",
      "Epoch 00002: val_loss improved from 0.12856 to 0.10688, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2006 - val_loss: 0.1069\n",
      "Epoch 3/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.1859\n",
      "Epoch 00003: val_loss did not improve from 0.10688\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1858 - val_loss: 0.1178\n",
      "Epoch 4/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.1671\n",
      "Epoch 00004: val_loss did not improve from 0.10688\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1672 - val_loss: 0.1435\n",
      " ###4 fold : val acc1 0.577, acc3 0.971, mae 0.226###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555/558 [============================>.] - ETA: 0s - loss: 0.5649\n",
      "Epoch 00001: val_loss improved from inf to 0.11882, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 3s 4ms/step - loss: 0.5637 - val_loss: 0.1188\n",
      "Epoch 2/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.2030\n",
      "Epoch 00002: val_loss improved from 0.11882 to 0.11542, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2028 - val_loss: 0.1154\n",
      "Epoch 3/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.1849\n",
      "Epoch 00003: val_loss did not improve from 0.11542\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1849 - val_loss: 0.1173\n",
      "Epoch 4/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.1692\n",
      "Epoch 00004: val_loss did not improve from 0.11542\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1692 - val_loss: 0.1157\n",
      " ###5 fold : val acc1 0.567, acc3 0.968, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551/558 [============================>.] - ETA: 0s - loss: 0.5613\n",
      "Epoch 00001: val_loss improved from inf to 0.11560, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "558/558 [==============================] - 3s 4ms/step - loss: 0.5578 - val_loss: 0.1156\n",
      "Epoch 2/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.2016\n",
      "Epoch 00002: val_loss did not improve from 0.11560\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2017 - val_loss: 0.1259\n",
      "Epoch 3/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.1853\n",
      "Epoch 00003: val_loss did not improve from 0.11560\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1852 - val_loss: 0.1160\n",
      " ###6 fold : val acc1 0.559, acc3 0.974, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557/558 [============================>.] - ETA: 0s - loss: 0.5562\n",
      "Epoch 00001: val_loss improved from inf to 0.12247, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "558/558 [==============================] - 3s 4ms/step - loss: 0.5561 - val_loss: 0.1225\n",
      "Epoch 2/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00002: val_loss did not improve from 0.12247\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1995 - val_loss: 0.1270\n",
      "Epoch 3/100\n",
      "554/558 [============================>.] - ETA: 0s - loss: 0.1855\n",
      "Epoch 00003: val_loss improved from 0.12247 to 0.12086, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1854 - val_loss: 0.1209\n",
      "Epoch 4/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.1693\n",
      "Epoch 00004: val_loss improved from 0.12086 to 0.09918, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1692 - val_loss: 0.0992\n",
      "Epoch 5/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.1544\n",
      "Epoch 00005: val_loss did not improve from 0.09918\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1542 - val_loss: 0.1040\n",
      "Epoch 6/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.1434\n",
      "Epoch 00006: val_loss improved from 0.09918 to 0.09608, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1434 - val_loss: 0.0961\n",
      "Epoch 7/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.1372\n",
      "Epoch 00007: val_loss did not improve from 0.09608\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1372 - val_loss: 0.1005\n",
      "Epoch 8/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.1318\n",
      "Epoch 00008: val_loss did not improve from 0.09608\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1319 - val_loss: 0.0969\n",
      " ###7 fold : val acc1 0.587, acc3 0.975, mae 0.219###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553/558 [============================>.] - ETA: 0s - loss: 0.5590\n",
      "Epoch 00001: val_loss improved from inf to 0.12474, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 3s 4ms/step - loss: 0.5568 - val_loss: 0.1247\n",
      "Epoch 2/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.2002\n",
      "Epoch 00002: val_loss did not improve from 0.12474\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2001 - val_loss: 0.1323\n",
      "Epoch 3/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.1839\n",
      "Epoch 00003: val_loss improved from 0.12474 to 0.11340, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1839 - val_loss: 0.1134\n",
      "Epoch 4/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.1679\n",
      "Epoch 00004: val_loss improved from 0.11340 to 0.10289, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1680 - val_loss: 0.1029\n",
      "Epoch 5/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.1537\n",
      "Epoch 00005: val_loss did not improve from 0.10289\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1536 - val_loss: 0.1106\n",
      "Epoch 6/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.1434\n",
      "Epoch 00006: val_loss improved from 0.10289 to 0.09826, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1434 - val_loss: 0.0983\n",
      "Epoch 7/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.1377\n",
      "Epoch 00007: val_loss did not improve from 0.09826\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1377 - val_loss: 0.1032\n",
      "Epoch 8/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.1321\n",
      "Epoch 00008: val_loss did not improve from 0.09826\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1321 - val_loss: 0.1001\n",
      " ###8 fold : val acc1 0.580, acc3 0.982, mae 0.219###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553/558 [============================>.] - ETA: 0s - loss: 0.5590\n",
      "Epoch 00001: val_loss improved from inf to 0.12432, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 3s 4ms/step - loss: 0.5568 - val_loss: 0.1243\n",
      "Epoch 2/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.2001\n",
      "Epoch 00002: val_loss did not improve from 0.12432\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2001 - val_loss: 0.1365\n",
      "Epoch 3/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.1840\n",
      "Epoch 00003: val_loss improved from 0.12432 to 0.11772, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1839 - val_loss: 0.1177\n",
      "Epoch 4/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.1681\n",
      "Epoch 00004: val_loss improved from 0.11772 to 0.10534, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1680 - val_loss: 0.1053\n",
      "Epoch 5/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.1537\n",
      "Epoch 00005: val_loss did not improve from 0.10534\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1536 - val_loss: 0.1106\n",
      "Epoch 6/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.1434\n",
      "Epoch 00006: val_loss improved from 0.10534 to 0.09991, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1434 - val_loss: 0.0999\n",
      "Epoch 7/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.1377\n",
      "Epoch 00007: val_loss did not improve from 0.09991\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1377 - val_loss: 0.1065\n",
      "Epoch 8/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.1325\n",
      "Epoch 00008: val_loss did not improve from 0.09991\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.1321 - val_loss: 0.1009\n",
      " ###9 fold : val acc1 0.586, acc3 0.985, mae 0.214###\n",
      "acc10.583_acc30.976\n",
      "random search 1/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/70 [=========================>....] - ETA: 0s - loss: 20.1293\n",
      "Epoch 00001: val_loss improved from inf to 14.76830, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 19.5308 - val_loss: 14.7683\n",
      "Epoch 2/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 11.6837\n",
      "Epoch 00002: val_loss improved from 14.76830 to 7.79814, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 11.2877 - val_loss: 7.7981\n",
      "Epoch 3/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 6.1747\n",
      "Epoch 00003: val_loss improved from 7.79814 to 4.02375, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 5.8759 - val_loss: 4.0237\n",
      "Epoch 4/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 3.5639\n",
      "Epoch 00004: val_loss improved from 4.02375 to 2.48224, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.4442 - val_loss: 2.4822\n",
      "Epoch 5/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 2.3843\n",
      "Epoch 00005: val_loss improved from 2.48224 to 1.57598, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.3005 - val_loss: 1.5760\n",
      "Epoch 6/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 1.6074\n",
      "Epoch 00006: val_loss improved from 1.57598 to 0.98416, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.5590 - val_loss: 0.9842\n",
      "Epoch 7/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 1.1377\n",
      "Epoch 00007: val_loss improved from 0.98416 to 0.60947, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0938 - val_loss: 0.6095\n",
      "Epoch 8/100\n",
      "53/70 [=====================>........] - ETA: 0s - loss: 0.8106\n",
      "Epoch 00008: val_loss improved from 0.60947 to 0.39185, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7787 - val_loss: 0.3919\n",
      "Epoch 9/100\n",
      "53/70 [=====================>........] - ETA: 0s - loss: 0.6268\n",
      "Epoch 00009: val_loss improved from 0.39185 to 0.26845, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6100 - val_loss: 0.2685\n",
      "Epoch 10/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.5390\n",
      "Epoch 00010: val_loss improved from 0.26845 to 0.19963, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5318 - val_loss: 0.1996\n",
      "Epoch 11/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.4839\n",
      "Epoch 00011: val_loss improved from 0.19963 to 0.16542, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4824 - val_loss: 0.1654\n",
      "Epoch 12/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.4548\n",
      "Epoch 00012: val_loss improved from 0.16542 to 0.14828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4530 - val_loss: 0.1483\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4341\n",
      "Epoch 00013: val_loss improved from 0.14828 to 0.13980, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4341 - val_loss: 0.1398\n",
      "Epoch 14/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4271\n",
      "Epoch 00014: val_loss improved from 0.13980 to 0.13434, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4276 - val_loss: 0.1343\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4098\n",
      "Epoch 00015: val_loss improved from 0.13434 to 0.13155, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4098 - val_loss: 0.1316\n",
      "Epoch 16/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.4122\n",
      "Epoch 00016: val_loss improved from 0.13155 to 0.12817, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4088 - val_loss: 0.1282\n",
      "Epoch 17/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.3971\n",
      "Epoch 00017: val_loss improved from 0.12817 to 0.12614, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3971 - val_loss: 0.1261\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3926\n",
      "Epoch 00018: val_loss improved from 0.12614 to 0.12456, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3926 - val_loss: 0.1246\n",
      "Epoch 19/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.3771\n",
      "Epoch 00019: val_loss improved from 0.12456 to 0.12334, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3772 - val_loss: 0.1233\n",
      "Epoch 20/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3757\n",
      "Epoch 00020: val_loss improved from 0.12334 to 0.12105, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3685 - val_loss: 0.1211\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3613\n",
      "Epoch 00021: val_loss improved from 0.12105 to 0.11925, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3613 - val_loss: 0.1192\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3560\n",
      "Epoch 00022: val_loss improved from 0.11925 to 0.11851, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3560 - val_loss: 0.1185\n",
      "Epoch 23/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3587\n",
      "Epoch 00023: val_loss improved from 0.11851 to 0.11724, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3573 - val_loss: 0.1172\n",
      "Epoch 24/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.3508\n",
      "Epoch 00024: val_loss improved from 0.11724 to 0.11610, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3522 - val_loss: 0.1161\n",
      "Epoch 25/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.3357\n",
      "Epoch 00025: val_loss improved from 0.11610 to 0.11484, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3385 - val_loss: 0.1148\n",
      "Epoch 26/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.3478\n",
      "Epoch 00026: val_loss improved from 0.11484 to 0.11370, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3429 - val_loss: 0.1137\n",
      "Epoch 27/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.3422\n",
      "Epoch 00027: val_loss improved from 0.11370 to 0.11260, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3412 - val_loss: 0.1126\n",
      "Epoch 28/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3335\n",
      "Epoch 00028: val_loss improved from 0.11260 to 0.11151, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3336 - val_loss: 0.1115\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3270\n",
      "Epoch 00029: val_loss improved from 0.11151 to 0.10984, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3270 - val_loss: 0.1098\n",
      "Epoch 30/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.3250\n",
      "Epoch 00030: val_loss improved from 0.10984 to 0.10878, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3228 - val_loss: 0.1088\n",
      "Epoch 31/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3197\n",
      "Epoch 00031: val_loss did not improve from 0.10878\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3191 - val_loss: 0.1090\n",
      "Epoch 32/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.3124\n",
      "Epoch 00032: val_loss improved from 0.10878 to 0.10871, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3133 - val_loss: 0.1087\n",
      "Epoch 33/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3090\n",
      "Epoch 00033: val_loss improved from 0.10871 to 0.10650, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3109 - val_loss: 0.1065\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3091\n",
      "Epoch 00034: val_loss improved from 0.10650 to 0.10634, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3091 - val_loss: 0.1063\n",
      "Epoch 35/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.3090\n",
      "Epoch 00035: val_loss improved from 0.10634 to 0.10542, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3059 - val_loss: 0.1054\n",
      "Epoch 36/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2994\n",
      "Epoch 00036: val_loss did not improve from 0.10542\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2990 - val_loss: 0.1059\n",
      "Epoch 37/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2982\n",
      "Epoch 00037: val_loss improved from 0.10542 to 0.10459, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2987 - val_loss: 0.1046\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2976\n",
      "Epoch 00038: val_loss did not improve from 0.10459\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2976 - val_loss: 0.1049\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2936\n",
      "Epoch 00039: val_loss improved from 0.10459 to 0.10451, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2936 - val_loss: 0.1045\n",
      "Epoch 40/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2951\n",
      "Epoch 00040: val_loss did not improve from 0.10451\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2950 - val_loss: 0.1047\n",
      "Epoch 41/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2898\n",
      "Epoch 00041: val_loss improved from 0.10451 to 0.10367, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2917 - val_loss: 0.1037\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2880\n",
      "Epoch 00042: val_loss improved from 0.10367 to 0.10232, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2880 - val_loss: 0.1023\n",
      "Epoch 43/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2837\n",
      "Epoch 00043: val_loss improved from 0.10232 to 0.10223, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2874 - val_loss: 0.1022\n",
      "Epoch 44/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2867\n",
      "Epoch 00044: val_loss did not improve from 0.10223\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2896 - val_loss: 0.1030\n",
      "Epoch 45/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2838\n",
      "Epoch 00045: val_loss did not improve from 0.10223\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2840 - val_loss: 0.1026\n",
      " ###0 fold : val acc1 0.596, acc3 0.977, mae 0.214###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/70 [=========================>....] - ETA: 0s - loss: 20.0618\n",
      "Epoch 00001: val_loss improved from inf to 14.77433, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 19.5226 - val_loss: 14.7743\n",
      "Epoch 2/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 11.5233\n",
      "Epoch 00002: val_loss improved from 14.77433 to 7.80976, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 11.2926 - val_loss: 7.8098\n",
      "Epoch 3/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 6.0928\n",
      "Epoch 00003: val_loss improved from 7.80976 to 4.02898, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 5.8845 - val_loss: 4.0290\n",
      "Epoch 4/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 3.5723\n",
      "Epoch 00004: val_loss improved from 4.02898 to 2.48061, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.4510 - val_loss: 2.4806\n",
      "Epoch 5/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 2.3252\n",
      "Epoch 00005: val_loss improved from 2.48061 to 1.57457, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 2.3003 - val_loss: 1.5746\n",
      "Epoch 6/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 1.5928\n",
      "Epoch 00006: val_loss improved from 1.57457 to 0.98102, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.5616 - val_loss: 0.9810\n",
      "Epoch 7/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 1.1340\n",
      "Epoch 00007: val_loss improved from 0.98102 to 0.60711, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0988 - val_loss: 0.6071\n",
      "Epoch 8/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.7972\n",
      "Epoch 00008: val_loss improved from 0.60711 to 0.39097, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7775 - val_loss: 0.3910\n",
      "Epoch 9/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6255\n",
      "Epoch 00009: val_loss improved from 0.39097 to 0.26809, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6138 - val_loss: 0.2681\n",
      "Epoch 10/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.5327\n",
      "Epoch 00010: val_loss improved from 0.26809 to 0.19929, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5292 - val_loss: 0.1993\n",
      "Epoch 11/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.4852\n",
      "Epoch 00011: val_loss improved from 0.19929 to 0.16514, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4833 - val_loss: 0.1651\n",
      "Epoch 12/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.4546\n",
      "Epoch 00012: val_loss improved from 0.16514 to 0.14805, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4551 - val_loss: 0.1480\n",
      "Epoch 13/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.4374\n",
      "Epoch 00013: val_loss improved from 0.14805 to 0.13957, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4376 - val_loss: 0.1396\n",
      "Epoch 14/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.4280\n",
      "Epoch 00014: val_loss improved from 0.13957 to 0.13411, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4293 - val_loss: 0.1341\n",
      "Epoch 15/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.4116\n",
      "Epoch 00015: val_loss improved from 0.13411 to 0.13133, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4064 - val_loss: 0.1313\n",
      "Epoch 16/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.4089\n",
      "Epoch 00016: val_loss improved from 0.13133 to 0.12817, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4049 - val_loss: 0.1282\n",
      "Epoch 17/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3965\n",
      "Epoch 00017: val_loss improved from 0.12817 to 0.12611, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3955 - val_loss: 0.1261\n",
      "Epoch 18/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3921\n",
      "Epoch 00018: val_loss improved from 0.12611 to 0.12430, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3911 - val_loss: 0.1243\n",
      "Epoch 19/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.3743\n",
      "Epoch 00019: val_loss improved from 0.12430 to 0.12321, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3751 - val_loss: 0.1232\n",
      "Epoch 20/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.3705\n",
      "Epoch 00020: val_loss improved from 0.12321 to 0.12093, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3683 - val_loss: 0.1209\n",
      "Epoch 21/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.3632\n",
      "Epoch 00021: val_loss improved from 0.12093 to 0.11933, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3648 - val_loss: 0.1193\n",
      "Epoch 22/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.3572\n",
      "Epoch 00022: val_loss improved from 0.11933 to 0.11835, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3587 - val_loss: 0.1184\n",
      "Epoch 23/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.3549\n",
      "Epoch 00023: val_loss improved from 0.11835 to 0.11707, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3559 - val_loss: 0.1171\n",
      "Epoch 24/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3529\n",
      "Epoch 00024: val_loss improved from 0.11707 to 0.11601, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3541 - val_loss: 0.1160\n",
      "Epoch 25/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.3383\n",
      "Epoch 00025: val_loss improved from 0.11601 to 0.11493, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3404 - val_loss: 0.1149\n",
      "Epoch 26/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3468\n",
      "Epoch 00026: val_loss improved from 0.11493 to 0.11331, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3425 - val_loss: 0.1133\n",
      "Epoch 27/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.3456\n",
      "Epoch 00027: val_loss improved from 0.11331 to 0.11215, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3444 - val_loss: 0.1121\n",
      "Epoch 28/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3343\n",
      "Epoch 00028: val_loss improved from 0.11215 to 0.11125, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3336 - val_loss: 0.1113\n",
      "Epoch 29/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.3261\n",
      "Epoch 00029: val_loss improved from 0.11125 to 0.10970, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3277 - val_loss: 0.1097\n",
      "Epoch 30/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.3225\n",
      "Epoch 00030: val_loss improved from 0.10970 to 0.10876, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3205 - val_loss: 0.1088\n",
      "Epoch 31/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.3208\n",
      "Epoch 00031: val_loss did not improve from 0.10876\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3210 - val_loss: 0.1090\n",
      "Epoch 32/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.3143\n",
      "Epoch 00032: val_loss improved from 0.10876 to 0.10860, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3149 - val_loss: 0.1086\n",
      "Epoch 33/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.3128\n",
      "Epoch 00033: val_loss improved from 0.10860 to 0.10659, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3127 - val_loss: 0.1066\n",
      "Epoch 34/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3074\n",
      "Epoch 00034: val_loss improved from 0.10659 to 0.10594, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3077 - val_loss: 0.1059\n",
      "Epoch 35/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.3086\n",
      "Epoch 00035: val_loss improved from 0.10594 to 0.10546, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3063 - val_loss: 0.1055\n",
      "Epoch 36/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2974\n",
      "Epoch 00036: val_loss improved from 0.10546 to 0.10533, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2975 - val_loss: 0.1053\n",
      "Epoch 37/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2979\n",
      "Epoch 00037: val_loss improved from 0.10533 to 0.10427, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3001 - val_loss: 0.1043\n",
      "Epoch 38/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2987\n",
      "Epoch 00038: val_loss did not improve from 0.10427\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2958 - val_loss: 0.1047\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2939\n",
      "Epoch 00039: val_loss did not improve from 0.10427\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2939 - val_loss: 0.1043\n",
      " ###1 fold : val acc1 0.591, acc3 0.976, mae 0.217###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/70 [==========================>...] - ETA: 0s - loss: 19.7954\n",
      "Epoch 00001: val_loss improved from inf to 14.76719, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 19.4920 - val_loss: 14.7672\n",
      "Epoch 2/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 11.5513\n",
      "Epoch 00002: val_loss improved from 14.76719 to 7.80270, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 11.2729 - val_loss: 7.8027\n",
      "Epoch 3/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 5.9716\n",
      "Epoch 00003: val_loss improved from 7.80270 to 4.02861, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 5.8753 - val_loss: 4.0286\n",
      "Epoch 4/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 3.5073\n",
      "Epoch 00004: val_loss improved from 4.02861 to 2.47972, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.4468 - val_loss: 2.4797\n",
      "Epoch 5/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 2.3628\n",
      "Epoch 00005: val_loss improved from 2.47972 to 1.57282, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.2946 - val_loss: 1.5728\n",
      "Epoch 6/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 1.5835\n",
      "Epoch 00006: val_loss improved from 1.57282 to 0.97778, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.5555 - val_loss: 0.9778\n",
      "Epoch 7/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 1.1120\n",
      "Epoch 00007: val_loss improved from 0.97778 to 0.60304, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 1.0940 - val_loss: 0.6030\n",
      "Epoch 8/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.7975\n",
      "Epoch 00008: val_loss improved from 0.60304 to 0.38623, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7822 - val_loss: 0.3862\n",
      "Epoch 9/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.6164\n",
      "Epoch 00009: val_loss improved from 0.38623 to 0.26500, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6115 - val_loss: 0.2650\n",
      "Epoch 10/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.5354\n",
      "Epoch 00010: val_loss improved from 0.26500 to 0.19707, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5321 - val_loss: 0.1971\n",
      "Epoch 11/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.4849\n",
      "Epoch 00011: val_loss improved from 0.19707 to 0.16433, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4803 - val_loss: 0.1643\n",
      "Epoch 12/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.4543\n",
      "Epoch 00012: val_loss improved from 0.16433 to 0.14772, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4548 - val_loss: 0.1477\n",
      "Epoch 13/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.4372\n",
      "Epoch 00013: val_loss improved from 0.14772 to 0.13920, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4393 - val_loss: 0.1392\n",
      "Epoch 14/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.4243\n",
      "Epoch 00014: val_loss improved from 0.13920 to 0.13405, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4260 - val_loss: 0.1341\n",
      "Epoch 15/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.4082\n",
      "Epoch 00015: val_loss improved from 0.13405 to 0.13102, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4042 - val_loss: 0.1310\n",
      "Epoch 16/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.4066\n",
      "Epoch 00016: val_loss improved from 0.13102 to 0.12866, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4035 - val_loss: 0.1287\n",
      "Epoch 17/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.3877\n",
      "Epoch 00017: val_loss improved from 0.12866 to 0.12584, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3905 - val_loss: 0.1258\n",
      "Epoch 18/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.3895\n",
      "Epoch 00018: val_loss improved from 0.12584 to 0.12423, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3892 - val_loss: 0.1242\n",
      "Epoch 19/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.3747\n",
      "Epoch 00019: val_loss improved from 0.12423 to 0.12374, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3745 - val_loss: 0.1237\n",
      "Epoch 20/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.3734\n",
      "Epoch 00020: val_loss improved from 0.12374 to 0.12137, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3695 - val_loss: 0.1214\n",
      "Epoch 21/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.3605\n",
      "Epoch 00021: val_loss improved from 0.12137 to 0.11933, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3631 - val_loss: 0.1193\n",
      "Epoch 22/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3565\n",
      "Epoch 00022: val_loss improved from 0.11933 to 0.11852, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3588 - val_loss: 0.1185\n",
      "Epoch 23/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.3543\n",
      "Epoch 00023: val_loss improved from 0.11852 to 0.11739, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3564 - val_loss: 0.1174\n",
      "Epoch 24/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.3523\n",
      "Epoch 00024: val_loss improved from 0.11739 to 0.11644, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3536 - val_loss: 0.1164\n",
      "Epoch 25/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3400\n",
      "Epoch 00025: val_loss improved from 0.11644 to 0.11504, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3391 - val_loss: 0.1150\n",
      "Epoch 26/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.3442\n",
      "Epoch 00026: val_loss improved from 0.11504 to 0.11370, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3407 - val_loss: 0.1137\n",
      "Epoch 27/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.3436\n",
      "Epoch 00027: val_loss improved from 0.11370 to 0.11252, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3417 - val_loss: 0.1125\n",
      "Epoch 28/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3319\n",
      "Epoch 00028: val_loss improved from 0.11252 to 0.11153, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3322 - val_loss: 0.1115\n",
      "Epoch 29/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.3232\n",
      "Epoch 00029: val_loss improved from 0.11153 to 0.10989, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3267 - val_loss: 0.1099\n",
      "Epoch 30/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.3217\n",
      "Epoch 00030: val_loss improved from 0.10989 to 0.10920, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3190 - val_loss: 0.1092\n",
      "Epoch 31/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.3192\n",
      "Epoch 00031: val_loss did not improve from 0.10920\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3199 - val_loss: 0.1095\n",
      "Epoch 32/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.3128\n",
      "Epoch 00032: val_loss did not improve from 0.10920\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3142 - val_loss: 0.1093\n",
      " ###2 fold : val acc1 0.576, acc3 0.973, mae 0.226###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 19.7786\n",
      "Epoch 00001: val_loss improved from inf to 14.74422, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 19.5467 - val_loss: 14.7442\n",
      "Epoch 2/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 11.4699\n",
      "Epoch 00002: val_loss improved from 14.74422 to 7.77596, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 11.3081 - val_loss: 7.7760\n",
      "Epoch 3/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 6.0396\n",
      "Epoch 00003: val_loss improved from 7.77596 to 4.01202, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 5.8900 - val_loss: 4.0120\n",
      "Epoch 4/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 3.4966\n",
      "Epoch 00004: val_loss improved from 4.01202 to 2.47859, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.4546 - val_loss: 2.4786\n",
      "Epoch 5/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 2.3704\n",
      "Epoch 00005: val_loss improved from 2.47859 to 1.57079, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.3034 - val_loss: 1.5708\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.5643\n",
      "Epoch 00006: val_loss improved from 1.57079 to 0.97488, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.5643 - val_loss: 0.9749\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0865\n",
      "Epoch 00007: val_loss improved from 0.97488 to 0.60258, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0865 - val_loss: 0.6026\n",
      "Epoch 8/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.8111\n",
      "Epoch 00008: val_loss improved from 0.60258 to 0.38581, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7848 - val_loss: 0.3858\n",
      "Epoch 9/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6285\n",
      "Epoch 00009: val_loss improved from 0.38581 to 0.26383, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6157 - val_loss: 0.2638\n",
      "Epoch 10/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.5382\n",
      "Epoch 00010: val_loss improved from 0.26383 to 0.19600, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5293 - val_loss: 0.1960\n",
      "Epoch 11/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.4814\n",
      "Epoch 00011: val_loss improved from 0.19600 to 0.16344, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4771 - val_loss: 0.1634\n",
      "Epoch 12/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.4529\n",
      "Epoch 00012: val_loss improved from 0.16344 to 0.14722, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4537 - val_loss: 0.1472\n",
      "Epoch 13/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.4423\n",
      "Epoch 00013: val_loss improved from 0.14722 to 0.13875, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4429 - val_loss: 0.1387\n",
      "Epoch 14/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.4231\n",
      "Epoch 00014: val_loss improved from 0.13875 to 0.13346, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4252 - val_loss: 0.1335\n",
      "Epoch 15/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.4081\n",
      "Epoch 00015: val_loss improved from 0.13346 to 0.13079, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4024 - val_loss: 0.1308\n",
      "Epoch 16/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.4045\n",
      "Epoch 00016: val_loss improved from 0.13079 to 0.12842, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4023 - val_loss: 0.1284\n",
      "Epoch 17/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3877\n",
      "Epoch 00017: val_loss improved from 0.12842 to 0.12563, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3893 - val_loss: 0.1256\n",
      "Epoch 18/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.3885\n",
      "Epoch 00018: val_loss improved from 0.12563 to 0.12419, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3885 - val_loss: 0.1242\n",
      "Epoch 19/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.3749\n",
      "Epoch 00019: val_loss improved from 0.12419 to 0.12317, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3758 - val_loss: 0.1232\n",
      "Epoch 20/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3731\n",
      "Epoch 00020: val_loss improved from 0.12317 to 0.12137, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3709 - val_loss: 0.1214\n",
      "Epoch 21/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3635\n",
      "Epoch 00021: val_loss improved from 0.12137 to 0.11925, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3656 - val_loss: 0.1193\n",
      "Epoch 22/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.3579\n",
      "Epoch 00022: val_loss improved from 0.11925 to 0.11874, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3611 - val_loss: 0.1187\n",
      "Epoch 23/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.3531\n",
      "Epoch 00023: val_loss improved from 0.11874 to 0.11713, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3563 - val_loss: 0.1171\n",
      "Epoch 24/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.3536\n",
      "Epoch 00024: val_loss improved from 0.11713 to 0.11613, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3528 - val_loss: 0.1161\n",
      "Epoch 25/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3415\n",
      "Epoch 00025: val_loss improved from 0.11613 to 0.11521, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3391 - val_loss: 0.1152\n",
      "Epoch 26/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.3433\n",
      "Epoch 00026: val_loss improved from 0.11521 to 0.11342, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3399 - val_loss: 0.1134\n",
      "Epoch 27/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.3417\n",
      "Epoch 00027: val_loss improved from 0.11342 to 0.11225, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3417 - val_loss: 0.1122\n",
      "Epoch 28/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.3333\n",
      "Epoch 00028: val_loss improved from 0.11225 to 0.11111, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3322 - val_loss: 0.1111\n",
      "Epoch 29/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3222\n",
      "Epoch 00029: val_loss improved from 0.11111 to 0.11001, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3252 - val_loss: 0.1100\n",
      "Epoch 30/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.3204\n",
      "Epoch 00030: val_loss improved from 0.11001 to 0.10952, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3180 - val_loss: 0.1095\n",
      "Epoch 31/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3206\n",
      "Epoch 00031: val_loss did not improve from 0.10952\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3202 - val_loss: 0.1097\n",
      "Epoch 32/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.3134\n",
      "Epoch 00032: val_loss improved from 0.10952 to 0.10930, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3146 - val_loss: 0.1093\n",
      "Epoch 33/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.3167\n",
      "Epoch 00033: val_loss improved from 0.10930 to 0.10704, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3170 - val_loss: 0.1070\n",
      "Epoch 34/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.3052\n",
      "Epoch 00034: val_loss improved from 0.10704 to 0.10643, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3055 - val_loss: 0.1064\n",
      "Epoch 35/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.3055\n",
      "Epoch 00035: val_loss improved from 0.10643 to 0.10554, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3060 - val_loss: 0.1055\n",
      "Epoch 36/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.3030\n",
      "Epoch 00036: val_loss did not improve from 0.10554\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3018 - val_loss: 0.1060\n",
      "Epoch 37/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.3020\n",
      "Epoch 00037: val_loss improved from 0.10554 to 0.10460, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3036 - val_loss: 0.1046\n",
      "Epoch 38/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2982\n",
      "Epoch 00038: val_loss did not improve from 0.10460\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2960 - val_loss: 0.1048\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2975\n",
      "Epoch 00039: val_loss improved from 0.10460 to 0.10450, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2975 - val_loss: 0.1045\n",
      "Epoch 40/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2956\n",
      "Epoch 00040: val_loss improved from 0.10450 to 0.10381, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2942 - val_loss: 0.1038\n",
      "Epoch 41/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2922\n",
      "Epoch 00041: val_loss did not improve from 0.10381\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2919 - val_loss: 0.1040\n",
      "Epoch 42/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2864\n",
      "Epoch 00042: val_loss improved from 0.10381 to 0.10300, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2885 - val_loss: 0.1030\n",
      "Epoch 43/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.2890\n",
      "Epoch 00043: val_loss improved from 0.10300 to 0.10242, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2891 - val_loss: 0.1024\n",
      "Epoch 44/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2907\n",
      "Epoch 00044: val_loss did not improve from 0.10242\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2933 - val_loss: 0.1027\n",
      "Epoch 45/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2835\n",
      "Epoch 00045: val_loss did not improve from 0.10242\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2859 - val_loss: 0.1038\n",
      " ###3 fold : val acc1 0.602, acc3 0.971, mae 0.214###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 19.5999\n",
      "Epoch 00001: val_loss improved from inf to 14.68888, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 19.4336 - val_loss: 14.6889\n",
      "Epoch 2/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 11.3631\n",
      "Epoch 00002: val_loss improved from 14.68888 to 7.73999, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 11.2236 - val_loss: 7.7400\n",
      "Epoch 3/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 5.8627\n",
      "Epoch 00003: val_loss improved from 7.73999 to 4.01041, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 5.8467 - val_loss: 4.0104\n",
      "Epoch 4/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 3.4589\n",
      "Epoch 00004: val_loss improved from 4.01041 to 2.47218, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 3.4251 - val_loss: 2.4722\n",
      "Epoch 5/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 2.3501\n",
      "Epoch 00005: val_loss improved from 2.47218 to 1.57013, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 2.3180 - val_loss: 1.5701\n",
      "Epoch 6/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.5797\n",
      "Epoch 00006: val_loss improved from 1.57013 to 0.97325, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 1.5713 - val_loss: 0.9733\n",
      "Epoch 7/100\n",
      "47/70 [===================>..........] - ETA: 0s - loss: 1.1581\n",
      "Epoch 00007: val_loss improved from 0.97325 to 0.60373, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 1.0886 - val_loss: 0.6037\n",
      "Epoch 8/100\n",
      "49/70 [====================>.........] - ETA: 0s - loss: 0.8343\n",
      "Epoch 00008: val_loss improved from 0.60373 to 0.38379, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.7959 - val_loss: 0.3838\n",
      "Epoch 9/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6288\n",
      "Epoch 00009: val_loss improved from 0.38379 to 0.26024, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6253 - val_loss: 0.2602\n",
      "Epoch 10/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5224\n",
      "Epoch 00010: val_loss improved from 0.26024 to 0.19582, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5219 - val_loss: 0.1958\n",
      "Epoch 11/100\n",
      "48/70 [===================>..........] - ETA: 0s - loss: 0.4824\n",
      "Epoch 00011: val_loss improved from 0.19582 to 0.16295, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4754 - val_loss: 0.1629\n",
      "Epoch 12/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.4462\n",
      "Epoch 00012: val_loss improved from 0.16295 to 0.14748, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4439 - val_loss: 0.1475\n",
      "Epoch 13/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.4294\n",
      "Epoch 00013: val_loss improved from 0.14748 to 0.13934, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4335 - val_loss: 0.1393\n",
      "Epoch 14/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.4251\n",
      "Epoch 00014: val_loss improved from 0.13934 to 0.13421, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4243 - val_loss: 0.1342\n",
      "Epoch 15/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.4140\n",
      "Epoch 00015: val_loss improved from 0.13421 to 0.13020, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4140 - val_loss: 0.1302\n",
      "Epoch 16/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.4078\n",
      "Epoch 00016: val_loss improved from 0.13020 to 0.12879, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4070 - val_loss: 0.1288\n",
      "Epoch 17/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.3977\n",
      "Epoch 00017: val_loss improved from 0.12879 to 0.12640, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3989 - val_loss: 0.1264\n",
      "Epoch 18/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.3904\n",
      "Epoch 00018: val_loss improved from 0.12640 to 0.12545, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3888 - val_loss: 0.1255\n",
      "Epoch 19/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.3822\n",
      "Epoch 00019: val_loss improved from 0.12545 to 0.12297, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3817 - val_loss: 0.1230\n",
      "Epoch 20/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.3812\n",
      "Epoch 00020: val_loss improved from 0.12297 to 0.12159, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3811 - val_loss: 0.1216\n",
      "Epoch 21/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3665\n",
      "Epoch 00021: val_loss improved from 0.12159 to 0.11999, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.3660 - val_loss: 0.1200\n",
      "Epoch 22/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.3616\n",
      "Epoch 00022: val_loss improved from 0.11999 to 0.11939, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3621 - val_loss: 0.1194\n",
      "Epoch 23/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.3485\n",
      "Epoch 00023: val_loss improved from 0.11939 to 0.11698, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3493 - val_loss: 0.1170\n",
      "Epoch 24/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.3498\n",
      "Epoch 00024: val_loss improved from 0.11698 to 0.11551, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3501 - val_loss: 0.1155\n",
      "Epoch 25/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.3377\n",
      "Epoch 00025: val_loss did not improve from 0.11551\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3373 - val_loss: 0.1157\n",
      "Epoch 26/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.3381\n",
      "Epoch 00026: val_loss improved from 0.11551 to 0.11371, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3382 - val_loss: 0.1137\n",
      "Epoch 27/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.3333\n",
      "Epoch 00027: val_loss improved from 0.11371 to 0.11313, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3330 - val_loss: 0.1131\n",
      "Epoch 28/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.3250\n",
      "Epoch 00028: val_loss improved from 0.11313 to 0.11197, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3254 - val_loss: 0.1120\n",
      "Epoch 29/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.3301\n",
      "Epoch 00029: val_loss improved from 0.11197 to 0.11043, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3305 - val_loss: 0.1104\n",
      "Epoch 30/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.3210\n",
      "Epoch 00030: val_loss improved from 0.11043 to 0.10924, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3220 - val_loss: 0.1092\n",
      "Epoch 31/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.3198\n",
      "Epoch 00031: val_loss improved from 0.10924 to 0.10871, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3182 - val_loss: 0.1087\n",
      "Epoch 32/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.3176\n",
      "Epoch 00032: val_loss improved from 0.10871 to 0.10763, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3148 - val_loss: 0.1076\n",
      "Epoch 33/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.3120\n",
      "Epoch 00033: val_loss did not improve from 0.10763\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3110 - val_loss: 0.1076\n",
      "Epoch 34/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.3104\n",
      "Epoch 00034: val_loss improved from 0.10763 to 0.10674, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3085 - val_loss: 0.1067\n",
      "Epoch 35/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.3080\n",
      "Epoch 00035: val_loss did not improve from 0.10674\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.3084 - val_loss: 0.1075\n",
      "Epoch 36/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.3054\n",
      "Epoch 00036: val_loss improved from 0.10674 to 0.10599, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3053 - val_loss: 0.1060\n",
      "Epoch 37/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.2956\n",
      "Epoch 00037: val_loss improved from 0.10599 to 0.10543, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2972 - val_loss: 0.1054\n",
      "Epoch 38/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.2994\n",
      "Epoch 00038: val_loss improved from 0.10543 to 0.10434, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3002 - val_loss: 0.1043\n",
      "Epoch 39/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2911\n",
      "Epoch 00039: val_loss improved from 0.10434 to 0.10423, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2933 - val_loss: 0.1042\n",
      "Epoch 40/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.2909\n",
      "Epoch 00040: val_loss improved from 0.10423 to 0.10293, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2933 - val_loss: 0.1029\n",
      "Epoch 41/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.2931\n",
      "Epoch 00041: val_loss did not improve from 0.10293\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.2909 - val_loss: 0.1032\n",
      "Epoch 42/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2932\n",
      "Epoch 00042: val_loss did not improve from 0.10293\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.2932 - val_loss: 0.1031\n",
      " ###4 fold : val acc1 0.589, acc3 0.971, mae 0.220###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/70 [=========================>....] - ETA: 0s - loss: 19.9958\n",
      "Epoch 00001: val_loss improved from inf to 14.68806, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 19.4542 - val_loss: 14.6881\n",
      "Epoch 2/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 11.3242\n",
      "Epoch 00002: val_loss improved from 14.68806 to 7.72973, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 11.2427 - val_loss: 7.7297\n",
      "Epoch 3/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 6.0535\n",
      "Epoch 00003: val_loss improved from 7.72973 to 4.00091, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 5.8592 - val_loss: 4.0009\n",
      "Epoch 4/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 3.5440\n",
      "Epoch 00004: val_loss improved from 4.00091 to 2.46145, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.4217 - val_loss: 2.4615\n",
      "Epoch 5/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 2.3757\n",
      "Epoch 00005: val_loss improved from 2.46145 to 1.56382, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.3044 - val_loss: 1.5638\n",
      "Epoch 6/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 1.6281\n",
      "Epoch 00006: val_loss improved from 1.56382 to 0.97042, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.5637 - val_loss: 0.9704\n",
      "Epoch 7/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 1.1353\n",
      "Epoch 00007: val_loss improved from 0.97042 to 0.60197, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0875 - val_loss: 0.6020\n",
      "Epoch 8/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.8181\n",
      "Epoch 00008: val_loss improved from 0.60197 to 0.38333, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7902 - val_loss: 0.3833\n",
      "Epoch 9/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6413\n",
      "Epoch 00009: val_loss improved from 0.38333 to 0.25882, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6262 - val_loss: 0.2588\n",
      "Epoch 10/100\n",
      " 1/70 [..............................] - ETA: 0s - loss: 0.6312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/140 [=========================>....] - ETA: 0s - loss: 0.1074\n",
      "Epoch 00010: val_loss improved from 0.10745 to 0.10568, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1072 - val_loss: 0.1057\n",
      "Epoch 11/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1061\n",
      "Epoch 00011: val_loss improved from 0.10568 to 0.10395, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1055 - val_loss: 0.1039\n",
      "Epoch 12/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.1040\n",
      "Epoch 00012: val_loss improved from 0.10395 to 0.10256, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1041 - val_loss: 0.1026\n",
      "Epoch 13/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1026\n",
      "Epoch 00013: val_loss improved from 0.10256 to 0.10157, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1029 - val_loss: 0.1016\n",
      "Epoch 14/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.1018\n",
      "Epoch 00014: val_loss improved from 0.10157 to 0.10022, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1019 - val_loss: 0.1002\n",
      "Epoch 15/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 00015: val_loss improved from 0.10022 to 0.09934, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1009 - val_loss: 0.0993\n",
      "Epoch 16/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1002\n",
      "Epoch 00016: val_loss improved from 0.09934 to 0.09844, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1000 - val_loss: 0.0984\n",
      "Epoch 17/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00017: val_loss improved from 0.09844 to 0.09766, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0992 - val_loss: 0.0977\n",
      "Epoch 18/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.0992\n",
      "Epoch 00018: val_loss improved from 0.09766 to 0.09715, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0987 - val_loss: 0.0972\n",
      "Epoch 19/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00019: val_loss improved from 0.09715 to 0.09659, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0983 - val_loss: 0.0966\n",
      "Epoch 20/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.0972\n",
      "Epoch 00020: val_loss did not improve from 0.09659\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0977 - val_loss: 0.0966\n",
      "Epoch 21/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00021: val_loss improved from 0.09659 to 0.09556, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0973 - val_loss: 0.0956\n",
      "Epoch 22/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.0977\n",
      "Epoch 00022: val_loss improved from 0.09556 to 0.09548, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0972 - val_loss: 0.0955\n",
      "Epoch 23/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.0964\n",
      "Epoch 00023: val_loss improved from 0.09548 to 0.09531, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0965 - val_loss: 0.0953\n",
      "Epoch 24/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00024: val_loss improved from 0.09531 to 0.09505, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 0.0950\n",
      "Epoch 25/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00025: val_loss did not improve from 0.09505\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0961 - val_loss: 0.0958\n",
      "Epoch 26/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.0962\n",
      "Epoch 00026: val_loss improved from 0.09505 to 0.09460, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0961 - val_loss: 0.0946\n",
      "Epoch 27/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00027: val_loss did not improve from 0.09460\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0960 - val_loss: 0.0955\n",
      "Epoch 28/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.0957\n",
      "Epoch 00028: val_loss did not improve from 0.09460\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0957 - val_loss: 0.0957\n",
      " ###5 fold : val acc1 0.590, acc3 0.982, mae 0.214###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/140 [============================>.] - ETA: 0s - loss: 12.4454\n",
      "Epoch 00001: val_loss improved from inf to 3.80905, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 12.3040 - val_loss: 3.8091\n",
      "Epoch 2/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 2.1017\n",
      "Epoch 00002: val_loss improved from 3.80905 to 0.98244, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 2.0842 - val_loss: 0.9824\n",
      "Epoch 3/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.5740\n",
      "Epoch 00003: val_loss improved from 0.98244 to 0.25666, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5449 - val_loss: 0.2567\n",
      "Epoch 4/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1841\n",
      "Epoch 00004: val_loss improved from 0.25666 to 0.13201, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1796 - val_loss: 0.1320\n",
      "Epoch 5/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00005: val_loss improved from 0.13201 to 0.11858, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1255 - val_loss: 0.1186\n",
      "Epoch 6/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1184\n",
      "Epoch 00006: val_loss improved from 0.11858 to 0.11415, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1183 - val_loss: 0.1141\n",
      "Epoch 7/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.1160\n",
      "Epoch 00007: val_loss improved from 0.11415 to 0.11183, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1148 - val_loss: 0.1118\n",
      "Epoch 8/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1125\n",
      "Epoch 00008: val_loss improved from 0.11183 to 0.10919, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1121 - val_loss: 0.1092\n",
      "Epoch 9/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1093\n",
      "Epoch 00009: val_loss improved from 0.10919 to 0.10697, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1098 - val_loss: 0.1070\n",
      "Epoch 10/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.1077\n",
      "Epoch 00010: val_loss improved from 0.10697 to 0.10499, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1078 - val_loss: 0.1050\n",
      "Epoch 11/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.1065\n",
      "Epoch 00011: val_loss improved from 0.10499 to 0.10339, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1061 - val_loss: 0.1034\n",
      "Epoch 12/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.1047\n",
      "Epoch 00012: val_loss improved from 0.10339 to 0.10228, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1045 - val_loss: 0.1023\n",
      "Epoch 13/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1028\n",
      "Epoch 00013: val_loss improved from 0.10228 to 0.10113, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1033 - val_loss: 0.1011\n",
      "Epoch 14/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1019\n",
      "Epoch 00014: val_loss improved from 0.10113 to 0.09992, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1021 - val_loss: 0.0999\n",
      "Epoch 15/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1011\n",
      "Epoch 00015: val_loss improved from 0.09992 to 0.09878, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 0.0988\n",
      "Epoch 16/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1003\n",
      "Epoch 00016: val_loss improved from 0.09878 to 0.09805, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1004 - val_loss: 0.0981\n",
      "Epoch 17/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00017: val_loss improved from 0.09805 to 0.09736, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0995 - val_loss: 0.0974\n",
      "Epoch 18/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00018: val_loss improved from 0.09736 to 0.09684, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0990 - val_loss: 0.0968\n",
      "Epoch 19/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00019: val_loss did not improve from 0.09684\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0987 - val_loss: 0.0975\n",
      "Epoch 20/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.0978\n",
      "Epoch 00020: val_loss improved from 0.09684 to 0.09618, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0980 - val_loss: 0.0962\n",
      "Epoch 21/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 00021: val_loss improved from 0.09618 to 0.09540, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0977 - val_loss: 0.0954\n",
      "Epoch 22/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.0983\n",
      "Epoch 00022: val_loss did not improve from 0.09540\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0974 - val_loss: 0.0954\n",
      "Epoch 23/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00023: val_loss improved from 0.09540 to 0.09509, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0969 - val_loss: 0.0951\n",
      "Epoch 24/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.0964\n",
      "Epoch 00024: val_loss improved from 0.09509 to 0.09477, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0966 - val_loss: 0.0948\n",
      "Epoch 25/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00025: val_loss did not improve from 0.09477\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0965 - val_loss: 0.0956\n",
      "Epoch 26/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00026: val_loss did not improve from 0.09477\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0965 - val_loss: 0.0950\n",
      " ###6 fold : val acc1 0.604, acc3 0.984, mae 0.206###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/140 [============================>.] - ETA: 0s - loss: 12.3750\n",
      "Epoch 00001: val_loss improved from inf to 3.81036, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 12.2977 - val_loss: 3.8104\n",
      "Epoch 2/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 2.1873\n",
      "Epoch 00002: val_loss improved from 3.81036 to 0.98345, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 2.0804 - val_loss: 0.9835\n",
      "Epoch 3/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.5480\n",
      "Epoch 00003: val_loss improved from 0.98345 to 0.25532, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5428 - val_loss: 0.2553\n",
      "Epoch 4/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1779\n",
      "Epoch 00004: val_loss improved from 0.25532 to 0.13198, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1763 - val_loss: 0.1320\n",
      "Epoch 5/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1232\n",
      "Epoch 00005: val_loss improved from 0.13198 to 0.11882, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 0.1188\n",
      "Epoch 6/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.1169\n",
      "Epoch 00006: val_loss improved from 0.11882 to 0.11485, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1165 - val_loss: 0.1149\n",
      "Epoch 7/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1136\n",
      "Epoch 00007: val_loss improved from 0.11485 to 0.11202, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1132 - val_loss: 0.1120\n",
      "Epoch 8/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1109\n",
      "Epoch 00008: val_loss improved from 0.11202 to 0.10953, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1106 - val_loss: 0.1095\n",
      "Epoch 9/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1083\n",
      "Epoch 00009: val_loss improved from 0.10953 to 0.10751, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 0.1075\n",
      "Epoch 10/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1066\n",
      "Epoch 00010: val_loss improved from 0.10751 to 0.10539, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1063 - val_loss: 0.1054\n",
      "Epoch 11/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1045\n",
      "Epoch 00011: val_loss improved from 0.10539 to 0.10387, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1046 - val_loss: 0.1039\n",
      "Epoch 12/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1035\n",
      "Epoch 00012: val_loss improved from 0.10387 to 0.10241, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1032 - val_loss: 0.1024\n",
      "Epoch 13/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1021\n",
      "Epoch 00013: val_loss improved from 0.10241 to 0.10179, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 14/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1010\n",
      "Epoch 00014: val_loss improved from 0.10179 to 0.10032, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1008 - val_loss: 0.1003\n",
      "Epoch 15/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.1003\n",
      "Epoch 00015: val_loss improved from 0.10032 to 0.09923, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1001 - val_loss: 0.0992\n",
      "Epoch 16/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.0990\n",
      "Epoch 00016: val_loss improved from 0.09923 to 0.09836, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0991 - val_loss: 0.0984\n",
      "Epoch 17/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.0983\n",
      "Epoch 00017: val_loss improved from 0.09836 to 0.09785, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 18/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00018: val_loss improved from 0.09785 to 0.09717, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0979 - val_loss: 0.0972\n",
      "Epoch 19/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00019: val_loss did not improve from 0.09717\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0975 - val_loss: 0.0978\n",
      "Epoch 20/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.0971\n",
      "Epoch 00020: val_loss improved from 0.09717 to 0.09618, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0969 - val_loss: 0.0962\n",
      "Epoch 21/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.0966\n",
      "Epoch 00021: val_loss improved from 0.09618 to 0.09609, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0966 - val_loss: 0.0961\n",
      "Epoch 22/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.0962\n",
      "Epoch 00022: val_loss improved from 0.09609 to 0.09561, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0963 - val_loss: 0.0956\n",
      "Epoch 23/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00023: val_loss improved from 0.09561 to 0.09536, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0959 - val_loss: 0.0954\n",
      "Epoch 24/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.0956\n",
      "Epoch 00024: val_loss improved from 0.09536 to 0.09488, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0956 - val_loss: 0.0949\n",
      "Epoch 25/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.0957\n",
      "Epoch 00025: val_loss did not improve from 0.09488\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0954 - val_loss: 0.0958\n",
      "Epoch 26/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0955\n",
      "Epoch 00026: val_loss did not improve from 0.09488\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0955 - val_loss: 0.0954\n",
      " ###7 fold : val acc1 0.590, acc3 0.973, mae 0.219###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/140 [==========================>...] - ETA: 0s - loss: 13.0937\n",
      "Epoch 00001: val_loss improved from inf to 3.79089, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 12.3257 - val_loss: 3.7909\n",
      "Epoch 2/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 2.1955\n",
      "Epoch 00002: val_loss improved from 3.79089 to 0.98759, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 2.0809 - val_loss: 0.9876\n",
      "Epoch 3/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.5785\n",
      "Epoch 00003: val_loss improved from 0.98759 to 0.26455, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5452 - val_loss: 0.2646\n",
      "Epoch 4/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.1839\n",
      "Epoch 00004: val_loss improved from 0.26455 to 0.13749, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1776 - val_loss: 0.1375\n",
      "Epoch 5/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1234\n",
      "Epoch 00005: val_loss improved from 0.13749 to 0.12296, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 0.1230\n",
      "Epoch 6/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1165\n",
      "Epoch 00006: val_loss improved from 0.12296 to 0.11880, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1164 - val_loss: 0.1188\n",
      "Epoch 7/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1142\n",
      "Epoch 00007: val_loss improved from 0.11880 to 0.11579, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1132 - val_loss: 0.1158\n",
      "Epoch 8/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1105\n",
      "Epoch 00008: val_loss improved from 0.11579 to 0.11318, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1106 - val_loss: 0.1132\n",
      "Epoch 9/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00009: val_loss improved from 0.11318 to 0.11120, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 0.1112\n",
      "Epoch 10/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1068\n",
      "Epoch 00010: val_loss improved from 0.11120 to 0.10905, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1064 - val_loss: 0.1090\n",
      "Epoch 11/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.1045\n",
      "Epoch 00011: val_loss improved from 0.10905 to 0.10737, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1047 - val_loss: 0.1074\n",
      "Epoch 12/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1035\n",
      "Epoch 00012: val_loss improved from 0.10737 to 0.10592, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1033 - val_loss: 0.1059\n",
      "Epoch 13/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.1024\n",
      "Epoch 00013: val_loss improved from 0.10592 to 0.10505, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1020 - val_loss: 0.1050\n",
      "Epoch 14/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1009\n",
      "Epoch 00014: val_loss improved from 0.10505 to 0.10362, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1008 - val_loss: 0.1036\n",
      "Epoch 15/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00015: val_loss improved from 0.10362 to 0.10241, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1001 - val_loss: 0.1024\n",
      "Epoch 16/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.0989\n",
      "Epoch 00016: val_loss improved from 0.10241 to 0.10158, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0992 - val_loss: 0.1016\n",
      "Epoch 17/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00017: val_loss improved from 0.10158 to 0.10105, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0984 - val_loss: 0.1010\n",
      "Epoch 18/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.0983\n",
      "Epoch 00018: val_loss improved from 0.10105 to 0.10027, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0979 - val_loss: 0.1003\n",
      "Epoch 19/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00019: val_loss did not improve from 0.10027\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0975 - val_loss: 0.1010\n",
      "Epoch 20/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00020: val_loss improved from 0.10027 to 0.09916, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0969 - val_loss: 0.0992\n",
      "Epoch 21/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00021: val_loss improved from 0.09916 to 0.09889, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0966 - val_loss: 0.0989\n",
      "Epoch 22/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00022: val_loss improved from 0.09889 to 0.09859, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0963 - val_loss: 0.0986\n",
      "Epoch 23/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.0960\n",
      "Epoch 00023: val_loss improved from 0.09859 to 0.09852, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0960 - val_loss: 0.0985\n",
      "Epoch 24/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.0953\n",
      "Epoch 00024: val_loss improved from 0.09852 to 0.09767, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0956 - val_loss: 0.0977\n",
      "Epoch 25/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.0962\n",
      "Epoch 00025: val_loss did not improve from 0.09767\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0954 - val_loss: 0.0983\n",
      "Epoch 26/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.0953\n",
      "Epoch 00026: val_loss did not improve from 0.09767\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0954 - val_loss: 0.0978\n",
      " ###8 fold : val acc1 0.591, acc3 0.983, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/140 [=========================>....] - ETA: 0s - loss: 13.3686\n",
      "Epoch 00001: val_loss improved from inf to 3.79770, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 12.3257 - val_loss: 3.7977\n",
      "Epoch 2/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 2.1869\n",
      "Epoch 00002: val_loss improved from 3.79770 to 0.99221, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 2.0809 - val_loss: 0.9922\n",
      "Epoch 3/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.5452\n",
      "Epoch 00003: val_loss improved from 0.99221 to 0.26793, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5452 - val_loss: 0.2679\n",
      "Epoch 4/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1813\n",
      "Epoch 00004: val_loss improved from 0.26793 to 0.14020, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1776 - val_loss: 0.1402\n",
      "Epoch 5/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1235\n",
      "Epoch 00005: val_loss improved from 0.14020 to 0.12534, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 0.1253\n",
      "Epoch 6/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1160\n",
      "Epoch 00006: val_loss improved from 0.12534 to 0.12126, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1164 - val_loss: 0.1213\n",
      "Epoch 7/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1137\n",
      "Epoch 00007: val_loss improved from 0.12126 to 0.11836, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1132 - val_loss: 0.1184\n",
      "Epoch 8/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1109\n",
      "Epoch 00008: val_loss improved from 0.11836 to 0.11574, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1106 - val_loss: 0.1157\n",
      "Epoch 9/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00009: val_loss improved from 0.11574 to 0.11370, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 0.1137\n",
      "Epoch 10/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1068\n",
      "Epoch 00010: val_loss improved from 0.11370 to 0.11159, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1064 - val_loss: 0.1116\n",
      "Epoch 11/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1049\n",
      "Epoch 00011: val_loss improved from 0.11159 to 0.10997, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1047 - val_loss: 0.1100\n",
      "Epoch 12/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1037\n",
      "Epoch 00012: val_loss improved from 0.10997 to 0.10865, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1033 - val_loss: 0.1087\n",
      "Epoch 13/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1025\n",
      "Epoch 00013: val_loss improved from 0.10865 to 0.10741, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1020 - val_loss: 0.1074\n",
      "Epoch 14/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1009\n",
      "Epoch 00014: val_loss improved from 0.10741 to 0.10638, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1008 - val_loss: 0.1064\n",
      "Epoch 15/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1004\n",
      "Epoch 00015: val_loss improved from 0.10638 to 0.10528, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1001 - val_loss: 0.1053\n",
      "Epoch 16/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.0990\n",
      "Epoch 00016: val_loss improved from 0.10528 to 0.10423, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.0992 - val_loss: 0.1042\n",
      "Epoch 17/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.0983\n",
      "Epoch 00017: val_loss improved from 0.10423 to 0.10337, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.0984 - val_loss: 0.1034\n",
      "Epoch 18/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00018: val_loss improved from 0.10337 to 0.10264, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.0979 - val_loss: 0.1026\n",
      "Epoch 19/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.0972\n",
      "Epoch 00019: val_loss did not improve from 0.10264\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0975 - val_loss: 0.1029\n",
      "Epoch 20/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00020: val_loss improved from 0.10264 to 0.10172, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.0969 - val_loss: 0.1017\n",
      "Epoch 21/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00021: val_loss improved from 0.10172 to 0.10147, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0966 - val_loss: 0.1015\n",
      "Epoch 22/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.0966\n",
      "Epoch 00022: val_loss improved from 0.10147 to 0.10091, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.0963 - val_loss: 0.1009\n",
      "Epoch 23/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00023: val_loss improved from 0.10091 to 0.10082, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.0960 - val_loss: 0.1008\n",
      "Epoch 24/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.0953\n",
      "Epoch 00024: val_loss improved from 0.10082 to 0.10008, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.0956 - val_loss: 0.1001\n",
      "Epoch 25/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.0957\n",
      "Epoch 00025: val_loss did not improve from 0.10008\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0954 - val_loss: 0.1012\n",
      "Epoch 26/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.0956\n",
      "Epoch 00026: val_loss did not improve from 0.10008\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0954 - val_loss: 0.1002\n",
      " ###9 fold : val acc1 0.602, acc3 0.983, mae 0.208###\n",
      "acc10.599_acc30.981\n",
      "random search 6/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/140 [============================>.] - ETA: 0s - loss: 14.2787\n",
      "Epoch 00001: val_loss improved from inf to 6.82232, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 14.2642 - val_loss: 6.8223\n",
      "Epoch 2/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 4.0548\n",
      "Epoch 00002: val_loss improved from 6.82232 to 2.19396, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 4.0548 - val_loss: 2.1940\n",
      "Epoch 3/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 1.6658\n",
      "Epoch 00003: val_loss improved from 2.19396 to 0.83800, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.6658 - val_loss: 0.8380\n",
      "Epoch 4/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.8151\n",
      "Epoch 00004: val_loss improved from 0.83800 to 0.32102, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.7857 - val_loss: 0.3210\n",
      "Epoch 5/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.4733\n",
      "Epoch 00005: val_loss improved from 0.32102 to 0.17425, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4723 - val_loss: 0.1742\n",
      "Epoch 6/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3997\n",
      "Epoch 00006: val_loss improved from 0.17425 to 0.13881, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3981 - val_loss: 0.1388\n",
      "Epoch 7/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3690\n",
      "Epoch 00007: val_loss improved from 0.13881 to 0.12781, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3664 - val_loss: 0.1278\n",
      "Epoch 8/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.3551\n",
      "Epoch 00008: val_loss improved from 0.12781 to 0.12325, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3537 - val_loss: 0.1233\n",
      "Epoch 9/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.3481\n",
      "Epoch 00009: val_loss improved from 0.12325 to 0.11962, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3484 - val_loss: 0.1196\n",
      "Epoch 10/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.3329\n",
      "Epoch 00010: val_loss improved from 0.11962 to 0.11675, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3335 - val_loss: 0.1168\n",
      "Epoch 11/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.3266\n",
      "Epoch 00011: val_loss improved from 0.11675 to 0.11465, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3270 - val_loss: 0.1147\n",
      "Epoch 12/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.3185\n",
      "Epoch 00012: val_loss improved from 0.11465 to 0.11330, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3186 - val_loss: 0.1133\n",
      "Epoch 13/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.3068\n",
      "Epoch 00013: val_loss improved from 0.11330 to 0.11137, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3089 - val_loss: 0.1114\n",
      "Epoch 14/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3021\n",
      "Epoch 00014: val_loss improved from 0.11137 to 0.11088, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3028 - val_loss: 0.1109\n",
      "Epoch 15/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.2986\n",
      "Epoch 00015: val_loss improved from 0.11088 to 0.10844, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2987 - val_loss: 0.1084\n",
      "Epoch 16/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2900\n",
      "Epoch 00016: val_loss improved from 0.10844 to 0.10729, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2901 - val_loss: 0.1073\n",
      "Epoch 17/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2863\n",
      "Epoch 00017: val_loss improved from 0.10729 to 0.10565, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2863 - val_loss: 0.1057\n",
      "Epoch 18/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2772\n",
      "Epoch 00018: val_loss improved from 0.10565 to 0.10495, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2774 - val_loss: 0.1049\n",
      "Epoch 19/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2752\n",
      "Epoch 00019: val_loss improved from 0.10495 to 0.10411, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2750 - val_loss: 0.1041\n",
      "Epoch 20/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2708\n",
      "Epoch 00020: val_loss improved from 0.10411 to 0.10324, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2711 - val_loss: 0.1032\n",
      "Epoch 21/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2683\n",
      "Epoch 00021: val_loss improved from 0.10324 to 0.10139, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2683 - val_loss: 0.1014\n",
      "Epoch 22/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2643\n",
      "Epoch 00022: val_loss did not improve from 0.10139\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2636 - val_loss: 0.1028\n",
      "Epoch 23/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2605\n",
      "Epoch 00023: val_loss did not improve from 0.10139\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2606 - val_loss: 0.1018\n",
      " ###0 fold : val acc1 0.600, acc3 0.977, mae 0.212###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/140 [==========================>...] - ETA: 0s - loss: 14.8663\n",
      "Epoch 00001: val_loss improved from inf to 6.82258, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 14.2568 - val_loss: 6.8226\n",
      "Epoch 2/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 4.1342\n",
      "Epoch 00002: val_loss improved from 6.82258 to 2.19358, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 4.0555 - val_loss: 2.1936\n",
      "Epoch 3/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 1.7381\n",
      "Epoch 00003: val_loss improved from 2.19358 to 0.83428, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.6680 - val_loss: 0.8343\n",
      "Epoch 4/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.7830\n",
      "Epoch 00004: val_loss improved from 0.83428 to 0.32030, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.7804 - val_loss: 0.3203\n",
      "Epoch 5/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.4715\n",
      "Epoch 00005: val_loss improved from 0.32030 to 0.17466, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4711 - val_loss: 0.1747\n",
      "Epoch 6/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.3987\n",
      "Epoch 00006: val_loss improved from 0.17466 to 0.13852, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3972 - val_loss: 0.1385\n",
      "Epoch 7/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.3690\n",
      "Epoch 00007: val_loss improved from 0.13852 to 0.12788, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3687 - val_loss: 0.1279\n",
      "Epoch 8/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.3559\n",
      "Epoch 00008: val_loss improved from 0.12788 to 0.12305, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3535 - val_loss: 0.1230\n",
      "Epoch 9/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3477\n",
      "Epoch 00009: val_loss improved from 0.12305 to 0.11980, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3474 - val_loss: 0.1198\n",
      "Epoch 10/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3326\n",
      "Epoch 00010: val_loss improved from 0.11980 to 0.11681, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3326 - val_loss: 0.1168\n",
      "Epoch 11/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3255\n",
      "Epoch 00011: val_loss improved from 0.11681 to 0.11403, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3256 - val_loss: 0.1140\n",
      "Epoch 12/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3194\n",
      "Epoch 00012: val_loss improved from 0.11403 to 0.11335, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3192 - val_loss: 0.1133\n",
      "Epoch 13/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3110\n",
      "Epoch 00013: val_loss improved from 0.11335 to 0.11159, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3111 - val_loss: 0.1116\n",
      "Epoch 14/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.3024\n",
      "Epoch 00014: val_loss improved from 0.11159 to 0.11030, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3017 - val_loss: 0.1103\n",
      "Epoch 15/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2971\n",
      "Epoch 00015: val_loss improved from 0.11030 to 0.10834, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2973 - val_loss: 0.1083\n",
      "Epoch 16/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.2894\n",
      "Epoch 00016: val_loss improved from 0.10834 to 0.10748, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2894 - val_loss: 0.1075\n",
      "Epoch 17/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2840\n",
      "Epoch 00017: val_loss improved from 0.10748 to 0.10569, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2835 - val_loss: 0.1057\n",
      "Epoch 18/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.2794\n",
      "Epoch 00018: val_loss improved from 0.10569 to 0.10466, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2796 - val_loss: 0.1047\n",
      "Epoch 19/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2764\n",
      "Epoch 00019: val_loss improved from 0.10466 to 0.10402, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2760 - val_loss: 0.1040\n",
      "Epoch 20/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.2737\n",
      "Epoch 00020: val_loss improved from 0.10402 to 0.10281, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2738 - val_loss: 0.1028\n",
      "Epoch 21/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2685\n",
      "Epoch 00021: val_loss improved from 0.10281 to 0.10137, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2684 - val_loss: 0.1014\n",
      "Epoch 22/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.2669\n",
      "Epoch 00022: val_loss did not improve from 0.10137\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2659 - val_loss: 0.1028\n",
      "Epoch 23/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.2620\n",
      "Epoch 00023: val_loss improved from 0.10137 to 0.10111, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2606 - val_loss: 0.1011\n",
      "Epoch 24/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2565\n",
      "Epoch 00024: val_loss improved from 0.10111 to 0.10026, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2555 - val_loss: 0.1003\n",
      "Epoch 25/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2475\n",
      "Epoch 00025: val_loss improved from 0.10026 to 0.10023, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2476 - val_loss: 0.1002\n",
      "Epoch 26/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2519\n",
      "Epoch 00026: val_loss improved from 0.10023 to 0.09948, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2515 - val_loss: 0.0995\n",
      "Epoch 27/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2482\n",
      "Epoch 00027: val_loss improved from 0.09948 to 0.09819, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2480 - val_loss: 0.0982\n",
      "Epoch 28/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2487\n",
      "Epoch 00028: val_loss did not improve from 0.09819\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2487 - val_loss: 0.0998\n",
      "Epoch 29/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2444\n",
      "Epoch 00029: val_loss did not improve from 0.09819\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2444 - val_loss: 0.0984\n",
      " ###1 fold : val acc1 0.592, acc3 0.981, mae 0.214###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/140 [==========================>...] - ETA: 0s - loss: 14.9334\n",
      "Epoch 00001: val_loss improved from inf to 6.82199, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 14.2359 - val_loss: 6.8220\n",
      "Epoch 2/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 4.0912\n",
      "Epoch 00002: val_loss improved from 6.82199 to 2.18185, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 4.0328 - val_loss: 2.1818\n",
      "Epoch 3/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 1.6985\n",
      "Epoch 00003: val_loss improved from 2.18185 to 0.83222, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.6615 - val_loss: 0.8322\n",
      "Epoch 4/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.7979\n",
      "Epoch 00004: val_loss improved from 0.83222 to 0.31599, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7796 - val_loss: 0.3160\n",
      "Epoch 5/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.4779\n",
      "Epoch 00005: val_loss improved from 0.31599 to 0.17345, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4728 - val_loss: 0.1734\n",
      "Epoch 6/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.3949\n",
      "Epoch 00006: val_loss improved from 0.17345 to 0.13771, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3936 - val_loss: 0.1377\n",
      "Epoch 7/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3714\n",
      "Epoch 00007: val_loss improved from 0.13771 to 0.12724, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3687 - val_loss: 0.1272\n",
      "Epoch 8/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.3517\n",
      "Epoch 00008: val_loss improved from 0.12724 to 0.12267, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3508 - val_loss: 0.1227\n",
      "Epoch 9/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3480\n",
      "Epoch 00009: val_loss improved from 0.12267 to 0.11980, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3479 - val_loss: 0.1198\n",
      "Epoch 10/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.3313\n",
      "Epoch 00010: val_loss improved from 0.11980 to 0.11718, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3318 - val_loss: 0.1172\n",
      "Epoch 11/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.3241\n",
      "Epoch 00011: val_loss improved from 0.11718 to 0.11421, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3240 - val_loss: 0.1142\n",
      "Epoch 12/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3176\n",
      "Epoch 00012: val_loss improved from 0.11421 to 0.11348, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3169 - val_loss: 0.1135\n",
      "Epoch 13/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.3102\n",
      "Epoch 00013: val_loss improved from 0.11348 to 0.11219, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3111 - val_loss: 0.1122\n",
      "Epoch 14/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.3010\n",
      "Epoch 00014: val_loss improved from 0.11219 to 0.11042, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3004 - val_loss: 0.1104\n",
      "Epoch 15/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.2973\n",
      "Epoch 00015: val_loss improved from 0.11042 to 0.10897, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2973 - val_loss: 0.1090\n",
      "Epoch 16/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2890\n",
      "Epoch 00016: val_loss improved from 0.10897 to 0.10726, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2891 - val_loss: 0.1073\n",
      "Epoch 17/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.2826\n",
      "Epoch 00017: val_loss improved from 0.10726 to 0.10636, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2825 - val_loss: 0.1064\n",
      "Epoch 18/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.2799\n",
      "Epoch 00018: val_loss improved from 0.10636 to 0.10495, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2801 - val_loss: 0.1050\n",
      "Epoch 19/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2767\n",
      "Epoch 00019: val_loss improved from 0.10495 to 0.10481, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2762 - val_loss: 0.1048\n",
      "Epoch 20/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2727\n",
      "Epoch 00020: val_loss improved from 0.10481 to 0.10262, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2734 - val_loss: 0.1026\n",
      "Epoch 21/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2686\n",
      "Epoch 00021: val_loss improved from 0.10262 to 0.10142, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2688 - val_loss: 0.1014\n",
      "Epoch 22/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2698\n",
      "Epoch 00022: val_loss did not improve from 0.10142\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2681 - val_loss: 0.1031\n",
      "Epoch 23/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2605\n",
      "Epoch 00023: val_loss improved from 0.10142 to 0.10086, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2611 - val_loss: 0.1009\n",
      "Epoch 24/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2569\n",
      "Epoch 00024: val_loss did not improve from 0.10086\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2566 - val_loss: 0.1009\n",
      "Epoch 25/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.2512\n",
      "Epoch 00025: val_loss improved from 0.10086 to 0.10042, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2509 - val_loss: 0.1004\n",
      "Epoch 26/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2528\n",
      "Epoch 00026: val_loss improved from 0.10042 to 0.09958, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2518 - val_loss: 0.0996\n",
      "Epoch 27/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2469\n",
      "Epoch 00027: val_loss improved from 0.09958 to 0.09840, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2464 - val_loss: 0.0984\n",
      "Epoch 28/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2461\n",
      "Epoch 00028: val_loss did not improve from 0.09840\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2459 - val_loss: 0.0998\n",
      "Epoch 29/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2469\n",
      "Epoch 00029: val_loss improved from 0.09840 to 0.09835, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2466 - val_loss: 0.0983\n",
      "Epoch 30/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2444\n",
      "Epoch 00030: val_loss improved from 0.09835 to 0.09800, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2442 - val_loss: 0.0980\n",
      "Epoch 31/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2405\n",
      "Epoch 00031: val_loss did not improve from 0.09800\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2406 - val_loss: 0.0983\n",
      "Epoch 32/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2351\n",
      "Epoch 00032: val_loss did not improve from 0.09800\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2355 - val_loss: 0.0981\n",
      " ###2 fold : val acc1 0.599, acc3 0.979, mae 0.212###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/140 [==========================>...] - ETA: 0s - loss: 14.9628\n",
      "Epoch 00001: val_loss improved from inf to 6.78477, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 14.2604 - val_loss: 6.7848\n",
      "Epoch 2/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 4.1978\n",
      "Epoch 00002: val_loss improved from 6.78477 to 2.17138, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 4.0337 - val_loss: 2.1714\n",
      "Epoch 3/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 1.7035\n",
      "Epoch 00003: val_loss improved from 2.17138 to 0.83523, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 1.6619 - val_loss: 0.8352\n",
      "Epoch 4/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.8068\n",
      "Epoch 00004: val_loss improved from 0.83523 to 0.31538, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7852 - val_loss: 0.3154\n",
      "Epoch 5/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.4785\n",
      "Epoch 00005: val_loss improved from 0.31538 to 0.17210, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.4744 - val_loss: 0.1721\n",
      "Epoch 6/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.3977\n",
      "Epoch 00006: val_loss improved from 0.17210 to 0.13742, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3942 - val_loss: 0.1374\n",
      "Epoch 7/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.3694\n",
      "Epoch 00007: val_loss improved from 0.13742 to 0.12757, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3674 - val_loss: 0.1276\n",
      "Epoch 8/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3535\n",
      "Epoch 00008: val_loss improved from 0.12757 to 0.12288, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3516 - val_loss: 0.1229\n",
      "Epoch 9/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3500\n",
      "Epoch 00009: val_loss improved from 0.12288 to 0.11962, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3498 - val_loss: 0.1196\n",
      "Epoch 10/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.3310\n",
      "Epoch 00010: val_loss improved from 0.11962 to 0.11774, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3311 - val_loss: 0.1177\n",
      "Epoch 11/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.3248\n",
      "Epoch 00011: val_loss improved from 0.11774 to 0.11453, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3246 - val_loss: 0.1145\n",
      "Epoch 12/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.3180\n",
      "Epoch 00012: val_loss improved from 0.11453 to 0.11403, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3168 - val_loss: 0.1140\n",
      "Epoch 13/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.3104\n",
      "Epoch 00013: val_loss improved from 0.11403 to 0.11237, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3107 - val_loss: 0.1124\n",
      "Epoch 14/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.3024\n",
      "Epoch 00014: val_loss improved from 0.11237 to 0.11048, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3023 - val_loss: 0.1105\n",
      "Epoch 15/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.2977\n",
      "Epoch 00015: val_loss improved from 0.11048 to 0.10870, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2981 - val_loss: 0.1087\n",
      "Epoch 16/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2911\n",
      "Epoch 00016: val_loss improved from 0.10870 to 0.10766, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2913 - val_loss: 0.1077\n",
      "Epoch 17/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2826\n",
      "Epoch 00017: val_loss improved from 0.10766 to 0.10623, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2829 - val_loss: 0.1062\n",
      "Epoch 18/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2796\n",
      "Epoch 00018: val_loss improved from 0.10623 to 0.10526, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2801 - val_loss: 0.1053\n",
      "Epoch 19/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2745\n",
      "Epoch 00019: val_loss improved from 0.10526 to 0.10444, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2739 - val_loss: 0.1044\n",
      "Epoch 20/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.2744\n",
      "Epoch 00020: val_loss improved from 0.10444 to 0.10273, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2745 - val_loss: 0.1027\n",
      "Epoch 21/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2683\n",
      "Epoch 00021: val_loss improved from 0.10273 to 0.10143, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2680 - val_loss: 0.1014\n",
      "Epoch 22/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2676\n",
      "Epoch 00022: val_loss did not improve from 0.10143\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2667 - val_loss: 0.1021\n",
      "Epoch 23/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2608\n",
      "Epoch 00023: val_loss improved from 0.10143 to 0.10061, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2608 - val_loss: 0.1006\n",
      "Epoch 24/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2567\n",
      "Epoch 00024: val_loss improved from 0.10061 to 0.10053, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2567 - val_loss: 0.1005\n",
      "Epoch 25/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2542\n",
      "Epoch 00025: val_loss improved from 0.10053 to 0.10032, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2520 - val_loss: 0.1003\n",
      "Epoch 26/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2531\n",
      "Epoch 00026: val_loss improved from 0.10032 to 0.09964, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2516 - val_loss: 0.0996\n",
      "Epoch 27/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2465\n",
      "Epoch 00027: val_loss improved from 0.09964 to 0.09886, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2471 - val_loss: 0.0989\n",
      "Epoch 28/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2463\n",
      "Epoch 00028: val_loss did not improve from 0.09886\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2467 - val_loss: 0.0999\n",
      "Epoch 29/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2488\n",
      "Epoch 00029: val_loss improved from 0.09886 to 0.09841, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2479 - val_loss: 0.0984\n",
      "Epoch 30/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.2451\n",
      "Epoch 00030: val_loss improved from 0.09841 to 0.09835, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2441 - val_loss: 0.0983\n",
      "Epoch 31/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.2429\n",
      "Epoch 00031: val_loss improved from 0.09835 to 0.09822, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2431 - val_loss: 0.0982\n",
      "Epoch 32/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2352\n",
      "Epoch 00032: val_loss improved from 0.09822 to 0.09757, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2352 - val_loss: 0.0976\n",
      "Epoch 33/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2378\n",
      "Epoch 00033: val_loss did not improve from 0.09757\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2380 - val_loss: 0.0998\n",
      "Epoch 34/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2313\n",
      "Epoch 00034: val_loss did not improve from 0.09757\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2308 - val_loss: 0.0983\n",
      " ###3 fold : val acc1 0.612, acc3 0.974, mae 0.208###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/140 [=========================>....] - ETA: 0s - loss: 14.8713\n",
      "Epoch 00001: val_loss improved from inf to 6.75522, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 14.1300 - val_loss: 6.7552\n",
      "Epoch 2/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 4.0447\n",
      "Epoch 00002: val_loss improved from 6.75522 to 2.19120, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 4.0435 - val_loss: 2.1912\n",
      "Epoch 3/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 1.7063\n",
      "Epoch 00003: val_loss improved from 2.19120 to 0.82798, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.6906 - val_loss: 0.8280\n",
      "Epoch 4/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.7952\n",
      "Epoch 00004: val_loss improved from 0.82798 to 0.31148, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.7811 - val_loss: 0.3115\n",
      "Epoch 5/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.4760\n",
      "Epoch 00005: val_loss improved from 0.31148 to 0.17158, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.4735 - val_loss: 0.1716\n",
      "Epoch 6/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.3943\n",
      "Epoch 00006: val_loss improved from 0.17158 to 0.13704, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3941 - val_loss: 0.1370\n",
      "Epoch 7/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.3705\n",
      "Epoch 00007: val_loss improved from 0.13704 to 0.12702, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3695 - val_loss: 0.1270\n",
      "Epoch 8/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3555\n",
      "Epoch 00008: val_loss improved from 0.12702 to 0.12560, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3527 - val_loss: 0.1256\n",
      "Epoch 9/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3458\n",
      "Epoch 00009: val_loss improved from 0.12560 to 0.11924, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3458 - val_loss: 0.1192\n",
      "Epoch 10/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3233\n",
      "Epoch 00010: val_loss improved from 0.11924 to 0.11759, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3234 - val_loss: 0.1176\n",
      "Epoch 11/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3224\n",
      "Epoch 00011: val_loss improved from 0.11759 to 0.11570, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3227 - val_loss: 0.1157\n",
      "Epoch 12/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.3168\n",
      "Epoch 00012: val_loss improved from 0.11570 to 0.11312, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3166 - val_loss: 0.1131\n",
      "Epoch 13/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.3077\n",
      "Epoch 00013: val_loss improved from 0.11312 to 0.11151, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3082 - val_loss: 0.1115\n",
      "Epoch 14/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.3074\n",
      "Epoch 00014: val_loss improved from 0.11151 to 0.11018, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3067 - val_loss: 0.1102\n",
      "Epoch 15/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2964\n",
      "Epoch 00015: val_loss did not improve from 0.11018\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2955 - val_loss: 0.1106\n",
      "Epoch 16/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2927\n",
      "Epoch 00016: val_loss improved from 0.11018 to 0.10959, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2937 - val_loss: 0.1096\n",
      "Epoch 17/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2857\n",
      "Epoch 00017: val_loss improved from 0.10959 to 0.10856, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2858 - val_loss: 0.1086\n",
      "Epoch 18/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.2821\n",
      "Epoch 00018: val_loss improved from 0.10856 to 0.10492, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2816 - val_loss: 0.1049\n",
      "Epoch 19/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.2806\n",
      "Epoch 00019: val_loss improved from 0.10492 to 0.10412, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2816 - val_loss: 0.1041\n",
      "Epoch 20/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2709\n",
      "Epoch 00020: val_loss improved from 0.10412 to 0.10330, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2706 - val_loss: 0.1033\n",
      "Epoch 21/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2677\n",
      "Epoch 00021: val_loss improved from 0.10330 to 0.10219, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2674 - val_loss: 0.1022\n",
      "Epoch 22/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2616\n",
      "Epoch 00022: val_loss improved from 0.10219 to 0.10095, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2606 - val_loss: 0.1009\n",
      "Epoch 23/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2581\n",
      "Epoch 00023: val_loss improved from 0.10095 to 0.10072, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2590 - val_loss: 0.1007\n",
      "Epoch 24/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.2593\n",
      "Epoch 00024: val_loss did not improve from 0.10072\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2582 - val_loss: 0.1010\n",
      "Epoch 25/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2492\n",
      "Epoch 00025: val_loss did not improve from 0.10072\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2491 - val_loss: 0.1019\n",
      " ###4 fold : val acc1 0.599, acc3 0.975, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/140 [===========================>..] - ETA: 0s - loss: 14.5952\n",
      "Epoch 00001: val_loss improved from inf to 6.74868, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 14.1651 - val_loss: 6.7487\n",
      "Epoch 2/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 4.0606\n",
      "Epoch 00002: val_loss improved from 6.74868 to 2.18411, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 4.0465 - val_loss: 2.1841\n",
      "Epoch 3/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 1.6926\n",
      "Epoch 00003: val_loss improved from 2.18411 to 0.82790, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.6770 - val_loss: 0.8279\n",
      "Epoch 4/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.8013\n",
      "Epoch 00004: val_loss improved from 0.82790 to 0.31218, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.7821 - val_loss: 0.3122\n",
      "Epoch 5/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.4765\n",
      "Epoch 00005: val_loss improved from 0.31218 to 0.17168, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4764 - val_loss: 0.1717\n",
      "Epoch 6/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3961\n",
      "Epoch 00006: val_loss improved from 0.17168 to 0.13738, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3961 - val_loss: 0.1374\n",
      "Epoch 7/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.3686\n",
      "Epoch 00007: val_loss improved from 0.13738 to 0.12674, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3676 - val_loss: 0.1267\n",
      "Epoch 8/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3533\n",
      "Epoch 00008: val_loss improved from 0.12674 to 0.12476, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3521 - val_loss: 0.1248\n",
      "Epoch 9/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.3444\n",
      "Epoch 00009: val_loss improved from 0.12476 to 0.11899, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3443 - val_loss: 0.1190\n",
      "Epoch 10/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.3245\n",
      "Epoch 00010: val_loss improved from 0.11899 to 0.11723, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3239 - val_loss: 0.1172\n",
      "Epoch 11/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3233\n",
      "Epoch 00011: val_loss improved from 0.11723 to 0.11631, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3233 - val_loss: 0.1163\n",
      "Epoch 12/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.3172\n",
      "Epoch 00012: val_loss improved from 0.11631 to 0.11322, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3161 - val_loss: 0.1132\n",
      "Epoch 13/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.3088\n",
      "Epoch 00013: val_loss improved from 0.11322 to 0.11134, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3091 - val_loss: 0.1113\n",
      "Epoch 14/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.3087\n",
      "Epoch 00014: val_loss improved from 0.11134 to 0.11079, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3083 - val_loss: 0.1108\n",
      "Epoch 15/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2961\n",
      "Epoch 00015: val_loss improved from 0.11079 to 0.10953, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2955 - val_loss: 0.1095\n",
      "Epoch 16/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.2943\n",
      "Epoch 00016: val_loss did not improve from 0.10953\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2943 - val_loss: 0.1111\n",
      "Epoch 17/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2855\n",
      "Epoch 00017: val_loss improved from 0.10953 to 0.10895, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2854 - val_loss: 0.1089\n",
      "Epoch 18/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.2804\n",
      "Epoch 00018: val_loss improved from 0.10895 to 0.10573, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2786 - val_loss: 0.1057\n",
      "Epoch 19/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2791\n",
      "Epoch 00019: val_loss improved from 0.10573 to 0.10424, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2789 - val_loss: 0.1042\n",
      "Epoch 20/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2703\n",
      "Epoch 00020: val_loss improved from 0.10424 to 0.10423, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2708 - val_loss: 0.1042\n",
      "Epoch 21/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2669\n",
      "Epoch 00021: val_loss improved from 0.10423 to 0.10207, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2667 - val_loss: 0.1021\n",
      "Epoch 22/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2597\n",
      "Epoch 00022: val_loss improved from 0.10207 to 0.10136, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2595 - val_loss: 0.1014\n",
      "Epoch 23/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2608\n",
      "Epoch 00023: val_loss improved from 0.10136 to 0.10093, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2609 - val_loss: 0.1009\n",
      "Epoch 24/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.2592\n",
      "Epoch 00024: val_loss did not improve from 0.10093\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2581 - val_loss: 0.1012\n",
      "Epoch 25/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2523\n",
      "Epoch 00025: val_loss did not improve from 0.10093\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2523 - val_loss: 0.1012\n",
      " ###5 fold : val acc1 0.583, acc3 0.975, mae 0.222###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/140 [==========================>...] - ETA: 0s - loss: 14.6619\n",
      "Epoch 00001: val_loss improved from inf to 6.77078, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 14.1877 - val_loss: 6.7708\n",
      "Epoch 2/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 4.1223\n",
      "Epoch 00002: val_loss improved from 6.77078 to 2.17896, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 4.0389 - val_loss: 2.1790\n",
      "Epoch 3/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 1.7413\n",
      "Epoch 00003: val_loss improved from 2.17896 to 0.82015, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.6733 - val_loss: 0.8202\n",
      "Epoch 4/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.8003\n",
      "Epoch 00004: val_loss improved from 0.82015 to 0.30986, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7793 - val_loss: 0.3099\n",
      "Epoch 5/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.4785\n",
      "Epoch 00005: val_loss improved from 0.30986 to 0.17243, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4786 - val_loss: 0.1724\n",
      "Epoch 6/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.3987\n",
      "Epoch 00006: val_loss improved from 0.17243 to 0.13734, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3977 - val_loss: 0.1373\n",
      "Epoch 7/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.3703\n",
      "Epoch 00007: val_loss improved from 0.13734 to 0.12659, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3688 - val_loss: 0.1266\n",
      "Epoch 8/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3559\n",
      "Epoch 00008: val_loss improved from 0.12659 to 0.12392, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3561 - val_loss: 0.1239\n",
      "Epoch 9/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3468\n",
      "Epoch 00009: val_loss improved from 0.12392 to 0.11814, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3475 - val_loss: 0.1181\n",
      "Epoch 10/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.3280\n",
      "Epoch 00010: val_loss improved from 0.11814 to 0.11653, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3273 - val_loss: 0.1165\n",
      "Epoch 11/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3246\n",
      "Epoch 00011: val_loss improved from 0.11653 to 0.11614, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3258 - val_loss: 0.1161\n",
      "Epoch 12/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.3194\n",
      "Epoch 00012: val_loss improved from 0.11614 to 0.11274, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3184 - val_loss: 0.1127\n",
      "Epoch 13/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.3116\n",
      "Epoch 00013: val_loss improved from 0.11274 to 0.11070, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3110 - val_loss: 0.1107\n",
      "Epoch 14/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3073\n",
      "Epoch 00014: val_loss improved from 0.11070 to 0.10975, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3073 - val_loss: 0.1098\n",
      "Epoch 15/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.2980\n",
      "Epoch 00015: val_loss improved from 0.10975 to 0.10824, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2965 - val_loss: 0.1082\n",
      "Epoch 16/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2953\n",
      "Epoch 00016: val_loss did not improve from 0.10824\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2963 - val_loss: 0.1104\n",
      "Epoch 17/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.2857\n",
      "Epoch 00017: val_loss improved from 0.10824 to 0.10777, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2854 - val_loss: 0.1078\n",
      "Epoch 18/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.2809\n",
      "Epoch 00018: val_loss improved from 0.10777 to 0.10509, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2801 - val_loss: 0.1051\n",
      "Epoch 19/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2805\n",
      "Epoch 00019: val_loss improved from 0.10509 to 0.10402, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2805 - val_loss: 0.1040\n",
      "Epoch 20/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.2713\n",
      "Epoch 00020: val_loss improved from 0.10402 to 0.10342, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2717 - val_loss: 0.1034\n",
      "Epoch 21/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2683\n",
      "Epoch 00021: val_loss improved from 0.10342 to 0.10152, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2681 - val_loss: 0.1015\n",
      "Epoch 22/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2594\n",
      "Epoch 00022: val_loss improved from 0.10152 to 0.10054, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2596 - val_loss: 0.1005\n",
      "Epoch 23/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2615\n",
      "Epoch 00023: val_loss did not improve from 0.10054\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2615 - val_loss: 0.1006\n",
      "Epoch 24/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.2573\n",
      "Epoch 00024: val_loss did not improve from 0.10054\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2578 - val_loss: 0.1006\n",
      " ###6 fold : val acc1 0.606, acc3 0.979, mae 0.207###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/140 [============================>.] - ETA: 0s - loss: 14.2965\n",
      "Epoch 00001: val_loss improved from inf to 6.78750, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 14.1856 - val_loss: 6.7875\n",
      "Epoch 2/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 4.1843\n",
      "Epoch 00002: val_loss improved from 6.78750 to 2.17073, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 4.0413 - val_loss: 2.1707\n",
      "Epoch 3/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 1.6755\n",
      "Epoch 00003: val_loss improved from 2.17073 to 0.82498, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.6737 - val_loss: 0.8250\n",
      "Epoch 4/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.8087\n",
      "Epoch 00004: val_loss improved from 0.82498 to 0.31236, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7824 - val_loss: 0.3124\n",
      "Epoch 5/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.4833\n",
      "Epoch 00005: val_loss improved from 0.31236 to 0.17391, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4789 - val_loss: 0.1739\n",
      "Epoch 6/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.3965\n",
      "Epoch 00006: val_loss improved from 0.17391 to 0.13827, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3960 - val_loss: 0.1383\n",
      "Epoch 7/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.3705\n",
      "Epoch 00007: val_loss improved from 0.13827 to 0.12756, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3687 - val_loss: 0.1276\n",
      "Epoch 8/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3539\n",
      "Epoch 00008: val_loss improved from 0.12756 to 0.12400, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3542 - val_loss: 0.1240\n",
      "Epoch 9/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.3476"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/558 [============================>.] - ETA: 0s - loss: 1.0888\n",
      "Epoch 00001: val_loss improved from inf to 0.11301, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 1.0798 - val_loss: 0.1130\n",
      "Epoch 2/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.1027\n",
      "Epoch 00002: val_loss did not improve from 0.11301\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1025 - val_loss: 0.1169\n",
      "Epoch 3/100\n",
      "541/558 [============================>.] - ETA: 0s - loss: 0.1007\n",
      "Epoch 00003: val_loss improved from 0.11301 to 0.10514, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1007 - val_loss: 0.1051\n",
      "Epoch 4/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.0995\n",
      "Epoch 00004: val_loss did not improve from 0.10514\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0995 - val_loss: 0.1060\n",
      "Epoch 5/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00005: val_loss improved from 0.10514 to 0.10369, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0993 - val_loss: 0.1037\n",
      "Epoch 6/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.1002\n",
      "Epoch 00006: val_loss improved from 0.10369 to 0.10199, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1002 - val_loss: 0.1020\n",
      "Epoch 7/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.1001\n",
      "Epoch 00007: val_loss improved from 0.10199 to 0.10055, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0999 - val_loss: 0.1005\n",
      "Epoch 8/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.1007\n",
      "Epoch 00008: val_loss did not improve from 0.10055\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1005 - val_loss: 0.1074\n",
      "Epoch 9/100\n",
      "554/558 [============================>.] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00009: val_loss did not improve from 0.10055\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0997 - val_loss: 0.1092\n",
      " ###9 fold : val acc1 0.605, acc3 0.980, mae 0.208###\n",
      "acc10.592_acc30.980\n",
      "random search 12/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/279 [===========================>..] - ETA: 0s - loss: 1.6621\n",
      "Epoch 00001: val_loss improved from inf to 0.11364, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.6074 - val_loss: 0.1136\n",
      "Epoch 2/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1959\n",
      "Epoch 00002: val_loss improved from 0.11364 to 0.10056, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1956 - val_loss: 0.1006\n",
      "Epoch 3/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1791\n",
      "Epoch 00003: val_loss improved from 0.10056 to 0.09873, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1790 - val_loss: 0.0987\n",
      "Epoch 4/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1765\n",
      "Epoch 00004: val_loss did not improve from 0.09873\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1761 - val_loss: 0.1052\n",
      "Epoch 5/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1692\n",
      "Epoch 00005: val_loss improved from 0.09873 to 0.09573, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1694 - val_loss: 0.0957\n",
      "Epoch 6/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1646\n",
      "Epoch 00006: val_loss did not improve from 0.09573\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1651 - val_loss: 0.1017\n",
      "Epoch 7/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1611\n",
      "Epoch 00007: val_loss did not improve from 0.09573\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1610 - val_loss: 0.0979\n",
      " ###0 fold : val acc1 0.602, acc3 0.984, mae 0.207###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262/279 [===========================>..] - ETA: 0s - loss: 1.6997\n",
      "Epoch 00001: val_loss improved from inf to 0.11194, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.6127 - val_loss: 0.1119\n",
      "Epoch 2/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1964\n",
      "Epoch 00002: val_loss improved from 0.11194 to 0.10224, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1964 - val_loss: 0.1022\n",
      "Epoch 3/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1794\n",
      "Epoch 00003: val_loss improved from 0.10224 to 0.10151, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1794 - val_loss: 0.1015\n",
      "Epoch 4/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.1757\n",
      "Epoch 00004: val_loss did not improve from 0.10151\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1753 - val_loss: 0.1018\n",
      "Epoch 5/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1700\n",
      "Epoch 00005: val_loss improved from 0.10151 to 0.09860, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1700 - val_loss: 0.0986\n",
      "Epoch 6/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1656\n",
      "Epoch 00006: val_loss did not improve from 0.09860\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1656 - val_loss: 0.0989\n",
      "Epoch 7/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1617\n",
      "Epoch 00007: val_loss improved from 0.09860 to 0.09790, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1613 - val_loss: 0.0979\n",
      "Epoch 8/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1612\n",
      "Epoch 00008: val_loss improved from 0.09790 to 0.09603, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1612 - val_loss: 0.0960\n",
      "Epoch 9/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1587\n",
      "Epoch 00009: val_loss improved from 0.09603 to 0.09435, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1584 - val_loss: 0.0943\n",
      "Epoch 10/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.1576\n",
      "Epoch 00010: val_loss did not improve from 0.09435\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1576 - val_loss: 0.1047\n",
      "Epoch 11/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1565\n",
      "Epoch 00011: val_loss did not improve from 0.09435\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1565 - val_loss: 0.0972\n",
      " ###1 fold : val acc1 0.599, acc3 0.982, mae 0.210###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/279 [============================>.] - ETA: 0s - loss: 1.6242\n",
      "Epoch 00001: val_loss improved from inf to 0.11257, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.6115 - val_loss: 0.1126\n",
      "Epoch 2/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1974\n",
      "Epoch 00002: val_loss improved from 0.11257 to 0.10206, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1969 - val_loss: 0.1021\n",
      "Epoch 3/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1800\n",
      "Epoch 00003: val_loss improved from 0.10206 to 0.10156, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1799 - val_loss: 0.1016\n",
      "Epoch 4/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1754\n",
      "Epoch 00004: val_loss improved from 0.10156 to 0.10043, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1751 - val_loss: 0.1004\n",
      "Epoch 5/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1698\n",
      "Epoch 00005: val_loss improved from 0.10043 to 0.09783, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1700 - val_loss: 0.0978\n",
      "Epoch 6/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1647\n",
      "Epoch 00006: val_loss improved from 0.09783 to 0.09702, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1654 - val_loss: 0.0970\n",
      "Epoch 7/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1623\n",
      "Epoch 00007: val_loss improved from 0.09702 to 0.09656, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1623 - val_loss: 0.0966\n",
      "Epoch 8/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1617\n",
      "Epoch 00008: val_loss improved from 0.09656 to 0.09534, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1618 - val_loss: 0.0953\n",
      "Epoch 9/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1583\n",
      "Epoch 00009: val_loss improved from 0.09534 to 0.09453, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1580 - val_loss: 0.0945\n",
      "Epoch 10/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1587\n",
      "Epoch 00010: val_loss did not improve from 0.09453\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1589 - val_loss: 0.1051\n",
      "Epoch 11/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1558\n",
      "Epoch 00011: val_loss did not improve from 0.09453\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1560 - val_loss: 0.1016\n",
      " ###2 fold : val acc1 0.598, acc3 0.982, mae 0.210###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/279 [===========================>..] - ETA: 0s - loss: 1.6565\n",
      "Epoch 00001: val_loss improved from inf to 0.11379, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.6020 - val_loss: 0.1138\n",
      "Epoch 2/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1975\n",
      "Epoch 00002: val_loss improved from 0.11379 to 0.10302, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1974 - val_loss: 0.1030\n",
      "Epoch 3/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1791\n",
      "Epoch 00003: val_loss improved from 0.10302 to 0.09829, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1790 - val_loss: 0.0983\n",
      "Epoch 4/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1734\n",
      "Epoch 00004: val_loss did not improve from 0.09829\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1734 - val_loss: 0.0985\n",
      "Epoch 5/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1697\n",
      "Epoch 00005: val_loss improved from 0.09829 to 0.09670, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1699 - val_loss: 0.0967\n",
      "Epoch 6/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1654\n",
      "Epoch 00006: val_loss improved from 0.09670 to 0.09545, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1657 - val_loss: 0.0954\n",
      "Epoch 7/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1644\n",
      "Epoch 00007: val_loss improved from 0.09545 to 0.09528, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1641 - val_loss: 0.0953\n",
      "Epoch 8/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1644\n",
      "Epoch 00008: val_loss did not improve from 0.09528\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1639 - val_loss: 0.0964\n",
      "Epoch 9/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1575\n",
      "Epoch 00009: val_loss improved from 0.09528 to 0.09456, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1574 - val_loss: 0.0946\n",
      "Epoch 10/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.1590\n",
      "Epoch 00010: val_loss did not improve from 0.09456\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1589 - val_loss: 0.1018\n",
      "Epoch 11/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1569\n",
      "Epoch 00011: val_loss did not improve from 0.09456\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1565 - val_loss: 0.1022\n",
      " ###3 fold : val acc1 0.598, acc3 0.980, mae 0.211###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/279 [============================>.] - ETA: 0s - loss: 1.5992\n",
      "Epoch 00001: val_loss improved from inf to 0.11882, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.5817 - val_loss: 0.1188\n",
      "Epoch 2/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1985\n",
      "Epoch 00002: val_loss improved from 0.11882 to 0.10350, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1972 - val_loss: 0.1035\n",
      "Epoch 3/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1792\n",
      "Epoch 00003: val_loss improved from 0.10350 to 0.10029, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1791 - val_loss: 0.1003\n",
      "Epoch 4/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1698\n",
      "Epoch 00004: val_loss improved from 0.10029 to 0.09740, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1696 - val_loss: 0.0974\n",
      "Epoch 5/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1659\n",
      "Epoch 00005: val_loss did not improve from 0.09740\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1659 - val_loss: 0.1131\n",
      "Epoch 6/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1643\n",
      "Epoch 00006: val_loss did not improve from 0.09740\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1641 - val_loss: 0.1017\n",
      " ###4 fold : val acc1 0.580, acc3 0.982, mae 0.219###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/279 [===========================>..] - ETA: 0s - loss: 1.6237\n",
      "Epoch 00001: val_loss improved from inf to 0.11718, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.5750 - val_loss: 0.1172\n",
      "Epoch 2/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1984\n",
      "Epoch 00002: val_loss improved from 0.11718 to 0.10483, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1971 - val_loss: 0.1048\n",
      "Epoch 3/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1790\n",
      "Epoch 00003: val_loss improved from 0.10483 to 0.09764, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1790 - val_loss: 0.0976\n",
      "Epoch 4/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1696\n",
      "Epoch 00004: val_loss improved from 0.09764 to 0.09605, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1697 - val_loss: 0.0961\n",
      "Epoch 5/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1655\n",
      "Epoch 00005: val_loss did not improve from 0.09605\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1655 - val_loss: 0.1146\n",
      "Epoch 6/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1654\n",
      "Epoch 00006: val_loss did not improve from 0.09605\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1653 - val_loss: 0.1021\n",
      " ###5 fold : val acc1 0.589, acc3 0.981, mae 0.215###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264/279 [===========================>..] - ETA: 0s - loss: 1.6487\n",
      "Epoch 00001: val_loss improved from inf to 0.11385, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.5730 - val_loss: 0.1138\n",
      "Epoch 2/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00002: val_loss improved from 0.11385 to 0.10535, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1981 - val_loss: 0.1054\n",
      "Epoch 3/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1785\n",
      "Epoch 00003: val_loss improved from 0.10535 to 0.10106, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1783 - val_loss: 0.1011\n",
      "Epoch 4/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1713\n",
      "Epoch 00004: val_loss did not improve from 0.10106\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1713 - val_loss: 0.1012\n",
      "Epoch 5/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1654\n",
      "Epoch 00005: val_loss did not improve from 0.10106\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1655 - val_loss: 0.1092\n",
      " ###6 fold : val acc1 0.587, acc3 0.982, mae 0.215###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/279 [============================>.] - ETA: 0s - loss: 1.5981\n",
      "Epoch 00001: val_loss improved from inf to 0.11125, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.5701 - val_loss: 0.1112\n",
      "Epoch 2/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.1971\n",
      "Epoch 00002: val_loss improved from 0.11125 to 0.10458, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1958 - val_loss: 0.1046\n",
      "Epoch 3/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1782\n",
      "Epoch 00003: val_loss improved from 0.10458 to 0.10102, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1778 - val_loss: 0.1010\n",
      "Epoch 4/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1703\n",
      "Epoch 00004: val_loss improved from 0.10102 to 0.09792, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1706 - val_loss: 0.0979\n",
      "Epoch 5/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1639\n",
      "Epoch 00005: val_loss did not improve from 0.09792\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1642 - val_loss: 0.1063\n",
      "Epoch 6/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1639\n",
      "Epoch 00006: val_loss did not improve from 0.09792\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1641 - val_loss: 0.0997\n",
      " ###7 fold : val acc1 0.590, acc3 0.974, mae 0.218###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/279 [============================>.] - ETA: 0s - loss: 1.5859\n",
      "Epoch 00001: val_loss improved from inf to 0.11632, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.5782 - val_loss: 0.1163\n",
      "Epoch 2/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1958\n",
      "Epoch 00002: val_loss improved from 0.11632 to 0.10789, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1952 - val_loss: 0.1079\n",
      "Epoch 3/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1772\n",
      "Epoch 00003: val_loss improved from 0.10789 to 0.10732, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1772 - val_loss: 0.1073\n",
      "Epoch 4/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1700\n",
      "Epoch 00004: val_loss improved from 0.10732 to 0.10060, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1701 - val_loss: 0.1006\n",
      "Epoch 5/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1644\n",
      "Epoch 00005: val_loss did not improve from 0.10060\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1646 - val_loss: 0.1108\n",
      "Epoch 6/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1632\n",
      "Epoch 00006: val_loss did not improve from 0.10060\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1634 - val_loss: 0.1052\n",
      " ###8 fold : val acc1 0.597, acc3 0.981, mae 0.211###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/279 [===========================>..] - ETA: 0s - loss: 1.6327\n",
      "Epoch 00001: val_loss improved from inf to 0.11954, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.5782 - val_loss: 0.1195\n",
      "Epoch 2/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.1965\n",
      "Epoch 00002: val_loss improved from 0.11954 to 0.11125, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1952 - val_loss: 0.1113\n",
      "Epoch 3/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1773\n",
      "Epoch 00003: val_loss improved from 0.11125 to 0.10748, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1772 - val_loss: 0.1075\n",
      "Epoch 4/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.1698\n",
      "Epoch 00004: val_loss improved from 0.10748 to 0.10273, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.4,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1701 - val_loss: 0.1027\n",
      "Epoch 5/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1645\n",
      "Epoch 00005: val_loss did not improve from 0.10273\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1646 - val_loss: 0.1123\n",
      "Epoch 6/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1633\n",
      "Epoch 00006: val_loss did not improve from 0.10273\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1634 - val_loss: 0.1075\n",
      " ###9 fold : val acc1 0.599, acc3 0.982, mae 0.210###\n",
      "acc10.594_acc30.981\n",
      "random search 13/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 17.4893\n",
      "Epoch 00001: val_loss improved from inf to 11.82022, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 17.4893 - val_loss: 11.8202\n",
      "Epoch 2/100\n",
      "49/70 [====================>.........] - ETA: 0s - loss: 9.2573 \n",
      "Epoch 00002: val_loss improved from 11.82022 to 4.43066, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 8.2111 - val_loss: 4.4307\n",
      "Epoch 3/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 3.6583\n",
      "Epoch 00003: val_loss improved from 4.43066 to 1.87693, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 3.6201 - val_loss: 1.8769\n",
      "Epoch 4/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 2.0901\n",
      "Epoch 00004: val_loss improved from 1.87693 to 0.90768, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 2.0898 - val_loss: 0.9077\n",
      "Epoch 5/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 1.4452\n",
      "Epoch 00005: val_loss improved from 0.90768 to 0.47256, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.4195 - val_loss: 0.4726\n",
      "Epoch 6/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 1.1535\n",
      "Epoch 00006: val_loss improved from 0.47256 to 0.28165, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1353 - val_loss: 0.2817\n",
      "Epoch 7/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.9668\n",
      "Epoch 00007: val_loss improved from 0.28165 to 0.21231, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9592 - val_loss: 0.2123\n",
      "Epoch 8/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.8945\n",
      "Epoch 00008: val_loss improved from 0.21231 to 0.18273, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8899 - val_loss: 0.1827\n",
      "Epoch 9/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.8637\n",
      "Epoch 00009: val_loss improved from 0.18273 to 0.17155, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8600 - val_loss: 0.1715\n",
      "Epoch 10/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.8077\n",
      "Epoch 00010: val_loss improved from 0.17155 to 0.16030, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8103 - val_loss: 0.1603\n",
      "Epoch 11/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.7827\n",
      "Epoch 00011: val_loss improved from 0.16030 to 0.15345, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7824 - val_loss: 0.1535\n",
      "Epoch 12/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.7542\n",
      "Epoch 00012: val_loss improved from 0.15345 to 0.15020, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7547 - val_loss: 0.1502\n",
      "Epoch 13/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.7474\n",
      "Epoch 00013: val_loss improved from 0.15020 to 0.14287, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7465 - val_loss: 0.1429\n",
      "Epoch 14/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.7114\n",
      "Epoch 00014: val_loss improved from 0.14287 to 0.14237, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7143 - val_loss: 0.1424\n",
      "Epoch 15/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6892\n",
      "Epoch 00015: val_loss improved from 0.14237 to 0.13969, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6933 - val_loss: 0.1397\n",
      "Epoch 16/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.6826\n",
      "Epoch 00016: val_loss improved from 0.13969 to 0.13455, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6806 - val_loss: 0.1346\n",
      "Epoch 17/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.6616\n",
      "Epoch 00017: val_loss did not improve from 0.13455\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6594 - val_loss: 0.1359\n",
      "Epoch 18/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6461\n",
      "Epoch 00018: val_loss improved from 0.13455 to 0.13001, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6423 - val_loss: 0.1300\n",
      "Epoch 19/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.6242\n",
      "Epoch 00019: val_loss improved from 0.13001 to 0.12931, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6221 - val_loss: 0.1293\n",
      "Epoch 20/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.6217\n",
      "Epoch 00020: val_loss did not improve from 0.12931\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6195 - val_loss: 0.1298\n",
      "Epoch 21/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.5904\n",
      "Epoch 00021: val_loss improved from 0.12931 to 0.12414, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5886 - val_loss: 0.1241\n",
      "Epoch 22/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.5906\n",
      "Epoch 00022: val_loss improved from 0.12414 to 0.12119, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5898 - val_loss: 0.1212\n",
      "Epoch 23/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.5834\n",
      "Epoch 00023: val_loss improved from 0.12119 to 0.12058, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5825 - val_loss: 0.1206\n",
      "Epoch 24/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.5664\n",
      "Epoch 00024: val_loss improved from 0.12058 to 0.12012, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5679 - val_loss: 0.1201\n",
      "Epoch 25/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.5600\n",
      "Epoch 00025: val_loss improved from 0.12012 to 0.11456, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5590 - val_loss: 0.1146\n",
      "Epoch 26/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.5554\n",
      "Epoch 00026: val_loss did not improve from 0.11456\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5545 - val_loss: 0.1165\n",
      "Epoch 27/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.5411\n",
      "Epoch 00027: val_loss did not improve from 0.11456\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5398 - val_loss: 0.1169\n",
      " ###0 fold : val acc1 0.569, acc3 0.968, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/70 [=========================>....] - ETA: 0s - loss: 18.1604\n",
      "Epoch 00001: val_loss improved from inf to 11.83857, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 17.4838 - val_loss: 11.8386\n",
      "Epoch 2/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 8.5671\n",
      "Epoch 00002: val_loss improved from 11.83857 to 4.44680, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 8.2254 - val_loss: 4.4468\n",
      "Epoch 3/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 3.7524\n",
      "Epoch 00003: val_loss improved from 4.44680 to 1.87347, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.6315 - val_loss: 1.8735\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 2.0940\n",
      "Epoch 00004: val_loss improved from 1.87347 to 0.88947, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.0940 - val_loss: 0.8895\n",
      "Epoch 5/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 1.4296\n",
      "Epoch 00005: val_loss improved from 0.88947 to 0.46041, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.3915 - val_loss: 0.4604\n",
      "Epoch 6/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 1.1526\n",
      "Epoch 00006: val_loss improved from 0.46041 to 0.27998, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1260 - val_loss: 0.2800\n",
      "Epoch 7/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.9633\n",
      "Epoch 00007: val_loss improved from 0.27998 to 0.21308, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9541 - val_loss: 0.2131\n",
      "Epoch 8/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.9070\n",
      "Epoch 00008: val_loss improved from 0.21308 to 0.18334, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8994 - val_loss: 0.1833\n",
      "Epoch 9/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.8650\n",
      "Epoch 00009: val_loss improved from 0.18334 to 0.17294, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8613 - val_loss: 0.1729\n",
      "Epoch 10/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.8138\n",
      "Epoch 00010: val_loss improved from 0.17294 to 0.16105, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8165 - val_loss: 0.1610\n",
      "Epoch 11/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.7862\n",
      "Epoch 00011: val_loss improved from 0.16105 to 0.15433, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7915 - val_loss: 0.1543\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7619\n",
      "Epoch 00012: val_loss improved from 0.15433 to 0.15085, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7619 - val_loss: 0.1508\n",
      "Epoch 13/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.7579\n",
      "Epoch 00013: val_loss improved from 0.15085 to 0.14405, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7513 - val_loss: 0.1440\n",
      "Epoch 14/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.7289\n",
      "Epoch 00014: val_loss improved from 0.14405 to 0.14203, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7248 - val_loss: 0.1420\n",
      "Epoch 15/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.6941\n",
      "Epoch 00015: val_loss improved from 0.14203 to 0.13835, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6981 - val_loss: 0.1383\n",
      "Epoch 16/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6961\n",
      "Epoch 00016: val_loss improved from 0.13835 to 0.13622, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6911 - val_loss: 0.1362\n",
      "Epoch 17/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.6741\n",
      "Epoch 00017: val_loss did not improve from 0.13622\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6697 - val_loss: 0.1372\n",
      "Epoch 18/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.6536\n",
      "Epoch 00018: val_loss improved from 0.13622 to 0.12883, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6493 - val_loss: 0.1288\n",
      "Epoch 19/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.6381\n",
      "Epoch 00019: val_loss did not improve from 0.12883\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6363 - val_loss: 0.1307\n",
      "Epoch 20/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.6318\n",
      "Epoch 00020: val_loss did not improve from 0.12883\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6298 - val_loss: 0.1296\n",
      " ###1 fold : val acc1 0.539, acc3 0.965, mae 0.249###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/70 [=========================>....] - ETA: 0s - loss: 18.1263\n",
      "Epoch 00001: val_loss improved from inf to 11.81740, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 17.4641 - val_loss: 11.8174\n",
      "Epoch 2/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 8.4354\n",
      "Epoch 00002: val_loss improved from 11.81740 to 4.44203, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 8.1964 - val_loss: 4.4420\n",
      "Epoch 3/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 3.7652\n",
      "Epoch 00003: val_loss improved from 4.44203 to 1.88009, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.6284 - val_loss: 1.8801\n",
      "Epoch 4/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 2.1722\n",
      "Epoch 00004: val_loss improved from 1.88009 to 0.89816, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.1036 - val_loss: 0.8982\n",
      "Epoch 5/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 1.4405\n",
      "Epoch 00005: val_loss improved from 0.89816 to 0.46605, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.4016 - val_loss: 0.4660\n",
      "Epoch 6/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 1.1507\n",
      "Epoch 00006: val_loss improved from 0.46605 to 0.28225, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1237 - val_loss: 0.2822\n",
      "Epoch 7/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.9669\n",
      "Epoch 00007: val_loss improved from 0.28225 to 0.21214, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9562 - val_loss: 0.2121\n",
      "Epoch 8/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.8996\n",
      "Epoch 00008: val_loss improved from 0.21214 to 0.18131, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8925 - val_loss: 0.1813\n",
      "Epoch 9/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.8502\n",
      "Epoch 00009: val_loss improved from 0.18131 to 0.17152, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8530 - val_loss: 0.1715\n",
      "Epoch 10/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.8104\n",
      "Epoch 00010: val_loss improved from 0.17152 to 0.16272, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8130 - val_loss: 0.1627\n",
      "Epoch 11/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.7790\n",
      "Epoch 00011: val_loss improved from 0.16272 to 0.15429, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7815 - val_loss: 0.1543\n",
      "Epoch 12/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7526\n",
      "Epoch 00012: val_loss improved from 0.15429 to 0.15104, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7532 - val_loss: 0.1510\n",
      "Epoch 13/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.7478\n",
      "Epoch 00013: val_loss improved from 0.15104 to 0.14335, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7472 - val_loss: 0.1433\n",
      "Epoch 14/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.7151\n",
      "Epoch 00014: val_loss improved from 0.14335 to 0.14225, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7143 - val_loss: 0.1423\n",
      "Epoch 15/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6877\n",
      "Epoch 00015: val_loss improved from 0.14225 to 0.13806, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6916 - val_loss: 0.1381\n",
      "Epoch 16/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6806\n",
      "Epoch 00016: val_loss improved from 0.13806 to 0.13483, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6800 - val_loss: 0.1348\n",
      "Epoch 17/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6676\n",
      "Epoch 00017: val_loss did not improve from 0.13483\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6631 - val_loss: 0.1350\n",
      "Epoch 18/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6548\n",
      "Epoch 00018: val_loss improved from 0.13483 to 0.12957, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6498 - val_loss: 0.1296\n",
      "Epoch 19/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6268\n",
      "Epoch 00019: val_loss did not improve from 0.12957\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6242 - val_loss: 0.1329\n",
      "Epoch 20/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6288\n",
      "Epoch 00020: val_loss did not improve from 0.12957\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6228 - val_loss: 0.1307\n",
      " ###2 fold : val acc1 0.537, acc3 0.958, mae 0.254###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/70 [============================>.] - ETA: 0s - loss: 17.6468\n",
      "Epoch 00001: val_loss improved from inf to 11.79941, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 17.5215 - val_loss: 11.7994\n",
      "Epoch 2/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 8.4104\n",
      "Epoch 00002: val_loss improved from 11.79941 to 4.43499, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 8.2449 - val_loss: 4.4350\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 3.6423\n",
      "Epoch 00003: val_loss improved from 4.43499 to 1.87216, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 3.6423 - val_loss: 1.8722\n",
      "Epoch 4/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 2.1012\n",
      "Epoch 00004: val_loss improved from 1.87216 to 0.88864, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 2.0994 - val_loss: 0.8886\n",
      "Epoch 5/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.4072\n",
      "Epoch 00005: val_loss improved from 0.88864 to 0.45923, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 1.3977 - val_loss: 0.4592\n",
      "Epoch 6/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 1.1386\n",
      "Epoch 00006: val_loss improved from 0.45923 to 0.27836, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1200 - val_loss: 0.2784\n",
      "Epoch 7/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.9736\n",
      "Epoch 00007: val_loss improved from 0.27836 to 0.21046, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9677 - val_loss: 0.2105\n",
      "Epoch 8/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.9029\n",
      "Epoch 00008: val_loss improved from 0.21046 to 0.18010, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8978 - val_loss: 0.1801\n",
      "Epoch 9/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.8534\n",
      "Epoch 00009: val_loss improved from 0.18010 to 0.17153, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8502 - val_loss: 0.1715\n",
      "Epoch 10/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.8106\n",
      "Epoch 00010: val_loss improved from 0.17153 to 0.16293, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8137 - val_loss: 0.1629\n",
      "Epoch 11/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.7909\n",
      "Epoch 00011: val_loss improved from 0.16293 to 0.15313, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7929 - val_loss: 0.1531\n",
      "Epoch 12/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.7593\n",
      "Epoch 00012: val_loss improved from 0.15313 to 0.15029, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7650 - val_loss: 0.1503\n",
      "Epoch 13/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.7462\n",
      "Epoch 00013: val_loss improved from 0.15029 to 0.14408, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7445 - val_loss: 0.1441\n",
      "Epoch 14/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.7167\n",
      "Epoch 00014: val_loss improved from 0.14408 to 0.14200, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7201 - val_loss: 0.1420\n",
      "Epoch 15/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.6947\n",
      "Epoch 00015: val_loss improved from 0.14200 to 0.13848, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6980 - val_loss: 0.1385\n",
      "Epoch 16/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.6862\n",
      "Epoch 00016: val_loss improved from 0.13848 to 0.13371, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6869 - val_loss: 0.1337\n",
      "Epoch 17/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6725\n",
      "Epoch 00017: val_loss did not improve from 0.13371\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6711 - val_loss: 0.1359\n",
      "Epoch 18/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6591\n",
      "Epoch 00018: val_loss improved from 0.13371 to 0.13014, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6548 - val_loss: 0.1301\n",
      "Epoch 19/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.6341\n",
      "Epoch 00019: val_loss did not improve from 0.13014\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6325 - val_loss: 0.1331\n",
      "Epoch 20/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6368\n",
      "Epoch 00020: val_loss did not improve from 0.13014\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6346 - val_loss: 0.1304\n",
      " ###3 fold : val acc1 0.555, acc3 0.953, mae 0.247###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/70 [========================>.....] - ETA: 0s - loss: 18.1190\n",
      "Epoch 00001: val_loss improved from inf to 11.81501, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 17.4213 - val_loss: 11.8150\n",
      "Epoch 2/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 8.6303\n",
      "Epoch 00002: val_loss improved from 11.81501 to 4.42779, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 8.2325 - val_loss: 4.4278\n",
      "Epoch 3/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 3.7745\n",
      "Epoch 00003: val_loss improved from 4.42779 to 1.86622, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.6062 - val_loss: 1.8662\n",
      "Epoch 4/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 2.1383\n",
      "Epoch 00004: val_loss improved from 1.86622 to 0.88934, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.0721 - val_loss: 0.8893\n",
      "Epoch 5/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 1.4425\n",
      "Epoch 00005: val_loss improved from 0.88934 to 0.46010, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.4094 - val_loss: 0.4601\n",
      "Epoch 6/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 1.1198\n",
      "Epoch 00006: val_loss improved from 0.46010 to 0.27558, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0982 - val_loss: 0.2756\n",
      "Epoch 7/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.9734\n",
      "Epoch 00007: val_loss improved from 0.27558 to 0.20780, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9635 - val_loss: 0.2078\n",
      "Epoch 8/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.8989\n",
      "Epoch 00008: val_loss improved from 0.20780 to 0.18403, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8917 - val_loss: 0.1840\n",
      "Epoch 9/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.8488\n",
      "Epoch 00009: val_loss improved from 0.18403 to 0.16507, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8429 - val_loss: 0.1651\n",
      "Epoch 10/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.8063\n",
      "Epoch 00010: val_loss improved from 0.16507 to 0.16342, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7962 - val_loss: 0.1634\n",
      "Epoch 11/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.7809\n",
      "Epoch 00011: val_loss improved from 0.16342 to 0.15186, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7746 - val_loss: 0.1519\n",
      "Epoch 12/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.7559\n",
      "Epoch 00012: val_loss improved from 0.15186 to 0.15031, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7518 - val_loss: 0.1503\n",
      "Epoch 13/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7254\n",
      "Epoch 00013: val_loss improved from 0.15031 to 0.14292, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7250 - val_loss: 0.1429\n",
      "Epoch 14/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.7147\n",
      "Epoch 00014: val_loss improved from 0.14292 to 0.14045, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7183 - val_loss: 0.1405\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7008\n",
      "Epoch 00015: val_loss improved from 0.14045 to 0.13822, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7008 - val_loss: 0.1382\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6744\n",
      "Epoch 00016: val_loss improved from 0.13822 to 0.13697, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6744 - val_loss: 0.1370\n",
      "Epoch 17/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.6768\n",
      "Epoch 00017: val_loss did not improve from 0.13697\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6756 - val_loss: 0.1382\n",
      "Epoch 18/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.6634\n",
      "Epoch 00018: val_loss improved from 0.13697 to 0.13124, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6588 - val_loss: 0.1312\n",
      "Epoch 19/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6266\n",
      "Epoch 00019: val_loss improved from 0.13124 to 0.12914, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6274 - val_loss: 0.1291\n",
      "Epoch 20/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6322\n",
      "Epoch 00020: val_loss improved from 0.12914 to 0.12793, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6255 - val_loss: 0.1279\n",
      "Epoch 21/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6288\n",
      "Epoch 00021: val_loss improved from 0.12793 to 0.12066, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6256 - val_loss: 0.1207\n",
      "Epoch 22/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.5940\n",
      "Epoch 00022: val_loss did not improve from 0.12066\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5962 - val_loss: 0.1268\n",
      "Epoch 23/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.5871\n",
      "Epoch 00023: val_loss did not improve from 0.12066\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5841 - val_loss: 0.1230\n",
      " ###4 fold : val acc1 0.548, acc3 0.954, mae 0.249###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/70 [=========================>....] - ETA: 0s - loss: 18.0551\n",
      "Epoch 00001: val_loss improved from inf to 11.82180, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 17.4403 - val_loss: 11.8218\n",
      "Epoch 2/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 8.6067\n",
      "Epoch 00002: val_loss improved from 11.82180 to 4.43230, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 8.2519 - val_loss: 4.4323\n",
      "Epoch 3/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 3.6768\n",
      "Epoch 00003: val_loss improved from 4.43230 to 1.86548, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 3.6005 - val_loss: 1.8655\n",
      "Epoch 4/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 2.1209\n",
      "Epoch 00004: val_loss improved from 1.86548 to 0.88491, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.0672 - val_loss: 0.8849\n",
      "Epoch 5/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 1.4208\n",
      "Epoch 00005: val_loss improved from 0.88491 to 0.45468, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.3999 - val_loss: 0.4547\n",
      "Epoch 6/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 1.1051\n",
      "Epoch 00006: val_loss improved from 0.45468 to 0.27464, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0949 - val_loss: 0.2746\n",
      "Epoch 7/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.9722\n",
      "Epoch 00007: val_loss improved from 0.27464 to 0.20689, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9606 - val_loss: 0.2069\n",
      "Epoch 8/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.8931\n",
      "Epoch 00008: val_loss improved from 0.20689 to 0.18187, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8863 - val_loss: 0.1819\n",
      "Epoch 9/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.8410\n",
      "Epoch 00009: val_loss improved from 0.18187 to 0.16561, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8388 - val_loss: 0.1656\n",
      "Epoch 10/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.7980\n",
      "Epoch 00010: val_loss improved from 0.16561 to 0.16178, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7881 - val_loss: 0.1618\n",
      "Epoch 11/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.7751\n",
      "Epoch 00011: val_loss improved from 0.16178 to 0.15148, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7718 - val_loss: 0.1515\n",
      "Epoch 12/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.7517\n",
      "Epoch 00012: val_loss improved from 0.15148 to 0.14906, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7501 - val_loss: 0.1491\n",
      "Epoch 13/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.7214\n",
      "Epoch 00013: val_loss improved from 0.14906 to 0.14172, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7227 - val_loss: 0.1417\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7060\n",
      "Epoch 00014: val_loss improved from 0.14172 to 0.13934, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7060 - val_loss: 0.1393\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6942\n",
      "Epoch 00015: val_loss improved from 0.13934 to 0.13906, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6942 - val_loss: 0.1391\n",
      "Epoch 16/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6736\n",
      "Epoch 00016: val_loss improved from 0.13906 to 0.13557, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6649 - val_loss: 0.1356\n",
      "Epoch 17/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.6652\n",
      "Epoch 00017: val_loss did not improve from 0.13557\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6611 - val_loss: 0.1360\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6453\n",
      "Epoch 00018: val_loss improved from 0.13557 to 0.12890, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6453 - val_loss: 0.1289\n",
      "Epoch 19/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.6227\n",
      "Epoch 00019: val_loss improved from 0.12890 to 0.12815, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6232 - val_loss: 0.1281\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6172\n",
      "Epoch 00020: val_loss did not improve from 0.12815\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6172 - val_loss: 0.1294\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6172\n",
      "Epoch 00021: val_loss improved from 0.12815 to 0.12123, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6172 - val_loss: 0.1212\n",
      "Epoch 22/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.5948\n",
      "Epoch 00022: val_loss did not improve from 0.12123\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5979 - val_loss: 0.1294\n",
      "Epoch 23/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.5851\n",
      "Epoch 00023: val_loss did not improve from 0.12123\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5844 - val_loss: 0.1236\n",
      " ###5 fold : val acc1 0.542, acc3 0.959, mae 0.251###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/70 [====================>.........] - ETA: 0s - loss: 18.9567\n",
      "Epoch 00001: val_loss improved from inf to 11.83391, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 17.4513 - val_loss: 11.8339\n",
      "Epoch 2/100\n",
      "48/70 [===================>..........] - ETA: 0s - loss: 9.3352 \n",
      "Epoch 00002: val_loss improved from 11.83391 to 4.43689, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 8.2705 - val_loss: 4.4369\n",
      "Epoch 3/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 3.6567\n",
      "Epoch 00003: val_loss improved from 4.43689 to 1.86307, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 3.6263 - val_loss: 1.8631\n",
      "Epoch 4/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 2.1171\n",
      "Epoch 00004: val_loss improved from 1.86307 to 0.87933, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.0673 - val_loss: 0.8793\n",
      "Epoch 5/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 1.4322\n",
      "Epoch 00005: val_loss improved from 0.87933 to 0.45142, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.3969 - val_loss: 0.4514\n",
      "Epoch 6/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 1.1083\n",
      "Epoch 00006: val_loss improved from 0.45142 to 0.27484, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0969 - val_loss: 0.2748\n",
      "Epoch 7/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.9810\n",
      "Epoch 00007: val_loss improved from 0.27484 to 0.20548, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9707 - val_loss: 0.2055\n",
      "Epoch 8/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.8946\n",
      "Epoch 00008: val_loss improved from 0.20548 to 0.18040, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8923 - val_loss: 0.1804\n",
      "Epoch 9/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.8472\n",
      "Epoch 00009: val_loss improved from 0.18040 to 0.16648, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8452 - val_loss: 0.1665\n",
      "Epoch 10/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.7953\n",
      "Epoch 00010: val_loss improved from 0.16648 to 0.16113, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7936 - val_loss: 0.1611\n",
      "Epoch 11/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.7821\n",
      "Epoch 00011: val_loss improved from 0.16113 to 0.15226, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7824 - val_loss: 0.1523\n",
      "Epoch 12/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.7598\n",
      "Epoch 00012: val_loss improved from 0.15226 to 0.14963, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7607 - val_loss: 0.1496\n",
      "Epoch 13/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.7316\n",
      "Epoch 00013: val_loss improved from 0.14963 to 0.14252, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7337 - val_loss: 0.1425\n",
      "Epoch 14/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.7164\n",
      "Epoch 00014: val_loss improved from 0.14252 to 0.13960, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7197 - val_loss: 0.1396\n",
      "Epoch 15/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.7041\n",
      "Epoch 00015: val_loss improved from 0.13960 to 0.13786, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7035 - val_loss: 0.1379\n",
      "Epoch 16/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.6722\n",
      "Epoch 00016: val_loss improved from 0.13786 to 0.13607, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6690 - val_loss: 0.1361\n",
      "Epoch 17/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6761\n",
      "Epoch 00017: val_loss improved from 0.13607 to 0.13454, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6752 - val_loss: 0.1345\n",
      "Epoch 18/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.6592\n",
      "Epoch 00018: val_loss improved from 0.13454 to 0.12897, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6613 - val_loss: 0.1290\n",
      "Epoch 19/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.6319\n",
      "Epoch 00019: val_loss improved from 0.12897 to 0.12714, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6338 - val_loss: 0.1271\n",
      "Epoch 20/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6322\n",
      "Epoch 00020: val_loss did not improve from 0.12714\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6278 - val_loss: 0.1281\n",
      "Epoch 21/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.6293\n",
      "Epoch 00021: val_loss improved from 0.12714 to 0.12077, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6272 - val_loss: 0.1208\n",
      "Epoch 22/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.6073\n",
      "Epoch 00022: val_loss did not improve from 0.12077\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6037 - val_loss: 0.1271\n",
      "Epoch 23/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.5950\n",
      "Epoch 00023: val_loss did not improve from 0.12077\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5918 - val_loss: 0.1215\n",
      " ###6 fold : val acc1 0.565, acc3 0.961, mae 0.237###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/70 [============================>.] - ETA: 0s - loss: 17.5779\n",
      "Epoch 00001: val_loss improved from inf to 11.83411, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 17.4627 - val_loss: 11.8341\n",
      "Epoch 2/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 8.3100\n",
      "Epoch 00002: val_loss improved from 11.83411 to 4.44271, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 8.2780 - val_loss: 4.4427\n",
      "Epoch 3/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 3.6673\n",
      "Epoch 00003: val_loss improved from 4.44271 to 1.86141, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 3.6347 - val_loss: 1.8614\n",
      "Epoch 4/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 2.1074\n",
      "Epoch 00004: val_loss improved from 1.86141 to 0.87674, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 2.0772 - val_loss: 0.8767\n",
      "Epoch 5/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 1.4124\n",
      "Epoch 00005: val_loss improved from 0.87674 to 0.44673, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 1.3972 - val_loss: 0.4467\n",
      "Epoch 6/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 1.0901\n",
      "Epoch 00006: val_loss improved from 0.44673 to 0.27255, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0860 - val_loss: 0.2726\n",
      "Epoch 7/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9695\n",
      "Epoch 00007: val_loss improved from 0.27255 to 0.20405, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.9697 - val_loss: 0.2041\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8817\n",
      "Epoch 00008: val_loss improved from 0.20405 to 0.18033, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.8817 - val_loss: 0.1803\n",
      "Epoch 9/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8517\n",
      "Epoch 00009: val_loss improved from 0.18033 to 0.16655, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.8470 - val_loss: 0.1665\n",
      "Epoch 10/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.7920\n",
      "Epoch 00010: val_loss improved from 0.16655 to 0.16000, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.7919 - val_loss: 0.1600\n",
      "Epoch 11/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.7814\n",
      "Epoch 00011: val_loss improved from 0.16000 to 0.15421, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.7822 - val_loss: 0.1542\n",
      "Epoch 12/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.7567\n",
      "Epoch 00012: val_loss improved from 0.15421 to 0.15051, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7573 - val_loss: 0.1505\n",
      "Epoch 13/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.7282\n",
      "Epoch 00013: val_loss improved from 0.15051 to 0.14267, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7303 - val_loss: 0.1427\n",
      "Epoch 14/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.7141\n",
      "Epoch 00014: val_loss improved from 0.14267 to 0.14083, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7174 - val_loss: 0.1408\n",
      "Epoch 15/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.6996\n",
      "Epoch 00015: val_loss improved from 0.14083 to 0.14044, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6986 - val_loss: 0.1404\n",
      "Epoch 16/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.6679\n",
      "Epoch 00016: val_loss improved from 0.14044 to 0.13588, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6640 - val_loss: 0.1359\n",
      "Epoch 17/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6733\n",
      "Epoch 00017: val_loss improved from 0.13588 to 0.13437, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6727 - val_loss: 0.1344\n",
      "Epoch 18/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.6517\n",
      "Epoch 00018: val_loss improved from 0.13437 to 0.13076, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6572 - val_loss: 0.1308\n",
      "Epoch 19/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.6383\n",
      "Epoch 00019: val_loss improved from 0.13076 to 0.13015, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6385 - val_loss: 0.1301\n",
      "Epoch 20/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6276\n",
      "Epoch 00020: val_loss improved from 0.13015 to 0.12952, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6223 - val_loss: 0.1295\n",
      "Epoch 21/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.6220\n",
      "Epoch 00021: val_loss improved from 0.12952 to 0.12172, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6191 - val_loss: 0.1217\n",
      "Epoch 22/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5924\n",
      "Epoch 00022: val_loss did not improve from 0.12172\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5944 - val_loss: 0.1267\n",
      "Epoch 23/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.5885\n",
      "Epoch 00023: val_loss did not improve from 0.12172\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5861 - val_loss: 0.1238\n",
      " ###7 fold : val acc1 0.545, acc3 0.949, mae 0.254###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/70 [==========================>...] - ETA: 0s - loss: 17.8793\n",
      "Epoch 00001: val_loss improved from inf to 11.81804, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 17.4956 - val_loss: 11.8180\n",
      "Epoch 2/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 8.6079\n",
      "Epoch 00002: val_loss improved from 11.81804 to 4.43125, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 8.2738 - val_loss: 4.4313\n",
      "Epoch 3/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 3.7729\n",
      "Epoch 00003: val_loss improved from 4.43125 to 1.87356, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.6220 - val_loss: 1.8736\n",
      "Epoch 4/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 2.1576\n",
      "Epoch 00004: val_loss improved from 1.87356 to 0.89402, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.0789 - val_loss: 0.8940\n",
      "Epoch 5/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 1.4386\n",
      "Epoch 00005: val_loss improved from 0.89402 to 0.45856, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.3915 - val_loss: 0.4586\n",
      "Epoch 6/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 1.0970\n",
      "Epoch 00006: val_loss improved from 0.45856 to 0.28183, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0882 - val_loss: 0.2818\n",
      "Epoch 7/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.9785\n",
      "Epoch 00007: val_loss improved from 0.28183 to 0.21070, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9718 - val_loss: 0.2107\n",
      "Epoch 8/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.8885\n",
      "Epoch 00008: val_loss improved from 0.21070 to 0.18656, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8817 - val_loss: 0.1866\n",
      "Epoch 9/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.8599\n",
      "Epoch 00009: val_loss improved from 0.18656 to 0.17242, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8497 - val_loss: 0.1724\n",
      "Epoch 10/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.7971\n",
      "Epoch 00010: val_loss improved from 0.17242 to 0.16463, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7922 - val_loss: 0.1646\n",
      "Epoch 11/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.7877\n",
      "Epoch 00011: val_loss improved from 0.16463 to 0.15916, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7834 - val_loss: 0.1592\n",
      "Epoch 12/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.7566\n",
      "Epoch 00012: val_loss improved from 0.15916 to 0.15551, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7569 - val_loss: 0.1555\n",
      "Epoch 13/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.7256\n",
      "Epoch 00013: val_loss improved from 0.15551 to 0.14670, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7298 - val_loss: 0.1467\n",
      "Epoch 14/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.7155\n",
      "Epoch 00014: val_loss improved from 0.14670 to 0.14532, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7197 - val_loss: 0.1453\n",
      "Epoch 15/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.6992\n",
      "Epoch 00015: val_loss improved from 0.14532 to 0.14471, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6992 - val_loss: 0.1447\n",
      "Epoch 16/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6736\n",
      "Epoch 00016: val_loss improved from 0.14471 to 0.14127, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6674 - val_loss: 0.1413\n",
      "Epoch 17/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6752\n",
      "Epoch 00017: val_loss improved from 0.14127 to 0.13858, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6733 - val_loss: 0.1386\n",
      "Epoch 18/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.6587\n",
      "Epoch 00018: val_loss improved from 0.13858 to 0.13531, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6587 - val_loss: 0.1353\n",
      "Epoch 19/100\n",
      "53/70 [=====================>........] - ETA: 0s - loss: 0.6431\n",
      "Epoch 00019: val_loss improved from 0.13531 to 0.13517, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6405 - val_loss: 0.1352\n",
      "Epoch 20/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6308\n",
      "Epoch 00020: val_loss improved from 0.13517 to 0.13494, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6267 - val_loss: 0.1349\n",
      "Epoch 21/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.6255\n",
      "Epoch 00021: val_loss improved from 0.13494 to 0.12576, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6202 - val_loss: 0.1258\n",
      "Epoch 22/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.5928\n",
      "Epoch 00022: val_loss did not improve from 0.12576\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5966 - val_loss: 0.1294\n",
      "Epoch 23/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5855\n",
      "Epoch 00023: val_loss did not improve from 0.12576\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5853 - val_loss: 0.1281\n",
      " ###8 fold : val acc1 0.549, acc3 0.955, mae 0.248###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/70 [==========================>...] - ETA: 0s - loss: 17.8793\n",
      "Epoch 00001: val_loss improved from inf to 11.75536, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 17.4956 - val_loss: 11.7554\n",
      "Epoch 2/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 8.3463\n",
      "Epoch 00002: val_loss improved from 11.75536 to 4.43084, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 8.2738 - val_loss: 4.4308\n",
      "Epoch 3/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 3.6394\n",
      "Epoch 00003: val_loss improved from 4.43084 to 1.87221, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 3.6220 - val_loss: 1.8722\n",
      "Epoch 4/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 2.1255\n",
      "Epoch 00004: val_loss improved from 1.87221 to 0.89487, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.0789 - val_loss: 0.8949\n",
      "Epoch 5/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 1.4115\n",
      "Epoch 00005: val_loss improved from 0.89487 to 0.46327, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.3915 - val_loss: 0.4633\n",
      "Epoch 6/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 1.0940\n",
      "Epoch 00006: val_loss improved from 0.46327 to 0.28709, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0882 - val_loss: 0.2871\n",
      "Epoch 7/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.9819\n",
      "Epoch 00007: val_loss improved from 0.28709 to 0.21558, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9718 - val_loss: 0.2156\n",
      "Epoch 8/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.8870\n",
      "Epoch 00008: val_loss improved from 0.21558 to 0.19180, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8817 - val_loss: 0.1918\n",
      "Epoch 9/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.8526\n",
      "Epoch 00009: val_loss improved from 0.19180 to 0.17732, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8497 - val_loss: 0.1773\n",
      "Epoch 10/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.7965\n",
      "Epoch 00010: val_loss improved from 0.17732 to 0.16976, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7922 - val_loss: 0.1698\n",
      "Epoch 11/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.7847\n",
      "Epoch 00011: val_loss improved from 0.16976 to 0.16429, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7834 - val_loss: 0.1643\n",
      "Epoch 12/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.7569\n",
      "Epoch 00012: val_loss improved from 0.16429 to 0.16068, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7569 - val_loss: 0.1607\n",
      "Epoch 13/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.7260\n",
      "Epoch 00013: val_loss improved from 0.16068 to 0.15100, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7298 - val_loss: 0.1510\n",
      "Epoch 14/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.7167\n",
      "Epoch 00014: val_loss improved from 0.15100 to 0.14978, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7197 - val_loss: 0.1498\n",
      "Epoch 15/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.6982\n",
      "Epoch 00015: val_loss improved from 0.14978 to 0.14964, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6992 - val_loss: 0.1496\n",
      "Epoch 16/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.6702\n",
      "Epoch 00016: val_loss improved from 0.14964 to 0.14578, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6674 - val_loss: 0.1458\n",
      "Epoch 17/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6735\n",
      "Epoch 00017: val_loss improved from 0.14578 to 0.14276, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6733 - val_loss: 0.1428\n",
      "Epoch 18/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.6534\n",
      "Epoch 00018: val_loss improved from 0.14276 to 0.13933, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6587 - val_loss: 0.1393\n",
      "Epoch 19/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.6412\n",
      "Epoch 00019: val_loss did not improve from 0.13933\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6405 - val_loss: 0.1395\n",
      "Epoch 20/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6310\n",
      "Epoch 00020: val_loss improved from 0.13933 to 0.13879, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6267 - val_loss: 0.1388\n",
      "Epoch 21/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.6239\n",
      "Epoch 00021: val_loss improved from 0.13879 to 0.12937, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6202 - val_loss: 0.1294\n",
      "Epoch 22/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.5990\n",
      "Epoch 00022: val_loss did not improve from 0.12937\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5966 - val_loss: 0.1335\n",
      "Epoch 23/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.5878\n",
      "Epoch 00023: val_loss did not improve from 0.12937\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5853 - val_loss: 0.1318\n",
      " ###9 fold : val acc1 0.564, acc3 0.966, mae 0.236###\n",
      "acc10.551_acc30.959\n",
      "random search 14/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555/558 [============================>.] - ETA: 0s - loss: 2.0679\n",
      "Epoch 00001: val_loss improved from inf to 0.11806, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0,lr0.002/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 2.0602 - val_loss: 0.1181\n",
      "Epoch 2/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00002: val_loss improved from 0.11806 to 0.10446, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0,lr0.002/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1114 - val_loss: 0.1045\n",
      "Epoch 3/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.1044\n",
      "Epoch 00003: val_loss improved from 0.10446 to 0.10204, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0,lr0.002/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1044 - val_loss: 0.1020\n",
      "Epoch 4/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.1007\n",
      "Epoch 00004: val_loss improved from 0.10204 to 0.09721, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0,lr0.002/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1006 - val_loss: 0.0972\n",
      "Epoch 5/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00005: val_loss did not improve from 0.09721\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0993 - val_loss: 0.1003\n",
      "Epoch 6/100\n",
      "139/558 [======>.......................] - ETA: 1s - loss: 0.0989"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542/558 [============================>.] - ETA: 0s - loss: 0.1138\n",
      "Epoch 00007: val_loss did not improve from 0.10060\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1134 - val_loss: 0.1052\n",
      "Epoch 8/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 0.1130\n",
      "Epoch 00008: val_loss did not improve from 0.10060\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1129 - val_loss: 0.1021\n",
      " ###8 fold : val acc1 0.573, acc3 0.984, mae 0.222###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548/558 [============================>.] - ETA: 0s - loss: 0.8352\n",
      "Epoch 00001: val_loss improved from inf to 0.10811, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.1,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.8234 - val_loss: 0.1081\n",
      "Epoch 2/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.1182\n",
      "Epoch 00002: val_loss did not improve from 0.10811\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1182 - val_loss: 0.1152\n",
      "Epoch 3/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.1173\n",
      "Epoch 00003: val_loss improved from 0.10811 to 0.10431, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.1,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1173 - val_loss: 0.1043\n",
      "Epoch 4/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.1161\n",
      "Epoch 00004: val_loss did not improve from 0.10431\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1164 - val_loss: 0.1100\n",
      "Epoch 5/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.1152\n",
      "Epoch 00005: val_loss improved from 0.10431 to 0.10176, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.1,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1154 - val_loss: 0.1018\n",
      "Epoch 6/100\n",
      "554/558 [============================>.] - ETA: 0s - loss: 0.1141\n",
      "Epoch 00006: val_loss improved from 0.10176 to 0.10116, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes512_dropout0.1,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1143 - val_loss: 0.1012\n",
      "Epoch 7/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.1136\n",
      "Epoch 00007: val_loss did not improve from 0.10116\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1134 - val_loss: 0.1073\n",
      "Epoch 8/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.1131\n",
      "Epoch 00008: val_loss did not improve from 0.10116\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1129 - val_loss: 0.1027\n",
      " ###9 fold : val acc1 0.565, acc3 0.986, mae 0.224###\n",
      "acc10.579_acc30.981\n",
      "random search 17/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/70 [============================>.] - ETA: 0s - loss: 21.6922\n",
      "Epoch 00001: val_loss improved from inf to 16.53123, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 21.5650 - val_loss: 16.5312\n",
      "Epoch 2/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 12.9414\n",
      "Epoch 00002: val_loss improved from 16.53123 to 9.13194, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 12.9159 - val_loss: 9.1319\n",
      "Epoch 3/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 7.0331\n",
      "Epoch 00003: val_loss improved from 9.13194 to 4.65166, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 7.0139 - val_loss: 4.6517\n",
      "Epoch 4/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 4.0965\n",
      "Epoch 00004: val_loss improved from 4.65166 to 2.74376, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 4.0826 - val_loss: 2.7438\n",
      "Epoch 5/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 2.9156\n",
      "Epoch 00005: val_loss improved from 2.74376 to 1.80195, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 2.8839 - val_loss: 1.8020\n",
      "Epoch 6/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 2.1491\n",
      "Epoch 00006: val_loss improved from 1.80195 to 1.17512, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.1330 - val_loss: 1.1751\n",
      "Epoch 7/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 1.6049\n",
      "Epoch 00007: val_loss improved from 1.17512 to 0.75681, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 1.5849 - val_loss: 0.7568\n",
      "Epoch 8/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.2613\n",
      "Epoch 00008: val_loss improved from 0.75681 to 0.48906, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 1.2599 - val_loss: 0.4891\n",
      "Epoch 9/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 1.0567\n",
      "Epoch 00009: val_loss improved from 0.48906 to 0.33206, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0551 - val_loss: 0.3321\n",
      "Epoch 10/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.9328\n",
      "Epoch 00010: val_loss improved from 0.33206 to 0.24478, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9291 - val_loss: 0.2448\n",
      "Epoch 11/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.8634\n",
      "Epoch 00011: val_loss improved from 0.24478 to 0.19909, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8624 - val_loss: 0.1991\n",
      "Epoch 12/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.8171\n",
      "Epoch 00012: val_loss improved from 0.19909 to 0.17168, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8198 - val_loss: 0.1717\n",
      "Epoch 13/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.7804\n",
      "Epoch 00013: val_loss improved from 0.17168 to 0.15562, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7745 - val_loss: 0.1556\n",
      "Epoch 14/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.7535\n",
      "Epoch 00014: val_loss improved from 0.15562 to 0.14325, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7616 - val_loss: 0.1433\n",
      "Epoch 15/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.7413\n",
      "Epoch 00015: val_loss improved from 0.14325 to 0.14259, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7408 - val_loss: 0.1426\n",
      "Epoch 16/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.7226\n",
      "Epoch 00016: val_loss improved from 0.14259 to 0.13614, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7230 - val_loss: 0.1361\n",
      "Epoch 17/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.7138\n",
      "Epoch 00017: val_loss did not improve from 0.13614\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.7158 - val_loss: 0.1398\n",
      "Epoch 18/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6905\n",
      "Epoch 00018: val_loss improved from 0.13614 to 0.13144, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6893 - val_loss: 0.1314\n",
      "Epoch 19/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.6789\n",
      "Epoch 00019: val_loss improved from 0.13144 to 0.12828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6783 - val_loss: 0.1283\n",
      "Epoch 20/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.6607\n",
      "Epoch 00020: val_loss did not improve from 0.12828\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6609 - val_loss: 0.1294\n",
      "Epoch 21/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.6518\n",
      "Epoch 00021: val_loss improved from 0.12828 to 0.12426, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6531 - val_loss: 0.1243\n",
      "Epoch 22/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.6374\n",
      "Epoch 00022: val_loss did not improve from 0.12426\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6335 - val_loss: 0.1266\n",
      "Epoch 23/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6297\n",
      "Epoch 00023: val_loss did not improve from 0.12426\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6287 - val_loss: 0.1245\n",
      " ###0 fold : val acc1 0.544, acc3 0.961, mae 0.249###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/70 [=========================>....] - ETA: 0s - loss: 22.1131\n",
      "Epoch 00001: val_loss improved from inf to 16.53717, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 21.5502 - val_loss: 16.5372\n",
      "Epoch 2/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 13.1779\n",
      "Epoch 00002: val_loss improved from 16.53717 to 9.14114, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 12.9267 - val_loss: 9.1411\n",
      "Epoch 3/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 7.2850\n",
      "Epoch 00003: val_loss improved from 9.14114 to 4.65242, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 7.0333 - val_loss: 4.6524\n",
      "Epoch 4/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 4.2495\n",
      "Epoch 00004: val_loss improved from 4.65242 to 2.73626, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 4.0905 - val_loss: 2.7363\n",
      "Epoch 5/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 2.9694\n",
      "Epoch 00005: val_loss improved from 2.73626 to 1.78937, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.8830 - val_loss: 1.7894\n",
      "Epoch 6/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 2.1647\n",
      "Epoch 00006: val_loss improved from 1.78937 to 1.17273, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.1091 - val_loss: 1.1727\n",
      "Epoch 7/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 1.6295\n",
      "Epoch 00007: val_loss improved from 1.17273 to 0.75499, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.5843 - val_loss: 0.7550\n",
      "Epoch 8/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 1.2898\n",
      "Epoch 00008: val_loss improved from 0.75499 to 0.48961, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2627 - val_loss: 0.4896\n",
      "Epoch 9/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 1.0717\n",
      "Epoch 00009: val_loss improved from 0.48961 to 0.33221, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0621 - val_loss: 0.3322\n",
      "Epoch 10/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.9341\n",
      "Epoch 00010: val_loss improved from 0.33221 to 0.24534, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9253 - val_loss: 0.2453\n",
      "Epoch 11/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.8488\n",
      "Epoch 00011: val_loss improved from 0.24534 to 0.20020, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8540 - val_loss: 0.2002\n",
      "Epoch 12/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.8287\n",
      "Epoch 00012: val_loss improved from 0.20020 to 0.17258, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8208 - val_loss: 0.1726\n",
      "Epoch 13/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.7847\n",
      "Epoch 00013: val_loss improved from 0.17258 to 0.15542, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7795 - val_loss: 0.1554\n",
      "Epoch 14/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.7519\n",
      "Epoch 00014: val_loss improved from 0.15542 to 0.14287, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7584 - val_loss: 0.1429\n",
      "Epoch 15/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.7452\n",
      "Epoch 00015: val_loss did not improve from 0.14287\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7387 - val_loss: 0.1430\n",
      "Epoch 16/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.7214\n",
      "Epoch 00016: val_loss improved from 0.14287 to 0.13553, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7197 - val_loss: 0.1355\n",
      "Epoch 17/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.7158\n",
      "Epoch 00017: val_loss did not improve from 0.13553\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7138 - val_loss: 0.1382\n",
      "Epoch 18/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.6885\n",
      "Epoch 00018: val_loss improved from 0.13553 to 0.12998, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6914 - val_loss: 0.1300\n",
      "Epoch 19/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.6752\n",
      "Epoch 00019: val_loss improved from 0.12998 to 0.12934, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6760 - val_loss: 0.1293\n",
      "Epoch 20/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.6625\n",
      "Epoch 00020: val_loss improved from 0.12934 to 0.12911, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6565 - val_loss: 0.1291\n",
      "Epoch 21/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.6522\n",
      "Epoch 00021: val_loss improved from 0.12911 to 0.12377, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6533 - val_loss: 0.1238\n",
      "Epoch 22/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.6501\n",
      "Epoch 00022: val_loss did not improve from 0.12377\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6431 - val_loss: 0.1263\n",
      "Epoch 23/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.6333\n",
      "Epoch 00023: val_loss did not improve from 0.12377\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6334 - val_loss: 0.1245\n",
      " ###1 fold : val acc1 0.554, acc3 0.968, mae 0.241###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/70 [==========================>...] - ETA: 0s - loss: 22.0003\n",
      "Epoch 00001: val_loss improved from inf to 16.52874, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 21.5354 - val_loss: 16.5287\n",
      "Epoch 2/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 13.2032\n",
      "Epoch 00002: val_loss improved from 16.52874 to 9.12880, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 12.9031 - val_loss: 9.1288\n",
      "Epoch 3/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 7.2858\n",
      "Epoch 00003: val_loss improved from 9.12880 to 4.64543, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 7.0037 - val_loss: 4.6454\n",
      "Epoch 4/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 4.1434\n",
      "Epoch 00004: val_loss improved from 4.64543 to 2.73226, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 4.0746 - val_loss: 2.7323\n",
      "Epoch 5/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 2.9639\n",
      "Epoch 00005: val_loss improved from 2.73226 to 1.79362, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.8819 - val_loss: 1.7936\n",
      "Epoch 6/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 2.1648\n",
      "Epoch 00006: val_loss improved from 1.79362 to 1.17495, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.1077 - val_loss: 1.1750\n",
      "Epoch 7/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 1.6287\n",
      "Epoch 00007: val_loss improved from 1.17495 to 0.75517, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.5845 - val_loss: 0.7552\n",
      "Epoch 8/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 1.3098\n",
      "Epoch 00008: val_loss improved from 0.75517 to 0.48850, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2693 - val_loss: 0.4885\n",
      "Epoch 9/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 1.0759\n",
      "Epoch 00009: val_loss improved from 0.48850 to 0.33027, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0601 - val_loss: 0.3303\n",
      "Epoch 10/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.9456\n",
      "Epoch 00010: val_loss improved from 0.33027 to 0.24420, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9330 - val_loss: 0.2442\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8565\n",
      "Epoch 00011: val_loss improved from 0.24420 to 0.19893, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8565 - val_loss: 0.1989\n",
      "Epoch 12/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.8285\n",
      "Epoch 00012: val_loss improved from 0.19893 to 0.17292, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8186 - val_loss: 0.1729\n",
      "Epoch 13/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.7873\n",
      "Epoch 00013: val_loss improved from 0.17292 to 0.15661, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7772 - val_loss: 0.1566\n",
      "Epoch 14/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.7523\n",
      "Epoch 00014: val_loss improved from 0.15661 to 0.14369, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7588 - val_loss: 0.1437\n",
      "Epoch 15/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.7379\n",
      "Epoch 00015: val_loss did not improve from 0.14369\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7328 - val_loss: 0.1445\n",
      "Epoch 16/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7197\n",
      "Epoch 00016: val_loss improved from 0.14369 to 0.13675, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7196 - val_loss: 0.1368\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7100\n",
      "Epoch 00017: val_loss did not improve from 0.13675\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7100 - val_loss: 0.1386\n",
      "Epoch 18/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6980\n",
      "Epoch 00018: val_loss improved from 0.13675 to 0.13118, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6992 - val_loss: 0.1312\n",
      "Epoch 19/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.6678\n",
      "Epoch 00019: val_loss improved from 0.13118 to 0.12985, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6695 - val_loss: 0.1298\n",
      "Epoch 20/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.6567\n",
      "Epoch 00020: val_loss improved from 0.12985 to 0.12879, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6545 - val_loss: 0.1288\n",
      "Epoch 21/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6545\n",
      "Epoch 00021: val_loss improved from 0.12879 to 0.12453, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6515 - val_loss: 0.1245\n",
      "Epoch 22/100\n",
      "53/70 [=====================>........] - ETA: 0s - loss: 0.6479\n",
      "Epoch 00022: val_loss did not improve from 0.12453\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6435 - val_loss: 0.1270\n",
      "Epoch 23/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6344\n",
      "Epoch 00023: val_loss improved from 0.12453 to 0.12453, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6325 - val_loss: 0.1245\n",
      "Epoch 24/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6218\n",
      "Epoch 00024: val_loss improved from 0.12453 to 0.12306, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6115 - val_loss: 0.1231\n",
      "Epoch 25/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6045\n",
      "Epoch 00025: val_loss improved from 0.12306 to 0.12263, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6069 - val_loss: 0.1226\n",
      "Epoch 26/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6061\n",
      "Epoch 00026: val_loss did not improve from 0.12263\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6033 - val_loss: 0.1234\n",
      "Epoch 27/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6010\n",
      "Epoch 00027: val_loss improved from 0.12263 to 0.11992, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6007 - val_loss: 0.1199\n",
      "Epoch 28/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.5744\n",
      "Epoch 00028: val_loss did not improve from 0.11992\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5729 - val_loss: 0.1212\n",
      "Epoch 29/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.5672\n",
      "Epoch 00029: val_loss improved from 0.11992 to 0.11883, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5704 - val_loss: 0.1188\n",
      "Epoch 30/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.5768\n",
      "Epoch 00030: val_loss improved from 0.11883 to 0.11848, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5724 - val_loss: 0.1185\n",
      "Epoch 31/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.5628\n",
      "Epoch 00031: val_loss improved from 0.11848 to 0.11670, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5651 - val_loss: 0.1167\n",
      "Epoch 32/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.5612\n",
      "Epoch 00032: val_loss did not improve from 0.11670\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5576 - val_loss: 0.1217\n",
      "Epoch 33/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.5478\n",
      "Epoch 00033: val_loss improved from 0.11670 to 0.11310, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5507 - val_loss: 0.1131\n",
      "Epoch 34/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.5267\n",
      "Epoch 00034: val_loss did not improve from 0.11310\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5311 - val_loss: 0.1131\n",
      "Epoch 35/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.5449\n",
      "Epoch 00035: val_loss improved from 0.11310 to 0.11152, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5445 - val_loss: 0.1115\n",
      "Epoch 36/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.5133\n",
      "Epoch 00036: val_loss did not improve from 0.11152\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5143 - val_loss: 0.1134\n",
      "Epoch 37/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.5223\n",
      "Epoch 00037: val_loss did not improve from 0.11152\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5273 - val_loss: 0.1122\n",
      " ###2 fold : val acc1 0.572, acc3 0.968, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/70 [==========================>...] - ETA: 0s - loss: 22.0364\n",
      "Epoch 00001: val_loss improved from inf to 16.49442, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 21.5709 - val_loss: 16.4944\n",
      "Epoch 2/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 13.1236\n",
      "Epoch 00002: val_loss improved from 16.49442 to 9.09228, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 12.9115 - val_loss: 9.0923\n",
      "Epoch 3/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 7.2933\n",
      "Epoch 00003: val_loss improved from 9.09228 to 4.63080, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 7.0116 - val_loss: 4.6308\n",
      "Epoch 4/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 4.2187\n",
      "Epoch 00004: val_loss improved from 4.63080 to 2.73664, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 4.0913 - val_loss: 2.7366\n",
      "Epoch 5/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 2.9528\n",
      "Epoch 00005: val_loss improved from 2.73664 to 1.79691, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.8935 - val_loss: 1.7969\n",
      "Epoch 6/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 2.1799\n",
      "Epoch 00006: val_loss improved from 1.79691 to 1.17447, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.1192 - val_loss: 1.1745\n",
      "Epoch 7/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 1.6351\n",
      "Epoch 00007: val_loss improved from 1.17447 to 0.75268, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.5935 - val_loss: 0.7527\n",
      "Epoch 8/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 1.3028\n",
      "Epoch 00008: val_loss improved from 0.75268 to 0.48559, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2734 - val_loss: 0.4856\n",
      "Epoch 9/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 1.0681\n",
      "Epoch 00009: val_loss improved from 0.48559 to 0.32753, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0598 - val_loss: 0.3275\n",
      "Epoch 10/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.9445\n",
      "Epoch 00010: val_loss improved from 0.32753 to 0.24208, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9325 - val_loss: 0.2421\n",
      "Epoch 11/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.8635\n",
      "Epoch 00011: val_loss improved from 0.24208 to 0.19789, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8673 - val_loss: 0.1979\n",
      "Epoch 12/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8143\n",
      "Epoch 00012: val_loss improved from 0.19789 to 0.17144, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8149 - val_loss: 0.1714\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7745\n",
      "Epoch 00013: val_loss improved from 0.17144 to 0.15573, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7745 - val_loss: 0.1557\n",
      "Epoch 14/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.7523\n",
      "Epoch 00014: val_loss improved from 0.15573 to 0.14275, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7571 - val_loss: 0.1427\n",
      "Epoch 15/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.7329\n",
      "Epoch 00015: val_loss improved from 0.14275 to 0.14198, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7294 - val_loss: 0.1420\n",
      "Epoch 16/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.7144\n",
      "Epoch 00016: val_loss improved from 0.14198 to 0.13698, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7150 - val_loss: 0.1370\n",
      "Epoch 17/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.7211\n",
      "Epoch 00017: val_loss improved from 0.13698 to 0.13677, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7123 - val_loss: 0.1368\n",
      "Epoch 18/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.7037\n",
      "Epoch 00018: val_loss improved from 0.13677 to 0.13100, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7047 - val_loss: 0.1310\n",
      "Epoch 19/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6753\n",
      "Epoch 00019: val_loss improved from 0.13100 to 0.12878, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6768 - val_loss: 0.1288\n",
      "Epoch 20/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.6608\n",
      "Epoch 00020: val_loss improved from 0.12878 to 0.12792, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6603 - val_loss: 0.1279\n",
      "Epoch 21/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6576\n",
      "Epoch 00021: val_loss improved from 0.12792 to 0.12455, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6554 - val_loss: 0.1245\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6493\n",
      "Epoch 00022: val_loss did not improve from 0.12455\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6493 - val_loss: 0.1267\n",
      "Epoch 23/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6385\n",
      "Epoch 00023: val_loss improved from 0.12455 to 0.12407, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6385 - val_loss: 0.1241\n",
      "Epoch 24/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.6173\n",
      "Epoch 00024: val_loss improved from 0.12407 to 0.12225, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6122 - val_loss: 0.1223\n",
      "Epoch 25/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6103\n",
      "Epoch 00025: val_loss improved from 0.12225 to 0.12181, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6110 - val_loss: 0.1218\n",
      "Epoch 26/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6142\n",
      "Epoch 00026: val_loss did not improve from 0.12181\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6078 - val_loss: 0.1240\n",
      "Epoch 27/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6013\n",
      "Epoch 00027: val_loss improved from 0.12181 to 0.11963, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6011 - val_loss: 0.1196\n",
      "Epoch 28/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.5733\n",
      "Epoch 00028: val_loss improved from 0.11963 to 0.11951, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5728 - val_loss: 0.1195\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5700\n",
      "Epoch 00029: val_loss improved from 0.11951 to 0.11772, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5700 - val_loss: 0.1177\n",
      "Epoch 30/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.5808\n",
      "Epoch 00030: val_loss did not improve from 0.11772\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5774 - val_loss: 0.1202\n",
      "Epoch 31/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.5635\n",
      "Epoch 00031: val_loss improved from 0.11772 to 0.11727, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5681 - val_loss: 0.1173\n",
      "Epoch 32/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.5647\n",
      "Epoch 00032: val_loss did not improve from 0.11727\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5620 - val_loss: 0.1196\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5516\n",
      "Epoch 00033: val_loss improved from 0.11727 to 0.11319, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5516 - val_loss: 0.1132\n",
      "Epoch 34/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.5318\n",
      "Epoch 00034: val_loss improved from 0.11319 to 0.11213, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5343 - val_loss: 0.1121\n",
      "Epoch 35/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.5428\n",
      "Epoch 00035: val_loss improved from 0.11213 to 0.11161, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5434 - val_loss: 0.1116\n",
      "Epoch 36/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.5179\n",
      "Epoch 00036: val_loss did not improve from 0.11161\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5186 - val_loss: 0.1140\n",
      "Epoch 37/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.5240\n",
      "Epoch 00037: val_loss improved from 0.11161 to 0.11132, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5300 - val_loss: 0.1113\n",
      "Epoch 38/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.5196\n",
      "Epoch 00038: val_loss did not improve from 0.11132\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5197 - val_loss: 0.1156\n",
      "Epoch 39/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.5132\n",
      "Epoch 00039: val_loss improved from 0.11132 to 0.10917, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5103 - val_loss: 0.1092\n",
      "Epoch 40/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.5068\n",
      "Epoch 00040: val_loss did not improve from 0.10917\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5111 - val_loss: 0.1119\n",
      "Epoch 41/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.5005\n",
      "Epoch 00041: val_loss improved from 0.10917 to 0.10786, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4996 - val_loss: 0.1079\n",
      "Epoch 42/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.5003\n",
      "Epoch 00042: val_loss improved from 0.10786 to 0.10736, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5016 - val_loss: 0.1074\n",
      "Epoch 43/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.4939\n",
      "Epoch 00043: val_loss did not improve from 0.10736\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4939 - val_loss: 0.1078\n",
      "Epoch 44/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.4876\n",
      "Epoch 00044: val_loss did not improve from 0.10736\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4912 - val_loss: 0.1088\n",
      " ###3 fold : val acc1 0.600, acc3 0.967, mae 0.217###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/70 [=========================>....] - ETA: 0s - loss: 21.9956\n",
      "Epoch 00001: val_loss improved from inf to 16.45220, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 21.4818 - val_loss: 16.4522\n",
      "Epoch 2/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 13.2984\n",
      "Epoch 00002: val_loss improved from 16.45220 to 9.07478, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 12.8736 - val_loss: 9.0748\n",
      "Epoch 3/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 7.2364\n",
      "Epoch 00003: val_loss improved from 9.07478 to 4.62120, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 6.9927 - val_loss: 4.6212\n",
      "Epoch 4/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 4.1865\n",
      "Epoch 00004: val_loss improved from 4.62120 to 2.70649, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 4.0185 - val_loss: 2.7065\n",
      "Epoch 5/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 2.9056\n",
      "Epoch 00005: val_loss improved from 2.70649 to 1.77783, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.8304 - val_loss: 1.7778\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 2.1299\n",
      "Epoch 00006: val_loss improved from 1.77783 to 1.16372, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.1299 - val_loss: 1.1637\n",
      "Epoch 7/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 1.6646\n",
      "Epoch 00007: val_loss improved from 1.16372 to 0.74483, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.6106 - val_loss: 0.7448\n",
      "Epoch 8/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 1.2952\n",
      "Epoch 00008: val_loss improved from 0.74483 to 0.48051, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2509 - val_loss: 0.4805\n",
      "Epoch 9/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 1.0859\n",
      "Epoch 00009: val_loss improved from 0.48051 to 0.32401, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0658 - val_loss: 0.3240\n",
      "Epoch 10/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.9473\n",
      "Epoch 00010: val_loss improved from 0.32401 to 0.24053, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9337 - val_loss: 0.2405\n",
      "Epoch 11/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.8826\n",
      "Epoch 00011: val_loss improved from 0.24053 to 0.19412, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8707 - val_loss: 0.1941\n",
      "Epoch 12/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.8168\n",
      "Epoch 00012: val_loss improved from 0.19412 to 0.16680, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8146 - val_loss: 0.1668\n",
      "Epoch 13/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.7716\n",
      "Epoch 00013: val_loss improved from 0.16680 to 0.15238, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7731 - val_loss: 0.1524\n",
      "Epoch 14/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.7574\n",
      "Epoch 00014: val_loss improved from 0.15238 to 0.14371, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7591 - val_loss: 0.1437\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7328\n",
      "Epoch 00015: val_loss improved from 0.14371 to 0.14120, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7328 - val_loss: 0.1412\n",
      "Epoch 16/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.7162\n",
      "Epoch 00016: val_loss improved from 0.14120 to 0.13621, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7130 - val_loss: 0.1362\n",
      "Epoch 17/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.7108\n",
      "Epoch 00017: val_loss improved from 0.13621 to 0.13288, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7152 - val_loss: 0.1329\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6793\n",
      "Epoch 00018: val_loss improved from 0.13288 to 0.13197, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6793 - val_loss: 0.1320\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6698\n",
      "Epoch 00019: val_loss improved from 0.13197 to 0.12991, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6698 - val_loss: 0.1299\n",
      "Epoch 20/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6727\n",
      "Epoch 00020: val_loss improved from 0.12991 to 0.12677, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6686 - val_loss: 0.1268\n",
      "Epoch 21/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6357\n",
      "Epoch 00021: val_loss did not improve from 0.12677\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6327 - val_loss: 0.1279\n",
      "Epoch 22/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6422\n",
      "Epoch 00022: val_loss did not improve from 0.12677\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6414 - val_loss: 0.1269\n",
      " ###4 fold : val acc1 0.538, acc3 0.949, mae 0.256###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/70 [=========================>....] - ETA: 0s - loss: 22.0859\n",
      "Epoch 00001: val_loss improved from inf to 16.44853, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 21.5000 - val_loss: 16.4485\n",
      "Epoch 2/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 13.3187\n",
      "Epoch 00002: val_loss improved from 16.44853 to 9.06553, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 12.8850 - val_loss: 9.0655\n",
      "Epoch 3/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 7.3849\n",
      "Epoch 00003: val_loss improved from 9.06553 to 4.61265, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 6.9969 - val_loss: 4.6127\n",
      "Epoch 4/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 4.0325\n",
      "Epoch 00004: val_loss improved from 4.61265 to 2.69522, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 4.0164 - val_loss: 2.6952\n",
      "Epoch 5/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 2.8857\n",
      "Epoch 00005: val_loss improved from 2.69522 to 1.77426, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.8122 - val_loss: 1.7743\n",
      "Epoch 6/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 2.1982\n",
      "Epoch 00006: val_loss improved from 1.77426 to 1.16522, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.1350 - val_loss: 1.1652\n",
      "Epoch 7/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 1.6425\n",
      "Epoch 00007: val_loss improved from 1.16522 to 0.74767, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.5945 - val_loss: 0.7477\n",
      "Epoch 8/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 1.2835\n",
      "Epoch 00008: val_loss improved from 0.74767 to 0.48574, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2475 - val_loss: 0.4857\n",
      "Epoch 9/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 1.0763\n",
      "Epoch 00009: val_loss improved from 0.48574 to 0.32644, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0713 - val_loss: 0.3264\n",
      "Epoch 10/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.9430\n",
      "Epoch 00010: val_loss improved from 0.32644 to 0.24158, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9314 - val_loss: 0.2416\n",
      "Epoch 11/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.8775\n",
      "Epoch 00011: val_loss improved from 0.24158 to 0.19485, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8714 - val_loss: 0.1949\n",
      "Epoch 12/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.8101\n",
      "Epoch 00012: val_loss improved from 0.19485 to 0.16731, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8118 - val_loss: 0.1673\n",
      "Epoch 13/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.7743\n",
      "Epoch 00013: val_loss improved from 0.16731 to 0.15266, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7736 - val_loss: 0.1527\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7539\n",
      "Epoch 00014: val_loss improved from 0.15266 to 0.14319, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7539 - val_loss: 0.1432\n",
      "Epoch 15/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.7355\n",
      "Epoch 00015: val_loss improved from 0.14319 to 0.14170, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7349 - val_loss: 0.1417\n",
      "Epoch 16/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.7167\n",
      "Epoch 00016: val_loss improved from 0.14170 to 0.13557, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7151 - val_loss: 0.1356\n",
      "Epoch 17/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7058\n",
      "Epoch 00017: val_loss improved from 0.13557 to 0.13209, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7063 - val_loss: 0.1321\n",
      "Epoch 18/100\n",
      "53/70 [=====================>........] - ETA: 0s - loss: 0.6780\n",
      "Epoch 00018: val_loss did not improve from 0.13209\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6761 - val_loss: 0.1324\n",
      "Epoch 19/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.6737\n",
      "Epoch 00019: val_loss improved from 0.13209 to 0.13030, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6692 - val_loss: 0.1303\n",
      "Epoch 20/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6609\n",
      "Epoch 00020: val_loss improved from 0.13030 to 0.12798, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6587 - val_loss: 0.1280\n",
      "Epoch 21/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6394\n",
      "Epoch 00021: val_loss did not improve from 0.12798\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6384 - val_loss: 0.1281\n",
      "Epoch 22/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6340\n",
      "Epoch 00022: val_loss improved from 0.12798 to 0.12674, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6333 - val_loss: 0.1267\n",
      "Epoch 23/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6296\n",
      "Epoch 00023: val_loss improved from 0.12674 to 0.12532, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6294 - val_loss: 0.1253\n",
      "Epoch 24/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6150\n",
      "Epoch 00024: val_loss improved from 0.12532 to 0.12258, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6141 - val_loss: 0.1226\n",
      "Epoch 25/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6196\n",
      "Epoch 00025: val_loss did not improve from 0.12258\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6183 - val_loss: 0.1242\n",
      "Epoch 26/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6073\n",
      "Epoch 00026: val_loss improved from 0.12258 to 0.12124, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6068 - val_loss: 0.1212\n",
      "Epoch 27/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.5925\n",
      "Epoch 00027: val_loss did not improve from 0.12124\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5853 - val_loss: 0.1219\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5708\n",
      "Epoch 00028: val_loss improved from 0.12124 to 0.12086, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5708 - val_loss: 0.1209\n",
      "Epoch 29/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.5857\n",
      "Epoch 00029: val_loss improved from 0.12086 to 0.11708, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5826 - val_loss: 0.1171\n",
      "Epoch 30/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.5589\n",
      "Epoch 00030: val_loss did not improve from 0.11708\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5579 - val_loss: 0.1182\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5684\n",
      "Epoch 00031: val_loss did not improve from 0.11708\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5684 - val_loss: 0.1206\n",
      " ###5 fold : val acc1 0.555, acc3 0.958, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 21.6815\n",
      "Epoch 00001: val_loss improved from inf to 16.46041, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 21.5068 - val_loss: 16.4604\n",
      "Epoch 2/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 13.2541\n",
      "Epoch 00002: val_loss improved from 16.46041 to 9.07392, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 12.8856 - val_loss: 9.0739\n",
      "Epoch 3/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 7.1614\n",
      "Epoch 00003: val_loss improved from 9.07392 to 4.62471, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 6.9934 - val_loss: 4.6247\n",
      "Epoch 4/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 4.1078\n",
      "Epoch 00004: val_loss improved from 4.62471 to 2.70468, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 4.0310 - val_loss: 2.7047\n",
      "Epoch 5/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 2.8746\n",
      "Epoch 00005: val_loss improved from 2.70468 to 1.77464, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.8205 - val_loss: 1.7746\n",
      "Epoch 6/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 2.1625\n",
      "Epoch 00006: val_loss improved from 1.77464 to 1.16243, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.1373 - val_loss: 1.1624\n",
      "Epoch 7/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 1.6119\n",
      "Epoch 00007: val_loss improved from 1.16243 to 0.74451, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.5893 - val_loss: 0.7445\n",
      "Epoch 8/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 1.2770\n",
      "Epoch 00008: val_loss improved from 0.74451 to 0.48358, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2509 - val_loss: 0.4836\n",
      "Epoch 9/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 1.0887\n",
      "Epoch 00009: val_loss improved from 0.48358 to 0.32468, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0771 - val_loss: 0.3247\n",
      "Epoch 10/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.9385\n",
      "Epoch 00010: val_loss improved from 0.32468 to 0.23887, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9309 - val_loss: 0.2389\n",
      "Epoch 11/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.8733\n",
      "Epoch 00011: val_loss improved from 0.23887 to 0.19491, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8645 - val_loss: 0.1949\n",
      "Epoch 12/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.8144\n",
      "Epoch 00012: val_loss improved from 0.19491 to 0.16714, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8141 - val_loss: 0.1671\n",
      "Epoch 13/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.7773\n",
      "Epoch 00013: val_loss improved from 0.16714 to 0.15088, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7768 - val_loss: 0.1509\n",
      "Epoch 14/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.7575\n",
      "Epoch 00014: val_loss improved from 0.15088 to 0.14253, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7558 - val_loss: 0.1425\n",
      "Epoch 15/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.7405\n",
      "Epoch 00015: val_loss improved from 0.14253 to 0.14114, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7370 - val_loss: 0.1411\n",
      "Epoch 16/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.7135\n",
      "Epoch 00016: val_loss improved from 0.14114 to 0.13604, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7136 - val_loss: 0.1360\n",
      "Epoch 17/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.7055\n",
      "Epoch 00017: val_loss improved from 0.13604 to 0.13162, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7056 - val_loss: 0.1316\n",
      "Epoch 18/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.6823\n",
      "Epoch 00018: val_loss improved from 0.13162 to 0.13146, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6824 - val_loss: 0.1315\n",
      "Epoch 19/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.6777\n",
      "Epoch 00019: val_loss improved from 0.13146 to 0.13023, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6760 - val_loss: 0.1302\n",
      "Epoch 20/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6632\n",
      "Epoch 00020: val_loss improved from 0.13023 to 0.12743, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6611 - val_loss: 0.1274\n",
      "Epoch 21/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.6454\n",
      "Epoch 00021: val_loss did not improve from 0.12743\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6453 - val_loss: 0.1275\n",
      "Epoch 22/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.6418\n",
      "Epoch 00022: val_loss improved from 0.12743 to 0.12647, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6335 - val_loss: 0.1265\n",
      "Epoch 23/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6319\n",
      "Epoch 00023: val_loss improved from 0.12647 to 0.12409, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6347 - val_loss: 0.1241\n",
      "Epoch 24/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.6189\n",
      "Epoch 00024: val_loss improved from 0.12409 to 0.12275, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6201 - val_loss: 0.1227\n",
      "Epoch 25/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6200\n",
      "Epoch 00025: val_loss did not improve from 0.12275\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6192 - val_loss: 0.1229\n",
      "Epoch 26/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.6097\n",
      "Epoch 00026: val_loss improved from 0.12275 to 0.12079, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6073 - val_loss: 0.1208\n",
      "Epoch 27/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.5947\n",
      "Epoch 00027: val_loss did not improve from 0.12079\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5883 - val_loss: 0.1220\n",
      "Epoch 28/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.5666\n",
      "Epoch 00028: val_loss improved from 0.12079 to 0.12001, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5704 - val_loss: 0.1200\n",
      "Epoch 29/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.5904\n",
      "Epoch 00029: val_loss improved from 0.12001 to 0.11677, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5883 - val_loss: 0.1168\n",
      "Epoch 30/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.5678\n",
      "Epoch 00030: val_loss did not improve from 0.11677\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5640 - val_loss: 0.1184\n",
      "Epoch 31/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.5622\n",
      "Epoch 00031: val_loss did not improve from 0.11677\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5694 - val_loss: 0.1185\n",
      " ###6 fold : val acc1 0.562, acc3 0.968, mae 0.235###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/70 [=========================>....] - ETA: 0s - loss: 22.0976\n",
      "Epoch 00001: val_loss improved from inf to 16.44644, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 21.5085 - val_loss: 16.4464\n",
      "Epoch 2/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 13.2815\n",
      "Epoch 00002: val_loss improved from 16.44644 to 9.06367, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 12.8811 - val_loss: 9.0637\n",
      "Epoch 3/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 7.4233\n",
      "Epoch 00003: val_loss improved from 9.06367 to 4.62435, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 6.9990 - val_loss: 4.6244\n",
      "Epoch 4/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 4.2209\n",
      "Epoch 00004: val_loss improved from 4.62435 to 2.70967, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 4.0488 - val_loss: 2.7097\n",
      "Epoch 5/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 2.9175\n",
      "Epoch 00005: val_loss improved from 2.70967 to 1.77426, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.8292 - val_loss: 1.7743\n",
      "Epoch 6/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 2.1781\n",
      "Epoch 00006: val_loss improved from 1.77426 to 1.16581, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.1383 - val_loss: 1.1658\n",
      "Epoch 7/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 1.6149\n",
      "Epoch 00007: val_loss improved from 1.16581 to 0.74729, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.5905 - val_loss: 0.7473\n",
      "Epoch 8/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 1.2764\n",
      "Epoch 00008: val_loss improved from 0.74729 to 0.48571, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2545 - val_loss: 0.4857\n",
      "Epoch 9/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 1.0854\n",
      "Epoch 00009: val_loss improved from 0.48571 to 0.32522, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0722 - val_loss: 0.3252\n",
      "Epoch 10/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.9344\n",
      "Epoch 00010: val_loss improved from 0.32522 to 0.23898, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9220 - val_loss: 0.2390\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8571\n",
      "Epoch 00011: val_loss improved from 0.23898 to 0.19606, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8571 - val_loss: 0.1961\n",
      "Epoch 12/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.8051\n",
      "Epoch 00012: val_loss improved from 0.19606 to 0.16766, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8081 - val_loss: 0.1677\n",
      "Epoch 13/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.7733\n",
      "Epoch 00013: val_loss improved from 0.16766 to 0.15202, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7708 - val_loss: 0.1520\n",
      "Epoch 14/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7513\n",
      "Epoch 00014: val_loss improved from 0.15202 to 0.14340, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7508 - val_loss: 0.1434\n",
      "Epoch 15/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7343\n",
      "Epoch 00015: val_loss improved from 0.14340 to 0.14074, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7334 - val_loss: 0.1407\n",
      "Epoch 16/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.7156\n",
      "Epoch 00016: val_loss improved from 0.14074 to 0.13647, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7122 - val_loss: 0.1365\n",
      "Epoch 17/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.7130\n",
      "Epoch 00017: val_loss improved from 0.13647 to 0.13241, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7101 - val_loss: 0.1324\n",
      "Epoch 18/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.6887\n",
      "Epoch 00018: val_loss improved from 0.13241 to 0.13209, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6875 - val_loss: 0.1321\n",
      "Epoch 19/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6809\n",
      "Epoch 00019: val_loss improved from 0.13209 to 0.13204, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6758 - val_loss: 0.1320\n",
      "Epoch 20/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6629\n",
      "Epoch 00020: val_loss improved from 0.13204 to 0.12917, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6614 - val_loss: 0.1292\n",
      "Epoch 21/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.6416\n",
      "Epoch 00021: val_loss improved from 0.12917 to 0.12798, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6440 - val_loss: 0.1280\n",
      "Epoch 22/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.6471\n",
      "Epoch 00022: val_loss improved from 0.12798 to 0.12754, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6397 - val_loss: 0.1275\n",
      "Epoch 23/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6294\n",
      "Epoch 00023: val_loss improved from 0.12754 to 0.12492, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6324 - val_loss: 0.1249\n",
      "Epoch 24/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6164\n",
      "Epoch 00024: val_loss improved from 0.12492 to 0.12403, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6195 - val_loss: 0.1240\n",
      "Epoch 25/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6127\n",
      "Epoch 00025: val_loss improved from 0.12403 to 0.12301, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6124 - val_loss: 0.1230\n",
      "Epoch 26/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.6058\n",
      "Epoch 00026: val_loss improved from 0.12301 to 0.12133, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6048 - val_loss: 0.1213\n",
      "Epoch 27/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.5948\n",
      "Epoch 00027: val_loss did not improve from 0.12133\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5896 - val_loss: 0.1227\n",
      "Epoch 28/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.5657\n",
      "Epoch 00028: val_loss improved from 0.12133 to 0.12091, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5702 - val_loss: 0.1209\n",
      "Epoch 29/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.5896\n",
      "Epoch 00029: val_loss improved from 0.12091 to 0.11785, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5870 - val_loss: 0.1179\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5594\n",
      "Epoch 00030: val_loss did not improve from 0.11785\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5594 - val_loss: 0.1196\n",
      "Epoch 31/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5626\n",
      "Epoch 00031: val_loss did not improve from 0.11785\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5638 - val_loss: 0.1192\n",
      " ###7 fold : val acc1 0.549, acc3 0.953, mae 0.250###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/70 [=======================>......] - ETA: 0s - loss: 22.3458\n",
      "Epoch 00001: val_loss improved from inf to 16.43527, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 21.5429 - val_loss: 16.4353\n",
      "Epoch 2/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 13.3064\n",
      "Epoch 00002: val_loss improved from 16.43527 to 9.04237, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 12.9087 - val_loss: 9.0424\n",
      "Epoch 3/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 7.2499\n",
      "Epoch 00003: val_loss improved from 9.04237 to 4.60091, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 7.0091 - val_loss: 4.6009\n",
      "Epoch 4/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 4.2263\n",
      "Epoch 00004: val_loss improved from 4.60091 to 2.70548, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 4.0610 - val_loss: 2.7055\n",
      "Epoch 5/100\n",
      "53/70 [=====================>........] - ETA: 0s - loss: 2.9232\n",
      "Epoch 00005: val_loss improved from 2.70548 to 1.77990, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.8273 - val_loss: 1.7799\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 2.1352\n",
      "Epoch 00006: val_loss improved from 1.77990 to 1.17828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.1352 - val_loss: 1.1783\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.5939\n",
      "Epoch 00007: val_loss improved from 1.17828 to 0.75848, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.5939 - val_loss: 0.7585\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2603\n",
      "Epoch 00008: val_loss improved from 0.75848 to 0.49582, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2603 - val_loss: 0.4958\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0712\n",
      "Epoch 00009: val_loss improved from 0.49582 to 0.33349, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0712 - val_loss: 0.3335\n",
      "Epoch 10/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.9331\n",
      "Epoch 00010: val_loss improved from 0.33349 to 0.24636, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9218 - val_loss: 0.2464\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8581\n",
      "Epoch 00011: val_loss improved from 0.24636 to 0.20312, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8581 - val_loss: 0.2031\n",
      "Epoch 12/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.8104\n",
      "Epoch 00012: val_loss improved from 0.20312 to 0.17472, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8110 - val_loss: 0.1747\n",
      "Epoch 13/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7759\n",
      "Epoch 00013: val_loss improved from 0.17472 to 0.15815, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7746 - val_loss: 0.1581\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7501\n",
      "Epoch 00014: val_loss improved from 0.15815 to 0.14904, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7501 - val_loss: 0.1490\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7348\n",
      "Epoch 00015: val_loss improved from 0.14904 to 0.14613, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7348 - val_loss: 0.1461\n",
      "Epoch 16/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.7195\n",
      "Epoch 00016: val_loss improved from 0.14613 to 0.14164, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7154 - val_loss: 0.1416\n",
      "Epoch 17/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.7171\n",
      "Epoch 00017: val_loss improved from 0.14164 to 0.13755, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7138 - val_loss: 0.1376\n",
      "Epoch 18/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6860\n",
      "Epoch 00018: val_loss improved from 0.13755 to 0.13690, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6874 - val_loss: 0.1369\n",
      "Epoch 19/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.6776\n",
      "Epoch 00019: val_loss improved from 0.13690 to 0.13676, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6750 - val_loss: 0.1368\n",
      "Epoch 20/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.6638\n",
      "Epoch 00020: val_loss improved from 0.13676 to 0.13384, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6615 - val_loss: 0.1338\n",
      "Epoch 21/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6427\n",
      "Epoch 00021: val_loss improved from 0.13384 to 0.13250, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6444 - val_loss: 0.1325\n",
      "Epoch 22/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6423\n",
      "Epoch 00022: val_loss improved from 0.13250 to 0.13196, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6418 - val_loss: 0.1320\n",
      "Epoch 23/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6318\n",
      "Epoch 00023: val_loss improved from 0.13196 to 0.12922, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6346 - val_loss: 0.1292\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6214\n",
      "Epoch 00024: val_loss improved from 0.12922 to 0.12861, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6214 - val_loss: 0.1286\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6127\n",
      "Epoch 00025: val_loss improved from 0.12861 to 0.12764, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6127 - val_loss: 0.1276\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6059\n",
      "Epoch 00026: val_loss improved from 0.12764 to 0.12528, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6059 - val_loss: 0.1253\n",
      "Epoch 27/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.5969\n",
      "Epoch 00027: val_loss did not improve from 0.12528\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5894 - val_loss: 0.1271\n",
      "Epoch 28/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.5681\n",
      "Epoch 00028: val_loss improved from 0.12528 to 0.12496, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5698 - val_loss: 0.1250\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5878\n",
      "Epoch 00029: val_loss improved from 0.12496 to 0.12221, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5878 - val_loss: 0.1222\n",
      "Epoch 30/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.5686\n",
      "Epoch 00030: val_loss did not improve from 0.12221\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5615 - val_loss: 0.1238\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5651\n",
      "Epoch 00031: val_loss did not improve from 0.12221\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5651 - val_loss: 0.1231\n",
      " ###8 fold : val acc1 0.551, acc3 0.958, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/70 [=========================>....] - ETA: 0s - loss: 22.1373\n",
      "Epoch 00001: val_loss improved from inf to 16.36841, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 21.5429 - val_loss: 16.3684\n",
      "Epoch 2/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 13.3064\n",
      "Epoch 00002: val_loss improved from 16.36841 to 9.02749, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 12.9087 - val_loss: 9.0275\n",
      "Epoch 3/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 7.3134\n",
      "Epoch 00003: val_loss improved from 9.02749 to 4.60343, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 7.0091 - val_loss: 4.6034\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 4.0610\n",
      "Epoch 00004: val_loss improved from 4.60343 to 2.70761, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 4.0610 - val_loss: 2.7076\n",
      "Epoch 5/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 2.9202\n",
      "Epoch 00005: val_loss improved from 2.70761 to 1.78156, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.8273 - val_loss: 1.7816\n",
      "Epoch 6/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 2.1778\n",
      "Epoch 00006: val_loss improved from 1.78156 to 1.18046, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.1352 - val_loss: 1.1805\n",
      "Epoch 7/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 1.6325\n",
      "Epoch 00007: val_loss improved from 1.18046 to 0.76180, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.5939 - val_loss: 0.7618\n",
      "Epoch 8/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 1.2829\n",
      "Epoch 00008: val_loss improved from 0.76180 to 0.50039, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2603 - val_loss: 0.5004\n",
      "Epoch 9/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 1.0838\n",
      "Epoch 00009: val_loss improved from 0.50039 to 0.33821, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0712 - val_loss: 0.3382\n",
      "Epoch 10/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.9331\n",
      "Epoch 00010: val_loss improved from 0.33821 to 0.25134, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9218 - val_loss: 0.2513\n",
      "Epoch 11/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.8724\n",
      "Epoch 00011: val_loss improved from 0.25134 to 0.20809, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8581 - val_loss: 0.2081\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8110\n",
      "Epoch 00012: val_loss improved from 0.20809 to 0.17954, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8110 - val_loss: 0.1795\n",
      "Epoch 13/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.7743\n",
      "Epoch 00013: val_loss improved from 0.17954 to 0.16269, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7746 - val_loss: 0.1627\n",
      "Epoch 14/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.7505\n",
      "Epoch 00014: val_loss improved from 0.16269 to 0.15358, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7501 - val_loss: 0.1536\n",
      "Epoch 15/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.7362\n",
      "Epoch 00015: val_loss improved from 0.15358 to 0.15118, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7348 - val_loss: 0.1512\n",
      "Epoch 16/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.7169\n",
      "Epoch 00016: val_loss improved from 0.15118 to 0.14636, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7154 - val_loss: 0.1464\n",
      "Epoch 17/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.7169\n",
      "Epoch 00017: val_loss improved from 0.14636 to 0.14211, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7138 - val_loss: 0.1421\n",
      "Epoch 18/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6860\n",
      "Epoch 00018: val_loss improved from 0.14211 to 0.14143, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6874 - val_loss: 0.1414\n",
      "Epoch 19/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.6792\n",
      "Epoch 00019: val_loss did not improve from 0.14143\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6750 - val_loss: 0.1417\n",
      "Epoch 20/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.6631\n",
      "Epoch 00020: val_loss improved from 0.14143 to 0.13835, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6615 - val_loss: 0.1384\n",
      "Epoch 21/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6427\n",
      "Epoch 00021: val_loss improved from 0.13835 to 0.13716, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6444 - val_loss: 0.1372\n",
      "Epoch 22/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6480\n",
      "Epoch 00022: val_loss improved from 0.13716 to 0.13670, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6418 - val_loss: 0.1367\n",
      "Epoch 23/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6314\n",
      "Epoch 00023: val_loss improved from 0.13670 to 0.13367, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6346 - val_loss: 0.1337\n",
      "Epoch 24/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6172\n",
      "Epoch 00024: val_loss improved from 0.13367 to 0.13324, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6214 - val_loss: 0.1332\n",
      "Epoch 25/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6141\n",
      "Epoch 00025: val_loss improved from 0.13324 to 0.13206, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6127 - val_loss: 0.1321\n",
      "Epoch 26/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6085\n",
      "Epoch 00026: val_loss improved from 0.13206 to 0.12932, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6059 - val_loss: 0.1293\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5894\n",
      "Epoch 00027: val_loss did not improve from 0.12932\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5894 - val_loss: 0.1314\n",
      "Epoch 28/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.5663\n",
      "Epoch 00028: val_loss improved from 0.12932 to 0.12910, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5698 - val_loss: 0.1291\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5878\n",
      "Epoch 00029: val_loss improved from 0.12910 to 0.12627, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5878 - val_loss: 0.1263\n",
      "Epoch 30/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.5675\n",
      "Epoch 00030: val_loss did not improve from 0.12627\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5615 - val_loss: 0.1279\n",
      "Epoch 31/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.5591\n",
      "Epoch 00031: val_loss did not improve from 0.12627\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5651 - val_loss: 0.1272\n",
      " ###9 fold : val acc1 0.570, acc3 0.970, mae 0.231###\n",
      "acc10.560_acc30.962\n",
      "random search 18/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/35 [=================>............] - ETA: 0s - loss: 19.7760 \n",
      "Epoch 00001: val_loss improved from inf to 13.54592, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 18.1388 - val_loss: 13.5459\n",
      "Epoch 2/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 11.3868\n",
      "Epoch 00002: val_loss improved from 13.54592 to 6.80058, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.1132 - val_loss: 6.8006\n",
      "Epoch 3/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 5.6321\n",
      "Epoch 00003: val_loss improved from 6.80058 to 3.59274, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.0399 - val_loss: 3.5927\n",
      "Epoch 4/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.2532\n",
      "Epoch 00004: val_loss improved from 3.59274 to 2.27677, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9940 - val_loss: 2.2768\n",
      "Epoch 5/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.0746\n",
      "Epoch 00005: val_loss improved from 2.27677 to 1.34736, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.8914 - val_loss: 1.3474\n",
      "Epoch 6/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.2234\n",
      "Epoch 00006: val_loss improved from 1.34736 to 0.77063, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.1221 - val_loss: 0.7706\n",
      "Epoch 7/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7453\n",
      "Epoch 00007: val_loss improved from 0.77063 to 0.45055, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6779 - val_loss: 0.4505\n",
      "Epoch 8/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.4649\n",
      "Epoch 00008: val_loss improved from 0.45055 to 0.28222, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4354 - val_loss: 0.2822\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3220\n",
      "Epoch 00009: val_loss improved from 0.28222 to 0.19460, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3091 - val_loss: 0.1946\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2618\n",
      "Epoch 00010: val_loss improved from 0.19460 to 0.15100, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2507 - val_loss: 0.1510\n",
      "Epoch 11/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2200\n",
      "Epoch 00011: val_loss improved from 0.15100 to 0.13193, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2164 - val_loss: 0.1319\n",
      "Epoch 12/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2066\n",
      "Epoch 00012: val_loss improved from 0.13193 to 0.12308, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2008 - val_loss: 0.1231\n",
      "Epoch 13/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1948\n",
      "Epoch 00013: val_loss improved from 0.12308 to 0.11901, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1961 - val_loss: 0.1190\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1973\n",
      "Epoch 00014: val_loss improved from 0.11901 to 0.11699, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1954 - val_loss: 0.1170\n",
      "Epoch 15/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1887\n",
      "Epoch 00015: val_loss improved from 0.11699 to 0.11538, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1888 - val_loss: 0.1154\n",
      "Epoch 16/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1922\n",
      "Epoch 00016: val_loss improved from 0.11538 to 0.11423, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1899 - val_loss: 0.1142\n",
      "Epoch 17/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1939\n",
      "Epoch 00017: val_loss improved from 0.11423 to 0.11323, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1901 - val_loss: 0.1132\n",
      "Epoch 18/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1839\n",
      "Epoch 00018: val_loss improved from 0.11323 to 0.11233, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1830 - val_loss: 0.1123\n",
      "Epoch 19/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1817\n",
      "Epoch 00019: val_loss improved from 0.11233 to 0.11156, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1823 - val_loss: 0.1116\n",
      "Epoch 20/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1829\n",
      "Epoch 00020: val_loss improved from 0.11156 to 0.11078, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1838 - val_loss: 0.1108\n",
      "Epoch 21/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1805\n",
      "Epoch 00021: val_loss improved from 0.11078 to 0.11009, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1823 - val_loss: 0.1101\n",
      "Epoch 22/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1857\n",
      "Epoch 00022: val_loss improved from 0.11009 to 0.10930, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1813 - val_loss: 0.1093\n",
      "Epoch 23/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1821\n",
      "Epoch 00023: val_loss improved from 0.10930 to 0.10890, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1810 - val_loss: 0.1089\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1763\n",
      "Epoch 00024: val_loss improved from 0.10890 to 0.10796, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1767 - val_loss: 0.1080\n",
      "Epoch 25/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1783\n",
      "Epoch 00025: val_loss improved from 0.10796 to 0.10739, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1760 - val_loss: 0.1074\n",
      "Epoch 26/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1740\n",
      "Epoch 00026: val_loss improved from 0.10739 to 0.10652, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1724 - val_loss: 0.1065\n",
      "Epoch 27/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1784\n",
      "Epoch 00027: val_loss improved from 0.10652 to 0.10600, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1744 - val_loss: 0.1060\n",
      "Epoch 28/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1673\n",
      "Epoch 00028: val_loss improved from 0.10600 to 0.10555, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1712 - val_loss: 0.1055\n",
      "Epoch 29/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1695\n",
      "Epoch 00029: val_loss improved from 0.10555 to 0.10496, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1706 - val_loss: 0.1050\n",
      "Epoch 30/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1721\n",
      "Epoch 00030: val_loss did not improve from 0.10496\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1703 - val_loss: 0.1050\n",
      "Epoch 31/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1678\n",
      "Epoch 00031: val_loss improved from 0.10496 to 0.10382, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1681 - val_loss: 0.1038\n",
      "Epoch 32/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1712\n",
      "Epoch 00032: val_loss improved from 0.10382 to 0.10346, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1693 - val_loss: 0.1035\n",
      "Epoch 33/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1664\n",
      "Epoch 00033: val_loss improved from 0.10346 to 0.10293, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1667 - val_loss: 0.1029\n",
      "Epoch 34/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1656\n",
      "Epoch 00034: val_loss improved from 0.10293 to 0.10264, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1653 - val_loss: 0.1026\n",
      "Epoch 35/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1650\n",
      "Epoch 00035: val_loss improved from 0.10264 to 0.10202, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1657 - val_loss: 0.1020\n",
      "Epoch 36/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1637\n",
      "Epoch 00036: val_loss improved from 0.10202 to 0.10186, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1643 - val_loss: 0.1019\n",
      "Epoch 37/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1648\n",
      "Epoch 00037: val_loss improved from 0.10186 to 0.10121, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1632 - val_loss: 0.1012\n",
      "Epoch 38/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1642\n",
      "Epoch 00038: val_loss improved from 0.10121 to 0.10113, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1621 - val_loss: 0.1011\n",
      "Epoch 39/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1644\n",
      "Epoch 00039: val_loss improved from 0.10113 to 0.10057, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1638 - val_loss: 0.1006\n",
      "Epoch 40/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1600\n",
      "Epoch 00040: val_loss improved from 0.10057 to 0.10020, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1618 - val_loss: 0.1002\n",
      "Epoch 41/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1606\n",
      "Epoch 00041: val_loss improved from 0.10020 to 0.09991, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1609 - val_loss: 0.0999\n",
      "Epoch 42/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1621\n",
      "Epoch 00042: val_loss improved from 0.09991 to 0.09967, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1611 - val_loss: 0.0997\n",
      "Epoch 43/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1580\n",
      "Epoch 00043: val_loss improved from 0.09967 to 0.09943, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1586 - val_loss: 0.0994\n",
      "Epoch 44/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1558\n",
      "Epoch 00044: val_loss improved from 0.09943 to 0.09930, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1597 - val_loss: 0.0993\n",
      "Epoch 45/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1583\n",
      "Epoch 00045: val_loss improved from 0.09930 to 0.09887, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1588 - val_loss: 0.0989\n",
      "Epoch 46/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1586\n",
      "Epoch 00046: val_loss did not improve from 0.09887\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1581 - val_loss: 0.0989\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1556\n",
      "Epoch 00047: val_loss improved from 0.09887 to 0.09840, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1556 - val_loss: 0.0984\n",
      "Epoch 48/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1593\n",
      "Epoch 00048: val_loss improved from 0.09840 to 0.09839, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1559 - val_loss: 0.0984\n",
      "Epoch 49/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1534\n",
      "Epoch 00049: val_loss improved from 0.09839 to 0.09801, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1553 - val_loss: 0.0980\n",
      "Epoch 50/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1558\n",
      "Epoch 00050: val_loss improved from 0.09801 to 0.09782, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1567 - val_loss: 0.0978\n",
      "Epoch 51/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1552\n",
      "Epoch 00051: val_loss did not improve from 0.09782\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1556 - val_loss: 0.0978\n",
      "Epoch 52/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1564\n",
      "Epoch 00052: val_loss improved from 0.09782 to 0.09771, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1560 - val_loss: 0.0977\n",
      "Epoch 53/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1547\n",
      "Epoch 00053: val_loss improved from 0.09771 to 0.09756, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1554 - val_loss: 0.0976\n",
      "Epoch 54/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1516\n",
      "Epoch 00054: val_loss improved from 0.09756 to 0.09726, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1530 - val_loss: 0.0973\n",
      "Epoch 55/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1514\n",
      "Epoch 00055: val_loss did not improve from 0.09726\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1521 - val_loss: 0.0976\n",
      "Epoch 56/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1563\n",
      "Epoch 00056: val_loss improved from 0.09726 to 0.09695, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1547 - val_loss: 0.0970\n",
      "Epoch 57/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1516\n",
      "Epoch 00057: val_loss did not improve from 0.09695\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1517 - val_loss: 0.0972\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1518\n",
      "Epoch 00058: val_loss improved from 0.09695 to 0.09691, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1518 - val_loss: 0.0969\n",
      "Epoch 59/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1499\n",
      "Epoch 00059: val_loss improved from 0.09691 to 0.09646, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1501 - val_loss: 0.0965\n",
      "Epoch 60/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1483\n",
      "Epoch 00060: val_loss did not improve from 0.09646\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1491 - val_loss: 0.0965\n",
      "Epoch 61/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1550\n",
      "Epoch 00061: val_loss did not improve from 0.09646\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1502 - val_loss: 0.0967\n",
      " ###0 fold : val acc1 0.607, acc3 0.981, mae 0.207###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/35 [==================>...........] - ETA: 0s - loss: 19.6451 \n",
      "Epoch 00001: val_loss improved from inf to 13.55329, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 18.1298 - val_loss: 13.5533\n",
      "Epoch 2/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 11.2876\n",
      "Epoch 00002: val_loss improved from 13.55329 to 6.79810, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.1240 - val_loss: 6.7981\n",
      "Epoch 3/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 5.6350\n",
      "Epoch 00003: val_loss improved from 6.79810 to 3.58466, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.0389 - val_loss: 3.5847\n",
      "Epoch 4/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 3.2122\n",
      "Epoch 00004: val_loss improved from 3.58466 to 2.27290, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.9953 - val_loss: 2.2729\n",
      "Epoch 5/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 2.0634\n",
      "Epoch 00005: val_loss improved from 2.27290 to 1.33697, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.8848 - val_loss: 1.3370\n",
      "Epoch 6/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 1.2058\n",
      "Epoch 00006: val_loss improved from 1.33697 to 0.76839, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.1172 - val_loss: 0.7684\n",
      "Epoch 7/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7404\n",
      "Epoch 00007: val_loss improved from 0.76839 to 0.44994, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6756 - val_loss: 0.4499\n",
      "Epoch 8/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.4649\n",
      "Epoch 00008: val_loss improved from 0.44994 to 0.28123, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4355 - val_loss: 0.2812\n",
      "Epoch 9/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.3193\n",
      "Epoch 00009: val_loss improved from 0.28123 to 0.19391, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3072 - val_loss: 0.1939\n",
      "Epoch 10/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.2562\n",
      "Epoch 00010: val_loss improved from 0.19391 to 0.14997, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2475 - val_loss: 0.1500\n",
      "Epoch 11/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.2157\n",
      "Epoch 00011: val_loss improved from 0.14997 to 0.13160, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2126 - val_loss: 0.1316\n",
      "Epoch 12/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.2028\n",
      "Epoch 00012: val_loss improved from 0.13160 to 0.12297, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2004 - val_loss: 0.1230\n",
      "Epoch 13/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1941\n",
      "Epoch 00013: val_loss improved from 0.12297 to 0.11900, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1959 - val_loss: 0.1190\n",
      "Epoch 14/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1950\n",
      "Epoch 00014: val_loss improved from 0.11900 to 0.11724, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1951 - val_loss: 0.1172\n",
      "Epoch 15/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1872\n",
      "Epoch 00015: val_loss improved from 0.11724 to 0.11555, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1879 - val_loss: 0.1156\n",
      "Epoch 16/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1903\n",
      "Epoch 00016: val_loss improved from 0.11555 to 0.11450, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1885 - val_loss: 0.1145\n",
      "Epoch 17/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1915\n",
      "Epoch 00017: val_loss improved from 0.11450 to 0.11348, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1886 - val_loss: 0.1135\n",
      "Epoch 18/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1861\n",
      "Epoch 00018: val_loss improved from 0.11348 to 0.11250, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1844 - val_loss: 0.1125\n",
      "Epoch 19/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1827\n",
      "Epoch 00019: val_loss improved from 0.11250 to 0.11162, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1827 - val_loss: 0.1116\n",
      "Epoch 20/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1825\n",
      "Epoch 00020: val_loss improved from 0.11162 to 0.11101, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1833 - val_loss: 0.1110\n",
      "Epoch 21/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1806\n",
      "Epoch 00021: val_loss improved from 0.11101 to 0.11018, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1819 - val_loss: 0.1102\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1830\n",
      "Epoch 00022: val_loss improved from 0.11018 to 0.10937, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1804 - val_loss: 0.1094\n",
      "Epoch 23/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1787\n",
      "Epoch 00023: val_loss improved from 0.10937 to 0.10893, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1792 - val_loss: 0.1089\n",
      "Epoch 24/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1787\n",
      "Epoch 00024: val_loss improved from 0.10893 to 0.10819, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1776 - val_loss: 0.1082\n",
      "Epoch 25/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1769\n",
      "Epoch 00025: val_loss improved from 0.10819 to 0.10735, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1762 - val_loss: 0.1074\n",
      "Epoch 26/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1742\n",
      "Epoch 00026: val_loss improved from 0.10735 to 0.10656, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1733 - val_loss: 0.1066\n",
      "Epoch 27/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1766\n",
      "Epoch 00027: val_loss improved from 0.10656 to 0.10598, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1734 - val_loss: 0.1060\n",
      "Epoch 28/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1668\n",
      "Epoch 00028: val_loss improved from 0.10598 to 0.10549, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1707 - val_loss: 0.1055\n",
      "Epoch 29/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1678\n",
      "Epoch 00029: val_loss improved from 0.10549 to 0.10491, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1696 - val_loss: 0.1049\n",
      "Epoch 30/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1736\n",
      "Epoch 00030: val_loss improved from 0.10491 to 0.10464, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1713 - val_loss: 0.1046\n",
      "Epoch 31/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1666\n",
      "Epoch 00031: val_loss improved from 0.10464 to 0.10370, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1675 - val_loss: 0.1037\n",
      "Epoch 32/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1722\n",
      "Epoch 00032: val_loss improved from 0.10370 to 0.10346, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1685 - val_loss: 0.1035\n",
      "Epoch 33/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1656\n",
      "Epoch 00033: val_loss improved from 0.10346 to 0.10301, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1660 - val_loss: 0.1030\n",
      "Epoch 34/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1651\n",
      "Epoch 00034: val_loss improved from 0.10301 to 0.10255, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1660 - val_loss: 0.1025\n",
      "Epoch 35/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1653\n",
      "Epoch 00035: val_loss improved from 0.10255 to 0.10224, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1649 - val_loss: 0.1022\n",
      "Epoch 36/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1651\n",
      "Epoch 00036: val_loss improved from 0.10224 to 0.10180, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1643 - val_loss: 0.1018\n",
      "Epoch 37/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1632\n",
      "Epoch 00037: val_loss improved from 0.10180 to 0.10116, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1628 - val_loss: 0.1012\n",
      "Epoch 38/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1629\n",
      "Epoch 00038: val_loss improved from 0.10116 to 0.10093, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1614 - val_loss: 0.1009\n",
      "Epoch 39/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1613\n",
      "Epoch 00039: val_loss improved from 0.10093 to 0.10049, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1633 - val_loss: 0.1005\n",
      "Epoch 40/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1601\n",
      "Epoch 00040: val_loss improved from 0.10049 to 0.10025, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1613 - val_loss: 0.1003\n",
      "Epoch 41/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1607\n",
      "Epoch 00041: val_loss improved from 0.10025 to 0.10005, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1610 - val_loss: 0.1001\n",
      "Epoch 42/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1616\n",
      "Epoch 00042: val_loss improved from 0.10005 to 0.09987, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1606 - val_loss: 0.0999\n",
      "Epoch 43/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1571\n",
      "Epoch 00043: val_loss improved from 0.09987 to 0.09953, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1588 - val_loss: 0.0995\n",
      "Epoch 44/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1550\n",
      "Epoch 00044: val_loss did not improve from 0.09953\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1573 - val_loss: 0.0995\n",
      "Epoch 45/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1571\n",
      "Epoch 00045: val_loss improved from 0.09953 to 0.09907, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1580 - val_loss: 0.0991\n",
      "Epoch 46/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1564\n",
      "Epoch 00046: val_loss did not improve from 0.09907\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1573 - val_loss: 0.0991\n",
      "Epoch 47/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1551\n",
      "Epoch 00047: val_loss improved from 0.09907 to 0.09853, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1566 - val_loss: 0.0985\n",
      "Epoch 48/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1574\n",
      "Epoch 00048: val_loss improved from 0.09853 to 0.09841, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1558 - val_loss: 0.0984\n",
      "Epoch 49/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1526\n",
      "Epoch 00049: val_loss improved from 0.09841 to 0.09810, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1550 - val_loss: 0.0981\n",
      "Epoch 50/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1565\n",
      "Epoch 00050: val_loss improved from 0.09810 to 0.09803, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1566 - val_loss: 0.0980\n",
      "Epoch 51/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1574\n",
      "Epoch 00051: val_loss improved from 0.09803 to 0.09773, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1564 - val_loss: 0.0977\n",
      "Epoch 52/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1579\n",
      "Epoch 00052: val_loss did not improve from 0.09773\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1562 - val_loss: 0.0978\n",
      "Epoch 53/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1543\n",
      "Epoch 00053: val_loss improved from 0.09773 to 0.09773, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1559 - val_loss: 0.0977\n",
      "Epoch 54/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1528\n",
      "Epoch 00054: val_loss improved from 0.09773 to 0.09729, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1534 - val_loss: 0.0973\n",
      "Epoch 55/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1524\n",
      "Epoch 00055: val_loss did not improve from 0.09729\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1521 - val_loss: 0.0974\n",
      "Epoch 56/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1541\n",
      "Epoch 00056: val_loss improved from 0.09729 to 0.09691, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1533 - val_loss: 0.0969\n",
      "Epoch 57/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1510\n",
      "Epoch 00057: val_loss did not improve from 0.09691\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1512 - val_loss: 0.0973\n",
      "Epoch 58/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1506\n",
      "Epoch 00058: val_loss improved from 0.09691 to 0.09668, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1514 - val_loss: 0.0967\n",
      "Epoch 59/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1506\n",
      "Epoch 00059: val_loss improved from 0.09668 to 0.09645, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1505 - val_loss: 0.0965\n",
      "Epoch 60/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1490\n",
      "Epoch 00060: val_loss did not improve from 0.09645\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1494 - val_loss: 0.0966\n",
      "Epoch 61/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1548\n",
      "Epoch 00061: val_loss did not improve from 0.09645\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1508 - val_loss: 0.0968\n",
      " ###1 fold : val acc1 0.594, acc3 0.982, mae 0.212###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/35 [=================>............] - ETA: 0s - loss: 19.7466 \n",
      "Epoch 00001: val_loss improved from inf to 13.55177, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 18.1103 - val_loss: 13.5518\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 11.4627\n",
      "Epoch 00002: val_loss improved from 13.55177 to 6.79584, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.1041 - val_loss: 6.7958\n",
      "Epoch 3/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 5.6650\n",
      "Epoch 00003: val_loss improved from 6.79584 to 3.58350, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.0294 - val_loss: 3.5835\n",
      "Epoch 4/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 3.2262\n",
      "Epoch 00004: val_loss improved from 3.58350 to 2.27127, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.9939 - val_loss: 2.2713\n",
      "Epoch 5/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 2.0580\n",
      "Epoch 00005: val_loss improved from 2.27127 to 1.33639, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.8839 - val_loss: 1.3364\n",
      "Epoch 6/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 1.2024\n",
      "Epoch 00006: val_loss improved from 1.33639 to 0.76758, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.1178 - val_loss: 0.7676\n",
      "Epoch 7/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7363\n",
      "Epoch 00007: val_loss improved from 0.76758 to 0.44795, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6756 - val_loss: 0.4480\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4696\n",
      "Epoch 00008: val_loss improved from 0.44795 to 0.27866, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4332 - val_loss: 0.2787\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3195\n",
      "Epoch 00009: val_loss improved from 0.27866 to 0.19193, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3076 - val_loss: 0.1919\n",
      "Epoch 10/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2565\n",
      "Epoch 00010: val_loss improved from 0.19193 to 0.14885, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2470 - val_loss: 0.1488\n",
      "Epoch 11/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2159\n",
      "Epoch 00011: val_loss improved from 0.14885 to 0.13077, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2114 - val_loss: 0.1308\n",
      "Epoch 12/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2012\n",
      "Epoch 00012: val_loss improved from 0.13077 to 0.12261, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2001 - val_loss: 0.1226\n",
      "Epoch 13/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1935\n",
      "Epoch 00013: val_loss improved from 0.12261 to 0.11883, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1949 - val_loss: 0.1188\n",
      "Epoch 14/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1949\n",
      "Epoch 00014: val_loss improved from 0.11883 to 0.11699, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1949 - val_loss: 0.1170\n",
      "Epoch 15/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00015: val_loss improved from 0.11699 to 0.11545, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1883 - val_loss: 0.1155\n",
      "Epoch 16/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1902\n",
      "Epoch 00016: val_loss improved from 0.11545 to 0.11443, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1891 - val_loss: 0.1144\n",
      "Epoch 17/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1937\n",
      "Epoch 00017: val_loss improved from 0.11443 to 0.11360, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1900 - val_loss: 0.1136\n",
      "Epoch 18/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1851\n",
      "Epoch 00018: val_loss improved from 0.11360 to 0.11259, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1856 - val_loss: 0.1126\n",
      "Epoch 19/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1827\n",
      "Epoch 00019: val_loss improved from 0.11259 to 0.11166, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1817 - val_loss: 0.1117\n",
      "Epoch 20/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1800\n",
      "Epoch 00020: val_loss improved from 0.11166 to 0.11090, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1832 - val_loss: 0.1109\n",
      "Epoch 21/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1805\n",
      "Epoch 00021: val_loss improved from 0.11090 to 0.11038, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1827 - val_loss: 0.1104\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1831\n",
      "Epoch 00022: val_loss improved from 0.11038 to 0.10945, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1803 - val_loss: 0.1094\n",
      "Epoch 23/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1797\n",
      "Epoch 00023: val_loss improved from 0.10945 to 0.10907, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1799 - val_loss: 0.1091\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1806\n",
      "Epoch 00024: val_loss improved from 0.10907 to 0.10803, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1773 - val_loss: 0.1080\n",
      "Epoch 25/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1785\n",
      "Epoch 00025: val_loss improved from 0.10803 to 0.10743, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1764 - val_loss: 0.1074\n",
      "Epoch 26/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1741\n",
      "Epoch 00026: val_loss improved from 0.10743 to 0.10670, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1732 - val_loss: 0.1067\n",
      "Epoch 27/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1748\n",
      "Epoch 00027: val_loss improved from 0.10670 to 0.10606, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1721 - val_loss: 0.1061\n",
      "Epoch 28/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1663\n",
      "Epoch 00028: val_loss improved from 0.10606 to 0.10550, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1699 - val_loss: 0.1055\n",
      "Epoch 29/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1666\n",
      "Epoch 00029: val_loss improved from 0.10550 to 0.10494, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1695 - val_loss: 0.1049\n",
      "Epoch 30/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1747\n",
      "Epoch 00030: val_loss improved from 0.10494 to 0.10458, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1720 - val_loss: 0.1046\n",
      "Epoch 31/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1668\n",
      "Epoch 00031: val_loss improved from 0.10458 to 0.10384, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1685 - val_loss: 0.1038\n",
      "Epoch 32/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1718\n",
      "Epoch 00032: val_loss improved from 0.10384 to 0.10346, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1677 - val_loss: 0.1035\n",
      "Epoch 33/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1636\n",
      "Epoch 00033: val_loss improved from 0.10346 to 0.10313, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1654 - val_loss: 0.1031\n",
      "Epoch 34/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1654\n",
      "Epoch 00034: val_loss improved from 0.10313 to 0.10258, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1658 - val_loss: 0.1026\n",
      "Epoch 35/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1650\n",
      "Epoch 00035: val_loss improved from 0.10258 to 0.10214, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1657 - val_loss: 0.1021\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1637\n",
      "Epoch 00036: val_loss improved from 0.10214 to 0.10182, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1637 - val_loss: 0.1018\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1632\n",
      "Epoch 00037: val_loss improved from 0.10182 to 0.10128, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1632 - val_loss: 0.1013\n",
      "Epoch 38/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1643\n",
      "Epoch 00038: val_loss improved from 0.10128 to 0.10090, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1619 - val_loss: 0.1009\n",
      "Epoch 39/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1592\n",
      "Epoch 00039: val_loss improved from 0.10090 to 0.10059, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1615 - val_loss: 0.1006\n",
      "Epoch 40/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1598\n",
      "Epoch 00040: val_loss improved from 0.10059 to 0.10037, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1613 - val_loss: 0.1004\n",
      "Epoch 41/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1629\n",
      "Epoch 00041: val_loss improved from 0.10037 to 0.10009, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1616 - val_loss: 0.1001\n",
      "Epoch 42/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1611\n",
      "Epoch 00042: val_loss improved from 0.10009 to 0.09985, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1602 - val_loss: 0.0998\n",
      "Epoch 43/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1586\n",
      "Epoch 00043: val_loss improved from 0.09985 to 0.09966, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1601 - val_loss: 0.0997\n",
      "Epoch 44/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1541\n",
      "Epoch 00044: val_loss improved from 0.09966 to 0.09953, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1572 - val_loss: 0.0995\n",
      "Epoch 45/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1577\n",
      "Epoch 00045: val_loss improved from 0.09953 to 0.09914, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1577 - val_loss: 0.0991\n",
      "Epoch 46/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1563\n",
      "Epoch 00046: val_loss improved from 0.09914 to 0.09901, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1577 - val_loss: 0.0990\n",
      "Epoch 47/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1570\n",
      "Epoch 00047: val_loss improved from 0.09901 to 0.09867, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1583 - val_loss: 0.0987\n",
      "Epoch 48/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1588\n",
      "Epoch 00048: val_loss improved from 0.09867 to 0.09855, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1557 - val_loss: 0.0986\n",
      "Epoch 49/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1527\n",
      "Epoch 00049: val_loss improved from 0.09855 to 0.09830, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1553 - val_loss: 0.0983\n",
      "Epoch 50/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1578\n",
      "Epoch 00050: val_loss improved from 0.09830 to 0.09809, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1564 - val_loss: 0.0981\n",
      "Epoch 51/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1563\n",
      "Epoch 00051: val_loss improved from 0.09809 to 0.09773, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1554 - val_loss: 0.0977\n",
      "Epoch 52/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1571\n",
      "Epoch 00052: val_loss did not improve from 0.09773\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1572 - val_loss: 0.0981\n",
      "Epoch 53/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1550\n",
      "Epoch 00053: val_loss improved from 0.09773 to 0.09764, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1561 - val_loss: 0.0976\n",
      "Epoch 54/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1552\n",
      "Epoch 00054: val_loss improved from 0.09764 to 0.09733, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1544 - val_loss: 0.0973\n",
      "Epoch 55/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1521\n",
      "Epoch 00055: val_loss did not improve from 0.09733\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1526 - val_loss: 0.0975\n",
      "Epoch 56/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1539\n",
      "Epoch 00056: val_loss improved from 0.09733 to 0.09708, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1535 - val_loss: 0.0971\n",
      "Epoch 57/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1521\n",
      "Epoch 00057: val_loss did not improve from 0.09708\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1514 - val_loss: 0.0975\n",
      "Epoch 58/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1517\n",
      "Epoch 00058: val_loss improved from 0.09708 to 0.09686, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1526 - val_loss: 0.0969\n",
      "Epoch 59/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1516\n",
      "Epoch 00059: val_loss improved from 0.09686 to 0.09670, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1510 - val_loss: 0.0967\n",
      "Epoch 60/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1498\n",
      "Epoch 00060: val_loss did not improve from 0.09670\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1503 - val_loss: 0.0967\n",
      "Epoch 61/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1539\n",
      "Epoch 00061: val_loss improved from 0.09670 to 0.09655, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1496 - val_loss: 0.0966\n",
      "Epoch 62/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1496\n",
      "Epoch 00062: val_loss improved from 0.09655 to 0.09628, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1478 - val_loss: 0.0963\n",
      "Epoch 63/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1526\n",
      "Epoch 00063: val_loss improved from 0.09628 to 0.09616, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1491 - val_loss: 0.0962\n",
      "Epoch 64/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1514\n",
      "Epoch 00064: val_loss did not improve from 0.09616\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1487 - val_loss: 0.0962\n",
      "Epoch 65/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1458\n",
      "Epoch 00065: val_loss improved from 0.09616 to 0.09605, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1477 - val_loss: 0.0960\n",
      "Epoch 66/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1494\n",
      "Epoch 00066: val_loss improved from 0.09605 to 0.09587, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1498 - val_loss: 0.0959\n",
      "Epoch 67/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 0.1441\n",
      "Epoch 00067: val_loss improved from 0.09587 to 0.09570, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1464 - val_loss: 0.0957\n",
      "Epoch 68/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1453\n",
      "Epoch 00068: val_loss improved from 0.09570 to 0.09559, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1468 - val_loss: 0.0956\n",
      "Epoch 69/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1491\n",
      "Epoch 00069: val_loss did not improve from 0.09559\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1455 - val_loss: 0.0958\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1473\n",
      "Epoch 00070: val_loss improved from 0.09559 to 0.09547, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1473 - val_loss: 0.0955\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1461\n",
      "Epoch 00071: val_loss improved from 0.09547 to 0.09538, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1461 - val_loss: 0.0954\n",
      "Epoch 72/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1473\n",
      "Epoch 00072: val_loss did not improve from 0.09538\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1483 - val_loss: 0.0956\n",
      "Epoch 73/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1465\n",
      "Epoch 00073: val_loss did not improve from 0.09538\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1465 - val_loss: 0.0954\n",
      " ###2 fold : val acc1 0.603, acc3 0.979, mae 0.209###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/35 [=================>............] - ETA: 0s - loss: 19.7923 \n",
      "Epoch 00001: val_loss improved from inf to 13.54265, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 18.1652 - val_loss: 13.5426\n",
      "Epoch 2/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 11.3704\n",
      "Epoch 00002: val_loss improved from 13.54265 to 6.78768, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.1435 - val_loss: 6.7877\n",
      "Epoch 3/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 5.5892\n",
      "Epoch 00003: val_loss improved from 6.78768 to 3.58275, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.0572 - val_loss: 3.5828\n",
      "Epoch 4/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 3.2316\n",
      "Epoch 00004: val_loss improved from 3.58275 to 2.27285, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.0059 - val_loss: 2.2728\n",
      "Epoch 5/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 2.0652\n",
      "Epoch 00005: val_loss improved from 2.27285 to 1.33814, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.8929 - val_loss: 1.3381\n",
      "Epoch 6/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.2260\n",
      "Epoch 00006: val_loss improved from 1.33814 to 0.77032, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.1264 - val_loss: 0.7703\n",
      "Epoch 7/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7502\n",
      "Epoch 00007: val_loss improved from 0.77032 to 0.44981, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6788 - val_loss: 0.4498\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4721\n",
      "Epoch 00008: val_loss improved from 0.44981 to 0.27964, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4335 - val_loss: 0.2796\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3220\n",
      "Epoch 00009: val_loss improved from 0.27964 to 0.19177, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3078 - val_loss: 0.1918\n",
      "Epoch 10/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.2554\n",
      "Epoch 00010: val_loss improved from 0.19177 to 0.14896, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2457 - val_loss: 0.1490\n",
      "Epoch 11/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2118\n",
      "Epoch 00011: val_loss improved from 0.14896 to 0.13092, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2104 - val_loss: 0.1309\n",
      "Epoch 12/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.2026\n",
      "Epoch 00012: val_loss improved from 0.13092 to 0.12280, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2007 - val_loss: 0.1228\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1930\n",
      "Epoch 00013: val_loss improved from 0.12280 to 0.11905, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1941 - val_loss: 0.1191\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1951\n",
      "Epoch 00014: val_loss improved from 0.11905 to 0.11732, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1950 - val_loss: 0.1173\n",
      "Epoch 15/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1866\n",
      "Epoch 00015: val_loss improved from 0.11732 to 0.11564, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1877 - val_loss: 0.1156\n",
      "Epoch 16/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00016: val_loss improved from 0.11564 to 0.11471, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1881 - val_loss: 0.1147\n",
      "Epoch 17/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1917\n",
      "Epoch 00017: val_loss improved from 0.11471 to 0.11398, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1886 - val_loss: 0.1140\n",
      "Epoch 18/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1860\n",
      "Epoch 00018: val_loss improved from 0.11398 to 0.11268, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1866 - val_loss: 0.1127\n",
      "Epoch 19/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1823\n",
      "Epoch 00019: val_loss improved from 0.11268 to 0.11183, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1805 - val_loss: 0.1118\n",
      "Epoch 20/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1791\n",
      "Epoch 00020: val_loss improved from 0.11183 to 0.11107, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1837 - val_loss: 0.1111\n",
      "Epoch 21/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1789\n",
      "Epoch 00021: val_loss improved from 0.11107 to 0.11038, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1815 - val_loss: 0.1104\n",
      "Epoch 22/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1825\n",
      "Epoch 00022: val_loss improved from 0.11038 to 0.10964, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1804 - val_loss: 0.1096\n",
      "Epoch 23/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1781\n",
      "Epoch 00023: val_loss improved from 0.10964 to 0.10904, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1789 - val_loss: 0.1090\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1787\n",
      "Epoch 00024: val_loss improved from 0.10904 to 0.10808, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1766 - val_loss: 0.1081\n",
      "Epoch 25/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1798\n",
      "Epoch 00025: val_loss improved from 0.10808 to 0.10743, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1770 - val_loss: 0.1074\n",
      "Epoch 26/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1749\n",
      "Epoch 00026: val_loss improved from 0.10743 to 0.10677, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1732 - val_loss: 0.1068\n",
      "Epoch 27/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1736\n",
      "Epoch 00027: val_loss improved from 0.10677 to 0.10623, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1718 - val_loss: 0.1062\n",
      "Epoch 28/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1663\n",
      "Epoch 00028: val_loss improved from 0.10623 to 0.10567, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1704 - val_loss: 0.1057\n",
      "Epoch 29/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1665\n",
      "Epoch 00029: val_loss improved from 0.10567 to 0.10501, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1691 - val_loss: 0.1050\n",
      "Epoch 30/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1740\n",
      "Epoch 00030: val_loss improved from 0.10501 to 0.10462, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1714 - val_loss: 0.1046\n",
      "Epoch 31/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1681\n",
      "Epoch 00031: val_loss improved from 0.10462 to 0.10418, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1684 - val_loss: 0.1042\n",
      "Epoch 32/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1702\n",
      "Epoch 00032: val_loss improved from 0.10418 to 0.10353, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1675 - val_loss: 0.1035\n",
      "Epoch 33/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1633\n",
      "Epoch 00033: val_loss improved from 0.10353 to 0.10313, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1653 - val_loss: 0.1031\n",
      "Epoch 34/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1647\n",
      "Epoch 00034: val_loss improved from 0.10313 to 0.10255, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1657 - val_loss: 0.1026\n",
      "Epoch 35/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1633\n",
      "Epoch 00035: val_loss improved from 0.10255 to 0.10241, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1653 - val_loss: 0.1024\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1631\n",
      "Epoch 00036: val_loss improved from 0.10241 to 0.10198, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1631 - val_loss: 0.1020\n",
      "Epoch 37/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1649\n",
      "Epoch 00037: val_loss improved from 0.10198 to 0.10146, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1633 - val_loss: 0.1015\n",
      "Epoch 38/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1644\n",
      "Epoch 00038: val_loss improved from 0.10146 to 0.10106, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1617 - val_loss: 0.1011\n",
      "Epoch 39/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1609\n",
      "Epoch 00039: val_loss improved from 0.10106 to 0.10062, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1625 - val_loss: 0.1006\n",
      "Epoch 40/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1601\n",
      "Epoch 00040: val_loss improved from 0.10062 to 0.10041, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1605 - val_loss: 0.1004\n",
      "Epoch 41/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1621\n",
      "Epoch 00041: val_loss improved from 0.10041 to 0.10014, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1611 - val_loss: 0.1001\n",
      "Epoch 42/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1605\n",
      "Epoch 00042: val_loss improved from 0.10014 to 0.09995, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1592 - val_loss: 0.0999\n",
      "Epoch 43/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1574\n",
      "Epoch 00043: val_loss improved from 0.09995 to 0.09976, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1593 - val_loss: 0.0998\n",
      "Epoch 44/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1565\n",
      "Epoch 00044: val_loss improved from 0.09976 to 0.09959, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1585 - val_loss: 0.0996\n",
      "Epoch 45/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1570\n",
      "Epoch 00045: val_loss improved from 0.09959 to 0.09928, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1568 - val_loss: 0.0993\n",
      "Epoch 46/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1561\n",
      "Epoch 00046: val_loss improved from 0.09928 to 0.09906, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1575 - val_loss: 0.0991\n",
      "Epoch 47/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1572\n",
      "Epoch 00047: val_loss improved from 0.09906 to 0.09886, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1580 - val_loss: 0.0989\n",
      "Epoch 48/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1570\n",
      "Epoch 00048: val_loss improved from 0.09886 to 0.09877, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1545 - val_loss: 0.0988\n",
      "Epoch 49/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1542\n",
      "Epoch 00049: val_loss improved from 0.09877 to 0.09843, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1552 - val_loss: 0.0984\n",
      "Epoch 50/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1582\n",
      "Epoch 00050: val_loss improved from 0.09843 to 0.09821, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1567 - val_loss: 0.0982\n",
      "Epoch 51/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1562\n",
      "Epoch 00051: val_loss improved from 0.09821 to 0.09788, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1550 - val_loss: 0.0979\n",
      "Epoch 52/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1554\n",
      "Epoch 00052: val_loss did not improve from 0.09788\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1558 - val_loss: 0.0981\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1548\n",
      "Epoch 00053: val_loss improved from 0.09788 to 0.09760, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1548 - val_loss: 0.0976\n",
      "Epoch 54/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1557\n",
      "Epoch 00054: val_loss improved from 0.09760 to 0.09737, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1547 - val_loss: 0.0974\n",
      "Epoch 55/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1521\n",
      "Epoch 00055: val_loss improved from 0.09737 to 0.09734, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1530 - val_loss: 0.0973\n",
      "Epoch 56/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1532\n",
      "Epoch 00056: val_loss improved from 0.09734 to 0.09710, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1530 - val_loss: 0.0971\n",
      "Epoch 57/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1527\n",
      "Epoch 00057: val_loss did not improve from 0.09710\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1516 - val_loss: 0.0975\n",
      "Epoch 58/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1519\n",
      "Epoch 00058: val_loss improved from 0.09710 to 0.09693, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1522 - val_loss: 0.0969\n",
      "Epoch 59/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1520\n",
      "Epoch 00059: val_loss improved from 0.09693 to 0.09663, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1509 - val_loss: 0.0966\n",
      "Epoch 60/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1510\n",
      "Epoch 00060: val_loss improved from 0.09663 to 0.09663, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1507 - val_loss: 0.0966\n",
      "Epoch 61/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1528\n",
      "Epoch 00061: val_loss did not improve from 0.09663\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1498 - val_loss: 0.0969\n",
      "Epoch 62/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1501\n",
      "Epoch 00062: val_loss improved from 0.09663 to 0.09643, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1485 - val_loss: 0.0964\n",
      "Epoch 63/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1509\n",
      "Epoch 00063: val_loss improved from 0.09643 to 0.09614, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1485 - val_loss: 0.0961\n",
      "Epoch 64/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1512\n",
      "Epoch 00064: val_loss did not improve from 0.09614\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1488 - val_loss: 0.0963\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1473\n",
      "Epoch 00065: val_loss improved from 0.09614 to 0.09601, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1473 - val_loss: 0.0960\n",
      "Epoch 66/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1503\n",
      "Epoch 00066: val_loss improved from 0.09601 to 0.09595, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1497 - val_loss: 0.0960\n",
      "Epoch 67/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1448\n",
      "Epoch 00067: val_loss improved from 0.09595 to 0.09568, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1465 - val_loss: 0.0957\n",
      "Epoch 68/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1467\n",
      "Epoch 00068: val_loss improved from 0.09568 to 0.09564, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1473 - val_loss: 0.0956\n",
      "Epoch 69/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1473\n",
      "Epoch 00069: val_loss did not improve from 0.09564\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1453 - val_loss: 0.0957\n",
      "Epoch 70/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1464\n",
      "Epoch 00070: val_loss did not improve from 0.09564\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1476 - val_loss: 0.0957\n",
      " ###3 fold : val acc1 0.617, acc3 0.977, mae 0.203###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/35 [==================>...........] - ETA: 0s - loss: 19.5984 \n",
      "Epoch 00001: val_loss improved from inf to 13.53100, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 18.0924 - val_loss: 13.5310\n",
      "Epoch 2/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 11.2594\n",
      "Epoch 00002: val_loss improved from 13.53100 to 6.79170, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.1178 - val_loss: 6.7917\n",
      "Epoch 3/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 5.5591\n",
      "Epoch 00003: val_loss improved from 6.79170 to 3.59325, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.0421 - val_loss: 3.5932\n",
      "Epoch 4/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 3.2515\n",
      "Epoch 00004: val_loss improved from 3.59325 to 2.26739, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.9986 - val_loss: 2.2674\n",
      "Epoch 5/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 2.0423\n",
      "Epoch 00005: val_loss improved from 2.26739 to 1.34610, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.8822 - val_loss: 1.3461\n",
      "Epoch 6/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 1.2210\n",
      "Epoch 00006: val_loss improved from 1.34610 to 0.76869, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.1225 - val_loss: 0.7687\n",
      "Epoch 7/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7493\n",
      "Epoch 00007: val_loss improved from 0.76869 to 0.44612, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6793 - val_loss: 0.4461\n",
      "Epoch 8/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.4626\n",
      "Epoch 00008: val_loss improved from 0.44612 to 0.27684, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4316 - val_loss: 0.2768\n",
      "Epoch 9/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.3187\n",
      "Epoch 00009: val_loss improved from 0.27684 to 0.19187, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3025 - val_loss: 0.1919\n",
      "Epoch 10/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.2532\n",
      "Epoch 00010: val_loss improved from 0.19187 to 0.14969, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2434 - val_loss: 0.1497\n",
      "Epoch 11/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2215\n",
      "Epoch 00011: val_loss improved from 0.14969 to 0.13156, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2148 - val_loss: 0.1316\n",
      "Epoch 12/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00012: val_loss improved from 0.13156 to 0.12363, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2002 - val_loss: 0.1236\n",
      "Epoch 13/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1970\n",
      "Epoch 00013: val_loss improved from 0.12363 to 0.11989, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1965 - val_loss: 0.1199\n",
      "Epoch 14/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1910\n",
      "Epoch 00014: val_loss improved from 0.11989 to 0.11776, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1932 - val_loss: 0.1178\n",
      "Epoch 15/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1897\n",
      "Epoch 00015: val_loss improved from 0.11776 to 0.11606, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1912 - val_loss: 0.1161\n",
      "Epoch 16/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00016: val_loss improved from 0.11606 to 0.11487, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1879 - val_loss: 0.1149\n",
      "Epoch 17/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1866\n",
      "Epoch 00017: val_loss improved from 0.11487 to 0.11384, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1877 - val_loss: 0.1138\n",
      "Epoch 18/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1902\n",
      "Epoch 00018: val_loss improved from 0.11384 to 0.11322, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1861 - val_loss: 0.1132\n",
      "Epoch 19/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1844\n",
      "Epoch 00019: val_loss improved from 0.11322 to 0.11228, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1824 - val_loss: 0.1123\n",
      "Epoch 20/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1810\n",
      "Epoch 00020: val_loss improved from 0.11228 to 0.11120, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1797 - val_loss: 0.1112\n",
      "Epoch 21/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1818\n",
      "Epoch 00021: val_loss improved from 0.11120 to 0.11043, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1820 - val_loss: 0.1104\n",
      "Epoch 22/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1807\n",
      "Epoch 00022: val_loss improved from 0.11043 to 0.10979, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1796 - val_loss: 0.1098\n",
      "Epoch 23/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1783\n",
      "Epoch 00023: val_loss improved from 0.10979 to 0.10895, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1771 - val_loss: 0.1089\n",
      "Epoch 24/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1796\n",
      "Epoch 00024: val_loss improved from 0.10895 to 0.10828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1767 - val_loss: 0.1083\n",
      "Epoch 25/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1757\n",
      "Epoch 00025: val_loss improved from 0.10828 to 0.10752, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1723 - val_loss: 0.1075\n",
      "Epoch 26/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1728\n",
      "Epoch 00026: val_loss improved from 0.10752 to 0.10714, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1752 - val_loss: 0.1071\n",
      "Epoch 27/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1752\n",
      "Epoch 00027: val_loss improved from 0.10714 to 0.10639, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1737 - val_loss: 0.1064\n",
      "Epoch 28/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1701\n",
      "Epoch 00028: val_loss improved from 0.10639 to 0.10564, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1720 - val_loss: 0.1056\n",
      "Epoch 29/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1697\n",
      "Epoch 00029: val_loss improved from 0.10564 to 0.10530, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1704 - val_loss: 0.1053\n",
      "Epoch 30/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1678\n",
      "Epoch 00030: val_loss improved from 0.10530 to 0.10464, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1678 - val_loss: 0.1046\n",
      "Epoch 31/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1685\n",
      "Epoch 00031: val_loss improved from 0.10464 to 0.10415, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1698 - val_loss: 0.1042\n",
      "Epoch 32/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1683\n",
      "Epoch 00032: val_loss improved from 0.10415 to 0.10372, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1669 - val_loss: 0.1037\n",
      "Epoch 33/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1645\n",
      "Epoch 00033: val_loss improved from 0.10372 to 0.10346, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1657 - val_loss: 0.1035\n",
      "Epoch 34/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1673\n",
      "Epoch 00034: val_loss improved from 0.10346 to 0.10284, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1679 - val_loss: 0.1028\n",
      "Epoch 35/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1659\n",
      "Epoch 00035: val_loss improved from 0.10284 to 0.10243, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1650 - val_loss: 0.1024\n",
      "Epoch 36/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1638\n",
      "Epoch 00036: val_loss improved from 0.10243 to 0.10212, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1639 - val_loss: 0.1021\n",
      "Epoch 37/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1627\n",
      "Epoch 00037: val_loss improved from 0.10212 to 0.10151, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1611 - val_loss: 0.1015\n",
      "Epoch 38/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1589\n",
      "Epoch 00038: val_loss did not improve from 0.10151\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1631 - val_loss: 0.1016\n",
      "Epoch 39/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1607\n",
      "Epoch 00039: val_loss improved from 0.10151 to 0.10076, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1626 - val_loss: 0.1008\n",
      "Epoch 40/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1607\n",
      "Epoch 00040: val_loss improved from 0.10076 to 0.10052, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1596 - val_loss: 0.1005\n",
      "Epoch 41/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1587\n",
      "Epoch 00041: val_loss improved from 0.10052 to 0.10046, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1584 - val_loss: 0.1005\n",
      "Epoch 42/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1604\n",
      "Epoch 00042: val_loss improved from 0.10046 to 0.09999, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1599 - val_loss: 0.1000\n",
      "Epoch 43/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1592\n",
      "Epoch 00043: val_loss improved from 0.09999 to 0.09953, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1582 - val_loss: 0.0995\n",
      "Epoch 44/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1586\n",
      "Epoch 00044: val_loss improved from 0.09953 to 0.09934, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1585 - val_loss: 0.0993\n",
      "Epoch 45/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1586\n",
      "Epoch 00045: val_loss improved from 0.09934 to 0.09927, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1573 - val_loss: 0.0993\n",
      "Epoch 46/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1581\n",
      "Epoch 00046: val_loss improved from 0.09927 to 0.09912, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1572 - val_loss: 0.0991\n",
      "Epoch 47/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1556\n",
      "Epoch 00047: val_loss improved from 0.09912 to 0.09872, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1565 - val_loss: 0.0987\n",
      "Epoch 48/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1562\n",
      "Epoch 00048: val_loss improved from 0.09872 to 0.09836, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1559 - val_loss: 0.0984\n",
      "Epoch 49/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1569\n",
      "Epoch 00049: val_loss improved from 0.09836 to 0.09814, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1560 - val_loss: 0.0981\n",
      "Epoch 50/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1525\n",
      "Epoch 00050: val_loss improved from 0.09814 to 0.09802, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1524 - val_loss: 0.0980\n",
      "Epoch 51/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1537\n",
      "Epoch 00051: val_loss improved from 0.09802 to 0.09780, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1526 - val_loss: 0.0978\n",
      "Epoch 52/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1526\n",
      "Epoch 00052: val_loss improved from 0.09780 to 0.09756, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1517 - val_loss: 0.0976\n",
      "Epoch 53/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1501\n",
      "Epoch 00053: val_loss improved from 0.09756 to 0.09738, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1514 - val_loss: 0.0974\n",
      "Epoch 54/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1493\n",
      "Epoch 00054: val_loss did not improve from 0.09738\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1529 - val_loss: 0.0975\n",
      "Epoch 55/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1525\n",
      "Epoch 00055: val_loss improved from 0.09738 to 0.09709, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1520 - val_loss: 0.0971\n",
      "Epoch 56/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1556\n",
      "Epoch 00056: val_loss improved from 0.09709 to 0.09692, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1524 - val_loss: 0.0969\n",
      "Epoch 57/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1507\n",
      "Epoch 00057: val_loss improved from 0.09692 to 0.09687, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1492 - val_loss: 0.0969\n",
      "Epoch 58/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1432\n",
      "Epoch 00058: val_loss improved from 0.09687 to 0.09683, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1471 - val_loss: 0.0968\n",
      "Epoch 59/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1520\n",
      "Epoch 00059: val_loss did not improve from 0.09683\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1492 - val_loss: 0.0972\n",
      "Epoch 60/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1499\n",
      "Epoch 00060: val_loss did not improve from 0.09683\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1496 - val_loss: 0.0973\n",
      " ###4 fold : val acc1 0.593, acc3 0.979, mae 0.214###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543/558 [============================>.] - ETA: 0s - loss: 0.2384\n",
      "Epoch 00002: val_loss improved from 0.11140 to 0.10310, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.2386 - val_loss: 0.1031\n",
      "Epoch 3/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.2084\n",
      "Epoch 00003: val_loss improved from 0.10310 to 0.09890, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.2082 - val_loss: 0.0989\n",
      "Epoch 4/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.1970\n",
      "Epoch 00004: val_loss improved from 0.09890 to 0.09803, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1970 - val_loss: 0.0980\n",
      "Epoch 5/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.1853\n",
      "Epoch 00005: val_loss did not improve from 0.09803\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1853 - val_loss: 0.1040\n",
      "Epoch 6/100\n",
      "543/558 [============================>.] - ETA: 0s - loss: 0.1833\n",
      "Epoch 00006: val_loss improved from 0.09803 to 0.09690, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1834 - val_loss: 0.0969\n",
      "Epoch 7/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.1777\n",
      "Epoch 00007: val_loss did not improve from 0.09690\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1777 - val_loss: 0.0988\n",
      "Epoch 8/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.1765\n",
      "Epoch 00008: val_loss did not improve from 0.09690\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1766 - val_loss: 0.1050\n",
      " ###6 fold : val acc1 0.597, acc3 0.986, mae 0.209###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545/558 [============================>.] - ETA: 0s - loss: 2.4033\n",
      "Epoch 00001: val_loss improved from inf to 0.11196, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 2.3565 - val_loss: 0.1120\n",
      "Epoch 2/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 0.2366\n",
      "Epoch 00002: val_loss improved from 0.11196 to 0.10297, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.2365 - val_loss: 0.1030\n",
      "Epoch 3/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.2086\n",
      "Epoch 00003: val_loss improved from 0.10297 to 0.09800, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.2082 - val_loss: 0.0980\n",
      "Epoch 4/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.1951\n",
      "Epoch 00004: val_loss improved from 0.09800 to 0.09765, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1952 - val_loss: 0.0976\n",
      "Epoch 5/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.1828\n",
      "Epoch 00005: val_loss did not improve from 0.09765\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1829 - val_loss: 0.1052\n",
      "Epoch 6/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.1823\n",
      "Epoch 00006: val_loss improved from 0.09765 to 0.09642, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1825 - val_loss: 0.0964\n",
      "Epoch 7/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.1757\n",
      "Epoch 00007: val_loss did not improve from 0.09642\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1757 - val_loss: 0.0978\n",
      "Epoch 8/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 0.1754\n",
      "Epoch 00008: val_loss did not improve from 0.09642\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1755 - val_loss: 0.1047\n",
      " ###7 fold : val acc1 0.584, acc3 0.975, mae 0.220###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543/558 [============================>.] - ETA: 0s - loss: 2.4223\n",
      "Epoch 00001: val_loss improved from inf to 0.11659, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 2.3672 - val_loss: 0.1166\n",
      "Epoch 2/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.2370\n",
      "Epoch 00002: val_loss improved from 0.11659 to 0.10597, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.2370 - val_loss: 0.1060\n",
      "Epoch 3/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.2084\n",
      "Epoch 00003: val_loss improved from 0.10597 to 0.10223, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.2080 - val_loss: 0.1022\n",
      "Epoch 4/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.1948\n",
      "Epoch 00004: val_loss improved from 0.10223 to 0.10043, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1948 - val_loss: 0.1004\n",
      "Epoch 5/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 0.1834\n",
      "Epoch 00005: val_loss did not improve from 0.10043\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1833 - val_loss: 0.1077\n",
      "Epoch 6/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.1829\n",
      "Epoch 00006: val_loss improved from 0.10043 to 0.10017, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1830 - val_loss: 0.1002\n",
      "Epoch 7/100\n",
      "540/558 [============================>.] - ETA: 0s - loss: 0.1765\n",
      "Epoch 00007: val_loss did not improve from 0.10017\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1762 - val_loss: 0.1027\n",
      "Epoch 8/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.1760\n",
      "Epoch 00008: val_loss did not improve from 0.10017\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1760 - val_loss: 0.1085\n",
      " ###8 fold : val acc1 0.600, acc3 0.981, mae 0.210###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545/558 [============================>.] - ETA: 0s - loss: 2.4143\n",
      "Epoch 00001: val_loss improved from inf to 0.11987, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 2.3672 - val_loss: 0.1199\n",
      "Epoch 2/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.2368\n",
      "Epoch 00002: val_loss improved from 0.11987 to 0.10901, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.2370 - val_loss: 0.1090\n",
      "Epoch 3/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.2085\n",
      "Epoch 00003: val_loss improved from 0.10901 to 0.10369, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.2080 - val_loss: 0.1037\n",
      "Epoch 4/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.1949\n",
      "Epoch 00004: val_loss improved from 0.10369 to 0.10209, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1948 - val_loss: 0.1021\n",
      "Epoch 5/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.1833\n",
      "Epoch 00005: val_loss did not improve from 0.10209\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1833 - val_loss: 0.1084\n",
      "Epoch 6/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.1830\n",
      "Epoch 00006: val_loss improved from 0.10209 to 0.10163, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes256_dropout0.3,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1830 - val_loss: 0.1016\n",
      "Epoch 7/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.1760\n",
      "Epoch 00007: val_loss did not improve from 0.10163\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1762 - val_loss: 0.1049\n",
      "Epoch 8/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.1760\n",
      "Epoch 00008: val_loss did not improve from 0.10163\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1760 - val_loss: 0.1104\n",
      " ###9 fold : val acc1 0.588, acc3 0.984, mae 0.214###\n",
      "acc10.597_acc30.980\n",
      "random search 23/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/279 [===========================>..] - ETA: 0s - loss: 2.9719\n",
      "Epoch 00001: val_loss improved from inf to 0.16402, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 2.8979 - val_loss: 0.1640\n",
      "Epoch 2/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.8388\n",
      "Epoch 00002: val_loss did not improve from 0.16402\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8387 - val_loss: 0.1648\n",
      "Epoch 3/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.8198\n",
      "Epoch 00003: val_loss improved from 0.16402 to 0.12578, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8206 - val_loss: 0.1258\n",
      "Epoch 4/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.8008\n",
      "Epoch 00004: val_loss did not improve from 0.12578\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8006 - val_loss: 0.1426\n",
      "Epoch 5/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.7761\n",
      "Epoch 00005: val_loss did not improve from 0.12578\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7762 - val_loss: 0.1801\n",
      " ###0 fold : val acc1 0.534, acc3 0.962, mae 0.254###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/279 [===========================>..] - ETA: 0s - loss: 3.0177\n",
      "Epoch 00001: val_loss improved from inf to 0.15403, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 2.9114 - val_loss: 0.1540\n",
      "Epoch 2/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.8389\n",
      "Epoch 00002: val_loss did not improve from 0.15403\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8410 - val_loss: 0.1592\n",
      "Epoch 3/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.8194\n",
      "Epoch 00003: val_loss improved from 0.15403 to 0.12098, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8203 - val_loss: 0.1210\n",
      "Epoch 4/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.8019\n",
      "Epoch 00004: val_loss did not improve from 0.12098\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8006 - val_loss: 0.1395\n",
      "Epoch 5/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.7743\n",
      "Epoch 00005: val_loss did not improve from 0.12098\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7745 - val_loss: 0.1695\n",
      " ###1 fold : val acc1 0.562, acc3 0.971, mae 0.235###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - ETA: 0s - loss: 2.9036\n",
      "Epoch 00001: val_loss improved from inf to 0.16009, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 2.9036 - val_loss: 0.1601\n",
      "Epoch 2/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.8405\n",
      "Epoch 00002: val_loss improved from 0.16009 to 0.15079, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8409 - val_loss: 0.1508\n",
      "Epoch 3/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.8185\n",
      "Epoch 00003: val_loss improved from 0.15079 to 0.12175, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8209 - val_loss: 0.1217\n",
      "Epoch 4/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.8053\n",
      "Epoch 00004: val_loss did not improve from 0.12175\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8010 - val_loss: 0.1463\n",
      "Epoch 5/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.7769\n",
      "Epoch 00005: val_loss did not improve from 0.12175\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7775 - val_loss: 0.1605\n",
      " ###2 fold : val acc1 0.549, acc3 0.966, mae 0.243###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/279 [===========================>..] - ETA: 0s - loss: 3.0096\n",
      "Epoch 00001: val_loss improved from inf to 0.15552, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 2.9033 - val_loss: 0.1555\n",
      "Epoch 2/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.8448\n",
      "Epoch 00002: val_loss improved from 0.15552 to 0.14708, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8441 - val_loss: 0.1471\n",
      "Epoch 3/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.8192\n",
      "Epoch 00003: val_loss improved from 0.14708 to 0.12131, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8213 - val_loss: 0.1213\n",
      "Epoch 4/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.8095\n",
      "Epoch 00004: val_loss did not improve from 0.12131\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8049 - val_loss: 0.1336\n",
      "Epoch 5/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.7824\n",
      "Epoch 00005: val_loss did not improve from 0.12131\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7820 - val_loss: 0.1527\n",
      " ###3 fold : val acc1 0.553, acc3 0.964, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/279 [============================>.] - ETA: 0s - loss: 2.9609\n",
      "Epoch 00001: val_loss improved from inf to 0.15404, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 2.8933 - val_loss: 0.1540\n",
      "Epoch 2/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.8599\n",
      "Epoch 00002: val_loss improved from 0.15404 to 0.14741, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8569 - val_loss: 0.1474\n",
      "Epoch 3/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.8083\n",
      "Epoch 00003: val_loss improved from 0.14741 to 0.14031, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8104 - val_loss: 0.1403\n",
      "Epoch 4/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.7794\n",
      "Epoch 00004: val_loss improved from 0.14031 to 0.10444, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7813 - val_loss: 0.1044\n",
      "Epoch 5/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.7758\n",
      "Epoch 00005: val_loss did not improve from 0.10444\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7768 - val_loss: 0.1491\n",
      "Epoch 6/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.7349\n",
      "Epoch 00006: val_loss did not improve from 0.10444\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7335 - val_loss: 0.1362\n",
      " ###4 fold : val acc1 0.567, acc3 0.975, mae 0.229###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267/279 [===========================>..] - ETA: 0s - loss: 2.9844\n",
      "Epoch 00001: val_loss improved from inf to 0.15809, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.8932 - val_loss: 0.1581\n",
      "Epoch 2/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.8566\n",
      "Epoch 00002: val_loss improved from 0.15809 to 0.15029, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8539 - val_loss: 0.1503\n",
      "Epoch 3/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.8119\n",
      "Epoch 00003: val_loss improved from 0.15029 to 0.12982, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8139 - val_loss: 0.1298\n",
      "Epoch 4/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.7804\n",
      "Epoch 00004: val_loss improved from 0.12982 to 0.12079, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7825 - val_loss: 0.1208\n",
      "Epoch 5/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.7758\n",
      "Epoch 00005: val_loss did not improve from 0.12079\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7763 - val_loss: 0.1338\n",
      "Epoch 6/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.7301\n",
      "Epoch 00006: val_loss did not improve from 0.12079\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7288 - val_loss: 0.1378\n",
      " ###5 fold : val acc1 0.552, acc3 0.961, mae 0.244###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271/279 [============================>.] - ETA: 0s - loss: 2.9480\n",
      "Epoch 00001: val_loss improved from inf to 0.15601, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 2.8899 - val_loss: 0.1560\n",
      "Epoch 2/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.8574\n",
      "Epoch 00002: val_loss improved from 0.15601 to 0.14254, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8573 - val_loss: 0.1425\n",
      "Epoch 3/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.8109\n",
      "Epoch 00003: val_loss improved from 0.14254 to 0.13924, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8109 - val_loss: 0.1392\n",
      "Epoch 4/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.7868\n",
      "Epoch 00004: val_loss improved from 0.13924 to 0.11950, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7873 - val_loss: 0.1195\n",
      "Epoch 5/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.7813\n",
      "Epoch 00005: val_loss did not improve from 0.11950\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7809 - val_loss: 0.1397\n",
      "Epoch 6/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.7360\n",
      "Epoch 00006: val_loss did not improve from 0.11950\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7339 - val_loss: 0.1365\n",
      " ###6 fold : val acc1 0.546, acc3 0.970, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267/279 [===========================>..] - ETA: 0s - loss: 2.9630\n",
      "Epoch 00001: val_loss improved from inf to 0.15440, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 2.8726 - val_loss: 0.1544\n",
      "Epoch 2/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.8571\n",
      "Epoch 00002: val_loss improved from 0.15440 to 0.13842, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8555 - val_loss: 0.1384\n",
      "Epoch 3/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.8090\n",
      "Epoch 00003: val_loss did not improve from 0.13842\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8116 - val_loss: 0.1417\n",
      "Epoch 4/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.7818\n",
      "Epoch 00004: val_loss improved from 0.13842 to 0.11967, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7836 - val_loss: 0.1197\n",
      "Epoch 5/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.7756\n",
      "Epoch 00005: val_loss did not improve from 0.11967\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7771 - val_loss: 0.1366\n",
      "Epoch 6/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.7296\n",
      "Epoch 00006: val_loss did not improve from 0.11967\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7291 - val_loss: 0.1316\n",
      " ###7 fold : val acc1 0.549, acc3 0.953, mae 0.250###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267/279 [===========================>..] - ETA: 0s - loss: 2.9756\n",
      "Epoch 00001: val_loss improved from inf to 0.16424, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 2.8845 - val_loss: 0.1642\n",
      "Epoch 2/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.8564\n",
      "Epoch 00002: val_loss improved from 0.16424 to 0.14476, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8553 - val_loss: 0.1448\n",
      "Epoch 3/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.8124\n",
      "Epoch 00003: val_loss improved from 0.14476 to 0.13986, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8153 - val_loss: 0.1399\n",
      "Epoch 4/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.7813\n",
      "Epoch 00004: val_loss improved from 0.13986 to 0.12212, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7829 - val_loss: 0.1221\n",
      "Epoch 5/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.7778\n",
      "Epoch 00005: val_loss did not improve from 0.12212\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7776 - val_loss: 0.1450\n",
      "Epoch 6/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.7289\n",
      "Epoch 00006: val_loss did not improve from 0.12212\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7297 - val_loss: 0.1320\n",
      " ###8 fold : val acc1 0.550, acc3 0.960, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/279 [===========================>..] - ETA: 0s - loss: 2.9683\n",
      "Epoch 00001: val_loss improved from inf to 0.16832, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 2.8845 - val_loss: 0.1683\n",
      "Epoch 2/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.8565\n",
      "Epoch 00002: val_loss improved from 0.16832 to 0.14797, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8553 - val_loss: 0.1480\n",
      "Epoch 3/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.8127\n",
      "Epoch 00003: val_loss improved from 0.14797 to 0.14316, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8153 - val_loss: 0.1432\n",
      "Epoch 4/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.7813\n",
      "Epoch 00004: val_loss improved from 0.14316 to 0.12546, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7829 - val_loss: 0.1255\n",
      "Epoch 5/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.7758\n",
      "Epoch 00005: val_loss did not improve from 0.12546\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7776 - val_loss: 0.1459\n",
      "Epoch 6/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.7298\n",
      "Epoch 00006: val_loss did not improve from 0.12546\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7297 - val_loss: 0.1356\n",
      " ###9 fold : val acc1 0.565, acc3 0.969, mae 0.234###\n",
      "acc10.553_acc30.965\n",
      "random search 24/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/279 [============================>.] - ETA: 0s - loss: 1.4172\n",
      "Epoch 00001: val_loss improved from inf to 0.13235, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 1.4024 - val_loss: 0.1324\n",
      "Epoch 2/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.2838\n",
      "Epoch 00002: val_loss improved from 0.13235 to 0.10734, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2844 - val_loss: 0.1073\n",
      "Epoch 3/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.2607\n",
      "Epoch 00003: val_loss did not improve from 0.10734\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2609 - val_loss: 0.1083\n",
      "Epoch 4/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.2610\n",
      "Epoch 00004: val_loss did not improve from 0.10734\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2608 - val_loss: 0.1121\n",
      " ###0 fold : val acc1 0.585, acc3 0.971, mae 0.223###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/279 [============================>.] - ETA: 0s - loss: 1.4329\n",
      "Epoch 00001: val_loss improved from inf to 0.13264, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 1.4144 - val_loss: 0.1326\n",
      "Epoch 2/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.2831\n",
      "Epoch 00002: val_loss improved from 0.13264 to 0.11111, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2838 - val_loss: 0.1111\n",
      "Epoch 3/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.2612\n",
      "Epoch 00003: val_loss did not improve from 0.11111\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2608 - val_loss: 0.1133\n",
      "Epoch 4/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.2592\n",
      "Epoch 00004: val_loss improved from 0.11111 to 0.10367, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2588 - val_loss: 0.1037\n",
      "Epoch 5/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.2455\n",
      "Epoch 00005: val_loss improved from 0.10367 to 0.10310, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2467 - val_loss: 0.1031\n",
      "Epoch 6/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.2389\n",
      "Epoch 00006: val_loss did not improve from 0.10310\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2387 - val_loss: 0.1046\n",
      "Epoch 7/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.2390\n",
      "Epoch 00007: val_loss did not improve from 0.10310\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2379 - val_loss: 0.1060\n",
      " ###1 fold : val acc1 0.573, acc3 0.980, mae 0.224###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/279 [============================>.] - ETA: 0s - loss: 1.4183\n",
      "Epoch 00001: val_loss improved from inf to 0.13175, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 1.4079 - val_loss: 0.1318\n",
      "Epoch 2/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2835\n",
      "Epoch 00002: val_loss improved from 0.13175 to 0.11204, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2833 - val_loss: 0.1120\n",
      "Epoch 3/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.2637\n",
      "Epoch 00003: val_loss improved from 0.11204 to 0.10858, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2637 - val_loss: 0.1086\n",
      "Epoch 4/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.2582\n",
      "Epoch 00004: val_loss improved from 0.10858 to 0.09906, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2575 - val_loss: 0.0991\n",
      "Epoch 5/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.2485\n",
      "Epoch 00005: val_loss did not improve from 0.09906\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2494 - val_loss: 0.1079\n",
      "Epoch 6/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.2385\n",
      "Epoch 00006: val_loss did not improve from 0.09906\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2385 - val_loss: 0.1025\n",
      " ###2 fold : val acc1 0.583, acc3 0.979, mae 0.220###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/279 [============================>.] - ETA: 0s - loss: 1.4269\n",
      "Epoch 00001: val_loss improved from inf to 0.13365, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 1.4047 - val_loss: 0.1337\n",
      "Epoch 2/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.2855\n",
      "Epoch 00002: val_loss improved from 0.13365 to 0.11697, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2846 - val_loss: 0.1170\n",
      "Epoch 3/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2638\n",
      "Epoch 00003: val_loss improved from 0.11697 to 0.10477, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2639 - val_loss: 0.1048\n",
      "Epoch 4/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.2556\n",
      "Epoch 00004: val_loss improved from 0.10477 to 0.10333, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2551 - val_loss: 0.1033\n",
      "Epoch 5/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.2477\n",
      "Epoch 00005: val_loss did not improve from 0.10333\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2486 - val_loss: 0.1068\n",
      "Epoch 6/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.2399\n",
      "Epoch 00006: val_loss improved from 0.10333 to 0.10005, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2399 - val_loss: 0.1001\n",
      "Epoch 7/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.2382\n",
      "Epoch 00007: val_loss did not improve from 0.10005\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2381 - val_loss: 0.1076\n",
      "Epoch 8/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.2348\n",
      "Epoch 00008: val_loss did not improve from 0.10005\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2338 - val_loss: 0.1004\n",
      " ###3 fold : val acc1 0.606, acc3 0.973, mae 0.211###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/279 [============================>.] - ETA: 0s - loss: 1.4108\n",
      "Epoch 00001: val_loss improved from inf to 0.13226, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 1.3844 - val_loss: 0.1323\n",
      "Epoch 2/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.2856\n",
      "Epoch 00002: val_loss improved from 0.13226 to 0.10604, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2858 - val_loss: 0.1060\n",
      "Epoch 3/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.2587\n",
      "Epoch 00003: val_loss improved from 0.10604 to 0.10463, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2586 - val_loss: 0.1046\n",
      "Epoch 4/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.2531\n",
      "Epoch 00004: val_loss did not improve from 0.10463\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2531 - val_loss: 0.1114\n",
      "Epoch 5/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.2454\n",
      "Epoch 00005: val_loss improved from 0.10463 to 0.10226, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2458 - val_loss: 0.1023\n",
      "Epoch 6/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2408\n",
      "Epoch 00006: val_loss did not improve from 0.10226\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2402 - val_loss: 0.1035\n",
      "Epoch 7/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.2299\n",
      "Epoch 00007: val_loss improved from 0.10226 to 0.09733, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2302 - val_loss: 0.0973\n",
      "Epoch 8/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.2257\n",
      "Epoch 00008: val_loss did not improve from 0.09733\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2269 - val_loss: 0.0990\n",
      "Epoch 9/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.2268\n",
      "Epoch 00009: val_loss did not improve from 0.09733\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2269 - val_loss: 0.1109\n",
      " ###4 fold : val acc1 0.583, acc3 0.977, mae 0.220###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/279 [============================>.] - ETA: 0s - loss: 1.4050\n",
      "Epoch 00001: val_loss improved from inf to 0.13704, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 1.3708 - val_loss: 0.1370\n",
      "Epoch 2/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.2849\n",
      "Epoch 00002: val_loss improved from 0.13704 to 0.10804, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2849 - val_loss: 0.1080\n",
      "Epoch 3/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.2576\n",
      "Epoch 00003: val_loss improved from 0.10804 to 0.10153, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2573 - val_loss: 0.1015\n",
      "Epoch 4/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.2520\n",
      "Epoch 00004: val_loss did not improve from 0.10153\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2526 - val_loss: 0.1034\n",
      "Epoch 5/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2455\n",
      "Epoch 00005: val_loss did not improve from 0.10153\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2456 - val_loss: 0.1029\n",
      " ###5 fold : val acc1 0.582, acc3 0.977, mae 0.221###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/279 [============================>.] - ETA: 0s - loss: 1.3673\n",
      "Epoch 00001: val_loss improved from inf to 0.12975, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 1.3652 - val_loss: 0.1297\n",
      "Epoch 2/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.2869\n",
      "Epoch 00002: val_loss improved from 0.12975 to 0.11195, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2851 - val_loss: 0.1120\n",
      "Epoch 3/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.2596\n",
      "Epoch 00003: val_loss improved from 0.11195 to 0.10492, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2599 - val_loss: 0.1049\n",
      "Epoch 4/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.2524"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.1470\n",
      "Epoch 00006: val_loss improved from 1.22101 to 0.71061, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.0325 - val_loss: 0.7106\n",
      "Epoch 7/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6884\n",
      "Epoch 00007: val_loss improved from 0.71061 to 0.41400, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6237 - val_loss: 0.4140\n",
      "Epoch 8/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.4272\n",
      "Epoch 00008: val_loss improved from 0.41400 to 0.26153, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4006 - val_loss: 0.2615\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3148\n",
      "Epoch 00009: val_loss improved from 0.26153 to 0.18961, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2998 - val_loss: 0.1896\n",
      "Epoch 10/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2547\n",
      "Epoch 00010: val_loss improved from 0.18961 to 0.15861, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2470 - val_loss: 0.1586\n",
      "Epoch 11/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2302\n",
      "Epoch 00011: val_loss improved from 0.15861 to 0.14542, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2256 - val_loss: 0.1454\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2249\n",
      "Epoch 00012: val_loss improved from 0.14542 to 0.13884, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2199 - val_loss: 0.1388\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2087\n",
      "Epoch 00013: val_loss improved from 0.13884 to 0.13555, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2093 - val_loss: 0.1356\n",
      "Epoch 14/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2064\n",
      "Epoch 00014: val_loss improved from 0.13555 to 0.13305, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2076 - val_loss: 0.1330\n",
      "Epoch 15/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00015: val_loss improved from 0.13305 to 0.13141, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2095 - val_loss: 0.1314\n",
      "Epoch 16/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2067\n",
      "Epoch 00016: val_loss improved from 0.13141 to 0.12956, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2043 - val_loss: 0.1296\n",
      "Epoch 17/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2080\n",
      "Epoch 00017: val_loss improved from 0.12956 to 0.12831, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2059 - val_loss: 0.1283\n",
      "Epoch 18/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1981\n",
      "Epoch 00018: val_loss improved from 0.12831 to 0.12687, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2000 - val_loss: 0.1269\n",
      "Epoch 19/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1967\n",
      "Epoch 00019: val_loss improved from 0.12687 to 0.12546, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1956 - val_loss: 0.1255\n",
      "Epoch 20/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1933\n",
      "Epoch 00020: val_loss improved from 0.12546 to 0.12403, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1917 - val_loss: 0.1240\n",
      "Epoch 21/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1973\n",
      "Epoch 00021: val_loss improved from 0.12403 to 0.12264, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1973 - val_loss: 0.1226\n",
      "Epoch 22/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1927\n",
      "Epoch 00022: val_loss improved from 0.12264 to 0.12206, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1924 - val_loss: 0.1221\n",
      "Epoch 23/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1917\n",
      "Epoch 00023: val_loss improved from 0.12206 to 0.12051, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1906 - val_loss: 0.1205\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1859\n",
      "Epoch 00024: val_loss improved from 0.12051 to 0.11939, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1891 - val_loss: 0.1194\n",
      "Epoch 25/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1902\n",
      "Epoch 00025: val_loss improved from 0.11939 to 0.11812, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1890 - val_loss: 0.1181\n",
      "Epoch 26/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1822\n",
      "Epoch 00026: val_loss improved from 0.11812 to 0.11700, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1853 - val_loss: 0.1170\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1838\n",
      "Epoch 00027: val_loss improved from 0.11700 to 0.11641, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1838 - val_loss: 0.1164\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1860\n",
      "Epoch 00028: val_loss improved from 0.11641 to 0.11549, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1860 - val_loss: 0.1155\n",
      "Epoch 29/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1812\n",
      "Epoch 00029: val_loss improved from 0.11549 to 0.11446, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1805 - val_loss: 0.1145\n",
      "Epoch 30/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1776\n",
      "Epoch 00030: val_loss improved from 0.11446 to 0.11403, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1785 - val_loss: 0.1140\n",
      "Epoch 31/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1798\n",
      "Epoch 00031: val_loss improved from 0.11403 to 0.11320, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1804 - val_loss: 0.1132\n",
      "Epoch 32/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1782\n",
      "Epoch 00032: val_loss improved from 0.11320 to 0.11233, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1787 - val_loss: 0.1123\n",
      "Epoch 33/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1783\n",
      "Epoch 00033: val_loss improved from 0.11233 to 0.11193, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1794 - val_loss: 0.1119\n",
      "Epoch 34/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1774\n",
      "Epoch 00034: val_loss improved from 0.11193 to 0.11103, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1774 - val_loss: 0.1110\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1758\n",
      "Epoch 00035: val_loss improved from 0.11103 to 0.11022, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1758 - val_loss: 0.1102\n",
      "Epoch 36/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1716\n",
      "Epoch 00036: val_loss improved from 0.11022 to 0.11005, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1727 - val_loss: 0.1101\n",
      "Epoch 37/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1714\n",
      "Epoch 00037: val_loss improved from 0.11005 to 0.10917, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1730 - val_loss: 0.1092\n",
      "Epoch 38/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1701\n",
      "Epoch 00038: val_loss improved from 0.10917 to 0.10884, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1721 - val_loss: 0.1088\n",
      "Epoch 39/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1728\n",
      "Epoch 00039: val_loss improved from 0.10884 to 0.10827, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1746 - val_loss: 0.1083\n",
      "Epoch 40/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1703\n",
      "Epoch 00040: val_loss improved from 0.10827 to 0.10785, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1717 - val_loss: 0.1079\n",
      "Epoch 41/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1700\n",
      "Epoch 00041: val_loss improved from 0.10785 to 0.10769, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1692 - val_loss: 0.1077\n",
      "Epoch 42/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1697\n",
      "Epoch 00042: val_loss improved from 0.10769 to 0.10686, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1703 - val_loss: 0.1069\n",
      "Epoch 43/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1716\n",
      "Epoch 00043: val_loss did not improve from 0.10686\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1692 - val_loss: 0.1071\n",
      "Epoch 44/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 0.1690\n",
      "Epoch 00044: val_loss improved from 0.10686 to 0.10595, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1697 - val_loss: 0.1060\n",
      "Epoch 45/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 0.1680\n",
      "Epoch 00045: val_loss improved from 0.10595 to 0.10589, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1661 - val_loss: 0.1059\n",
      "Epoch 46/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1663\n",
      "Epoch 00046: val_loss improved from 0.10589 to 0.10548, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1658 - val_loss: 0.1055\n",
      "Epoch 47/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1672\n",
      "Epoch 00047: val_loss improved from 0.10548 to 0.10530, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1654 - val_loss: 0.1053\n",
      "Epoch 48/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1663\n",
      "Epoch 00048: val_loss improved from 0.10530 to 0.10522, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1650 - val_loss: 0.1052\n",
      "Epoch 49/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1659\n",
      "Epoch 00049: val_loss improved from 0.10522 to 0.10473, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1655 - val_loss: 0.1047\n",
      "Epoch 50/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1612\n",
      "Epoch 00050: val_loss improved from 0.10473 to 0.10454, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1627 - val_loss: 0.1045\n",
      "Epoch 51/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1636\n",
      "Epoch 00051: val_loss improved from 0.10454 to 0.10417, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1636 - val_loss: 0.1042\n",
      "Epoch 52/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1619\n",
      "Epoch 00052: val_loss improved from 0.10417 to 0.10411, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1621 - val_loss: 0.1041\n",
      "Epoch 53/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1606\n",
      "Epoch 00053: val_loss improved from 0.10411 to 0.10390, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1616 - val_loss: 0.1039\n",
      "Epoch 54/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1593\n",
      "Epoch 00054: val_loss improved from 0.10390 to 0.10360, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1601 - val_loss: 0.1036\n",
      "Epoch 55/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1625\n",
      "Epoch 00055: val_loss improved from 0.10360 to 0.10349, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1612 - val_loss: 0.1035\n",
      "Epoch 56/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1652\n",
      "Epoch 00056: val_loss did not improve from 0.10349\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1615 - val_loss: 0.1044\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1580\n",
      "Epoch 00057: val_loss improved from 0.10349 to 0.10326, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1580 - val_loss: 0.1033\n",
      "Epoch 58/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1586\n",
      "Epoch 00058: val_loss did not improve from 0.10326\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1593 - val_loss: 0.1040\n",
      "Epoch 59/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1615\n",
      "Epoch 00059: val_loss improved from 0.10326 to 0.10310, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1594 - val_loss: 0.1031\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1578\n",
      "Epoch 00060: val_loss did not improve from 0.10310\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1578 - val_loss: 0.1032\n",
      "Epoch 61/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1554\n",
      "Epoch 00061: val_loss did not improve from 0.10310\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1588 - val_loss: 0.1032\n",
      " ###9 fold : val acc1 0.597, acc3 0.983, mae 0.210###\n",
      "acc10.597_acc30.979\n",
      "random search 28/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/140 [==========================>...] - ETA: 0s - loss: 1.8580\n",
      "Epoch 00001: val_loss improved from inf to 0.13355, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 1.7466 - val_loss: 0.1336\n",
      "Epoch 2/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1686\n",
      "Epoch 00002: val_loss improved from 0.13355 to 0.10810, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1685 - val_loss: 0.1081\n",
      "Epoch 3/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1544\n",
      "Epoch 00003: val_loss improved from 0.10810 to 0.10452, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1545 - val_loss: 0.1045\n",
      "Epoch 4/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1527\n",
      "Epoch 00004: val_loss improved from 0.10452 to 0.10282, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1525 - val_loss: 0.1028\n",
      "Epoch 5/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1460\n",
      "Epoch 00005: val_loss did not improve from 0.10282\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1457 - val_loss: 0.1032\n",
      "Epoch 6/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1441\n",
      "Epoch 00006: val_loss did not improve from 0.10282\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1441 - val_loss: 0.1049\n",
      " ###0 fold : val acc1 0.581, acc3 0.973, mae 0.224###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/140 [===========================>..] - ETA: 0s - loss: 1.8156\n",
      "Epoch 00001: val_loss improved from inf to 0.12841, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 1.7419 - val_loss: 0.1284\n",
      "Epoch 2/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1671\n",
      "Epoch 00002: val_loss improved from 0.12841 to 0.10620, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1669 - val_loss: 0.1062\n",
      "Epoch 3/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1537\n",
      "Epoch 00003: val_loss improved from 0.10620 to 0.10124, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1537 - val_loss: 0.1012\n",
      "Epoch 4/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1496\n",
      "Epoch 00004: val_loss did not improve from 0.10124\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1499 - val_loss: 0.1029\n",
      "Epoch 5/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1452\n",
      "Epoch 00005: val_loss did not improve from 0.10124\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1452 - val_loss: 0.1038\n",
      " ###1 fold : val acc1 0.578, acc3 0.979, mae 0.222###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/140 [===========================>..] - ETA: 0s - loss: 1.7985\n",
      "Epoch 00001: val_loss improved from inf to 0.12574, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 1.7373 - val_loss: 0.1257\n",
      "Epoch 2/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1664\n",
      "Epoch 00002: val_loss improved from 0.12574 to 0.10658, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1661 - val_loss: 0.1066\n",
      "Epoch 3/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1548\n",
      "Epoch 00003: val_loss improved from 0.10658 to 0.10253, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1548 - val_loss: 0.1025\n",
      "Epoch 4/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1498\n",
      "Epoch 00004: val_loss improved from 0.10253 to 0.10122, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1499 - val_loss: 0.1012\n",
      "Epoch 5/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1470\n",
      "Epoch 00005: val_loss did not improve from 0.10122\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1470 - val_loss: 0.1019\n",
      "Epoch 6/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1437\n",
      "Epoch 00006: val_loss did not improve from 0.10122\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1438 - val_loss: 0.1016\n",
      " ###2 fold : val acc1 0.595, acc3 0.975, mae 0.215###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/140 [==========================>...] - ETA: 0s - loss: 1.8618\n",
      "Epoch 00001: val_loss improved from inf to 0.12288, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 1.7263 - val_loss: 0.1229\n",
      "Epoch 2/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1662\n",
      "Epoch 00002: val_loss improved from 0.12288 to 0.10562, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1663 - val_loss: 0.1056\n",
      "Epoch 3/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1536\n",
      "Epoch 00003: val_loss improved from 0.10562 to 0.10257, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1539 - val_loss: 0.1026\n",
      "Epoch 4/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1479\n",
      "Epoch 00004: val_loss improved from 0.10257 to 0.09886, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1478 - val_loss: 0.0989\n",
      "Epoch 5/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1456\n",
      "Epoch 00005: val_loss did not improve from 0.09886\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1459 - val_loss: 0.0992\n",
      "Epoch 6/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1444\n",
      "Epoch 00006: val_loss did not improve from 0.09886\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1443 - val_loss: 0.0991\n",
      " ###3 fold : val acc1 0.599, acc3 0.974, mae 0.214###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/140 [=========================>....] - ETA: 0s - loss: 1.8486\n",
      "Epoch 00001: val_loss improved from inf to 0.12080, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 1.6655 - val_loss: 0.1208\n",
      "Epoch 2/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1682\n",
      "Epoch 00002: val_loss improved from 0.12080 to 0.10367, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1673 - val_loss: 0.1037\n",
      "Epoch 3/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1519\n",
      "Epoch 00003: val_loss improved from 0.10367 to 0.09913, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1512 - val_loss: 0.0991\n",
      "Epoch 4/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1475\n",
      "Epoch 00004: val_loss did not improve from 0.09913\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1474 - val_loss: 0.1009\n",
      "Epoch 5/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.1454\n",
      "Epoch 00005: val_loss did not improve from 0.09913\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1458 - val_loss: 0.0995\n",
      " ###4 fold : val acc1 0.576, acc3 0.978, mae 0.223###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/140 [===========================>..] - ETA: 0s - loss: 1.7373\n",
      "Epoch 00001: val_loss improved from inf to 0.12290, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 1.6671 - val_loss: 0.1229\n",
      "Epoch 2/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1678\n",
      "Epoch 00002: val_loss improved from 0.12290 to 0.10589, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1678 - val_loss: 0.1059\n",
      "Epoch 3/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1536\n",
      "Epoch 00003: val_loss improved from 0.10589 to 0.10053, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1527 - val_loss: 0.1005\n",
      "Epoch 4/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1485\n",
      "Epoch 00004: val_loss did not improve from 0.10053\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1484 - val_loss: 0.1021\n",
      "Epoch 5/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1467\n",
      "Epoch 00005: val_loss improved from 0.10053 to 0.09817, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1467 - val_loss: 0.0982\n",
      "Epoch 6/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1430\n",
      "Epoch 00006: val_loss improved from 0.09817 to 0.09568, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1430 - val_loss: 0.0957\n",
      "Epoch 7/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1428\n",
      "Epoch 00007: val_loss did not improve from 0.09568\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1429 - val_loss: 0.1002\n",
      "Epoch 8/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1404\n",
      "Epoch 00008: val_loss did not improve from 0.09568\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1410 - val_loss: 0.1066\n",
      " ###5 fold : val acc1 0.580, acc3 0.982, mae 0.219###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/140 [============================>.] - ETA: 0s - loss: 1.6866\n",
      "Epoch 00001: val_loss improved from inf to 0.12155, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 1.6618 - val_loss: 0.1216\n",
      "Epoch 2/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1672\n",
      "Epoch 00002: val_loss improved from 0.12155 to 0.10489, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1673 - val_loss: 0.1049\n",
      "Epoch 3/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1533\n",
      "Epoch 00003: val_loss improved from 0.10489 to 0.10238, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1530 - val_loss: 0.1024\n",
      "Epoch 4/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1502\n",
      "Epoch 00004: val_loss improved from 0.10238 to 0.10224, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1502 - val_loss: 0.1022\n",
      "Epoch 5/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1485\n",
      "Epoch 00005: val_loss improved from 0.10224 to 0.09766, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1487 - val_loss: 0.0977\n",
      "Epoch 6/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1440\n",
      "Epoch 00006: val_loss improved from 0.09766 to 0.09691, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1429 - val_loss: 0.0969\n",
      "Epoch 7/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1418\n",
      "Epoch 00007: val_loss did not improve from 0.09691\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1418 - val_loss: 0.0979\n",
      "Epoch 8/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1421\n",
      "Epoch 00008: val_loss did not improve from 0.09691\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1423 - val_loss: 0.1140\n",
      " ###6 fold : val acc1 0.587, acc3 0.987, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/140 [============================>.] - ETA: 0s - loss: 1.7053\n",
      "Epoch 00001: val_loss improved from inf to 0.12149, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 1.6694 - val_loss: 0.1215\n",
      "Epoch 2/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1663\n",
      "Epoch 00002: val_loss improved from 0.12149 to 0.10613, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1659 - val_loss: 0.1061\n",
      "Epoch 3/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1525\n",
      "Epoch 00003: val_loss improved from 0.10613 to 0.10256, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1518 - val_loss: 0.1026\n",
      "Epoch 4/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1491\n",
      "Epoch 00004: val_loss did not improve from 0.10256\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1487 - val_loss: 0.1031\n",
      "Epoch 5/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1461\n",
      "Epoch 00005: val_loss improved from 0.10256 to 0.10212, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1462 - val_loss: 0.1021\n",
      "Epoch 6/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1427\n",
      "Epoch 00006: val_loss improved from 0.10212 to 0.09856, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1426 - val_loss: 0.0986\n",
      "Epoch 7/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.1403\n",
      "Epoch 00007: val_loss improved from 0.09856 to 0.09556, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1402 - val_loss: 0.0956\n",
      "Epoch 8/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1411\n",
      "Epoch 00008: val_loss did not improve from 0.09556\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1418 - val_loss: 0.1109\n",
      "Epoch 9/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1459\n",
      "Epoch 00009: val_loss did not improve from 0.09556\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1465 - val_loss: 0.0990\n",
      " ###7 fold : val acc1 0.589, acc3 0.975, mae 0.218###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/140 [==========================>...] - ETA: 0s - loss: 1.7816\n",
      "Epoch 00001: val_loss improved from inf to 0.12646, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 1.6749 - val_loss: 0.1265\n",
      "Epoch 2/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1661\n",
      "Epoch 00002: val_loss improved from 0.12646 to 0.11041, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1658 - val_loss: 0.1104\n",
      "Epoch 3/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1518\n",
      "Epoch 00003: val_loss improved from 0.11041 to 0.10829, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1517 - val_loss: 0.1083\n",
      "Epoch 4/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1485\n",
      "Epoch 00004: val_loss improved from 0.10829 to 0.10738, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1488 - val_loss: 0.1074\n",
      "Epoch 5/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1467\n",
      "Epoch 00005: val_loss improved from 0.10738 to 0.10663, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1470 - val_loss: 0.1066\n",
      "Epoch 6/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1431\n",
      "Epoch 00006: val_loss improved from 0.10663 to 0.10032, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1432 - val_loss: 0.1003\n",
      "Epoch 7/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1404\n",
      "Epoch 00007: val_loss improved from 0.10032 to 0.09804, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1404 - val_loss: 0.0980\n",
      "Epoch 8/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1422\n",
      "Epoch 00008: val_loss did not improve from 0.09804\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1421 - val_loss: 0.1098\n",
      "Epoch 9/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1450\n",
      "Epoch 00009: val_loss did not improve from 0.09804\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1453 - val_loss: 0.1013\n",
      " ###8 fold : val acc1 0.590, acc3 0.984, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/140 [===========================>..] - ETA: 0s - loss: 1.7222\n",
      "Epoch 00001: val_loss improved from inf to 0.12809, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 1.6749 - val_loss: 0.1281\n",
      "Epoch 2/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1675\n",
      "Epoch 00002: val_loss improved from 0.12809 to 0.11338, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1658 - val_loss: 0.1134\n",
      "Epoch 3/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1527\n",
      "Epoch 00003: val_loss improved from 0.11338 to 0.10903, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1517 - val_loss: 0.1090\n",
      "Epoch 4/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1491\n",
      "Epoch 00004: val_loss improved from 0.10903 to 0.10829, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1488 - val_loss: 0.1083\n",
      "Epoch 5/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1471\n",
      "Epoch 00005: val_loss did not improve from 0.10829\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1470 - val_loss: 0.1087\n",
      "Epoch 6/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1431\n",
      "Epoch 00006: val_loss improved from 0.10829 to 0.10218, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1432 - val_loss: 0.1022\n",
      "Epoch 7/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1404\n",
      "Epoch 00007: val_loss improved from 0.10218 to 0.10055, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1404 - val_loss: 0.1005\n",
      "Epoch 8/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1422\n",
      "Epoch 00008: val_loss did not improve from 0.10055\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1421 - val_loss: 0.1123\n",
      "Epoch 9/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1447\n",
      "Epoch 00009: val_loss did not improve from 0.10055\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1453 - val_loss: 0.1028\n",
      " ###9 fold : val acc1 0.605, acc3 0.983, mae 0.206###\n",
      "acc10.588_acc30.979\n",
      "random search 29/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/140 [============================>.] - ETA: 0s - loss: 11.6192\n",
      "Epoch 00001: val_loss improved from inf to 2.66810, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 11.4295 - val_loss: 2.6681\n",
      "Epoch 2/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 2.4457\n",
      "Epoch 00002: val_loss improved from 2.66810 to 0.98076, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 2.4070 - val_loss: 0.9808\n",
      "Epoch 3/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 1.2950\n",
      "Epoch 00003: val_loss improved from 0.98076 to 0.38175, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.2648 - val_loss: 0.3818\n",
      "Epoch 4/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.9307\n",
      "Epoch 00004: val_loss improved from 0.38175 to 0.24582, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.9248 - val_loss: 0.2458\n",
      "Epoch 5/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.7561\n",
      "Epoch 00005: val_loss improved from 0.24582 to 0.18670, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7558 - val_loss: 0.1867\n",
      "Epoch 6/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.6467\n",
      "Epoch 00006: val_loss improved from 0.18670 to 0.16525, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6454 - val_loss: 0.1653\n",
      "Epoch 7/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.5601\n",
      "Epoch 00007: val_loss improved from 0.16525 to 0.14473, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5581 - val_loss: 0.1447\n",
      "Epoch 8/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.4889\n",
      "Epoch 00008: val_loss improved from 0.14473 to 0.13431, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4895 - val_loss: 0.1343\n",
      "Epoch 9/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.4411\n",
      "Epoch 00009: val_loss improved from 0.13431 to 0.12727, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4408 - val_loss: 0.1273\n",
      "Epoch 10/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.4134\n",
      "Epoch 00010: val_loss improved from 0.12727 to 0.11685, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4120 - val_loss: 0.1169\n",
      "Epoch 11/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3839\n",
      "Epoch 00011: val_loss improved from 0.11685 to 0.11637, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3840 - val_loss: 0.1164\n",
      "Epoch 12/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.3572\n",
      "Epoch 00012: val_loss improved from 0.11637 to 0.11365, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3580 - val_loss: 0.1137\n",
      "Epoch 13/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3436\n",
      "Epoch 00013: val_loss did not improve from 0.11365\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3432 - val_loss: 0.1153\n",
      "Epoch 14/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.3307\n",
      "Epoch 00014: val_loss improved from 0.11365 to 0.11184, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3310 - val_loss: 0.1118\n",
      "Epoch 15/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.3304\n",
      "Epoch 00015: val_loss improved from 0.11184 to 0.10729, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3294 - val_loss: 0.1073\n",
      "Epoch 16/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.3230\n",
      "Epoch 00016: val_loss improved from 0.10729 to 0.10723, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3223 - val_loss: 0.1072\n",
      "Epoch 17/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.3132\n",
      "Epoch 00017: val_loss improved from 0.10723 to 0.10373, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3132 - val_loss: 0.1037\n",
      "Epoch 18/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.3073\n",
      "Epoch 00018: val_loss did not improve from 0.10373\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3065 - val_loss: 0.1072\n",
      "Epoch 19/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3039\n",
      "Epoch 00019: val_loss did not improve from 0.10373\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3037 - val_loss: 0.1056\n",
      " ###0 fold : val acc1 0.558, acc3 0.977, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/140 [===========================>..] - ETA: 0s - loss: 11.8217\n",
      "Epoch 00001: val_loss improved from inf to 2.66501, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 11.4399 - val_loss: 2.6650\n",
      "Epoch 2/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 2.4709\n",
      "Epoch 00002: val_loss improved from 2.66501 to 0.96632, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 2.3991 - val_loss: 0.9663\n",
      "Epoch 3/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 1.2499\n",
      "Epoch 00003: val_loss improved from 0.96632 to 0.38177, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.2496 - val_loss: 0.3818\n",
      "Epoch 4/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.9193\n",
      "Epoch 00004: val_loss improved from 0.38177 to 0.24519, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.9172 - val_loss: 0.2452\n",
      "Epoch 5/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.7577\n",
      "Epoch 00005: val_loss improved from 0.24519 to 0.18550, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7535 - val_loss: 0.1855\n",
      "Epoch 6/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.6458\n",
      "Epoch 00006: val_loss improved from 0.18550 to 0.16403, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6443 - val_loss: 0.1640\n",
      "Epoch 7/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.5609\n",
      "Epoch 00007: val_loss improved from 0.16403 to 0.14672, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5590 - val_loss: 0.1467\n",
      "Epoch 8/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.4862\n",
      "Epoch 00008: val_loss improved from 0.14672 to 0.13861, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4861 - val_loss: 0.1386\n",
      "Epoch 9/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.4390\n",
      "Epoch 00009: val_loss improved from 0.13861 to 0.12900, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4385 - val_loss: 0.1290\n",
      "Epoch 10/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.4116\n",
      "Epoch 00010: val_loss improved from 0.12900 to 0.11704, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4119 - val_loss: 0.1170\n",
      "Epoch 11/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.3843\n",
      "Epoch 00011: val_loss did not improve from 0.11704\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3845 - val_loss: 0.1173\n",
      "Epoch 12/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3590\n",
      "Epoch 00012: val_loss improved from 0.11704 to 0.11034, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3598 - val_loss: 0.1103\n",
      "Epoch 13/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3451\n",
      "Epoch 00013: val_loss did not improve from 0.11034\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3451 - val_loss: 0.1133\n",
      "Epoch 14/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3332\n",
      "Epoch 00014: val_loss improved from 0.11034 to 0.10965, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3332 - val_loss: 0.1097\n",
      "Epoch 15/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3294\n",
      "Epoch 00015: val_loss improved from 0.10965 to 0.10770, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3287 - val_loss: 0.1077\n",
      "Epoch 16/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3240\n",
      "Epoch 00016: val_loss did not improve from 0.10770\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3240 - val_loss: 0.1088\n",
      "Epoch 17/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3126\n",
      "Epoch 00017: val_loss improved from 0.10770 to 0.10347, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3126 - val_loss: 0.1035\n",
      "Epoch 18/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3063\n",
      "Epoch 00018: val_loss did not improve from 0.10347\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3057 - val_loss: 0.1074\n",
      "Epoch 19/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.3044\n",
      "Epoch 00019: val_loss did not improve from 0.10347\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3043 - val_loss: 0.1046\n",
      " ###1 fold : val acc1 0.551, acc3 0.979, mae 0.235###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/140 [============================>.] - ETA: 0s - loss: 11.5538\n",
      "Epoch 00001: val_loss improved from inf to 2.66085, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 11.4211 - val_loss: 2.6609\n",
      "Epoch 2/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 2.4146\n",
      "Epoch 00002: val_loss improved from 2.66085 to 0.96031, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 2.4146 - val_loss: 0.9603\n",
      "Epoch 3/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 1.2611\n",
      "Epoch 00003: val_loss improved from 0.96031 to 0.38461, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.2504 - val_loss: 0.3846\n",
      "Epoch 4/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.9274\n",
      "Epoch 00004: val_loss improved from 0.38461 to 0.23839, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.9182 - val_loss: 0.2384\n",
      "Epoch 5/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.7544\n",
      "Epoch 00005: val_loss improved from 0.23839 to 0.18454, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7544 - val_loss: 0.1845\n",
      "Epoch 6/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.6447\n",
      "Epoch 00006: val_loss improved from 0.18454 to 0.16303, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6439 - val_loss: 0.1630\n",
      "Epoch 7/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.5596\n",
      "Epoch 00007: val_loss improved from 0.16303 to 0.14540, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5575 - val_loss: 0.1454\n",
      "Epoch 8/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.4816\n",
      "Epoch 00008: val_loss improved from 0.14540 to 0.13901, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4833 - val_loss: 0.1390\n",
      "Epoch 9/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.4418\n",
      "Epoch 00009: val_loss improved from 0.13901 to 0.12834, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4406 - val_loss: 0.1283\n",
      "Epoch 10/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.4105\n",
      "Epoch 00010: val_loss improved from 0.12834 to 0.11541, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4117 - val_loss: 0.1154\n",
      "Epoch 11/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3829\n",
      "Epoch 00011: val_loss did not improve from 0.11541\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3829 - val_loss: 0.1187\n",
      "Epoch 12/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3568\n",
      "Epoch 00012: val_loss improved from 0.11541 to 0.11190, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3568 - val_loss: 0.1119\n",
      "Epoch 13/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3456\n",
      "Epoch 00013: val_loss did not improve from 0.11190\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3456 - val_loss: 0.1172\n",
      "Epoch 14/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3335\n",
      "Epoch 00014: val_loss improved from 0.11190 to 0.11008, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3334 - val_loss: 0.1101\n",
      "Epoch 15/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3286\n",
      "Epoch 00015: val_loss improved from 0.11008 to 0.10895, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3286 - val_loss: 0.1090\n",
      "Epoch 16/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.3223\n",
      "Epoch 00016: val_loss did not improve from 0.10895\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3223 - val_loss: 0.1096\n",
      "Epoch 17/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3134\n",
      "Epoch 00017: val_loss improved from 0.10895 to 0.10406, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3134 - val_loss: 0.1041\n",
      "Epoch 18/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3089\n",
      "Epoch 00018: val_loss did not improve from 0.10406\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3074 - val_loss: 0.1085\n",
      "Epoch 19/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3014\n",
      "Epoch 00019: val_loss did not improve from 0.10406\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3017 - val_loss: 0.1063\n",
      " ###2 fold : val acc1 0.555, acc3 0.978, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/140 [===========================>..] - ETA: 0s - loss: 11.9468\n",
      "Epoch 00001: val_loss improved from inf to 2.66365, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 11.4340 - val_loss: 2.6637\n",
      "Epoch 2/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 2.4549\n",
      "Epoch 00002: val_loss improved from 2.66365 to 0.96490, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 2.4216 - val_loss: 0.9649\n",
      "Epoch 3/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 1.2662\n",
      "Epoch 00003: val_loss improved from 0.96490 to 0.38666, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.2609 - val_loss: 0.3867\n",
      "Epoch 4/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.9184\n",
      "Epoch 00004: val_loss improved from 0.38666 to 0.23536, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.9184 - val_loss: 0.2354\n",
      "Epoch 5/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.7551\n",
      "Epoch 00005: val_loss improved from 0.23536 to 0.18519, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7542 - val_loss: 0.1852\n",
      "Epoch 6/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.6398\n",
      "Epoch 00006: val_loss improved from 0.18519 to 0.16208, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6399 - val_loss: 0.1621\n",
      "Epoch 7/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.5557\n",
      "Epoch 00007: val_loss improved from 0.16208 to 0.14472, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5548 - val_loss: 0.1447\n",
      "Epoch 8/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.4851\n",
      "Epoch 00008: val_loss improved from 0.14472 to 0.14206, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4860 - val_loss: 0.1421\n",
      "Epoch 9/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.4426\n",
      "Epoch 00009: val_loss improved from 0.14206 to 0.12892, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4423 - val_loss: 0.1289\n",
      "Epoch 10/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.4129\n",
      "Epoch 00010: val_loss improved from 0.12892 to 0.11545, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4129 - val_loss: 0.1155\n",
      "Epoch 11/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.3791\n",
      "Epoch 00011: val_loss did not improve from 0.11545\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3800 - val_loss: 0.1181\n",
      "Epoch 12/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.3568\n",
      "Epoch 00012: val_loss improved from 0.11545 to 0.11304, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3574 - val_loss: 0.1130\n",
      "Epoch 13/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.3467\n",
      "Epoch 00013: val_loss did not improve from 0.11304\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3463 - val_loss: 0.1175\n",
      "Epoch 14/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.3360\n",
      "Epoch 00014: val_loss improved from 0.11304 to 0.11103, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3364 - val_loss: 0.1110\n",
      "Epoch 15/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3285\n",
      "Epoch 00015: val_loss improved from 0.11103 to 0.10856, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3279 - val_loss: 0.1086\n",
      "Epoch 16/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3208\n",
      "Epoch 00016: val_loss did not improve from 0.10856\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3206 - val_loss: 0.1090\n",
      "Epoch 17/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3151\n",
      "Epoch 00017: val_loss improved from 0.10856 to 0.10680, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3144 - val_loss: 0.1068\n",
      "Epoch 18/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.3090\n",
      "Epoch 00018: val_loss did not improve from 0.10680\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3090 - val_loss: 0.1086\n",
      "Epoch 19/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3035\n",
      "Epoch 00019: val_loss did not improve from 0.10680\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3038 - val_loss: 0.1076\n",
      " ###3 fold : val acc1 0.552, acc3 0.973, mae 0.237###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/140 [=========================>....] - ETA: 0s - loss: 12.2616\n",
      "Epoch 00001: val_loss improved from inf to 2.68485, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 11.3696 - val_loss: 2.6848\n",
      "Epoch 2/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 2.4404\n",
      "Epoch 00002: val_loss improved from 2.68485 to 0.96005, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 2.4394 - val_loss: 0.9601\n",
      "Epoch 3/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 1.3015\n",
      "Epoch 00003: val_loss improved from 0.96005 to 0.38030, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.2861 - val_loss: 0.3803\n",
      "Epoch 4/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.9106\n",
      "Epoch 00004: val_loss improved from 0.38030 to 0.24427, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.9078 - val_loss: 0.2443\n",
      "Epoch 5/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.7522\n",
      "Epoch 00005: val_loss improved from 0.24427 to 0.18131, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7500 - val_loss: 0.1813\n",
      "Epoch 6/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.6377\n",
      "Epoch 00006: val_loss improved from 0.18131 to 0.16195, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6361 - val_loss: 0.1619\n",
      "Epoch 7/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.5480\n",
      "Epoch 00007: val_loss improved from 0.16195 to 0.14504, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5469 - val_loss: 0.1450\n",
      "Epoch 8/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.4919\n",
      "Epoch 00008: val_loss improved from 0.14504 to 0.12693, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4920 - val_loss: 0.1269\n",
      "Epoch 9/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.4441\n",
      "Epoch 00009: val_loss improved from 0.12693 to 0.12572, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4445 - val_loss: 0.1257\n",
      "Epoch 10/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.4079\n",
      "Epoch 00010: val_loss improved from 0.12572 to 0.11774, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4061 - val_loss: 0.1177\n",
      "Epoch 11/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3787\n",
      "Epoch 00011: val_loss improved from 0.11774 to 0.11702, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3782 - val_loss: 0.1170\n",
      "Epoch 12/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3603\n",
      "Epoch 00012: val_loss improved from 0.11702 to 0.11118, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3586 - val_loss: 0.1112\n",
      "Epoch 13/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.3453\n",
      "Epoch 00013: val_loss did not improve from 0.11118\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3450 - val_loss: 0.1119\n",
      "Epoch 14/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.3346\n",
      "Epoch 00014: val_loss did not improve from 0.11118\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3339 - val_loss: 0.1116\n",
      " ###4 fold : val acc1 0.543, acc3 0.966, mae 0.245###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/140 [==========================>...] - ETA: 0s - loss: 11.9926\n",
      "Epoch 00001: val_loss improved from inf to 2.68166, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 11.3656 - val_loss: 2.6817\n",
      "Epoch 2/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 2.4907\n",
      "Epoch 00002: val_loss improved from 2.68166 to 0.96363, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 2.4280 - val_loss: 0.9636\n",
      "Epoch 3/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 1.3078\n",
      "Epoch 00003: val_loss improved from 0.96363 to 0.38329, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.2885 - val_loss: 0.3833\n",
      "Epoch 4/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.9081\n",
      "Epoch 00004: val_loss improved from 0.38329 to 0.24281, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.9068 - val_loss: 0.2428\n",
      "Epoch 5/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.7466\n",
      "Epoch 00005: val_loss improved from 0.24281 to 0.18229, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7462 - val_loss: 0.1823\n",
      "Epoch 6/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.6386\n",
      "Epoch 00006: val_loss improved from 0.18229 to 0.16114, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6379 - val_loss: 0.1611\n",
      "Epoch 7/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.5471\n",
      "Epoch 00007: val_loss improved from 0.16114 to 0.14511, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5471 - val_loss: 0.1451\n",
      "Epoch 8/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.4946\n",
      "Epoch 00008: val_loss improved from 0.14511 to 0.12703, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4929 - val_loss: 0.1270\n",
      "Epoch 9/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4411\n",
      "Epoch 00009: val_loss improved from 0.12703 to 0.12605, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4411 - val_loss: 0.1261\n",
      "Epoch 10/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.4091\n",
      "Epoch 00010: val_loss improved from 0.12605 to 0.11711, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4079 - val_loss: 0.1171\n",
      "Epoch 11/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3811\n",
      "Epoch 00011: val_loss did not improve from 0.11711\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3810 - val_loss: 0.1181\n",
      "Epoch 12/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3602\n",
      "Epoch 00012: val_loss improved from 0.11711 to 0.11371, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3599 - val_loss: 0.1137\n",
      "Epoch 13/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3488\n",
      "Epoch 00013: val_loss did not improve from 0.11371\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3489 - val_loss: 0.1146\n",
      "Epoch 14/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.3369\n",
      "Epoch 00014: val_loss improved from 0.11371 to 0.11227, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3354 - val_loss: 0.1123\n",
      "Epoch 15/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.3240\n",
      "Epoch 00015: val_loss improved from 0.11227 to 0.10590, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3233 - val_loss: 0.1059\n",
      "Epoch 16/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.3221\n",
      "Epoch 00016: val_loss did not improve from 0.10590\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3212 - val_loss: 0.1084\n",
      "Epoch 17/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3164\n",
      "Epoch 00017: val_loss did not improve from 0.10590\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3163 - val_loss: 0.1117\n",
      " ###5 fold : val acc1 0.544, acc3 0.978, mae 0.239###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/140 [==========================>...] - ETA: 0s - loss: 12.0518\n",
      "Epoch 00001: val_loss improved from inf to 2.67441, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 11.3594 - val_loss: 2.6744\n",
      "Epoch 2/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 2.4310\n",
      "Epoch 00002: val_loss improved from 2.67441 to 0.95570, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 2.4234 - val_loss: 0.9557\n",
      "Epoch 3/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 1.3007\n",
      "Epoch 00003: val_loss improved from 0.95570 to 0.38358, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.2832 - val_loss: 0.3836\n",
      "Epoch 4/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.9152\n",
      "Epoch 00004: val_loss improved from 0.38358 to 0.24267, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.9111 - val_loss: 0.2427\n",
      "Epoch 5/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.7551\n",
      "Epoch 00005: val_loss improved from 0.24267 to 0.18419, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7531 - val_loss: 0.1842\n",
      "Epoch 6/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.6416\n",
      "Epoch 00006: val_loss improved from 0.18419 to 0.15838, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6365 - val_loss: 0.1584\n",
      "Epoch 7/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.5579\n",
      "Epoch 00007: val_loss improved from 0.15838 to 0.14608, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5570 - val_loss: 0.1461\n",
      "Epoch 8/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.4975\n",
      "Epoch 00008: val_loss improved from 0.14608 to 0.12671, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4953 - val_loss: 0.1267\n",
      "Epoch 9/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.4412\n",
      "Epoch 00009: val_loss did not improve from 0.12671\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4416 - val_loss: 0.1277\n",
      "Epoch 10/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.4099\n",
      "Epoch 00010: val_loss improved from 0.12671 to 0.11941, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4097 - val_loss: 0.1194\n",
      "Epoch 11/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3819\n",
      "Epoch 00011: val_loss did not improve from 0.11941\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3821 - val_loss: 0.1211\n",
      "Epoch 12/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3647\n",
      "Epoch 00012: val_loss improved from 0.11941 to 0.11676, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3626 - val_loss: 0.1168\n",
      "Epoch 13/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3492\n",
      "Epoch 00013: val_loss improved from 0.11676 to 0.11645, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3504 - val_loss: 0.1164\n",
      "Epoch 14/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3345\n",
      "Epoch 00014: val_loss improved from 0.11645 to 0.11449, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3346 - val_loss: 0.1145\n",
      "Epoch 15/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3276\n",
      "Epoch 00015: val_loss improved from 0.11449 to 0.10800, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3259 - val_loss: 0.1080\n",
      "Epoch 16/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3233\n",
      "Epoch 00016: val_loss did not improve from 0.10800\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3230 - val_loss: 0.1099\n",
      "Epoch 17/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3171\n",
      "Epoch 00017: val_loss did not improve from 0.10800\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3171 - val_loss: 0.1136\n",
      " ###6 fold : val acc1 0.547, acc3 0.977, mae 0.238###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/140 [==========================>...] - ETA: 0s - loss: 11.9849\n",
      "Epoch 00001: val_loss improved from inf to 2.67675, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 11.3632 - val_loss: 2.6768\n",
      "Epoch 2/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 2.4402\n",
      "Epoch 00002: val_loss improved from 2.67675 to 0.95154, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 2.4323 - val_loss: 0.9515\n",
      "Epoch 3/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 1.2943\n",
      "Epoch 00003: val_loss improved from 0.95154 to 0.38432, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.2879 - val_loss: 0.3843\n",
      "Epoch 4/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.9158\n",
      "Epoch 00004: val_loss improved from 0.38432 to 0.23886, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.9117 - val_loss: 0.2389\n",
      "Epoch 5/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.7558\n",
      "Epoch 00005: val_loss improved from 0.23886 to 0.18312, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7512 - val_loss: 0.1831\n",
      "Epoch 6/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.6315\n",
      "Epoch 00006: val_loss improved from 0.18312 to 0.15858, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6295 - val_loss: 0.1586\n",
      "Epoch 7/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.5636\n",
      "Epoch 00007: val_loss improved from 0.15858 to 0.14764, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5592 - val_loss: 0.1476\n",
      "Epoch 8/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.4981\n",
      "Epoch 00008: val_loss improved from 0.14764 to 0.12547, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4963 - val_loss: 0.1255\n",
      "Epoch 9/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.4403\n",
      "Epoch 00009: val_loss improved from 0.12547 to 0.12507, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4402 - val_loss: 0.1251\n",
      "Epoch 10/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4116\n",
      "Epoch 00010: val_loss improved from 0.12507 to 0.11812, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4116 - val_loss: 0.1181\n",
      "Epoch 11/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.3824\n",
      "Epoch 00011: val_loss did not improve from 0.11812\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3817 - val_loss: 0.1231\n",
      "Epoch 12/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.3601\n",
      "Epoch 00012: val_loss improved from 0.11812 to 0.11720, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3591 - val_loss: 0.1172\n",
      "Epoch 13/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.3472\n",
      "Epoch 00013: val_loss improved from 0.11720 to 0.11331, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3469 - val_loss: 0.1133\n",
      "Epoch 14/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.3319\n",
      "Epoch 00014: val_loss improved from 0.11331 to 0.11240, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3314 - val_loss: 0.1124\n",
      "Epoch 15/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3247\n",
      "Epoch 00015: val_loss improved from 0.11240 to 0.10787, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3245 - val_loss: 0.1079\n",
      "Epoch 16/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3223\n",
      "Epoch 00016: val_loss did not improve from 0.10787\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3232 - val_loss: 0.1095\n",
      "Epoch 17/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3163\n",
      "Epoch 00017: val_loss did not improve from 0.10787\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3154 - val_loss: 0.1141\n",
      " ###7 fold : val acc1 0.533, acc3 0.968, mae 0.250###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/140 [==========================>...] - ETA: 0s - loss: 12.1377\n",
      "Epoch 00001: val_loss improved from inf to 2.61859, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 11.3869 - val_loss: 2.6186\n",
      "Epoch 2/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 2.4447\n",
      "Epoch 00002: val_loss improved from 2.61859 to 0.92971, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 2.4342 - val_loss: 0.9297\n",
      "Epoch 3/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 1.3008\n",
      "Epoch 00003: val_loss improved from 0.92971 to 0.37887, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.2883 - val_loss: 0.3789\n",
      "Epoch 4/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.9151\n",
      "Epoch 00004: val_loss improved from 0.37887 to 0.23801, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.9093 - val_loss: 0.2380\n",
      "Epoch 5/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.7544\n",
      "Epoch 00005: val_loss improved from 0.23801 to 0.18541, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7526 - val_loss: 0.1854\n",
      "Epoch 6/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.6319\n",
      "Epoch 00006: val_loss improved from 0.18541 to 0.16219, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6277 - val_loss: 0.1622\n",
      "Epoch 7/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.5645\n",
      "Epoch 00007: val_loss improved from 0.16219 to 0.15058, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5617 - val_loss: 0.1506\n",
      "Epoch 8/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.4983\n",
      "Epoch 00008: val_loss improved from 0.15058 to 0.13007, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4960 - val_loss: 0.1301\n",
      "Epoch 9/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.4423\n",
      "Epoch 00009: val_loss improved from 0.13007 to 0.12965, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4417 - val_loss: 0.1297\n",
      "Epoch 10/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.4137\n",
      "Epoch 00010: val_loss improved from 0.12965 to 0.12230, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4116 - val_loss: 0.1223\n",
      "Epoch 11/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3835\n",
      "Epoch 00011: val_loss did not improve from 0.12230\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3819 - val_loss: 0.1286\n",
      "Epoch 12/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3601\n",
      "Epoch 00012: val_loss improved from 0.12230 to 0.12161, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3573 - val_loss: 0.1216\n",
      "Epoch 13/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.3474\n",
      "Epoch 00013: val_loss improved from 0.12161 to 0.11759, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3472 - val_loss: 0.1176\n",
      "Epoch 14/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3339\n",
      "Epoch 00014: val_loss improved from 0.11759 to 0.11674, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3333 - val_loss: 0.1167\n",
      "Epoch 15/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3272\n",
      "Epoch 00015: val_loss improved from 0.11674 to 0.11265, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3254 - val_loss: 0.1127\n",
      "Epoch 16/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.3246\n",
      "Epoch 00016: val_loss did not improve from 0.11265\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3232 - val_loss: 0.1128\n",
      "Epoch 17/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3142\n",
      "Epoch 00017: val_loss did not improve from 0.11265\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3147 - val_loss: 0.1161\n",
      " ###8 fold : val acc1 0.543, acc3 0.972, mae 0.243###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/140 [============================>.] - ETA: 0s - loss: 11.5192\n",
      "Epoch 00001: val_loss improved from inf to 2.65962, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 11.3869 - val_loss: 2.6596\n",
      "Epoch 2/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 2.4365\n",
      "Epoch 00002: val_loss improved from 2.65962 to 0.95009, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 2.4342 - val_loss: 0.9501\n",
      "Epoch 3/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 1.3028\n",
      "Epoch 00003: val_loss improved from 0.95009 to 0.39242, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.2883 - val_loss: 0.3924\n",
      "Epoch 4/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.9148\n",
      "Epoch 00004: val_loss improved from 0.39242 to 0.24901, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.9093 - val_loss: 0.2490\n",
      "Epoch 5/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.7577\n",
      "Epoch 00005: val_loss improved from 0.24901 to 0.19255, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7526 - val_loss: 0.1925\n",
      "Epoch 6/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.6334\n",
      "Epoch 00006: val_loss improved from 0.19255 to 0.16832, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6277 - val_loss: 0.1683\n",
      "Epoch 7/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.5624\n",
      "Epoch 00007: val_loss improved from 0.16832 to 0.15573, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5617 - val_loss: 0.1557\n",
      "Epoch 8/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.4983\n",
      "Epoch 00008: val_loss improved from 0.15573 to 0.13391, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4960 - val_loss: 0.1339\n",
      "Epoch 9/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.4419\n",
      "Epoch 00009: val_loss improved from 0.13391 to 0.13339, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4417 - val_loss: 0.1334\n",
      "Epoch 10/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.4132\n",
      "Epoch 00010: val_loss improved from 0.13339 to 0.12567, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4116 - val_loss: 0.1257\n",
      "Epoch 11/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3831\n",
      "Epoch 00011: val_loss did not improve from 0.12567\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3819 - val_loss: 0.1327\n",
      "Epoch 12/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.3596\n",
      "Epoch 00012: val_loss improved from 0.12567 to 0.12516, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3573 - val_loss: 0.1252\n",
      "Epoch 13/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.3474\n",
      "Epoch 00013: val_loss improved from 0.12516 to 0.12092, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3472 - val_loss: 0.1209\n",
      "Epoch 14/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3338\n",
      "Epoch 00014: val_loss improved from 0.12092 to 0.11960, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3333 - val_loss: 0.1196\n",
      "Epoch 15/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3251\n",
      "Epoch 00015: val_loss improved from 0.11960 to 0.11554, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3254 - val_loss: 0.1155\n",
      "Epoch 16/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3231\n",
      "Epoch 00016: val_loss did not improve from 0.11554\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3232 - val_loss: 0.1157\n",
      "Epoch 17/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3156\n",
      "Epoch 00017: val_loss did not improve from 0.11554\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3147 - val_loss: 0.1190\n",
      " ###9 fold : val acc1 0.544, acc3 0.979, mae 0.239###\n",
      "acc10.547_acc30.975\n",
      "random search 30/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 23.4553\n",
      "Epoch 00001: val_loss improved from inf to 19.79986, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 23.3136 - val_loss: 19.7999\n",
      "Epoch 2/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 17.3270\n",
      "Epoch 00002: val_loss improved from 19.79986 to 14.24874, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 17.2054 - val_loss: 14.2487\n",
      "Epoch 3/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 12.5053\n",
      "Epoch 00003: val_loss improved from 14.24874 to 9.82148, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 12.3116 - val_loss: 9.8215\n",
      "Epoch 4/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 8.7775\n",
      "Epoch 00004: val_loss improved from 9.82148 to 6.60110, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 8.6374 - val_loss: 6.6011\n",
      "Epoch 5/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 6.2409\n",
      "Epoch 00005: val_loss improved from 6.60110 to 4.35876, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 6.1259 - val_loss: 4.3588\n",
      "Epoch 6/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 4.4866\n",
      "Epoch 00006: val_loss improved from 4.35876 to 2.76300, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 4.4132 - val_loss: 2.7630\n",
      "Epoch 7/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 3.2910\n",
      "Epoch 00007: val_loss improved from 2.76300 to 1.77620, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.2231 - val_loss: 1.7762\n",
      "Epoch 8/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 2.5626\n",
      "Epoch 00008: val_loss improved from 1.77620 to 1.26753, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.5321 - val_loss: 1.2675\n",
      "Epoch 9/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 2.1629\n",
      "Epoch 00009: val_loss improved from 1.26753 to 0.97481, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.1718 - val_loss: 0.9748\n",
      "Epoch 10/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 1.9328\n",
      "Epoch 00010: val_loss improved from 0.97481 to 0.77031, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9275 - val_loss: 0.7703\n",
      "Epoch 11/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 1.7655\n",
      "Epoch 00011: val_loss improved from 0.77031 to 0.62300, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.7648 - val_loss: 0.6230\n",
      "Epoch 12/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 1.6107\n",
      "Epoch 00012: val_loss improved from 0.62300 to 0.50899, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.5938 - val_loss: 0.5090\n",
      "Epoch 13/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 1.4891\n",
      "Epoch 00013: val_loss improved from 0.50899 to 0.42137, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.4826 - val_loss: 0.4214\n",
      "Epoch 14/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 1.4203\n",
      "Epoch 00014: val_loss improved from 0.42137 to 0.35145, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.4080 - val_loss: 0.3514\n",
      "Epoch 15/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 1.3697\n",
      "Epoch 00015: val_loss improved from 0.35145 to 0.30027, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.3547 - val_loss: 0.3003\n",
      "Epoch 16/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 1.2661\n",
      "Epoch 00016: val_loss improved from 0.30027 to 0.26339, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2719 - val_loss: 0.2634\n",
      "Epoch 17/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 1.2553\n",
      "Epoch 00017: val_loss improved from 0.26339 to 0.23445, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2584 - val_loss: 0.2345\n",
      "Epoch 18/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 1.2036\n",
      "Epoch 00018: val_loss improved from 0.23445 to 0.20924, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2004 - val_loss: 0.2092\n",
      "Epoch 19/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 1.1424\n",
      "Epoch 00019: val_loss improved from 0.20924 to 0.18578, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1476 - val_loss: 0.1858\n",
      "Epoch 20/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 1.1279\n",
      "Epoch 00020: val_loss improved from 0.18578 to 0.17478, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1235 - val_loss: 0.1748\n",
      "Epoch 21/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 1.0935\n",
      "Epoch 00021: val_loss improved from 0.17478 to 0.15959, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0884 - val_loss: 0.1596\n",
      "Epoch 22/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 1.0300\n",
      "Epoch 00022: val_loss improved from 0.15959 to 0.15087, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0203 - val_loss: 0.1509\n",
      "Epoch 23/100\n",
      "41/70 [================>.............] - ETA: 0s - loss: 0.9749"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss improved from 0.26792 to 0.14148, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.0072 - val_loss: 0.1415\n",
      "Epoch 3/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.8391\n",
      "Epoch 00003: val_loss improved from 0.14148 to 0.12020, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8390 - val_loss: 0.1202\n",
      "Epoch 4/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.7701\n",
      "Epoch 00004: val_loss did not improve from 0.12020\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7700 - val_loss: 0.1436\n",
      "Epoch 5/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.7140\n",
      "Epoch 00005: val_loss did not improve from 0.12020\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7114 - val_loss: 0.1244\n",
      " ###3 fold : val acc1 0.568, acc3 0.959, mae 0.237###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/140 [==========================>...] - ETA: 0s - loss: 6.2295\n",
      "Epoch 00001: val_loss improved from inf to 0.26130, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 5.8665 - val_loss: 0.2613\n",
      "Epoch 2/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 1.0386\n",
      "Epoch 00002: val_loss improved from 0.26130 to 0.14977, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.0291 - val_loss: 0.1498\n",
      "Epoch 3/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.8444\n",
      "Epoch 00003: val_loss did not improve from 0.14977\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.8395 - val_loss: 0.1607\n",
      "Epoch 4/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.7548\n",
      "Epoch 00004: val_loss improved from 0.14977 to 0.11333, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7541 - val_loss: 0.1133\n",
      "Epoch 5/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.7002\n",
      "Epoch 00005: val_loss did not improve from 0.11333\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6998 - val_loss: 0.1361\n",
      "Epoch 6/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.6515\n",
      "Epoch 00006: val_loss did not improve from 0.11333\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6518 - val_loss: 0.1209\n",
      " ###4 fold : val acc1 0.566, acc3 0.961, mae 0.237###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/140 [==========================>...] - ETA: 0s - loss: 6.3033\n",
      "Epoch 00001: val_loss improved from inf to 0.26168, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 5.8621 - val_loss: 0.2617\n",
      "Epoch 2/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 1.0283\n",
      "Epoch 00002: val_loss improved from 0.26168 to 0.16626, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.0267 - val_loss: 0.1663\n",
      "Epoch 3/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.8451\n",
      "Epoch 00003: val_loss improved from 0.16626 to 0.16493, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8464 - val_loss: 0.1649\n",
      "Epoch 4/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.7655\n",
      "Epoch 00004: val_loss improved from 0.16493 to 0.11431, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7652 - val_loss: 0.1143\n",
      "Epoch 5/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.6989\n",
      "Epoch 00005: val_loss did not improve from 0.11431\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6963 - val_loss: 0.1433\n",
      "Epoch 6/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.6525\n",
      "Epoch 00006: val_loss did not improve from 0.11431\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6523 - val_loss: 0.1201\n",
      " ###5 fold : val acc1 0.539, acc3 0.965, mae 0.249###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/140 [==========================>...] - ETA: 0s - loss: 6.3025\n",
      "Epoch 00001: val_loss improved from inf to 0.25901, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 5.8623 - val_loss: 0.2590\n",
      "Epoch 2/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 1.0253\n",
      "Epoch 00002: val_loss improved from 0.25901 to 0.16154, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.0228 - val_loss: 0.1615\n",
      "Epoch 3/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.8453\n",
      "Epoch 00003: val_loss did not improve from 0.16154\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8464 - val_loss: 0.1677\n",
      "Epoch 4/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.7644\n",
      "Epoch 00004: val_loss improved from 0.16154 to 0.11573, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7636 - val_loss: 0.1157\n",
      "Epoch 5/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.7005\n",
      "Epoch 00005: val_loss did not improve from 0.11573\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7002 - val_loss: 0.1510\n",
      "Epoch 6/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.6592\n",
      "Epoch 00006: val_loss did not improve from 0.11573\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6592 - val_loss: 0.1172\n",
      " ###6 fold : val acc1 0.571, acc3 0.966, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/140 [==========================>...] - ETA: 0s - loss: 6.2224\n",
      "Epoch 00001: val_loss improved from inf to 0.26551, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 5.8638 - val_loss: 0.2655\n",
      "Epoch 2/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 1.0273\n",
      "Epoch 00002: val_loss improved from 0.26551 to 0.15167, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.0195 - val_loss: 0.1517\n",
      "Epoch 3/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.8508\n",
      "Epoch 00003: val_loss did not improve from 0.15167\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8489 - val_loss: 0.1727\n",
      "Epoch 4/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.7634\n",
      "Epoch 00004: val_loss improved from 0.15167 to 0.12648, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7624 - val_loss: 0.1265\n",
      "Epoch 5/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.7047\n",
      "Epoch 00005: val_loss did not improve from 0.12648\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7054 - val_loss: 0.1505\n",
      "Epoch 6/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.6553\n",
      "Epoch 00006: val_loss did not improve from 0.12648\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6575 - val_loss: 0.1272\n",
      " ###7 fold : val acc1 0.529, acc3 0.945, mae 0.264###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/140 [==========================>...] - ETA: 0s - loss: 6.2800\n",
      "Epoch 00001: val_loss improved from inf to 0.27909, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 5.8813 - val_loss: 0.2791\n",
      "Epoch 2/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 1.0219\n",
      "Epoch 00002: val_loss improved from 0.27909 to 0.15621, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.0200 - val_loss: 0.1562\n",
      "Epoch 3/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.8507\n",
      "Epoch 00003: val_loss did not improve from 0.15621\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8488 - val_loss: 0.1810\n",
      "Epoch 4/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.7674\n",
      "Epoch 00004: val_loss improved from 0.15621 to 0.13302, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7669 - val_loss: 0.1330\n",
      "Epoch 5/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.7063\n",
      "Epoch 00005: val_loss did not improve from 0.13302\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7061 - val_loss: 0.1537\n",
      "Epoch 6/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.6582\n",
      "Epoch 00006: val_loss improved from 0.13302 to 0.13230, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6585 - val_loss: 0.1323\n",
      "Epoch 7/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.6288\n",
      "Epoch 00007: val_loss improved from 0.13230 to 0.11653, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6229 - val_loss: 0.1165\n",
      "Epoch 8/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.5739\n",
      "Epoch 00008: val_loss improved from 0.11653 to 0.11420, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5732 - val_loss: 0.1142\n",
      "Epoch 9/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.5471\n",
      "Epoch 00009: val_loss improved from 0.11420 to 0.11216, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5464 - val_loss: 0.1122\n",
      "Epoch 10/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.5121\n",
      "Epoch 00010: val_loss improved from 0.11216 to 0.11111, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5114 - val_loss: 0.1111\n",
      "Epoch 11/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.4638\n",
      "Epoch 00011: val_loss improved from 0.11111 to 0.10671, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4637 - val_loss: 0.1067\n",
      "Epoch 12/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.4297\n",
      "Epoch 00012: val_loss did not improve from 0.10671\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4274 - val_loss: 0.1074\n",
      "Epoch 13/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.4097\n",
      "Epoch 00013: val_loss did not improve from 0.10671\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4049 - val_loss: 0.1072\n",
      " ###8 fold : val acc1 0.580, acc3 0.973, mae 0.224###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/140 [===========================>..] - ETA: 0s - loss: 6.0253\n",
      "Epoch 00001: val_loss improved from inf to 0.28591, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 5.8813 - val_loss: 0.2859\n",
      "Epoch 2/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 1.0263\n",
      "Epoch 00002: val_loss improved from 0.28591 to 0.15986, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.0200 - val_loss: 0.1599\n",
      "Epoch 3/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.8515\n",
      "Epoch 00003: val_loss did not improve from 0.15986\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8488 - val_loss: 0.1859\n",
      "Epoch 4/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.7674\n",
      "Epoch 00004: val_loss improved from 0.15986 to 0.13659, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7669 - val_loss: 0.1366\n",
      "Epoch 5/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.7107\n",
      "Epoch 00005: val_loss did not improve from 0.13659\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7061 - val_loss: 0.1570\n",
      "Epoch 6/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.6551\n",
      "Epoch 00006: val_loss improved from 0.13659 to 0.13595, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6585 - val_loss: 0.1359\n",
      "Epoch 7/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.6286\n",
      "Epoch 00007: val_loss improved from 0.13595 to 0.11950, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6229 - val_loss: 0.1195\n",
      "Epoch 8/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.5739\n",
      "Epoch 00008: val_loss improved from 0.11950 to 0.11703, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5732 - val_loss: 0.1170\n",
      "Epoch 9/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.5483\n",
      "Epoch 00009: val_loss improved from 0.11703 to 0.11523, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5464 - val_loss: 0.1152\n",
      "Epoch 10/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.5121\n",
      "Epoch 00010: val_loss improved from 0.11523 to 0.11346, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5114 - val_loss: 0.1135\n",
      "Epoch 11/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.4656\n",
      "Epoch 00011: val_loss improved from 0.11346 to 0.10886, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes16_dropout0.1,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4637 - val_loss: 0.1089\n",
      "Epoch 12/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.4278\n",
      "Epoch 00012: val_loss did not improve from 0.10886\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4274 - val_loss: 0.1093\n",
      "Epoch 13/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.4094\n",
      "Epoch 00013: val_loss did not improve from 0.10886\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4049 - val_loss: 0.1104\n",
      " ###9 fold : val acc1 0.576, acc3 0.981, mae 0.222###\n",
      "acc10.558_acc30.964\n",
      "random search 36/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/558 [============================>.] - ETA: 0s - loss: 7.6775\n",
      "Epoch 00001: val_loss improved from inf to 1.43908, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 7.6013 - val_loss: 1.4391\n",
      "Epoch 2/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.6315\n",
      "Epoch 00002: val_loss improved from 1.43908 to 0.21849, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.6262 - val_loss: 0.2185\n",
      "Epoch 3/100\n",
      "540/558 [============================>.] - ETA: 0s - loss: 0.1491\n",
      "Epoch 00003: val_loss improved from 0.21849 to 0.11676, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1480 - val_loss: 0.1168\n",
      "Epoch 4/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.1135\n",
      "Epoch 00004: val_loss improved from 0.11676 to 0.10873, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1136 - val_loss: 0.1087\n",
      "Epoch 5/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00005: val_loss improved from 0.10873 to 0.10483, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1080 - val_loss: 0.1048\n",
      "Epoch 6/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.1039\n",
      "Epoch 00006: val_loss improved from 0.10483 to 0.10020, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1038 - val_loss: 0.1002\n",
      "Epoch 7/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.1007\n",
      "Epoch 00007: val_loss improved from 0.10020 to 0.09790, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1006 - val_loss: 0.0979\n",
      "Epoch 8/100\n",
      "540/558 [============================>.] - ETA: 0s - loss: 0.0992\n",
      "Epoch 00008: val_loss improved from 0.09790 to 0.09642, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0989 - val_loss: 0.0964\n",
      "Epoch 9/100\n",
      "541/558 [============================>.] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00009: val_loss improved from 0.09642 to 0.09637, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0982 - val_loss: 0.0964\n",
      "Epoch 10/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.0977\n",
      "Epoch 00010: val_loss improved from 0.09637 to 0.09587, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0977 - val_loss: 0.0959\n",
      "Epoch 11/100\n",
      "541/558 [============================>.] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00011: val_loss improved from 0.09587 to 0.09494, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0969 - val_loss: 0.0949\n",
      "Epoch 12/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00012: val_loss did not improve from 0.09494\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0969 - val_loss: 0.0967\n",
      "Epoch 13/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00013: val_loss did not improve from 0.09494\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0969 - val_loss: 0.0952\n",
      " ###0 fold : val acc1 0.597, acc3 0.981, mae 0.211###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545/558 [============================>.] - ETA: 0s - loss: 7.7324\n",
      "Epoch 00001: val_loss improved from inf to 1.42019, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 7.6007 - val_loss: 1.4202\n",
      "Epoch 2/100\n",
      "541/558 [============================>.] - ETA: 0s - loss: 0.6302\n",
      "Epoch 00002: val_loss improved from 1.42019 to 0.21763, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.6189 - val_loss: 0.2176\n",
      "Epoch 3/100\n",
      "540/558 [============================>.] - ETA: 0s - loss: 0.1578\n",
      "Epoch 00003: val_loss improved from 0.21763 to 0.12109, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1567 - val_loss: 0.1211\n",
      "Epoch 4/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.1151\n",
      "Epoch 00004: val_loss improved from 0.12109 to 0.10986, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1151 - val_loss: 0.1099\n",
      "Epoch 5/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.1081\n",
      "Epoch 00005: val_loss improved from 0.10986 to 0.10563, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1084 - val_loss: 0.1056\n",
      "Epoch 6/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.1042\n",
      "Epoch 00006: val_loss improved from 0.10563 to 0.10091, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1042 - val_loss: 0.1009\n",
      "Epoch 7/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.1008\n",
      "Epoch 00007: val_loss improved from 0.10091 to 0.09820, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1007 - val_loss: 0.0982\n",
      "Epoch 8/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00008: val_loss improved from 0.09820 to 0.09647, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0990 - val_loss: 0.0965\n",
      "Epoch 9/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.0978\n",
      "Epoch 00009: val_loss improved from 0.09647 to 0.09622, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0978 - val_loss: 0.0962\n",
      "Epoch 10/100\n",
      "543/558 [============================>.] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00010: val_loss improved from 0.09622 to 0.09506, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0972 - val_loss: 0.0951\n",
      "Epoch 11/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00011: val_loss improved from 0.09506 to 0.09472, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0966 - val_loss: 0.0947\n",
      "Epoch 12/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.0964\n",
      "Epoch 00012: val_loss did not improve from 0.09472\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 13/100\n",
      "543/558 [============================>.] - ETA: 0s - loss: 0.0962\n",
      "Epoch 00013: val_loss did not improve from 0.09472\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0963 - val_loss: 0.0960\n",
      " ###1 fold : val acc1 0.585, acc3 0.983, mae 0.216###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553/558 [============================>.] - ETA: 0s - loss: 7.6383\n",
      "Epoch 00001: val_loss improved from inf to 1.42125, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 7.5931 - val_loss: 1.4213\n",
      "Epoch 2/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 0.6315\n",
      "Epoch 00002: val_loss improved from 1.42125 to 0.21867, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.6208 - val_loss: 0.2187\n",
      "Epoch 3/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.1534\n",
      "Epoch 00003: val_loss improved from 0.21867 to 0.11815, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1534 - val_loss: 0.1182\n",
      "Epoch 4/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.1143\n",
      "Epoch 00004: val_loss improved from 0.11815 to 0.10880, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1142 - val_loss: 0.1088\n",
      "Epoch 5/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00005: val_loss improved from 0.10880 to 0.10525, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1082 - val_loss: 0.1053\n",
      "Epoch 6/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.1042\n",
      "Epoch 00006: val_loss improved from 0.10525 to 0.10145, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1041 - val_loss: 0.1015\n",
      "Epoch 7/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.1009\n",
      "Epoch 00007: val_loss improved from 0.10145 to 0.09784, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1008 - val_loss: 0.0978\n",
      "Epoch 8/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 00008: val_loss improved from 0.09784 to 0.09620, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0990 - val_loss: 0.0962\n",
      "Epoch 9/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.0979\n",
      "Epoch 00009: val_loss improved from 0.09620 to 0.09583, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0978 - val_loss: 0.0958\n",
      "Epoch 10/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00010: val_loss did not improve from 0.09583\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0975 - val_loss: 0.0960\n",
      "Epoch 11/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00011: val_loss improved from 0.09583 to 0.09498, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0968 - val_loss: 0.0950\n",
      "Epoch 12/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00012: val_loss improved from 0.09498 to 0.09497, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0965 - val_loss: 0.0950\n",
      "Epoch 13/100\n",
      "554/558 [============================>.] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00013: val_loss did not improve from 0.09497\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 14/100\n",
      "540/558 [============================>.] - ETA: 0s - loss: 0.0963\n",
      "Epoch 00014: val_loss did not improve from 0.09497\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0964 - val_loss: 0.0956\n",
      " ###2 fold : val acc1 0.583, acc3 0.981, mae 0.219###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545/558 [============================>.] - ETA: 0s - loss: 7.7398\n",
      "Epoch 00001: val_loss improved from inf to 1.42635, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 7.6050 - val_loss: 1.4264\n",
      "Epoch 2/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.6284\n",
      "Epoch 00002: val_loss improved from 1.42635 to 0.21768, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.6240 - val_loss: 0.2177\n",
      "Epoch 3/100\n",
      "543/558 [============================>.] - ETA: 0s - loss: 0.1479\n",
      "Epoch 00003: val_loss improved from 0.21768 to 0.11668, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1475 - val_loss: 0.1167\n",
      "Epoch 4/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.1135\n",
      "Epoch 00004: val_loss improved from 0.11668 to 0.10842, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1134 - val_loss: 0.1084\n",
      "Epoch 5/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.1077\n",
      "Epoch 00005: val_loss improved from 0.10842 to 0.10423, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1077 - val_loss: 0.1042\n",
      "Epoch 6/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.1032\n",
      "Epoch 00006: val_loss improved from 0.10423 to 0.10076, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1031 - val_loss: 0.1008\n",
      "Epoch 7/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.1006\n",
      "Epoch 00007: val_loss improved from 0.10076 to 0.09754, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1005 - val_loss: 0.0975\n",
      "Epoch 8/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00008: val_loss improved from 0.09754 to 0.09619, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0987 - val_loss: 0.0962\n",
      "Epoch 9/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00009: val_loss improved from 0.09619 to 0.09582, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0976 - val_loss: 0.0958\n",
      "Epoch 10/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00010: val_loss did not improve from 0.09582\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0974 - val_loss: 0.0959\n",
      "Epoch 11/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.0968\n",
      "Epoch 00011: val_loss improved from 0.09582 to 0.09545, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0968 - val_loss: 0.0955\n",
      "Epoch 12/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00012: val_loss improved from 0.09545 to 0.09526, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0966 - val_loss: 0.0953\n",
      "Epoch 13/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00013: val_loss did not improve from 0.09526\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0968 - val_loss: 0.0970\n",
      "Epoch 14/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.0962\n",
      "Epoch 00014: val_loss did not improve from 0.09526\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0964 - val_loss: 0.0956\n",
      " ###3 fold : val acc1 0.598, acc3 0.980, mae 0.211###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 7.7803\n",
      "Epoch 00001: val_loss improved from inf to 1.44307, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 7.6082 - val_loss: 1.4431\n",
      "Epoch 2/100\n",
      "554/558 [============================>.] - ETA: 0s - loss: 0.6314\n",
      "Epoch 00002: val_loss improved from 1.44307 to 0.21880, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.6291 - val_loss: 0.2188\n",
      "Epoch 3/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.1556\n",
      "Epoch 00003: val_loss improved from 0.21880 to 0.12436, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1555 - val_loss: 0.1244\n",
      "Epoch 4/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.1172\n",
      "Epoch 00004: val_loss improved from 0.12436 to 0.11152, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1172 - val_loss: 0.1115\n",
      "Epoch 5/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.1086\n",
      "Epoch 00005: val_loss improved from 0.11152 to 0.10556, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1087 - val_loss: 0.1056\n",
      "Epoch 6/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.1037\n",
      "Epoch 00006: val_loss improved from 0.10556 to 0.10153, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1036 - val_loss: 0.1015\n",
      "Epoch 7/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.1009\n",
      "Epoch 00007: val_loss improved from 0.10153 to 0.09893, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1007 - val_loss: 0.0989\n",
      "Epoch 8/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00008: val_loss improved from 0.09893 to 0.09730, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0989 - val_loss: 0.0973\n",
      "Epoch 9/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.0977\n",
      "Epoch 00009: val_loss improved from 0.09730 to 0.09641, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0976 - val_loss: 0.0964\n",
      "Epoch 10/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00010: val_loss did not improve from 0.09641\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0969 - val_loss: 0.0972\n",
      "Epoch 11/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.0966\n",
      "Epoch 00011: val_loss did not improve from 0.09641\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0966 - val_loss: 0.0966\n",
      " ###4 fold : val acc1 0.595, acc3 0.979, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539/558 [===========================>..] - ETA: 0s - loss: 7.7867\n",
      "Epoch 00001: val_loss improved from inf to 1.42849, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 7.5813 - val_loss: 1.4285\n",
      "Epoch 2/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.6273\n",
      "Epoch 00002: val_loss improved from 1.42849 to 0.21766, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.6236 - val_loss: 0.2177\n",
      "Epoch 3/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.1541\n",
      "Epoch 00003: val_loss improved from 0.21766 to 0.11940, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1534 - val_loss: 0.1194\n",
      "Epoch 4/100\n",
      "554/558 [============================>.] - ETA: 0s - loss: 0.1143\n",
      "Epoch 00004: val_loss improved from 0.11940 to 0.10920, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1142 - val_loss: 0.1092\n",
      "Epoch 5/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.1075\n",
      "Epoch 00005: val_loss improved from 0.10920 to 0.10367, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1074 - val_loss: 0.1037\n",
      "Epoch 6/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.1032\n",
      "Epoch 00006: val_loss improved from 0.10367 to 0.09985, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1032 - val_loss: 0.0999\n",
      "Epoch 7/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 0.1005\n",
      "Epoch 00007: val_loss improved from 0.09985 to 0.09761, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1003 - val_loss: 0.0976\n",
      "Epoch 8/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00008: val_loss improved from 0.09761 to 0.09676, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0987 - val_loss: 0.0968\n",
      "Epoch 9/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00009: val_loss improved from 0.09676 to 0.09597, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0976 - val_loss: 0.0960\n",
      "Epoch 10/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00010: val_loss improved from 0.09597 to 0.09492, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0969 - val_loss: 0.0949\n",
      "Epoch 11/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00011: val_loss did not improve from 0.09492\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0967 - val_loss: 0.0961\n",
      "Epoch 12/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00012: val_loss did not improve from 0.09492\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0964 - val_loss: 0.0957\n",
      " ###5 fold : val acc1 0.591, acc3 0.983, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539/558 [===========================>..] - ETA: 0s - loss: 7.8018\n",
      "Epoch 00001: val_loss improved from inf to 1.41745, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 7.5943 - val_loss: 1.4174\n",
      "Epoch 2/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.6246\n",
      "Epoch 00002: val_loss improved from 1.41745 to 0.21432, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0,lr0.001/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.6201 - val_loss: 0.2143\n",
      "Epoch 3/100\n",
      "371/558 [==================>...........] - ETA: 0s - loss: 0.1564"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09538\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1279 - val_loss: 0.0956\n",
      "Epoch 10/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1269\n",
      "Epoch 00010: val_loss did not improve from 0.09538\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1271 - val_loss: 0.0955\n",
      " ###2 fold : val acc1 0.591, acc3 0.981, mae 0.214###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/279 [============================>.] - ETA: 0s - loss: 3.5851\n",
      "Epoch 00001: val_loss improved from inf to 0.14681, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.5171 - val_loss: 0.1468\n",
      "Epoch 2/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1669\n",
      "Epoch 00002: val_loss improved from 0.14681 to 0.11695, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1659 - val_loss: 0.1170\n",
      "Epoch 3/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1516\n",
      "Epoch 00003: val_loss improved from 0.11695 to 0.10772, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1511 - val_loss: 0.1077\n",
      "Epoch 4/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1423\n",
      "Epoch 00004: val_loss improved from 0.10772 to 0.10170, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1422 - val_loss: 0.1017\n",
      "Epoch 5/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1371\n",
      "Epoch 00005: val_loss improved from 0.10170 to 0.10031, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1372 - val_loss: 0.1003\n",
      "Epoch 6/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1336\n",
      "Epoch 00006: val_loss improved from 0.10031 to 0.09824, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1336 - val_loss: 0.0982\n",
      "Epoch 7/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1307\n",
      "Epoch 00007: val_loss improved from 0.09824 to 0.09604, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1308 - val_loss: 0.0960\n",
      "Epoch 8/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1312\n",
      "Epoch 00008: val_loss improved from 0.09604 to 0.09577, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1311 - val_loss: 0.0958\n",
      "Epoch 9/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1270\n",
      "Epoch 00009: val_loss improved from 0.09577 to 0.09554, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1269 - val_loss: 0.0955\n",
      "Epoch 10/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.1266\n",
      "Epoch 00010: val_loss did not improve from 0.09554\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1269 - val_loss: 0.0961\n",
      "Epoch 11/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1246\n",
      "Epoch 00011: val_loss did not improve from 0.09554\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1245 - val_loss: 0.0957\n",
      " ###3 fold : val acc1 0.598, acc3 0.979, mae 0.212###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271/279 [============================>.] - ETA: 0s - loss: 3.6137\n",
      "Epoch 00001: val_loss improved from inf to 0.14599, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 3.5206 - val_loss: 0.1460\n",
      "Epoch 2/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1665\n",
      "Epoch 00002: val_loss improved from 0.14599 to 0.11887, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1655 - val_loss: 0.1189\n",
      "Epoch 3/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1526\n",
      "Epoch 00003: val_loss improved from 0.11887 to 0.10852, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1525 - val_loss: 0.1085\n",
      "Epoch 4/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.1421\n",
      "Epoch 00004: val_loss improved from 0.10852 to 0.10223, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1416 - val_loss: 0.1022\n",
      "Epoch 5/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1362\n",
      "Epoch 00005: val_loss improved from 0.10223 to 0.09931, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1363 - val_loss: 0.0993\n",
      "Epoch 6/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1316\n",
      "Epoch 00006: val_loss improved from 0.09931 to 0.09770, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1316 - val_loss: 0.0977\n",
      "Epoch 7/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1312\n",
      "Epoch 00007: val_loss improved from 0.09770 to 0.09739, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1312 - val_loss: 0.0974\n",
      "Epoch 8/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1286\n",
      "Epoch 00008: val_loss improved from 0.09739 to 0.09727, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1287 - val_loss: 0.0973\n",
      "Epoch 9/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1264\n",
      "Epoch 00009: val_loss improved from 0.09727 to 0.09721, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1265 - val_loss: 0.0972\n",
      "Epoch 10/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00010: val_loss improved from 0.09721 to 0.09619, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1247 - val_loss: 0.0962\n",
      "Epoch 11/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00011: val_loss did not improve from 0.09619\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1253 - val_loss: 0.1001\n",
      "Epoch 12/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1226\n",
      "Epoch 00012: val_loss did not improve from 0.09619\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1226 - val_loss: 0.0967\n",
      " ###4 fold : val acc1 0.582, acc3 0.982, mae 0.218###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/279 [============================>.] - ETA: 0s - loss: 3.5652\n",
      "Epoch 00001: val_loss improved from inf to 0.14765, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 3.5223 - val_loss: 0.1477\n",
      "Epoch 2/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1660\n",
      "Epoch 00002: val_loss improved from 0.14765 to 0.11918, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1649 - val_loss: 0.1192\n",
      "Epoch 3/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1530\n",
      "Epoch 00003: val_loss improved from 0.11918 to 0.10802, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1526 - val_loss: 0.1080\n",
      "Epoch 4/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1416\n",
      "Epoch 00004: val_loss improved from 0.10802 to 0.10236, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1414 - val_loss: 0.1024\n",
      "Epoch 5/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1356\n",
      "Epoch 00005: val_loss improved from 0.10236 to 0.10099, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1361 - val_loss: 0.1010\n",
      "Epoch 6/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1329\n",
      "Epoch 00006: val_loss improved from 0.10099 to 0.09694, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1328 - val_loss: 0.0969\n",
      "Epoch 7/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1309\n",
      "Epoch 00007: val_loss improved from 0.09694 to 0.09639, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1313 - val_loss: 0.0964\n",
      "Epoch 8/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1276\n",
      "Epoch 00008: val_loss did not improve from 0.09639\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1276 - val_loss: 0.0970\n",
      "Epoch 9/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1267\n",
      "Epoch 00009: val_loss improved from 0.09639 to 0.09621, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1267 - val_loss: 0.0962\n",
      "Epoch 10/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00010: val_loss improved from 0.09621 to 0.09553, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1253 - val_loss: 0.0955\n",
      "Epoch 11/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00011: val_loss did not improve from 0.09553\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1257 - val_loss: 0.0987\n",
      "Epoch 12/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1222\n",
      "Epoch 00012: val_loss did not improve from 0.09553\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1222 - val_loss: 0.0977\n",
      " ###5 fold : val acc1 0.590, acc3 0.981, mae 0.214###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/279 [============================>.] - ETA: 0s - loss: 3.5459\n",
      "Epoch 00001: val_loss improved from inf to 0.14718, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 3.5270 - val_loss: 0.1472\n",
      "Epoch 2/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1670\n",
      "Epoch 00002: val_loss improved from 0.14718 to 0.11811, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1668 - val_loss: 0.1181\n",
      "Epoch 3/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1527\n",
      "Epoch 00003: val_loss improved from 0.11811 to 0.10750, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1525 - val_loss: 0.1075\n",
      "Epoch 4/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1428\n",
      "Epoch 00004: val_loss improved from 0.10750 to 0.10213, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1428 - val_loss: 0.1021\n",
      "Epoch 5/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.1365\n",
      "Epoch 00005: val_loss improved from 0.10213 to 0.10045, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1368 - val_loss: 0.1004\n",
      "Epoch 6/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1329\n",
      "Epoch 00006: val_loss improved from 0.10045 to 0.09675, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1331 - val_loss: 0.0967\n",
      "Epoch 7/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1320\n",
      "Epoch 00007: val_loss improved from 0.09675 to 0.09648, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1319 - val_loss: 0.0965\n",
      "Epoch 8/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1279\n",
      "Epoch 00008: val_loss did not improve from 0.09648\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1279 - val_loss: 0.0977\n",
      "Epoch 9/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1267\n",
      "Epoch 00009: val_loss improved from 0.09648 to 0.09525, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1267 - val_loss: 0.0953\n",
      "Epoch 10/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00010: val_loss did not improve from 0.09525\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1257 - val_loss: 0.0953\n",
      "Epoch 11/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00011: val_loss did not improve from 0.09525\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1249 - val_loss: 0.0980\n",
      " ###6 fold : val acc1 0.609, acc3 0.982, mae 0.204###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/279 [============================>.] - ETA: 0s - loss: 3.5495\n",
      "Epoch 00001: val_loss improved from inf to 0.14740, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 3.5064 - val_loss: 0.1474\n",
      "Epoch 2/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1670\n",
      "Epoch 00002: val_loss improved from 0.14740 to 0.11873, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1655 - val_loss: 0.1187\n",
      "Epoch 3/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1508\n",
      "Epoch 00003: val_loss improved from 0.11873 to 0.10785, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1508 - val_loss: 0.1079\n",
      "Epoch 4/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.1422\n",
      "Epoch 00004: val_loss improved from 0.10785 to 0.10288, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1418 - val_loss: 0.1029\n",
      "Epoch 5/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1353\n",
      "Epoch 00005: val_loss improved from 0.10288 to 0.10072, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1354 - val_loss: 0.1007\n",
      "Epoch 6/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.1322\n",
      "Epoch 00006: val_loss improved from 0.10072 to 0.09748, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1326 - val_loss: 0.0975\n",
      "Epoch 7/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1305\n",
      "Epoch 00007: val_loss improved from 0.09748 to 0.09676, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1304 - val_loss: 0.0968\n",
      "Epoch 8/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00008: val_loss improved from 0.09676 to 0.09653, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1271 - val_loss: 0.0965\n",
      "Epoch 9/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00009: val_loss improved from 0.09653 to 0.09641, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1250 - val_loss: 0.0964\n",
      "Epoch 10/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00010: val_loss improved from 0.09641 to 0.09465, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1248 - val_loss: 0.0946\n",
      "Epoch 11/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1233\n",
      "Epoch 00011: val_loss did not improve from 0.09465\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1232 - val_loss: 0.0970\n",
      "Epoch 12/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1227\n",
      "Epoch 00012: val_loss did not improve from 0.09465\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1222 - val_loss: 0.0982\n",
      " ###7 fold : val acc1 0.590, acc3 0.975, mae 0.217###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/279 [===========================>..] - ETA: 0s - loss: 3.6915\n",
      "Epoch 00001: val_loss improved from inf to 0.15275, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 3.5205 - val_loss: 0.1528\n",
      "Epoch 2/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1658\n",
      "Epoch 00002: val_loss improved from 0.15275 to 0.12227, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1651 - val_loss: 0.1223\n",
      "Epoch 3/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1511\n",
      "Epoch 00003: val_loss improved from 0.12227 to 0.11159, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1509 - val_loss: 0.1116\n",
      "Epoch 4/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1414\n",
      "Epoch 00004: val_loss improved from 0.11159 to 0.10639, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1415 - val_loss: 0.1064\n",
      "Epoch 5/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.1355\n",
      "Epoch 00005: val_loss improved from 0.10639 to 0.10423, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1358 - val_loss: 0.1042\n",
      "Epoch 6/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1319\n",
      "Epoch 00006: val_loss improved from 0.10423 to 0.10127, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1013\n",
      "Epoch 7/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1306\n",
      "Epoch 00007: val_loss improved from 0.10127 to 0.10061, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1305 - val_loss: 0.1006\n",
      "Epoch 8/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1274\n",
      "Epoch 00008: val_loss improved from 0.10061 to 0.09986, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1272 - val_loss: 0.0999\n",
      "Epoch 9/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00009: val_loss improved from 0.09986 to 0.09948, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1247 - val_loss: 0.0995\n",
      "Epoch 10/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00010: val_loss improved from 0.09948 to 0.09755, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1248 - val_loss: 0.0975\n",
      "Epoch 11/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00011: val_loss did not improve from 0.09755\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1238 - val_loss: 0.0998\n",
      "Epoch 12/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1225\n",
      "Epoch 00012: val_loss did not improve from 0.09755\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1224 - val_loss: 0.1021\n",
      " ###8 fold : val acc1 0.591, acc3 0.982, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/279 [===========================>..] - ETA: 0s - loss: 3.6915\n",
      "Epoch 00001: val_loss improved from inf to 0.15491, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 3.5205 - val_loss: 0.1549\n",
      "Epoch 2/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1658\n",
      "Epoch 00002: val_loss improved from 0.15491 to 0.12502, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1651 - val_loss: 0.1250\n",
      "Epoch 3/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1511\n",
      "Epoch 00003: val_loss improved from 0.12502 to 0.11409, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1509 - val_loss: 0.1141\n",
      "Epoch 4/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1415\n",
      "Epoch 00004: val_loss improved from 0.11409 to 0.10897, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1415 - val_loss: 0.1090\n",
      "Epoch 5/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1354\n",
      "Epoch 00005: val_loss improved from 0.10897 to 0.10651, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1358 - val_loss: 0.1065\n",
      "Epoch 6/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1322\n",
      "Epoch 00006: val_loss improved from 0.10651 to 0.10409, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1041\n",
      "Epoch 7/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1306\n",
      "Epoch 00007: val_loss improved from 0.10409 to 0.10353, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1305 - val_loss: 0.1035\n",
      "Epoch 8/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1273\n",
      "Epoch 00008: val_loss improved from 0.10353 to 0.10146, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1271 - val_loss: 0.1015\n",
      "Epoch 9/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1246\n",
      "Epoch 00009: val_loss did not improve from 0.10146\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1247 - val_loss: 0.1023\n",
      "Epoch 10/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00010: val_loss improved from 0.10146 to 0.09995, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1248 - val_loss: 0.1000\n",
      "Epoch 11/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00011: val_loss did not improve from 0.09995\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1238 - val_loss: 0.1019\n",
      "Epoch 12/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1225\n",
      "Epoch 00012: val_loss did not improve from 0.09995\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1224 - val_loss: 0.1051\n",
      " ###9 fold : val acc1 0.603, acc3 0.984, mae 0.207###\n",
      "acc10.594_acc30.981\n",
      "random search 40/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/279 [===========================>..] - ETA: 0s - loss: 2.1335\n",
      "Epoch 00001: val_loss improved from inf to 0.11768, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.0230 - val_loss: 0.1177\n",
      "Epoch 2/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1474\n",
      "Epoch 00002: val_loss improved from 0.11768 to 0.10538, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1474 - val_loss: 0.1054\n",
      "Epoch 3/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1375\n",
      "Epoch 00003: val_loss improved from 0.10538 to 0.09942, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1374 - val_loss: 0.0994\n",
      "Epoch 4/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1340\n",
      "Epoch 00004: val_loss did not improve from 0.09942\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1337 - val_loss: 0.1038\n",
      "Epoch 5/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.1306\n",
      "Epoch 00005: val_loss improved from 0.09942 to 0.09629, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1307 - val_loss: 0.0963\n",
      "Epoch 6/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1296\n",
      "Epoch 00006: val_loss did not improve from 0.09629\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1296 - val_loss: 0.1033\n",
      "Epoch 7/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1264\n",
      "Epoch 00007: val_loss did not improve from 0.09629\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1262 - val_loss: 0.0985\n",
      " ###0 fold : val acc1 0.614, acc3 0.980, mae 0.204###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/279 [===========================>..] - ETA: 0s - loss: 2.0962\n",
      "Epoch 00001: val_loss improved from inf to 0.11577, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.0294 - val_loss: 0.1158\n",
      "Epoch 2/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1469\n",
      "Epoch 00002: val_loss improved from 0.11577 to 0.10480, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1469 - val_loss: 0.1048\n",
      "Epoch 3/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1377\n",
      "Epoch 00003: val_loss improved from 0.10480 to 0.09991, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1376 - val_loss: 0.0999\n",
      "Epoch 4/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1331\n",
      "Epoch 00004: val_loss improved from 0.09991 to 0.09959, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1330 - val_loss: 0.0996\n",
      "Epoch 5/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1298\n",
      "Epoch 00005: val_loss improved from 0.09959 to 0.09755, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1301 - val_loss: 0.0975\n",
      "Epoch 6/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1286\n",
      "Epoch 00006: val_loss did not improve from 0.09755\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1286 - val_loss: 0.0993\n",
      "Epoch 7/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00007: val_loss improved from 0.09755 to 0.09662, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1256 - val_loss: 0.0966\n",
      "Epoch 8/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00008: val_loss did not improve from 0.09662\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1250 - val_loss: 0.0981\n",
      "Epoch 9/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1223\n",
      "Epoch 00009: val_loss improved from 0.09662 to 0.09476, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1222 - val_loss: 0.0948\n",
      "Epoch 10/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1208\n",
      "Epoch 00010: val_loss did not improve from 0.09476\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1208 - val_loss: 0.1015\n",
      "Epoch 11/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1197\n",
      "Epoch 00011: val_loss did not improve from 0.09476\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1197 - val_loss: 0.0956\n",
      " ###1 fold : val acc1 0.594, acc3 0.981, mae 0.212###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271/279 [============================>.] - ETA: 0s - loss: 2.0847\n",
      "Epoch 00001: val_loss improved from inf to 0.11578, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 2.0323 - val_loss: 0.1158\n",
      "Epoch 2/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.1468\n",
      "Epoch 00002: val_loss improved from 0.11578 to 0.10398, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1468 - val_loss: 0.1040\n",
      "Epoch 3/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1372\n",
      "Epoch 00003: val_loss improved from 0.10398 to 0.10051, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1375 - val_loss: 0.1005\n",
      "Epoch 4/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1333\n",
      "Epoch 00004: val_loss improved from 0.10051 to 0.09769, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1327 - val_loss: 0.0977\n",
      "Epoch 5/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1304\n",
      "Epoch 00005: val_loss improved from 0.09769 to 0.09685, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1302 - val_loss: 0.0968\n",
      "Epoch 6/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1283\n",
      "Epoch 00006: val_loss did not improve from 0.09685\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1284 - val_loss: 0.0985\n",
      "Epoch 7/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00007: val_loss improved from 0.09685 to 0.09543, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1262 - val_loss: 0.0954\n",
      "Epoch 8/100\n",
      "261/279 [===========================>..] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00008: val_loss did not improve from 0.09543\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1251 - val_loss: 0.1007\n",
      "Epoch 9/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00009: val_loss improved from 0.09543 to 0.09341, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1228 - val_loss: 0.0934\n",
      "Epoch 10/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1224\n",
      "Epoch 00010: val_loss did not improve from 0.09341\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1222 - val_loss: 0.1006\n",
      "Epoch 11/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1203\n",
      "Epoch 00011: val_loss did not improve from 0.09341\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1203 - val_loss: 0.0949\n",
      " ###2 fold : val acc1 0.608, acc3 0.982, mae 0.205###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/279 [============================>.] - ETA: 0s - loss: 2.0570\n",
      "Epoch 00001: val_loss improved from inf to 0.11811, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.0324 - val_loss: 0.1181\n",
      "Epoch 2/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1472\n",
      "Epoch 00002: val_loss improved from 0.11811 to 0.10374, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1472 - val_loss: 0.1037\n",
      "Epoch 3/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1369\n",
      "Epoch 00003: val_loss improved from 0.10374 to 0.09839, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1372 - val_loss: 0.0984\n",
      "Epoch 4/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.1334\n",
      "Epoch 00004: val_loss improved from 0.09839 to 0.09677, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1328 - val_loss: 0.0968\n",
      "Epoch 5/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00005: val_loss did not improve from 0.09677\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1288 - val_loss: 0.0972\n",
      "Epoch 6/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00006: val_loss improved from 0.09677 to 0.09649, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1283 - val_loss: 0.0965\n",
      "Epoch 7/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1264\n",
      "Epoch 00007: val_loss improved from 0.09649 to 0.09551, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1262 - val_loss: 0.0955\n",
      "Epoch 8/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00008: val_loss did not improve from 0.09551\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1249 - val_loss: 0.0983\n",
      "Epoch 9/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1217\n",
      "Epoch 00009: val_loss improved from 0.09551 to 0.09369, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1217 - val_loss: 0.0937\n",
      "Epoch 10/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1216\n",
      "Epoch 00010: val_loss did not improve from 0.09369\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1216 - val_loss: 0.0963\n",
      "Epoch 11/100\n",
      "262/279 [===========================>..] - ETA: 0s - loss: 0.1204\n",
      "Epoch 00011: val_loss did not improve from 0.09369\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1202 - val_loss: 0.0947\n",
      " ###3 fold : val acc1 0.607, acc3 0.981, mae 0.206###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/279 [===========================>..] - ETA: 0s - loss: 2.1455\n",
      "Epoch 00001: val_loss improved from inf to 0.12104, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.0343 - val_loss: 0.1210\n",
      "Epoch 2/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1460\n",
      "Epoch 00002: val_loss improved from 0.12104 to 0.10309, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1460 - val_loss: 0.1031\n",
      "Epoch 3/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1367\n",
      "Epoch 00003: val_loss improved from 0.10309 to 0.10029, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1367 - val_loss: 0.1003\n",
      "Epoch 4/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1309\n",
      "Epoch 00004: val_loss improved from 0.10029 to 0.09673, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1305 - val_loss: 0.0967\n",
      "Epoch 5/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1289\n",
      "Epoch 00005: val_loss did not improve from 0.09673\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1291 - val_loss: 0.0997\n",
      "Epoch 6/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1259\n",
      "Epoch 00006: val_loss improved from 0.09673 to 0.09499, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1258 - val_loss: 0.0950\n",
      "Epoch 7/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00007: val_loss did not improve from 0.09499\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1245 - val_loss: 0.0986\n",
      "Epoch 8/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00008: val_loss did not improve from 0.09499\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1240 - val_loss: 0.0957\n",
      " ###4 fold : val acc1 0.598, acc3 0.976, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271/279 [============================>.] - ETA: 0s - loss: 2.0790\n",
      "Epoch 00001: val_loss improved from inf to 0.11854, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.0266 - val_loss: 0.1185\n",
      "Epoch 2/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1455\n",
      "Epoch 00002: val_loss improved from 0.11854 to 0.10315, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1455 - val_loss: 0.1032\n",
      "Epoch 3/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1375\n",
      "Epoch 00003: val_loss improved from 0.10315 to 0.09840, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1374 - val_loss: 0.0984\n",
      "Epoch 4/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1310\n",
      "Epoch 00004: val_loss improved from 0.09840 to 0.09653, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1308 - val_loss: 0.0965\n",
      "Epoch 5/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1292\n",
      "Epoch 00005: val_loss did not improve from 0.09653\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1290 - val_loss: 0.1010\n",
      "Epoch 6/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.1268\n",
      "Epoch 00006: val_loss improved from 0.09653 to 0.09630, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1269 - val_loss: 0.0963\n",
      "Epoch 7/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00007: val_loss did not improve from 0.09630\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1250 - val_loss: 0.1000\n",
      "Epoch 8/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1234\n",
      "Epoch 00008: val_loss did not improve from 0.09630\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1239 - val_loss: 0.0989\n",
      " ###5 fold : val acc1 0.588, acc3 0.980, mae 0.216###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/279 [============================>.] - ETA: 0s - loss: 2.0634\n",
      "Epoch 00001: val_loss improved from inf to 0.11564, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.0251 - val_loss: 0.1156\n",
      "Epoch 2/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1458\n",
      "Epoch 00002: val_loss improved from 0.11564 to 0.10401, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1458 - val_loss: 0.1040\n",
      "Epoch 3/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1374\n",
      "Epoch 00003: val_loss improved from 0.10401 to 0.09832, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1374 - val_loss: 0.0983\n",
      "Epoch 4/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.1321\n",
      "Epoch 00004: val_loss improved from 0.09832 to 0.09739, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1317 - val_loss: 0.0974\n",
      "Epoch 5/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1297\n",
      "Epoch 00005: val_loss did not improve from 0.09739\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1295 - val_loss: 0.0993\n",
      "Epoch 6/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1266\n",
      "Epoch 00006: val_loss improved from 0.09739 to 0.09709, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1268 - val_loss: 0.0971\n",
      "Epoch 7/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1251\n",
      "Epoch 00007: val_loss did not improve from 0.09709\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1251 - val_loss: 0.0981\n",
      "Epoch 8/100\n",
      "261/279 [===========================>..] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00008: val_loss did not improve from 0.09709\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1246 - val_loss: 0.1016\n",
      " ###6 fold : val acc1 0.593, acc3 0.985, mae 0.211###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/279 [============================>.] - ETA: 0s - loss: 2.0503\n",
      "Epoch 00001: val_loss improved from inf to 0.11473, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.0120 - val_loss: 0.1147\n",
      "Epoch 2/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1452\n",
      "Epoch 00002: val_loss improved from 0.11473 to 0.10402, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1452 - val_loss: 0.1040\n",
      "Epoch 3/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1352\n",
      "Epoch 00003: val_loss improved from 0.10402 to 0.09967, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1351 - val_loss: 0.0997\n",
      "Epoch 4/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.1307\n",
      "Epoch 00004: val_loss improved from 0.09967 to 0.09791, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1302 - val_loss: 0.0979\n",
      "Epoch 5/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00005: val_loss did not improve from 0.09791\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1284 - val_loss: 0.1020\n",
      "Epoch 6/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00006: val_loss improved from 0.09791 to 0.09629, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1264 - val_loss: 0.0963\n",
      "Epoch 7/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00007: val_loss did not improve from 0.09629\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1242 - val_loss: 0.0971\n",
      "Epoch 8/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1235\n",
      "Epoch 00008: val_loss did not improve from 0.09629\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1234 - val_loss: 0.0970\n",
      " ###7 fold : val acc1 0.583, acc3 0.976, mae 0.220###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/279 [============================>.] - ETA: 0s - loss: 2.0669\n",
      "Epoch 00001: val_loss improved from inf to 0.11855, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.0214 - val_loss: 0.1186\n",
      "Epoch 2/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1456\n",
      "Epoch 00002: val_loss improved from 0.11855 to 0.10731, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1455 - val_loss: 0.1073\n",
      "Epoch 3/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1353\n",
      "Epoch 00003: val_loss improved from 0.10731 to 0.10262, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1351 - val_loss: 0.1026\n",
      "Epoch 4/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1306\n",
      "Epoch 00004: val_loss improved from 0.10262 to 0.10032, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1301 - val_loss: 0.1003\n",
      "Epoch 5/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1284\n",
      "Epoch 00005: val_loss did not improve from 0.10032\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1286 - val_loss: 0.1049\n",
      "Epoch 6/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00006: val_loss improved from 0.10032 to 0.09949, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1260 - val_loss: 0.0995\n",
      "Epoch 7/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00007: val_loss improved from 0.09949 to 0.09903, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1243 - val_loss: 0.0990\n",
      "Epoch 8/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1234\n",
      "Epoch 00008: val_loss did not improve from 0.09903\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1235 - val_loss: 0.0999\n",
      "Epoch 9/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1218\n",
      "Epoch 00009: val_loss did not improve from 0.09903\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1217 - val_loss: 0.1014\n",
      " ###8 fold : val acc1 0.604, acc3 0.980, mae 0.208###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/279 [============================>.] - ETA: 0s - loss: 2.0528\n",
      "Epoch 00001: val_loss improved from inf to 0.12134, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.0214 - val_loss: 0.1213\n",
      "Epoch 2/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1455\n",
      "Epoch 00002: val_loss improved from 0.12134 to 0.11000, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1455 - val_loss: 0.1100\n",
      "Epoch 3/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1355"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/279 [===========================>..] - ETA: 0s - loss: 0.2917\n",
      "Epoch 00004: val_loss improved from 0.10489 to 0.10043, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,dnodes128_dropout0.3,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2920 - val_loss: 0.1004\n",
      "Epoch 5/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2795\n",
      "Epoch 00005: val_loss did not improve from 0.10043\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2790 - val_loss: 0.1096\n",
      "Epoch 6/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.2682\n",
      "Epoch 00006: val_loss did not improve from 0.10043\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2682 - val_loss: 0.1169\n",
      " ###8 fold : val acc1 0.597, acc3 0.982, mae 0.210###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/279 [===========================>..] - ETA: 0s - loss: 1.0444\n",
      "Epoch 00001: val_loss improved from inf to 0.11866, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,dnodes128_dropout0.3,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 1.0193 - val_loss: 0.1187\n",
      "Epoch 2/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.3085\n",
      "Epoch 00002: val_loss improved from 0.11866 to 0.11149, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,dnodes128_dropout0.3,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3079 - val_loss: 0.1115\n",
      "Epoch 3/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.2895\n",
      "Epoch 00003: val_loss improved from 0.11149 to 0.10544, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,dnodes128_dropout0.3,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2893 - val_loss: 0.1054\n",
      "Epoch 4/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2915\n",
      "Epoch 00004: val_loss improved from 0.10544 to 0.10183, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.1,dnodes128_dropout0.3,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2919 - val_loss: 0.1018\n",
      "Epoch 5/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.2792\n",
      "Epoch 00005: val_loss did not improve from 0.10183\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2789 - val_loss: 0.1101\n",
      "Epoch 6/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2680\n",
      "Epoch 00006: val_loss did not improve from 0.10183\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2683 - val_loss: 0.1206\n",
      " ###9 fold : val acc1 0.597, acc3 0.982, mae 0.211###\n",
      "acc10.590_acc30.977\n",
      "random search 44/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/70 [==========================>...] - ETA: 0s - loss: 19.4049\n",
      "Epoch 00001: val_loss improved from inf to 12.59367, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 18.8224 - val_loss: 12.5937\n",
      "Epoch 2/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 8.6671\n",
      "Epoch 00002: val_loss improved from 12.59367 to 4.89336, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 8.4595 - val_loss: 4.8934\n",
      "Epoch 3/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 3.6585\n",
      "Epoch 00003: val_loss improved from 4.89336 to 2.39690, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 3.5740 - val_loss: 2.3969\n",
      "Epoch 4/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 2.0091\n",
      "Epoch 00004: val_loss improved from 2.39690 to 1.28655, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 1.9865 - val_loss: 1.2866\n",
      "Epoch 5/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 1.1514\n",
      "Epoch 00005: val_loss improved from 1.28655 to 0.64205, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1161 - val_loss: 0.6420\n",
      "Epoch 6/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6415\n",
      "Epoch 00006: val_loss improved from 0.64205 to 0.33840, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6358 - val_loss: 0.3384\n",
      "Epoch 7/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4430\n",
      "Epoch 00007: val_loss improved from 0.33840 to 0.20245, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4367 - val_loss: 0.2025\n",
      "Epoch 8/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.3382\n",
      "Epoch 00008: val_loss improved from 0.20245 to 0.14919, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.3369 - val_loss: 0.1492\n",
      "Epoch 9/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.3044\n",
      "Epoch 00009: val_loss improved from 0.14919 to 0.12872, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3024 - val_loss: 0.1287\n",
      "Epoch 10/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.2857\n",
      "Epoch 00010: val_loss improved from 0.12872 to 0.12115, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2853 - val_loss: 0.1211\n",
      "Epoch 11/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2791\n",
      "Epoch 00011: val_loss improved from 0.12115 to 0.11713, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2785 - val_loss: 0.1171\n",
      "Epoch 12/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2792\n",
      "Epoch 00012: val_loss improved from 0.11713 to 0.11551, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2795 - val_loss: 0.1155\n",
      "Epoch 13/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2704\n",
      "Epoch 00013: val_loss improved from 0.11551 to 0.11351, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2725 - val_loss: 0.1135\n",
      "Epoch 14/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2672\n",
      "Epoch 00014: val_loss improved from 0.11351 to 0.11206, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2657 - val_loss: 0.1121\n",
      "Epoch 15/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2642\n",
      "Epoch 00015: val_loss improved from 0.11206 to 0.11125, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2651 - val_loss: 0.1113\n",
      "Epoch 16/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2583\n",
      "Epoch 00016: val_loss improved from 0.11125 to 0.10983, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2584 - val_loss: 0.1098\n",
      "Epoch 17/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2583\n",
      "Epoch 00017: val_loss improved from 0.10983 to 0.10899, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2581 - val_loss: 0.1090\n",
      "Epoch 18/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2586\n",
      "Epoch 00018: val_loss improved from 0.10899 to 0.10800, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2583 - val_loss: 0.1080\n",
      "Epoch 19/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2500\n",
      "Epoch 00019: val_loss improved from 0.10800 to 0.10694, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2485 - val_loss: 0.1069\n",
      "Epoch 20/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2472\n",
      "Epoch 00020: val_loss improved from 0.10694 to 0.10621, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2466 - val_loss: 0.1062\n",
      "Epoch 21/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2455\n",
      "Epoch 00021: val_loss improved from 0.10621 to 0.10570, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2441 - val_loss: 0.1057\n",
      "Epoch 22/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2357\n",
      "Epoch 00022: val_loss improved from 0.10570 to 0.10446, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2354 - val_loss: 0.1045\n",
      "Epoch 23/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2400\n",
      "Epoch 00023: val_loss improved from 0.10446 to 0.10396, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2377 - val_loss: 0.1040\n",
      "Epoch 24/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2357\n",
      "Epoch 00024: val_loss improved from 0.10396 to 0.10298, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2347 - val_loss: 0.1030\n",
      "Epoch 25/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2323\n",
      "Epoch 00025: val_loss improved from 0.10298 to 0.10262, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2345 - val_loss: 0.1026\n",
      "Epoch 26/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2282\n",
      "Epoch 00026: val_loss improved from 0.10262 to 0.10185, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2281 - val_loss: 0.1019\n",
      "Epoch 27/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2294\n",
      "Epoch 00027: val_loss improved from 0.10185 to 0.10158, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2298 - val_loss: 0.1016\n",
      "Epoch 28/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2283\n",
      "Epoch 00028: val_loss improved from 0.10158 to 0.10151, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2293 - val_loss: 0.1015\n",
      "Epoch 29/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2265\n",
      "Epoch 00029: val_loss improved from 0.10151 to 0.10086, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2276 - val_loss: 0.1009\n",
      "Epoch 30/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2245\n",
      "Epoch 00030: val_loss improved from 0.10086 to 0.10073, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2224 - val_loss: 0.1007\n",
      "Epoch 31/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2258\n",
      "Epoch 00031: val_loss improved from 0.10073 to 0.10009, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2254 - val_loss: 0.1001\n",
      "Epoch 32/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2196\n",
      "Epoch 00032: val_loss did not improve from 0.10009\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2205 - val_loss: 0.1007\n",
      "Epoch 33/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2178\n",
      "Epoch 00033: val_loss improved from 0.10009 to 0.09890, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2166 - val_loss: 0.0989\n",
      "Epoch 34/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2158\n",
      "Epoch 00034: val_loss did not improve from 0.09890\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2168 - val_loss: 0.0992\n",
      "Epoch 35/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2151\n",
      "Epoch 00035: val_loss improved from 0.09890 to 0.09862, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2158 - val_loss: 0.0986\n",
      "Epoch 36/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2134\n",
      "Epoch 00036: val_loss improved from 0.09862 to 0.09862, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2139 - val_loss: 0.0986\n",
      "Epoch 37/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2115\n",
      "Epoch 00037: val_loss improved from 0.09862 to 0.09825, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2118 - val_loss: 0.0982\n",
      "Epoch 38/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2096\n",
      "Epoch 00038: val_loss did not improve from 0.09825\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2092 - val_loss: 0.0985\n",
      "Epoch 39/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2088\n",
      "Epoch 00039: val_loss improved from 0.09825 to 0.09806, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2072 - val_loss: 0.0981\n",
      "Epoch 40/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2088\n",
      "Epoch 00040: val_loss improved from 0.09806 to 0.09786, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2093 - val_loss: 0.0979\n",
      "Epoch 41/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2074\n",
      "Epoch 00041: val_loss improved from 0.09786 to 0.09728, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2069 - val_loss: 0.0973\n",
      "Epoch 42/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2075\n",
      "Epoch 00042: val_loss did not improve from 0.09728\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2091 - val_loss: 0.0977\n",
      "Epoch 43/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2060\n",
      "Epoch 00043: val_loss did not improve from 0.09728\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2056 - val_loss: 0.0974\n",
      " ###0 fold : val acc1 0.605, acc3 0.980, mae 0.208###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/70 [=========================>....] - ETA: 0s - loss: 19.5893\n",
      "Epoch 00001: val_loss improved from inf to 12.60506, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 18.8161 - val_loss: 12.6051\n",
      "Epoch 2/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 8.9562 \n",
      "Epoch 00002: val_loss improved from 12.60506 to 4.93165, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 8.4945 - val_loss: 4.9316\n",
      "Epoch 3/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 3.8085\n",
      "Epoch 00003: val_loss improved from 4.93165 to 2.39263, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.5999 - val_loss: 2.3926\n",
      "Epoch 4/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 2.0617\n",
      "Epoch 00004: val_loss improved from 2.39263 to 1.27505, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9833 - val_loss: 1.2750\n",
      "Epoch 5/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 1.1742\n",
      "Epoch 00005: val_loss improved from 1.27505 to 0.63828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1102 - val_loss: 0.6383\n",
      "Epoch 6/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.6573\n",
      "Epoch 00006: val_loss improved from 0.63828 to 0.33681, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6338 - val_loss: 0.3368\n",
      "Epoch 7/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.4527\n",
      "Epoch 00007: val_loss improved from 0.33681 to 0.20252, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4337 - val_loss: 0.2025\n",
      "Epoch 8/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3357\n",
      "Epoch 00008: val_loss improved from 0.20252 to 0.15004, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3358 - val_loss: 0.1500\n",
      "Epoch 9/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3031\n",
      "Epoch 00009: val_loss improved from 0.15004 to 0.12989, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3019 - val_loss: 0.1299\n",
      "Epoch 10/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2873\n",
      "Epoch 00010: val_loss improved from 0.12989 to 0.12153, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2879 - val_loss: 0.1215\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2817\n",
      "Epoch 00011: val_loss improved from 0.12153 to 0.11746, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2817 - val_loss: 0.1175\n",
      "Epoch 12/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2803\n",
      "Epoch 00012: val_loss improved from 0.11746 to 0.11573, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2801 - val_loss: 0.1157\n",
      "Epoch 13/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.2719\n",
      "Epoch 00013: val_loss improved from 0.11573 to 0.11372, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2727 - val_loss: 0.1137\n",
      "Epoch 14/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2674\n",
      "Epoch 00014: val_loss improved from 0.11372 to 0.11219, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2672 - val_loss: 0.1122\n",
      "Epoch 15/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2642\n",
      "Epoch 00015: val_loss improved from 0.11219 to 0.11139, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2644 - val_loss: 0.1114\n",
      "Epoch 16/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2580\n",
      "Epoch 00016: val_loss improved from 0.11139 to 0.10999, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2585 - val_loss: 0.1100\n",
      "Epoch 17/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2599\n",
      "Epoch 00017: val_loss improved from 0.10999 to 0.10900, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2594 - val_loss: 0.1090\n",
      "Epoch 18/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2600\n",
      "Epoch 00018: val_loss improved from 0.10900 to 0.10792, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2592 - val_loss: 0.1079\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2483\n",
      "Epoch 00019: val_loss improved from 0.10792 to 0.10696, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2483 - val_loss: 0.1070\n",
      "Epoch 20/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.2469\n",
      "Epoch 00020: val_loss improved from 0.10696 to 0.10616, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2483 - val_loss: 0.1062\n",
      "Epoch 21/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2444\n",
      "Epoch 00021: val_loss improved from 0.10616 to 0.10560, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2444 - val_loss: 0.1056\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2359\n",
      "Epoch 00022: val_loss improved from 0.10560 to 0.10454, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2359 - val_loss: 0.1045\n",
      "Epoch 23/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2381\n",
      "Epoch 00023: val_loss improved from 0.10454 to 0.10398, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2378 - val_loss: 0.1040\n",
      "Epoch 24/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2358\n",
      "Epoch 00024: val_loss improved from 0.10398 to 0.10312, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2351 - val_loss: 0.1031\n",
      "Epoch 25/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2342\n",
      "Epoch 00025: val_loss improved from 0.10312 to 0.10253, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2363 - val_loss: 0.1025\n",
      "Epoch 26/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.2283\n",
      "Epoch 00026: val_loss improved from 0.10253 to 0.10181, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2284 - val_loss: 0.1018\n",
      "Epoch 27/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.2291\n",
      "Epoch 00027: val_loss improved from 0.10181 to 0.10177, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2297 - val_loss: 0.1018\n",
      "Epoch 28/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2279\n",
      "Epoch 00028: val_loss improved from 0.10177 to 0.10139, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2280 - val_loss: 0.1014\n",
      "Epoch 29/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2303\n",
      "Epoch 00029: val_loss improved from 0.10139 to 0.10102, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2300 - val_loss: 0.1010\n",
      "Epoch 30/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.2224\n",
      "Epoch 00030: val_loss did not improve from 0.10102\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2215 - val_loss: 0.1011\n",
      "Epoch 31/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2263\n",
      "Epoch 00031: val_loss improved from 0.10102 to 0.10000, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2257 - val_loss: 0.1000\n",
      "Epoch 32/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2209\n",
      "Epoch 00032: val_loss did not improve from 0.10000\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2207 - val_loss: 0.1004\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2165\n",
      "Epoch 00033: val_loss improved from 0.10000 to 0.09891, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2165 - val_loss: 0.0989\n",
      "Epoch 34/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2171\n",
      "Epoch 00034: val_loss did not improve from 0.09891\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2174 - val_loss: 0.0990\n",
      "Epoch 35/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2163\n",
      "Epoch 00035: val_loss improved from 0.09891 to 0.09869, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2166 - val_loss: 0.0987\n",
      "Epoch 36/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2145\n",
      "Epoch 00036: val_loss improved from 0.09869 to 0.09840, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2143 - val_loss: 0.0984\n",
      "Epoch 37/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2110\n",
      "Epoch 00037: val_loss improved from 0.09840 to 0.09813, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2113 - val_loss: 0.0981\n",
      "Epoch 38/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2090\n",
      "Epoch 00038: val_loss did not improve from 0.09813\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2096 - val_loss: 0.0987\n",
      "Epoch 39/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2072\n",
      "Epoch 00039: val_loss improved from 0.09813 to 0.09783, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2075 - val_loss: 0.0978\n",
      "Epoch 40/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2113\n",
      "Epoch 00040: val_loss improved from 0.09783 to 0.09755, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2109 - val_loss: 0.0975\n",
      "Epoch 41/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2082\n",
      "Epoch 00041: val_loss improved from 0.09755 to 0.09741, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2086 - val_loss: 0.0974\n",
      "Epoch 42/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2080\n",
      "Epoch 00042: val_loss did not improve from 0.09741\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2086 - val_loss: 0.0975\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2065\n",
      "Epoch 00043: val_loss did not improve from 0.09741\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2065 - val_loss: 0.0976\n",
      " ###1 fold : val acc1 0.597, acc3 0.981, mae 0.211###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/70 [=========================>....] - ETA: 0s - loss: 19.5506\n",
      "Epoch 00001: val_loss improved from inf to 12.60105, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 18.7941 - val_loss: 12.6011\n",
      "Epoch 2/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 8.8645\n",
      "Epoch 00002: val_loss improved from 12.60105 to 4.93390, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 8.4743 - val_loss: 4.9339\n",
      "Epoch 3/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 3.8190\n",
      "Epoch 00003: val_loss improved from 4.93390 to 2.38955, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.5900 - val_loss: 2.3895\n",
      "Epoch 4/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 2.0684\n",
      "Epoch 00004: val_loss improved from 2.38955 to 1.26998, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9808 - val_loss: 1.2700\n",
      "Epoch 5/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 1.1559\n",
      "Epoch 00005: val_loss improved from 1.26998 to 0.63459, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1015 - val_loss: 0.6346\n",
      "Epoch 6/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.6446\n",
      "Epoch 00006: val_loss improved from 0.63459 to 0.33499, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6310 - val_loss: 0.3350\n",
      "Epoch 7/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.4469\n",
      "Epoch 00007: val_loss improved from 0.33499 to 0.20195, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4307 - val_loss: 0.2019\n",
      "Epoch 8/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3401\n",
      "Epoch 00008: val_loss improved from 0.20195 to 0.14860, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3366 - val_loss: 0.1486\n",
      "Epoch 9/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3029\n",
      "Epoch 00009: val_loss improved from 0.14860 to 0.12913, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3014 - val_loss: 0.1291\n",
      "Epoch 10/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2878\n",
      "Epoch 00010: val_loss improved from 0.12913 to 0.12122, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2873 - val_loss: 0.1212\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2807\n",
      "Epoch 00011: val_loss improved from 0.12122 to 0.11716, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2807 - val_loss: 0.1172\n",
      "Epoch 12/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2788\n",
      "Epoch 00012: val_loss improved from 0.11716 to 0.11545, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2799 - val_loss: 0.1154\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2716\n",
      "Epoch 00013: val_loss improved from 0.11545 to 0.11362, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2716 - val_loss: 0.1136\n",
      "Epoch 14/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2695\n",
      "Epoch 00014: val_loss improved from 0.11362 to 0.11217, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2666 - val_loss: 0.1122\n",
      "Epoch 15/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2646\n",
      "Epoch 00015: val_loss improved from 0.11217 to 0.11147, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2651 - val_loss: 0.1115\n",
      "Epoch 16/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2581\n",
      "Epoch 00016: val_loss improved from 0.11147 to 0.10994, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2589 - val_loss: 0.1099\n",
      "Epoch 17/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2575\n",
      "Epoch 00017: val_loss improved from 0.10994 to 0.10907, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2561 - val_loss: 0.1091\n",
      "Epoch 18/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2597\n",
      "Epoch 00018: val_loss improved from 0.10907 to 0.10799, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2597 - val_loss: 0.1080\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2471\n",
      "Epoch 00019: val_loss improved from 0.10799 to 0.10730, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2471 - val_loss: 0.1073\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2491\n",
      "Epoch 00020: val_loss improved from 0.10730 to 0.10636, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2491 - val_loss: 0.1064\n",
      "Epoch 21/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2414\n",
      "Epoch 00021: val_loss improved from 0.10636 to 0.10557, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2420 - val_loss: 0.1056\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2356\n",
      "Epoch 00022: val_loss improved from 0.10557 to 0.10472, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2356 - val_loss: 0.1047\n",
      "Epoch 23/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2363\n",
      "Epoch 00023: val_loss improved from 0.10472 to 0.10442, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2369 - val_loss: 0.1044\n",
      "Epoch 24/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2339\n",
      "Epoch 00024: val_loss improved from 0.10442 to 0.10329, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2339 - val_loss: 0.1033\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2345\n",
      "Epoch 00025: val_loss improved from 0.10329 to 0.10281, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2345 - val_loss: 0.1028\n",
      "Epoch 26/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2273\n",
      "Epoch 00026: val_loss improved from 0.10281 to 0.10197, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2273 - val_loss: 0.1020\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2299\n",
      "Epoch 00027: val_loss improved from 0.10197 to 0.10192, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2299 - val_loss: 0.1019\n",
      "Epoch 28/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2264\n",
      "Epoch 00028: val_loss improved from 0.10192 to 0.10163, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2268 - val_loss: 0.1016\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2267\n",
      "Epoch 00029: val_loss improved from 0.10163 to 0.10109, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2267 - val_loss: 0.1011\n",
      "Epoch 30/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2227\n",
      "Epoch 00030: val_loss did not improve from 0.10109\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2223 - val_loss: 0.1016\n",
      "Epoch 31/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2244\n",
      "Epoch 00031: val_loss improved from 0.10109 to 0.10011, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2237 - val_loss: 0.1001\n",
      "Epoch 32/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2197\n",
      "Epoch 00032: val_loss improved from 0.10011 to 0.09976, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2201 - val_loss: 0.0998\n",
      "Epoch 33/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2167\n",
      "Epoch 00033: val_loss improved from 0.09976 to 0.09919, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2163 - val_loss: 0.0992\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2159\n",
      "Epoch 00034: val_loss did not improve from 0.09919\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2159 - val_loss: 0.0992\n",
      "Epoch 35/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2164\n",
      "Epoch 00035: val_loss improved from 0.09919 to 0.09885, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2179 - val_loss: 0.0988\n",
      "Epoch 36/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2147\n",
      "Epoch 00036: val_loss improved from 0.09885 to 0.09869, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2147 - val_loss: 0.0987\n",
      "Epoch 37/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2099\n",
      "Epoch 00037: val_loss improved from 0.09869 to 0.09816, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2101 - val_loss: 0.0982\n",
      "Epoch 38/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2091\n",
      "Epoch 00038: val_loss did not improve from 0.09816\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2088 - val_loss: 0.0990\n",
      "Epoch 39/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2079\n",
      "Epoch 00039: val_loss improved from 0.09816 to 0.09783, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2068 - val_loss: 0.0978\n",
      "Epoch 40/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2104\n",
      "Epoch 00040: val_loss improved from 0.09783 to 0.09764, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2102 - val_loss: 0.0976\n",
      "Epoch 41/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2064\n",
      "Epoch 00041: val_loss improved from 0.09764 to 0.09727, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2077 - val_loss: 0.0973\n",
      "Epoch 42/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2055\n",
      "Epoch 00042: val_loss did not improve from 0.09727\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2071 - val_loss: 0.0976\n",
      "Epoch 43/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2059\n",
      "Epoch 00043: val_loss did not improve from 0.09727\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2060 - val_loss: 0.0983\n",
      " ###2 fold : val acc1 0.601, acc3 0.978, mae 0.211###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/70 [=======================>......] - ETA: 0s - loss: 19.8597\n",
      "Epoch 00001: val_loss improved from inf to 12.57477, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 18.8441 - val_loss: 12.5748\n",
      "Epoch 2/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 9.0687 \n",
      "Epoch 00002: val_loss improved from 12.57477 to 4.90648, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 8.4961 - val_loss: 4.9065\n",
      "Epoch 3/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 3.8253\n",
      "Epoch 00003: val_loss improved from 4.90648 to 2.38704, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.5939 - val_loss: 2.3870\n",
      "Epoch 4/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 2.0764\n",
      "Epoch 00004: val_loss improved from 2.38704 to 1.27385, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9873 - val_loss: 1.2738\n",
      "Epoch 5/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 1.1673\n",
      "Epoch 00005: val_loss improved from 1.27385 to 0.63625, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1030 - val_loss: 0.6362\n",
      "Epoch 6/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6373\n",
      "Epoch 00006: val_loss improved from 0.63625 to 0.33442, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6341 - val_loss: 0.3344\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4276\n",
      "Epoch 00007: val_loss improved from 0.33442 to 0.20141, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4276 - val_loss: 0.2014\n",
      "Epoch 8/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.3369\n",
      "Epoch 00008: val_loss improved from 0.20141 to 0.14861, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3360 - val_loss: 0.1486\n",
      "Epoch 9/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.3019\n",
      "Epoch 00009: val_loss improved from 0.14861 to 0.12918, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3017 - val_loss: 0.1292\n",
      "Epoch 10/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2891\n",
      "Epoch 00010: val_loss improved from 0.12918 to 0.12113, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2874 - val_loss: 0.1211\n",
      "Epoch 11/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2809\n",
      "Epoch 00011: val_loss improved from 0.12113 to 0.11737, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2781 - val_loss: 0.1174\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2815\n",
      "Epoch 00012: val_loss improved from 0.11737 to 0.11562, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2815 - val_loss: 0.1156\n",
      "Epoch 13/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.2693\n",
      "Epoch 00013: val_loss improved from 0.11562 to 0.11382, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2705 - val_loss: 0.1138\n",
      "Epoch 14/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.2639\n",
      "Epoch 00014: val_loss improved from 0.11382 to 0.11225, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2645 - val_loss: 0.1123\n",
      "Epoch 15/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2662\n",
      "Epoch 00015: val_loss improved from 0.11225 to 0.11145, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2660 - val_loss: 0.1114\n",
      "Epoch 16/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2571\n",
      "Epoch 00016: val_loss improved from 0.11145 to 0.11003, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2578 - val_loss: 0.1100\n",
      "Epoch 17/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2546\n",
      "Epoch 00017: val_loss improved from 0.11003 to 0.10923, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2543 - val_loss: 0.1092\n",
      "Epoch 18/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.2577\n",
      "Epoch 00018: val_loss improved from 0.10923 to 0.10818, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2579 - val_loss: 0.1082\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2459\n",
      "Epoch 00019: val_loss improved from 0.10818 to 0.10736, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2459 - val_loss: 0.1074\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2488\n",
      "Epoch 00020: val_loss improved from 0.10736 to 0.10631, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2488 - val_loss: 0.1063\n",
      "Epoch 21/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2410\n",
      "Epoch 00021: val_loss improved from 0.10631 to 0.10593, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2411 - val_loss: 0.1059\n",
      "Epoch 22/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2370\n",
      "Epoch 00022: val_loss improved from 0.10593 to 0.10507, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2364 - val_loss: 0.1051\n",
      "Epoch 23/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2350\n",
      "Epoch 00023: val_loss improved from 0.10507 to 0.10448, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2356 - val_loss: 0.1045\n",
      "Epoch 24/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2348\n",
      "Epoch 00024: val_loss improved from 0.10448 to 0.10322, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2341 - val_loss: 0.1032\n",
      "Epoch 25/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2336\n",
      "Epoch 00025: val_loss improved from 0.10322 to 0.10280, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2342 - val_loss: 0.1028\n",
      "Epoch 26/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2299\n",
      "Epoch 00026: val_loss improved from 0.10280 to 0.10219, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2293 - val_loss: 0.1022\n",
      "Epoch 27/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2296\n",
      "Epoch 00027: val_loss improved from 0.10219 to 0.10204, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2305 - val_loss: 0.1020\n",
      "Epoch 28/100\n",
      "53/70 [=====================>........] - ETA: 0s - loss: 0.2290\n",
      "Epoch 00028: val_loss improved from 0.10204 to 0.10173, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2281 - val_loss: 0.1017\n",
      "Epoch 29/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2262\n",
      "Epoch 00029: val_loss improved from 0.10173 to 0.10124, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2259 - val_loss: 0.1012\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2232\n",
      "Epoch 00030: val_loss did not improve from 0.10124\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2232 - val_loss: 0.1016\n",
      "Epoch 31/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2231\n",
      "Epoch 00031: val_loss improved from 0.10124 to 0.09997, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2231 - val_loss: 0.1000\n",
      "Epoch 32/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2192\n",
      "Epoch 00032: val_loss improved from 0.09997 to 0.09966, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2193 - val_loss: 0.0997\n",
      "Epoch 33/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2152\n",
      "Epoch 00033: val_loss improved from 0.09966 to 0.09946, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2147 - val_loss: 0.0995\n",
      "Epoch 34/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2149\n",
      "Epoch 00034: val_loss improved from 0.09946 to 0.09921, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2151 - val_loss: 0.0992\n",
      "Epoch 35/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2157\n",
      "Epoch 00035: val_loss improved from 0.09921 to 0.09896, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2175 - val_loss: 0.0990\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2157\n",
      "Epoch 00036: val_loss improved from 0.09896 to 0.09884, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2157 - val_loss: 0.0988\n",
      "Epoch 37/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2072\n",
      "Epoch 00037: val_loss improved from 0.09884 to 0.09832, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2087 - val_loss: 0.0983\n",
      "Epoch 38/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2088\n",
      "Epoch 00038: val_loss did not improve from 0.09832\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2091 - val_loss: 0.0988\n",
      "Epoch 39/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2068\n",
      "Epoch 00039: val_loss improved from 0.09832 to 0.09810, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2072 - val_loss: 0.0981\n",
      "Epoch 40/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2109\n",
      "Epoch 00040: val_loss improved from 0.09810 to 0.09741, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2105 - val_loss: 0.0974\n",
      "Epoch 41/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2073\n",
      "Epoch 00041: val_loss improved from 0.09741 to 0.09739, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2086 - val_loss: 0.0974\n",
      "Epoch 42/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2051\n",
      "Epoch 00042: val_loss did not improve from 0.09739\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2067 - val_loss: 0.0975\n",
      "Epoch 43/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00043: val_loss did not improve from 0.09739\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2060 - val_loss: 0.0980\n",
      " ###3 fold : val acc1 0.611, acc3 0.974, mae 0.208###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/70 [=========================>....] - ETA: 0s - loss: 19.4745\n",
      "Epoch 00001: val_loss improved from inf to 12.52934, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 18.7458 - val_loss: 12.5293\n",
      "Epoch 2/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 8.8689\n",
      "Epoch 00002: val_loss improved from 12.52934 to 4.89533, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 8.4496 - val_loss: 4.8953\n",
      "Epoch 3/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 3.7448\n",
      "Epoch 00003: val_loss improved from 4.89533 to 2.38725, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.5686 - val_loss: 2.3873\n",
      "Epoch 4/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 2.0881\n",
      "Epoch 00004: val_loss improved from 2.38725 to 1.27545, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9744 - val_loss: 1.2755\n",
      "Epoch 5/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 1.1505\n",
      "Epoch 00005: val_loss improved from 1.27545 to 0.63541, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1140 - val_loss: 0.6354\n",
      "Epoch 6/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.6586\n",
      "Epoch 00006: val_loss improved from 0.63541 to 0.33078, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6326 - val_loss: 0.3308\n",
      "Epoch 7/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.4402\n",
      "Epoch 00007: val_loss improved from 0.33078 to 0.20007, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4249 - val_loss: 0.2001\n",
      "Epoch 8/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.3330\n",
      "Epoch 00008: val_loss improved from 0.20007 to 0.14877, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3276 - val_loss: 0.1488\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2974\n",
      "Epoch 00009: val_loss improved from 0.14877 to 0.12956, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2974 - val_loss: 0.1296\n",
      "Epoch 10/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2890\n",
      "Epoch 00010: val_loss improved from 0.12956 to 0.12208, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2862 - val_loss: 0.1221\n",
      "Epoch 11/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2764\n",
      "Epoch 00011: val_loss improved from 0.12208 to 0.11834, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2753 - val_loss: 0.1183\n",
      "Epoch 12/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2772\n",
      "Epoch 00012: val_loss improved from 0.11834 to 0.11623, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2763 - val_loss: 0.1162\n",
      "Epoch 13/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2721\n",
      "Epoch 00013: val_loss improved from 0.11623 to 0.11410, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2711 - val_loss: 0.1141\n",
      "Epoch 14/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2673\n",
      "Epoch 00014: val_loss improved from 0.11410 to 0.11283, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2668 - val_loss: 0.1128\n",
      "Epoch 15/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2640\n",
      "Epoch 00015: val_loss improved from 0.11283 to 0.11165, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2638 - val_loss: 0.1116\n",
      "Epoch 16/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2629\n",
      "Epoch 00016: val_loss improved from 0.11165 to 0.11009, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2612 - val_loss: 0.1101\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2534\n",
      "Epoch 00017: val_loss did not improve from 0.11009\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2534 - val_loss: 0.1105\n",
      "Epoch 18/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2537\n",
      "Epoch 00018: val_loss improved from 0.11009 to 0.10863, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2539 - val_loss: 0.1086\n",
      "Epoch 19/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2489\n",
      "Epoch 00019: val_loss improved from 0.10863 to 0.10766, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2481 - val_loss: 0.1077\n",
      "Epoch 20/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2452\n",
      "Epoch 00020: val_loss improved from 0.10766 to 0.10658, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2445 - val_loss: 0.1066\n",
      "Epoch 21/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2434\n",
      "Epoch 00021: val_loss improved from 0.10658 to 0.10595, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2430 - val_loss: 0.1059\n",
      "Epoch 22/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2409\n",
      "Epoch 00022: val_loss improved from 0.10595 to 0.10570, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2407 - val_loss: 0.1057\n",
      "Epoch 23/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2343\n",
      "Epoch 00023: val_loss improved from 0.10570 to 0.10460, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2337 - val_loss: 0.1046\n",
      "Epoch 24/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2340\n",
      "Epoch 00024: val_loss improved from 0.10460 to 0.10401, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2338 - val_loss: 0.1040\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2318\n",
      "Epoch 00025: val_loss improved from 0.10401 to 0.10337, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2318 - val_loss: 0.1034\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2300\n",
      "Epoch 00026: val_loss improved from 0.10337 to 0.10230, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2300 - val_loss: 0.1023\n",
      "Epoch 27/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2319\n",
      "Epoch 00027: val_loss did not improve from 0.10230\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2312 - val_loss: 0.1023\n",
      "Epoch 28/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2298\n",
      "Epoch 00028: val_loss improved from 0.10230 to 0.10148, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2298 - val_loss: 0.1015\n",
      "Epoch 29/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2252\n",
      "Epoch 00029: val_loss improved from 0.10148 to 0.10080, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2250 - val_loss: 0.1008\n",
      "Epoch 30/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2221\n",
      "Epoch 00030: val_loss improved from 0.10080 to 0.10046, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2239 - val_loss: 0.1005\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2245\n",
      "Epoch 00031: val_loss improved from 0.10046 to 0.09964, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2245 - val_loss: 0.0996\n",
      "Epoch 32/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2216\n",
      "Epoch 00032: val_loss did not improve from 0.09964\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2202 - val_loss: 0.0997\n",
      "Epoch 33/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2188\n",
      "Epoch 00033: val_loss improved from 0.09964 to 0.09940, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2189 - val_loss: 0.0994\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2142\n",
      "Epoch 00034: val_loss improved from 0.09940 to 0.09910, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2142 - val_loss: 0.0991\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2116\n",
      "Epoch 00035: val_loss improved from 0.09910 to 0.09899, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2116 - val_loss: 0.0990\n",
      "Epoch 36/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2087\n",
      "Epoch 00036: val_loss improved from 0.09899 to 0.09819, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2085 - val_loss: 0.0982\n",
      "Epoch 37/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2175\n",
      "Epoch 00037: val_loss improved from 0.09819 to 0.09805, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2150 - val_loss: 0.0981\n",
      "Epoch 38/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2080\n",
      "Epoch 00038: val_loss improved from 0.09805 to 0.09783, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2111 - val_loss: 0.0978\n",
      "Epoch 39/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2076\n",
      "Epoch 00039: val_loss did not improve from 0.09783\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2087 - val_loss: 0.0980\n",
      "Epoch 40/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2086\n",
      "Epoch 00040: val_loss improved from 0.09783 to 0.09746, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2088 - val_loss: 0.0975\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2053\n",
      "Epoch 00041: val_loss improved from 0.09746 to 0.09722, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2053 - val_loss: 0.0972\n",
      "Epoch 42/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2077\n",
      "Epoch 00042: val_loss did not improve from 0.09722\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2086 - val_loss: 0.0977\n",
      "Epoch 43/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2042\n",
      "Epoch 00043: val_loss improved from 0.09722 to 0.09711, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2042 - val_loss: 0.0971\n",
      "Epoch 44/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2065\n",
      "Epoch 00044: val_loss improved from 0.09711 to 0.09692, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2060 - val_loss: 0.0969\n",
      "Epoch 45/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.1983\n",
      "Epoch 00045: val_loss did not improve from 0.09692\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1989 - val_loss: 0.0984\n",
      "Epoch 46/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2004\n",
      "Epoch 00046: val_loss improved from 0.09692 to 0.09669, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2004 - val_loss: 0.0967\n",
      "Epoch 47/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.1996\n",
      "Epoch 00047: val_loss did not improve from 0.09669\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1998 - val_loss: 0.0972\n",
      "Epoch 48/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2009\n",
      "Epoch 00048: val_loss did not improve from 0.09669\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2009 - val_loss: 0.0967\n",
      " ###4 fold : val acc1 0.597, acc3 0.978, mae 0.212###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/70 [==========================>...] - ETA: 0s - loss: 19.1484\n",
      "Epoch 00001: val_loss improved from inf to 12.52464, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 18.7607 - val_loss: 12.5246\n",
      "Epoch 2/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 8.6853\n",
      "Epoch 00002: val_loss improved from 12.52464 to 4.87749, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 8.4570 - val_loss: 4.8775\n",
      "Epoch 3/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 3.6584\n",
      "Epoch 00003: val_loss improved from 4.87749 to 2.38040, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.5567 - val_loss: 2.3804\n",
      "Epoch 4/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 2.0044\n",
      "Epoch 00004: val_loss improved from 2.38040 to 1.27595, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9691 - val_loss: 1.2760\n",
      "Epoch 5/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 1.1700\n",
      "Epoch 00005: val_loss improved from 1.27595 to 0.63828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1146 - val_loss: 0.6383\n",
      "Epoch 6/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.6634\n",
      "Epoch 00006: val_loss improved from 0.63828 to 0.33307, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6320 - val_loss: 0.3331\n",
      "Epoch 7/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.4382\n",
      "Epoch 00007: val_loss improved from 0.33307 to 0.20111, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4244 - val_loss: 0.2011\n",
      "Epoch 8/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.3336\n",
      "Epoch 00008: val_loss improved from 0.20111 to 0.14898, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3284 - val_loss: 0.1490\n",
      "Epoch 9/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.3013\n",
      "Epoch 00009: val_loss improved from 0.14898 to 0.12971, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2978 - val_loss: 0.1297\n",
      "Epoch 10/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2878\n",
      "Epoch 00010: val_loss improved from 0.12971 to 0.12204, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2856 - val_loss: 0.1220\n",
      "Epoch 11/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2762\n",
      "Epoch 00011: val_loss improved from 0.12204 to 0.11819, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2751 - val_loss: 0.1182\n",
      "Epoch 12/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2786\n",
      "Epoch 00012: val_loss improved from 0.11819 to 0.11630, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2773 - val_loss: 0.1163\n",
      "Epoch 13/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2693\n",
      "Epoch 00013: val_loss improved from 0.11630 to 0.11418, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2686 - val_loss: 0.1142\n",
      "Epoch 14/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2657\n",
      "Epoch 00014: val_loss improved from 0.11418 to 0.11279, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2652 - val_loss: 0.1128\n",
      "Epoch 15/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2631\n",
      "Epoch 00015: val_loss improved from 0.11279 to 0.11163, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2624 - val_loss: 0.1116\n",
      "Epoch 16/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2619\n",
      "Epoch 00016: val_loss improved from 0.11163 to 0.11027, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2591 - val_loss: 0.1103\n",
      "Epoch 17/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2532\n",
      "Epoch 00017: val_loss did not improve from 0.11027\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2541 - val_loss: 0.1105\n",
      "Epoch 18/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2527\n",
      "Epoch 00018: val_loss improved from 0.11027 to 0.10854, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2521 - val_loss: 0.1085\n",
      "Epoch 19/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2527\n",
      "Epoch 00019: val_loss improved from 0.10854 to 0.10764, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2509 - val_loss: 0.1076\n",
      "Epoch 20/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2462\n",
      "Epoch 00020: val_loss improved from 0.10764 to 0.10681, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2452 - val_loss: 0.1068\n",
      "Epoch 21/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2439\n",
      "Epoch 00021: val_loss improved from 0.10681 to 0.10608, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2435 - val_loss: 0.1061\n",
      "Epoch 22/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2409\n",
      "Epoch 00022: val_loss improved from 0.10608 to 0.10569, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2405 - val_loss: 0.1057\n",
      "Epoch 23/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2367\n",
      "Epoch 00023: val_loss improved from 0.10569 to 0.10485, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2338 - val_loss: 0.1049\n",
      "Epoch 24/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2354\n",
      "Epoch 00024: val_loss improved from 0.10485 to 0.10394, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2343 - val_loss: 0.1039\n",
      "Epoch 25/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.2319\n",
      "Epoch 00025: val_loss improved from 0.10394 to 0.10324, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2322 - val_loss: 0.1032\n",
      "Epoch 26/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2279\n",
      "Epoch 00026: val_loss improved from 0.10324 to 0.10270, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2292 - val_loss: 0.1027\n",
      "Epoch 27/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2312\n",
      "Epoch 00027: val_loss improved from 0.10270 to 0.10261, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2300 - val_loss: 0.1026\n",
      "Epoch 28/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2280\n",
      "Epoch 00028: val_loss improved from 0.10261 to 0.10171, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2282 - val_loss: 0.1017\n",
      "Epoch 29/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2242\n",
      "Epoch 00029: val_loss improved from 0.10171 to 0.10089, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2253 - val_loss: 0.1009\n",
      "Epoch 30/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2215\n",
      "Epoch 00030: val_loss improved from 0.10089 to 0.10067, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2239 - val_loss: 0.1007\n",
      "Epoch 31/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2229\n",
      "Epoch 00031: val_loss improved from 0.10067 to 0.09993, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2226 - val_loss: 0.0999\n",
      "Epoch 32/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2208\n",
      "Epoch 00032: val_loss improved from 0.09993 to 0.09963, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2201 - val_loss: 0.0996\n",
      "Epoch 33/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2200\n",
      "Epoch 00033: val_loss improved from 0.09963 to 0.09959, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2205 - val_loss: 0.0996\n",
      "Epoch 34/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2141\n",
      "Epoch 00034: val_loss improved from 0.09959 to 0.09926, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2138 - val_loss: 0.0993\n",
      "Epoch 35/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2119\n",
      "Epoch 00035: val_loss did not improve from 0.09926\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2106 - val_loss: 0.0994\n",
      "Epoch 36/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2072\n",
      "Epoch 00036: val_loss improved from 0.09926 to 0.09844, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2077 - val_loss: 0.0984\n",
      "Epoch 37/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2158\n",
      "Epoch 00037: val_loss improved from 0.09844 to 0.09822, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2126 - val_loss: 0.0982\n",
      "Epoch 38/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2098\n",
      "Epoch 00038: val_loss improved from 0.09822 to 0.09810, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2121 - val_loss: 0.0981\n",
      "Epoch 39/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2109\n",
      "Epoch 00039: val_loss did not improve from 0.09810\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2113 - val_loss: 0.0981\n",
      "Epoch 40/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2071\n",
      "Epoch 00040: val_loss improved from 0.09810 to 0.09782, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2085 - val_loss: 0.0978\n",
      "Epoch 41/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2065\n",
      "Epoch 00041: val_loss improved from 0.09782 to 0.09756, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2042 - val_loss: 0.0976\n",
      "Epoch 42/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00042: val_loss did not improve from 0.09756\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2065 - val_loss: 0.0976\n",
      "Epoch 43/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00043: val_loss improved from 0.09756 to 0.09711, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2030 - val_loss: 0.0971\n",
      "Epoch 44/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2064\n",
      "Epoch 00044: val_loss did not improve from 0.09711\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2059 - val_loss: 0.0972\n",
      "Epoch 45/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.1985\n",
      "Epoch 00045: val_loss did not improve from 0.09711\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1982 - val_loss: 0.0982\n",
      " ###5 fold : val acc1 0.588, acc3 0.980, mae 0.217###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/70 [========================>.....] - ETA: 0s - loss: 19.6767\n",
      "Epoch 00001: val_loss improved from inf to 12.54507, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 18.7805 - val_loss: 12.5451\n",
      "Epoch 2/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 8.7335\n",
      "Epoch 00002: val_loss improved from 12.54507 to 4.89210, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 8.4586 - val_loss: 4.8921\n",
      "Epoch 3/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 3.7021\n",
      "Epoch 00003: val_loss improved from 4.89210 to 2.37576, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.5714 - val_loss: 2.3758\n",
      "Epoch 4/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 2.0485\n",
      "Epoch 00004: val_loss improved from 2.37576 to 1.26570, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9717 - val_loss: 1.2657\n",
      "Epoch 5/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 1.1556\n",
      "Epoch 00005: val_loss improved from 1.26570 to 0.63210, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1077 - val_loss: 0.6321\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6300\n",
      "Epoch 00006: val_loss improved from 0.63210 to 0.32940, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6300 - val_loss: 0.3294\n",
      "Epoch 7/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.4404\n",
      "Epoch 00007: val_loss improved from 0.32940 to 0.20028, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4252 - val_loss: 0.2003\n",
      "Epoch 8/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.3367\n",
      "Epoch 00008: val_loss improved from 0.20028 to 0.14851, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3302 - val_loss: 0.1485\n",
      "Epoch 9/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.3043\n",
      "Epoch 00009: val_loss improved from 0.14851 to 0.12960, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3011 - val_loss: 0.1296\n",
      "Epoch 10/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2889\n",
      "Epoch 00010: val_loss improved from 0.12960 to 0.12169, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2876 - val_loss: 0.1217\n",
      "Epoch 11/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2804\n",
      "Epoch 00011: val_loss improved from 0.12169 to 0.11789, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2782 - val_loss: 0.1179\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2789\n",
      "Epoch 00012: val_loss improved from 0.11789 to 0.11604, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2789 - val_loss: 0.1160\n",
      "Epoch 13/100\n",
      "53/70 [=====================>........] - ETA: 0s - loss: 0.2712\n",
      "Epoch 00013: val_loss improved from 0.11604 to 0.11380, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2694 - val_loss: 0.1138\n",
      "Epoch 14/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2659\n",
      "Epoch 00014: val_loss improved from 0.11380 to 0.11231, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2667 - val_loss: 0.1123\n",
      "Epoch 15/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2662\n",
      "Epoch 00015: val_loss improved from 0.11231 to 0.11092, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2658 - val_loss: 0.1109\n",
      "Epoch 16/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2628\n",
      "Epoch 00016: val_loss improved from 0.11092 to 0.10972, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2586 - val_loss: 0.1097\n",
      "Epoch 17/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2559\n",
      "Epoch 00017: val_loss did not improve from 0.10972\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2568 - val_loss: 0.1102\n",
      "Epoch 18/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2509\n",
      "Epoch 00018: val_loss improved from 0.10972 to 0.10828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2516 - val_loss: 0.1083\n",
      "Epoch 19/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2525\n",
      "Epoch 00019: val_loss improved from 0.10828 to 0.10725, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2507 - val_loss: 0.1072\n",
      "Epoch 20/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2515\n",
      "Epoch 00020: val_loss improved from 0.10725 to 0.10639, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2499 - val_loss: 0.1064\n",
      "Epoch 21/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2444\n",
      "Epoch 00021: val_loss improved from 0.10639 to 0.10551, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2451 - val_loss: 0.1055\n",
      "Epoch 22/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2399\n",
      "Epoch 00022: val_loss improved from 0.10551 to 0.10498, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2393 - val_loss: 0.1050\n",
      "Epoch 23/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2390\n",
      "Epoch 00023: val_loss improved from 0.10498 to 0.10423, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2362 - val_loss: 0.1042\n",
      "Epoch 24/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2356\n",
      "Epoch 00024: val_loss improved from 0.10423 to 0.10349, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2354 - val_loss: 0.1035\n",
      "Epoch 25/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2348\n",
      "Epoch 00025: val_loss improved from 0.10349 to 0.10281, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2348 - val_loss: 0.1028\n",
      "Epoch 26/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2293\n",
      "Epoch 00026: val_loss improved from 0.10281 to 0.10232, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2302 - val_loss: 0.1023\n",
      "Epoch 27/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2331\n",
      "Epoch 00027: val_loss improved from 0.10232 to 0.10197, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2315 - val_loss: 0.1020\n",
      "Epoch 28/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2278\n",
      "Epoch 00028: val_loss improved from 0.10197 to 0.10133, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2278 - val_loss: 0.1013\n",
      "Epoch 29/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2264\n",
      "Epoch 00029: val_loss improved from 0.10133 to 0.10056, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2272 - val_loss: 0.1006\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2251\n",
      "Epoch 00030: val_loss improved from 0.10056 to 0.10040, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2251 - val_loss: 0.1004\n",
      "Epoch 31/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2231\n",
      "Epoch 00031: val_loss improved from 0.10040 to 0.09965, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2234 - val_loss: 0.0996\n",
      "Epoch 32/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2224\n",
      "Epoch 00032: val_loss improved from 0.09965 to 0.09912, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2220 - val_loss: 0.0991\n",
      "Epoch 33/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2193\n",
      "Epoch 00033: val_loss did not improve from 0.09912\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2206 - val_loss: 0.0994\n",
      "Epoch 34/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2146\n",
      "Epoch 00034: val_loss improved from 0.09912 to 0.09860, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2145 - val_loss: 0.0986\n",
      "Epoch 35/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2116\n",
      "Epoch 00035: val_loss did not improve from 0.09860\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2114 - val_loss: 0.0992\n",
      "Epoch 36/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2118\n",
      "Epoch 00036: val_loss improved from 0.09860 to 0.09839, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2107 - val_loss: 0.0984\n",
      "Epoch 37/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2178\n",
      "Epoch 00037: val_loss improved from 0.09839 to 0.09810, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2150 - val_loss: 0.0981\n",
      "Epoch 38/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2127\n",
      "Epoch 00038: val_loss improved from 0.09810 to 0.09768, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2138 - val_loss: 0.0977\n",
      "Epoch 39/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2101\n",
      "Epoch 00039: val_loss did not improve from 0.09768\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2110 - val_loss: 0.0977\n",
      "Epoch 40/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2086\n",
      "Epoch 00040: val_loss did not improve from 0.09768\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2102 - val_loss: 0.0979\n",
      " ###6 fold : val acc1 0.605, acc3 0.984, mae 0.205###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/70 [=========================>....] - ETA: 0s - loss: 19.5111\n",
      "Epoch 00001: val_loss improved from inf to 12.52545, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 18.7815 - val_loss: 12.5254\n",
      "Epoch 2/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 8.8088\n",
      "Epoch 00002: val_loss improved from 12.52545 to 4.88398, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 8.4499 - val_loss: 4.8840\n",
      "Epoch 3/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 3.6859\n",
      "Epoch 00003: val_loss improved from 4.88398 to 2.38093, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.5696 - val_loss: 2.3809\n",
      "Epoch 4/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 2.0764\n",
      "Epoch 00004: val_loss improved from 2.38093 to 1.27005, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9749 - val_loss: 1.2700\n",
      "Epoch 5/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1120\n",
      "Epoch 00005: val_loss improved from 1.27005 to 0.63629, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1100 - val_loss: 0.6363\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6304\n",
      "Epoch 00006: val_loss improved from 0.63629 to 0.33357, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6304 - val_loss: 0.3336\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4252\n",
      "Epoch 00007: val_loss improved from 0.33357 to 0.20171, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4252 - val_loss: 0.2017\n",
      "Epoch 8/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3298\n",
      "Epoch 00008: val_loss improved from 0.20171 to 0.14890, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3294 - val_loss: 0.1489\n",
      "Epoch 9/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2983\n",
      "Epoch 00009: val_loss improved from 0.14890 to 0.12969, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2989 - val_loss: 0.1297\n",
      "Epoch 10/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2848\n",
      "Epoch 00010: val_loss improved from 0.12969 to 0.12193, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2842 - val_loss: 0.1219\n",
      "Epoch 11/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.2782\n",
      "Epoch 00011: val_loss improved from 0.12193 to 0.11785, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2774 - val_loss: 0.1179\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2787\n",
      "Epoch 00012: val_loss improved from 0.11785 to 0.11582, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2787 - val_loss: 0.1158\n",
      "Epoch 13/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2705\n",
      "Epoch 00013: val_loss improved from 0.11582 to 0.11386, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2675 - val_loss: 0.1139\n",
      "Epoch 14/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2638\n",
      "Epoch 00014: val_loss improved from 0.11386 to 0.11254, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2636 - val_loss: 0.1125\n",
      "Epoch 15/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2629\n",
      "Epoch 00015: val_loss improved from 0.11254 to 0.11111, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2636 - val_loss: 0.1111\n",
      "Epoch 16/100\n",
      "53/70 [=====================>........] - ETA: 0s - loss: 0.2568\n",
      "Epoch 00016: val_loss improved from 0.11111 to 0.10990, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2551 - val_loss: 0.1099\n",
      "Epoch 17/100\n",
      "53/70 [=====================>........] - ETA: 0s - loss: 0.2536\n",
      "Epoch 00017: val_loss improved from 0.10990 to 0.10984, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2546 - val_loss: 0.1098\n",
      "Epoch 18/100\n",
      "53/70 [=====================>........] - ETA: 0s - loss: 0.2490\n",
      "Epoch 00018: val_loss improved from 0.10984 to 0.10847, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2494 - val_loss: 0.1085\n",
      "Epoch 19/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2502\n",
      "Epoch 00019: val_loss improved from 0.10847 to 0.10763, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2476 - val_loss: 0.1076\n",
      "Epoch 20/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2495\n",
      "Epoch 00020: val_loss improved from 0.10763 to 0.10684, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2496 - val_loss: 0.1068\n",
      "Epoch 21/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2422\n",
      "Epoch 00021: val_loss improved from 0.10684 to 0.10565, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2423 - val_loss: 0.1056\n",
      "Epoch 22/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2379\n",
      "Epoch 00022: val_loss improved from 0.10565 to 0.10506, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2372 - val_loss: 0.1051\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2330\n",
      "Epoch 00023: val_loss improved from 0.10506 to 0.10448, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2330 - val_loss: 0.1045\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2315\n",
      "Epoch 00024: val_loss improved from 0.10448 to 0.10373, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2315 - val_loss: 0.1037\n",
      "Epoch 25/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2347\n",
      "Epoch 00025: val_loss improved from 0.10373 to 0.10319, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2344 - val_loss: 0.1032\n",
      "Epoch 26/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2273\n",
      "Epoch 00026: val_loss improved from 0.10319 to 0.10274, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2286 - val_loss: 0.1027\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2286\n",
      "Epoch 00027: val_loss improved from 0.10274 to 0.10238, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2286 - val_loss: 0.1024\n",
      "Epoch 28/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2252\n",
      "Epoch 00028: val_loss improved from 0.10238 to 0.10175, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2253 - val_loss: 0.1017\n",
      "Epoch 29/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2249\n",
      "Epoch 00029: val_loss improved from 0.10175 to 0.10105, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2247 - val_loss: 0.1010\n",
      "Epoch 30/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2227\n",
      "Epoch 00030: val_loss improved from 0.10105 to 0.10079, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2228 - val_loss: 0.1008\n",
      "Epoch 31/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2207\n",
      "Epoch 00031: val_loss improved from 0.10079 to 0.10031, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2205 - val_loss: 0.1003\n",
      "Epoch 32/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2197\n",
      "Epoch 00032: val_loss improved from 0.10031 to 0.09961, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2191 - val_loss: 0.0996\n",
      "Epoch 33/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2185\n",
      "Epoch 00033: val_loss did not improve from 0.09961\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2189 - val_loss: 0.0999\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2135\n",
      "Epoch 00034: val_loss improved from 0.09961 to 0.09935, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2135 - val_loss: 0.0993\n",
      "Epoch 35/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2107\n",
      "Epoch 00035: val_loss did not improve from 0.09935\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2104 - val_loss: 0.0998\n",
      "Epoch 36/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2096\n",
      "Epoch 00036: val_loss improved from 0.09935 to 0.09895, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2095 - val_loss: 0.0990\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2106\n",
      "Epoch 00037: val_loss improved from 0.09895 to 0.09817, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2106 - val_loss: 0.0982\n",
      "Epoch 38/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2119\n",
      "Epoch 00038: val_loss improved from 0.09817 to 0.09795, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2117 - val_loss: 0.0980\n",
      "Epoch 39/100\n",
      "53/70 [=====================>........] - ETA: 0s - loss: 0.2087\n",
      "Epoch 00039: val_loss did not improve from 0.09795\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2095 - val_loss: 0.0980\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2091\n",
      "Epoch 00040: val_loss improved from 0.09795 to 0.09772, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2091 - val_loss: 0.0977\n",
      "Epoch 41/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2038\n",
      "Epoch 00041: val_loss improved from 0.09772 to 0.09745, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2035 - val_loss: 0.0975\n",
      "Epoch 42/100\n",
      "53/70 [=====================>........] - ETA: 0s - loss: 0.2056\n",
      "Epoch 00042: val_loss improved from 0.09745 to 0.09740, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2056 - val_loss: 0.0974\n",
      "Epoch 43/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2033\n",
      "Epoch 00043: val_loss improved from 0.09740 to 0.09710, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2038 - val_loss: 0.0971\n",
      "Epoch 44/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2034\n",
      "Epoch 00044: val_loss improved from 0.09710 to 0.09693, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2040 - val_loss: 0.0969\n",
      "Epoch 45/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2001\n",
      "Epoch 00045: val_loss did not improve from 0.09693\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1991 - val_loss: 0.0979\n",
      "Epoch 46/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1992\n",
      "Epoch 00046: val_loss improved from 0.09693 to 0.09679, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1990 - val_loss: 0.0968\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00047: val_loss did not improve from 0.09679\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1998 - val_loss: 0.0980\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2005\n",
      "Epoch 00048: val_loss did not improve from 0.09679\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2005 - val_loss: 0.0968\n",
      " ###7 fold : val acc1 0.595, acc3 0.972, mae 0.217###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/70 [===========================>..] - ETA: 0s - loss: 19.0390\n",
      "Epoch 00001: val_loss improved from inf to 12.52198, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 18.8158 - val_loss: 12.5220\n",
      "Epoch 2/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 8.5430\n",
      "Epoch 00002: val_loss improved from 12.52198 to 4.87701, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 8.4697 - val_loss: 4.8770\n",
      "Epoch 3/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 3.6652\n",
      "Epoch 00003: val_loss improved from 4.87701 to 2.37558, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 3.5723 - val_loss: 2.3756\n",
      "Epoch 4/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 2.0116\n",
      "Epoch 00004: val_loss improved from 2.37558 to 1.27564, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9775 - val_loss: 1.2756\n",
      "Epoch 5/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 1.1473\n",
      "Epoch 00005: val_loss improved from 1.27564 to 0.64481, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1071 - val_loss: 0.6448\n",
      "Epoch 6/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6488\n",
      "Epoch 00006: val_loss improved from 0.64481 to 0.34163, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6293 - val_loss: 0.3416\n",
      "Epoch 7/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.4319\n",
      "Epoch 00007: val_loss improved from 0.34163 to 0.20798, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4252 - val_loss: 0.2080\n",
      "Epoch 8/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.3335\n",
      "Epoch 00008: val_loss improved from 0.20798 to 0.15380, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3284 - val_loss: 0.1538\n",
      "Epoch 9/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.3007\n",
      "Epoch 00009: val_loss improved from 0.15380 to 0.13453, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2989 - val_loss: 0.1345\n",
      "Epoch 10/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.2851\n",
      "Epoch 00010: val_loss improved from 0.13453 to 0.12629, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2846 - val_loss: 0.1263\n",
      "Epoch 11/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2801\n",
      "Epoch 00011: val_loss improved from 0.12629 to 0.12205, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2779 - val_loss: 0.1220\n",
      "Epoch 12/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.2785\n",
      "Epoch 00012: val_loss improved from 0.12205 to 0.11980, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2783 - val_loss: 0.1198\n",
      "Epoch 13/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.2696\n",
      "Epoch 00013: val_loss improved from 0.11980 to 0.11779, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2668 - val_loss: 0.1178\n",
      "Epoch 14/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2633\n",
      "Epoch 00014: val_loss improved from 0.11779 to 0.11641, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2635 - val_loss: 0.1164\n",
      "Epoch 15/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.2648\n",
      "Epoch 00015: val_loss improved from 0.11641 to 0.11486, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2642 - val_loss: 0.1149\n",
      "Epoch 16/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.2556\n",
      "Epoch 00016: val_loss improved from 0.11486 to 0.11364, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2553 - val_loss: 0.1136\n",
      "Epoch 17/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2540\n",
      "Epoch 00017: val_loss did not improve from 0.11364\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.2547 - val_loss: 0.1137\n",
      "Epoch 18/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.2512\n",
      "Epoch 00018: val_loss improved from 0.11364 to 0.11222, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2504 - val_loss: 0.1122\n",
      "Epoch 19/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.2484\n",
      "Epoch 00019: val_loss improved from 0.11222 to 0.11129, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2467 - val_loss: 0.1113\n",
      "Epoch 20/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.2504\n",
      "Epoch 00020: val_loss improved from 0.11129 to 0.11057, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2497 - val_loss: 0.1106\n",
      "Epoch 21/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2432\n",
      "Epoch 00021: val_loss improved from 0.11057 to 0.10907, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2417 - val_loss: 0.1091\n",
      "Epoch 22/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.2389\n",
      "Epoch 00022: val_loss improved from 0.10907 to 0.10846, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2377 - val_loss: 0.1085\n",
      "Epoch 23/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.2347\n",
      "Epoch 00023: val_loss improved from 0.10846 to 0.10788, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2331 - val_loss: 0.1079\n",
      "Epoch 24/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2322\n",
      "Epoch 00024: val_loss improved from 0.10788 to 0.10712, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.2319 - val_loss: 0.1071\n",
      "Epoch 25/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.2352\n",
      "Epoch 00025: val_loss improved from 0.10712 to 0.10654, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2348 - val_loss: 0.1065\n",
      "Epoch 26/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.2269\n",
      "Epoch 00026: val_loss improved from 0.10654 to 0.10601, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2278 - val_loss: 0.1060\n",
      "Epoch 27/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.2289\n",
      "Epoch 00027: val_loss improved from 0.10601 to 0.10562, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2280 - val_loss: 0.1056\n",
      "Epoch 28/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.2254\n",
      "Epoch 00028: val_loss improved from 0.10562 to 0.10525, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2249 - val_loss: 0.1052\n",
      "Epoch 29/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.2255\n",
      "Epoch 00029: val_loss improved from 0.10525 to 0.10416, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2248 - val_loss: 0.1042\n",
      "Epoch 30/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2218\n",
      "Epoch 00030: val_loss improved from 0.10416 to 0.10408, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2227 - val_loss: 0.1041\n",
      "Epoch 31/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2215\n",
      "Epoch 00031: val_loss improved from 0.10408 to 0.10361, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2216 - val_loss: 0.1036\n",
      "Epoch 32/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2178\n",
      "Epoch 00032: val_loss improved from 0.10361 to 0.10288, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2188 - val_loss: 0.1029\n",
      "Epoch 33/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.2179\n",
      "Epoch 00033: val_loss did not improve from 0.10288\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.2188 - val_loss: 0.1035\n",
      "Epoch 34/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2128\n",
      "Epoch 00034: val_loss improved from 0.10288 to 0.10262, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2136 - val_loss: 0.1026\n",
      "Epoch 35/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.2103\n",
      "Epoch 00035: val_loss did not improve from 0.10262\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.2101 - val_loss: 0.1030\n",
      "Epoch 36/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2102\n",
      "Epoch 00036: val_loss improved from 0.10262 to 0.10212, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2095 - val_loss: 0.1021\n",
      "Epoch 37/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2138\n",
      "Epoch 00037: val_loss improved from 0.10212 to 0.10144, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2110 - val_loss: 0.1014\n",
      "Epoch 38/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.2109\n",
      "Epoch 00038: val_loss improved from 0.10144 to 0.10130, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2120 - val_loss: 0.1013\n",
      "Epoch 39/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2093\n",
      "Epoch 00039: val_loss did not improve from 0.10130\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.2095 - val_loss: 0.1014\n",
      "Epoch 40/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.2079\n",
      "Epoch 00040: val_loss improved from 0.10130 to 0.10090, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2091 - val_loss: 0.1009\n",
      "Epoch 41/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.2053\n",
      "Epoch 00041: val_loss improved from 0.10090 to 0.10086, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2033 - val_loss: 0.1009\n",
      "Epoch 42/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2071\n",
      "Epoch 00042: val_loss improved from 0.10086 to 0.10064, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2062 - val_loss: 0.1006\n",
      "Epoch 43/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.2041\n",
      "Epoch 00043: val_loss improved from 0.10064 to 0.10045, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2037 - val_loss: 0.1005\n",
      "Epoch 44/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.2038\n",
      "Epoch 00044: val_loss improved from 0.10045 to 0.10029, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2042 - val_loss: 0.1003\n",
      "Epoch 45/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.1987\n",
      "Epoch 00045: val_loss did not improve from 0.10029\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1987 - val_loss: 0.1007\n",
      "Epoch 46/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.1985\n",
      "Epoch 00046: val_loss improved from 0.10029 to 0.10023, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1987 - val_loss: 0.1002\n",
      "Epoch 47/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.2011\n",
      "Epoch 00047: val_loss did not improve from 0.10023\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2005 - val_loss: 0.1012\n",
      "Epoch 48/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.1999\n",
      "Epoch 00048: val_loss improved from 0.10023 to 0.10012, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2004 - val_loss: 0.1001\n",
      "Epoch 49/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.1988\n",
      "Epoch 00049: val_loss improved from 0.10012 to 0.09981, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1984 - val_loss: 0.0998\n",
      "Epoch 50/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.1969\n",
      "Epoch 00050: val_loss improved from 0.09981 to 0.09979, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1951 - val_loss: 0.0998\n",
      "Epoch 51/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.1969\n",
      "Epoch 00051: val_loss improved from 0.09979 to 0.09923, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1956 - val_loss: 0.0992\n",
      "Epoch 52/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.1931\n",
      "Epoch 00052: val_loss did not improve from 0.09923\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1931 - val_loss: 0.0997\n",
      "Epoch 53/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.1949\n",
      "Epoch 00053: val_loss did not improve from 0.09923\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.1951 - val_loss: 0.0996\n",
      " ###8 fold : val acc1 0.596, acc3 0.980, mae 0.212###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/70 [==========================>...] - ETA: 0s - loss: 19.2792\n",
      "Epoch 00001: val_loss improved from inf to 12.45770, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 18.8158 - val_loss: 12.4577\n",
      "Epoch 2/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 8.7331\n",
      "Epoch 00002: val_loss improved from 12.45770 to 4.87242, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 8.4697 - val_loss: 4.8724\n",
      "Epoch 3/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 3.7047\n",
      "Epoch 00003: val_loss improved from 4.87242 to 2.38042, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.5723 - val_loss: 2.3804\n",
      "Epoch 4/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 2.0757\n",
      "Epoch 00004: val_loss improved from 2.38042 to 1.27745, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9775 - val_loss: 1.2774\n",
      "Epoch 5/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 1.1655\n",
      "Epoch 00005: val_loss improved from 1.27745 to 0.64754, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1071 - val_loss: 0.6475\n",
      "Epoch 6/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.6613\n",
      "Epoch 00006: val_loss improved from 0.64754 to 0.34464, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6293 - val_loss: 0.3446\n",
      "Epoch 7/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.4375\n",
      "Epoch 00007: val_loss improved from 0.34464 to 0.21077, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4252 - val_loss: 0.2108\n",
      "Epoch 8/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3288\n",
      "Epoch 00008: val_loss improved from 0.21077 to 0.15669, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3284 - val_loss: 0.1567\n",
      "Epoch 9/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.3017\n",
      "Epoch 00009: val_loss improved from 0.15669 to 0.13674, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2989 - val_loss: 0.1367\n",
      "Epoch 10/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2842\n",
      "Epoch 00010: val_loss improved from 0.13674 to 0.12896, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2846 - val_loss: 0.1290\n",
      "Epoch 11/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2809\n",
      "Epoch 00011: val_loss improved from 0.12896 to 0.12497, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2779 - val_loss: 0.1250\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2783\n",
      "Epoch 00012: val_loss improved from 0.12497 to 0.12284, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2783 - val_loss: 0.1228\n",
      "Epoch 13/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2707\n",
      "Epoch 00013: val_loss improved from 0.12284 to 0.12056, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2668 - val_loss: 0.1206\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2635\n",
      "Epoch 00014: val_loss improved from 0.12056 to 0.11905, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2635 - val_loss: 0.1190\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2642\n",
      "Epoch 00015: val_loss improved from 0.11905 to 0.11768, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2642 - val_loss: 0.1177\n",
      "Epoch 16/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2553\n",
      "Epoch 00016: val_loss improved from 0.11768 to 0.11637, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2553 - val_loss: 0.1164\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2547\n",
      "Epoch 00017: val_loss did not improve from 0.11637\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2547 - val_loss: 0.1168\n",
      "Epoch 18/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2501\n",
      "Epoch 00018: val_loss improved from 0.11637 to 0.11514, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2504 - val_loss: 0.1151\n",
      "Epoch 19/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2468\n",
      "Epoch 00019: val_loss improved from 0.11514 to 0.11434, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2467 - val_loss: 0.1143\n",
      "Epoch 20/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2525\n",
      "Epoch 00020: val_loss improved from 0.11434 to 0.11328, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2497 - val_loss: 0.1133\n",
      "Epoch 21/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2417\n",
      "Epoch 00021: val_loss improved from 0.11328 to 0.11175, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2417 - val_loss: 0.1117\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2377\n",
      "Epoch 00022: val_loss improved from 0.11175 to 0.11116, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2377 - val_loss: 0.1112\n",
      "Epoch 23/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2343\n",
      "Epoch 00023: val_loss improved from 0.11116 to 0.11063, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2331 - val_loss: 0.1106\n",
      "Epoch 24/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2318\n",
      "Epoch 00024: val_loss improved from 0.11063 to 0.10978, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2319 - val_loss: 0.1098\n",
      "Epoch 25/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2355\n",
      "Epoch 00025: val_loss improved from 0.10978 to 0.10925, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2348 - val_loss: 0.1093\n",
      "Epoch 26/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2269\n",
      "Epoch 00026: val_loss improved from 0.10925 to 0.10867, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2278 - val_loss: 0.1087\n",
      "Epoch 27/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.2294\n",
      "Epoch 00027: val_loss improved from 0.10867 to 0.10828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2280 - val_loss: 0.1083\n",
      "Epoch 28/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2249\n",
      "Epoch 00028: val_loss improved from 0.10828 to 0.10792, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2249 - val_loss: 0.1079\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2248\n",
      "Epoch 00029: val_loss improved from 0.10792 to 0.10694, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2248 - val_loss: 0.1069\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2227\n",
      "Epoch 00030: val_loss improved from 0.10694 to 0.10632, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2227 - val_loss: 0.1063\n",
      "Epoch 31/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2216\n",
      "Epoch 00031: val_loss improved from 0.10632 to 0.10627, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2216 - val_loss: 0.1063\n",
      "Epoch 32/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2185\n",
      "Epoch 00032: val_loss improved from 0.10627 to 0.10525, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2188 - val_loss: 0.1052\n",
      "Epoch 33/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2179\n",
      "Epoch 00033: val_loss did not improve from 0.10525\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2188 - val_loss: 0.1058\n",
      "Epoch 34/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2136\n",
      "Epoch 00034: val_loss improved from 0.10525 to 0.10488, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2136 - val_loss: 0.1049\n",
      "Epoch 35/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2103\n",
      "Epoch 00035: val_loss did not improve from 0.10488\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2101 - val_loss: 0.1053\n",
      "Epoch 36/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.2087\n",
      "Epoch 00036: val_loss improved from 0.10488 to 0.10469, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2095 - val_loss: 0.1047\n",
      "Epoch 37/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2124\n",
      "Epoch 00037: val_loss improved from 0.10469 to 0.10390, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2110 - val_loss: 0.1039\n",
      "Epoch 38/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.2102\n",
      "Epoch 00038: val_loss improved from 0.10390 to 0.10352, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2120 - val_loss: 0.1035\n",
      "Epoch 39/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.2096\n",
      "Epoch 00039: val_loss did not improve from 0.10352\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2095 - val_loss: 0.1039\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2091\n",
      "Epoch 00040: val_loss improved from 0.10352 to 0.10338, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2091 - val_loss: 0.1034\n",
      "Epoch 41/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 0.2061\n",
      "Epoch 00041: val_loss improved from 0.10338 to 0.10314, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2033 - val_loss: 0.1031\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00042: val_loss improved from 0.10314 to 0.10285, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2062 - val_loss: 0.1029\n",
      "Epoch 43/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2032\n",
      "Epoch 00043: val_loss improved from 0.10285 to 0.10272, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2037 - val_loss: 0.1027\n",
      "Epoch 44/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.2033\n",
      "Epoch 00044: val_loss improved from 0.10272 to 0.10244, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2042 - val_loss: 0.1024\n",
      "Epoch 45/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 0.1996\n",
      "Epoch 00045: val_loss did not improve from 0.10244\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1987 - val_loss: 0.1035\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1987\n",
      "Epoch 00046: val_loss improved from 0.10244 to 0.10238, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1987 - val_loss: 0.1024\n",
      "Epoch 47/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.2000\n",
      "Epoch 00047: val_loss did not improve from 0.10238\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2005 - val_loss: 0.1031\n",
      "Epoch 48/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.2009\n",
      "Epoch 00048: val_loss improved from 0.10238 to 0.10232, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2004 - val_loss: 0.1023\n",
      "Epoch 49/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.1991\n",
      "Epoch 00049: val_loss improved from 0.10232 to 0.10227, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1984 - val_loss: 0.1023\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1951\n",
      "Epoch 00050: val_loss improved from 0.10227 to 0.10226, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1951 - val_loss: 0.1023\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1956\n",
      "Epoch 00051: val_loss improved from 0.10226 to 0.10165, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes256_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1956 - val_loss: 0.1016\n",
      "Epoch 52/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.1926\n",
      "Epoch 00052: val_loss did not improve from 0.10165\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1931 - val_loss: 0.1022\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1951\n",
      "Epoch 00053: val_loss did not improve from 0.10165\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1951 - val_loss: 0.1020\n",
      " ###9 fold : val acc1 0.601, acc3 0.983, mae 0.208###\n",
      "acc10.599_acc30.979\n",
      "random search 45/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/140 [==========================>...] - ETA: 0s - loss: 7.7284\n",
      "Epoch 00001: val_loss improved from inf to 1.19107, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 7.3264 - val_loss: 1.1911\n",
      "Epoch 2/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.7817\n",
      "Epoch 00002: val_loss improved from 1.19107 to 0.15380, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7746 - val_loss: 0.1538\n",
      "Epoch 3/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.4468\n",
      "Epoch 00003: val_loss improved from 0.15380 to 0.12147, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4471 - val_loss: 0.1215\n",
      "Epoch 4/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.4035\n",
      "Epoch 00004: val_loss improved from 0.12147 to 0.11723, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4035 - val_loss: 0.1172\n",
      "Epoch 5/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.3808\n",
      "Epoch 00005: val_loss improved from 0.11723 to 0.11223, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3813 - val_loss: 0.1122\n",
      "Epoch 6/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3649\n",
      "Epoch 00006: val_loss improved from 0.11223 to 0.10982, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3651 - val_loss: 0.1098\n",
      "Epoch 7/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.3365\n",
      "Epoch 00007: val_loss improved from 0.10982 to 0.10888, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3368 - val_loss: 0.1089\n",
      "Epoch 8/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3196\n",
      "Epoch 00008: val_loss improved from 0.10888 to 0.10453, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3189 - val_loss: 0.1045\n",
      "Epoch 9/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.3128\n",
      "Epoch 00009: val_loss did not improve from 0.10453\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3126 - val_loss: 0.1055\n",
      "Epoch 10/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3029\n",
      "Epoch 00010: val_loss improved from 0.10453 to 0.10195, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3023 - val_loss: 0.1019\n",
      "Epoch 11/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3028\n",
      "Epoch 00011: val_loss improved from 0.10195 to 0.10051, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3026 - val_loss: 0.1005\n",
      "Epoch 12/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2883\n",
      "Epoch 00012: val_loss improved from 0.10051 to 0.10011, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2880 - val_loss: 0.1001\n",
      "Epoch 13/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.2841\n",
      "Epoch 00013: val_loss improved from 0.10011 to 0.09955, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2842 - val_loss: 0.0996\n",
      "Epoch 14/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2794\n",
      "Epoch 00014: val_loss did not improve from 0.09955\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2794 - val_loss: 0.1043\n",
      "Epoch 15/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2710\n",
      "Epoch 00015: val_loss improved from 0.09955 to 0.09876, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2719 - val_loss: 0.0988\n",
      "Epoch 16/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.2795\n",
      "Epoch 00016: val_loss improved from 0.09876 to 0.09872, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2785 - val_loss: 0.0987\n",
      "Epoch 17/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2729\n",
      "Epoch 00017: val_loss improved from 0.09872 to 0.09833, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2726 - val_loss: 0.0983\n",
      "Epoch 18/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2704\n",
      "Epoch 00018: val_loss did not improve from 0.09833\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2701 - val_loss: 0.0994\n",
      "Epoch 19/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2676\n",
      "Epoch 00019: val_loss did not improve from 0.09833\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2671 - val_loss: 0.0993\n",
      " ###0 fold : val acc1 0.606, acc3 0.976, mae 0.209###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/140 [===========================>..] - ETA: 0s - loss: 7.6732\n",
      "Epoch 00001: val_loss improved from inf to 1.18711, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 7.3185 - val_loss: 1.1871\n",
      "Epoch 2/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.7896\n",
      "Epoch 00002: val_loss improved from 1.18711 to 0.15296, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.5,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.7713 - val_loss: 0.1530\n",
      "Epoch 3/100\n",
      " 96/140 [===================>..........] - ETA: 0s - loss: 0.4527"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555/558 [============================>.] - ETA: 0s - loss: 0.3342\n",
      "Epoch 00007: val_loss improved from 0.10324 to 0.10165, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.3340 - val_loss: 0.1016\n",
      "Epoch 8/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.3037\n",
      "Epoch 00008: val_loss did not improve from 0.10165\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.3030 - val_loss: 0.1023\n",
      "Epoch 9/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.2840\n",
      "Epoch 00009: val_loss did not improve from 0.10165\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2840 - val_loss: 0.1085\n",
      " ###5 fold : val acc1 0.582, acc3 0.973, mae 0.223###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545/558 [============================>.] - ETA: 0s - loss: 3.2806\n",
      "Epoch 00001: val_loss improved from inf to 0.14535, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_6.hdf5\n",
      "558/558 [==============================] - 3s 4ms/step - loss: 3.2249 - val_loss: 0.1454\n",
      "Epoch 2/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.6557\n",
      "Epoch 00002: val_loss improved from 0.14535 to 0.11981, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.6557 - val_loss: 0.1198\n",
      "Epoch 3/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.5405\n",
      "Epoch 00003: val_loss improved from 0.11981 to 0.11558, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.5399 - val_loss: 0.1156\n",
      "Epoch 4/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.4813\n",
      "Epoch 00004: val_loss improved from 0.11558 to 0.11395, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.4807 - val_loss: 0.1139\n",
      "Epoch 5/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.4298\n",
      "Epoch 00005: val_loss improved from 0.11395 to 0.11137, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.4298 - val_loss: 0.1114\n",
      "Epoch 6/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.3860\n",
      "Epoch 00006: val_loss improved from 0.11137 to 0.10666, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.3860 - val_loss: 0.1067\n",
      "Epoch 7/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.3344\n",
      "Epoch 00007: val_loss improved from 0.10666 to 0.10478, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.3339 - val_loss: 0.1048\n",
      "Epoch 8/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.3018\n",
      "Epoch 00008: val_loss did not improve from 0.10478\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.3013 - val_loss: 0.1048\n",
      "Epoch 9/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.2830\n",
      "Epoch 00009: val_loss improved from 0.10478 to 0.10360, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2821 - val_loss: 0.1036\n",
      "Epoch 10/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.2638\n",
      "Epoch 00010: val_loss improved from 0.10360 to 0.09743, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2635 - val_loss: 0.0974\n",
      "Epoch 11/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.2523\n",
      "Epoch 00011: val_loss did not improve from 0.09743\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.2525 - val_loss: 0.0993\n",
      "Epoch 12/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.2409\n",
      "Epoch 00012: val_loss did not improve from 0.09743\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2407 - val_loss: 0.1003\n",
      " ###6 fold : val acc1 0.583, acc3 0.984, mae 0.216###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548/558 [============================>.] - ETA: 0s - loss: 3.2665\n",
      "Epoch 00001: val_loss improved from inf to 0.14079, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "558/558 [==============================] - 3s 4ms/step - loss: 3.2244 - val_loss: 0.1408\n",
      "Epoch 2/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.6558\n",
      "Epoch 00002: val_loss improved from 0.14079 to 0.12081, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.6558 - val_loss: 0.1208\n",
      "Epoch 3/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.5369\n",
      "Epoch 00003: val_loss improved from 0.12081 to 0.11312, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.5363 - val_loss: 0.1131\n",
      "Epoch 4/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.4763\n",
      "Epoch 00004: val_loss improved from 0.11312 to 0.11224, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.4763 - val_loss: 0.1122\n",
      "Epoch 5/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.4291\n",
      "Epoch 00005: val_loss did not improve from 0.11224\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.4272 - val_loss: 0.1144\n",
      "Epoch 6/100\n",
      "554/558 [============================>.] - ETA: 0s - loss: 0.3799\n",
      "Epoch 00006: val_loss improved from 0.11224 to 0.10474, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.3800 - val_loss: 0.1047\n",
      "Epoch 7/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.3334\n",
      "Epoch 00007: val_loss did not improve from 0.10474\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.3325 - val_loss: 0.1063\n",
      "Epoch 8/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.3015\n",
      "Epoch 00008: val_loss did not improve from 0.10474\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.3008 - val_loss: 0.1060\n",
      " ###7 fold : val acc1 0.567, acc3 0.965, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556/558 [============================>.] - ETA: 0s - loss: 3.2362\n",
      "Epoch 00001: val_loss improved from inf to 0.14572, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "558/558 [==============================] - 3s 4ms/step - loss: 3.2307 - val_loss: 0.1457\n",
      "Epoch 2/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.6522\n",
      "Epoch 00002: val_loss improved from 0.14572 to 0.12616, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.6522 - val_loss: 0.1262\n",
      "Epoch 3/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.5366\n",
      "Epoch 00003: val_loss improved from 0.12616 to 0.11867, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.5359 - val_loss: 0.1187\n",
      "Epoch 4/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.4764\n",
      "Epoch 00004: val_loss improved from 0.11867 to 0.11644, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.4766 - val_loss: 0.1164\n",
      "Epoch 5/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.4289\n",
      "Epoch 00005: val_loss did not improve from 0.11644\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.4272 - val_loss: 0.1186\n",
      "Epoch 6/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.3797\n",
      "Epoch 00006: val_loss improved from 0.11644 to 0.10826, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.3803 - val_loss: 0.1083\n",
      "Epoch 7/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.3333\n",
      "Epoch 00007: val_loss improved from 0.10826 to 0.10809, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.3333 - val_loss: 0.1081\n",
      "Epoch 8/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.3010\n",
      "Epoch 00008: val_loss did not improve from 0.10809\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.3010 - val_loss: 0.1111\n",
      "Epoch 9/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.2805\n",
      "Epoch 00009: val_loss improved from 0.10809 to 0.10532, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2797 - val_loss: 0.1053\n",
      "Epoch 10/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.2647\n",
      "Epoch 00010: val_loss improved from 0.10532 to 0.10094, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2643 - val_loss: 0.1009\n",
      "Epoch 11/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.2527\n",
      "Epoch 00011: val_loss improved from 0.10094 to 0.10085, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2520 - val_loss: 0.1008\n",
      "Epoch 12/100\n",
      "554/558 [============================>.] - ETA: 0s - loss: 0.2402\n",
      "Epoch 00012: val_loss did not improve from 0.10085\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2401 - val_loss: 0.1034\n",
      "Epoch 13/100\n",
      "554/558 [============================>.] - ETA: 0s - loss: 0.2376\n",
      "Epoch 00013: val_loss did not improve from 0.10085\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.2373 - val_loss: 0.1022\n",
      " ###8 fold : val acc1 0.586, acc3 0.982, mae 0.216###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549/558 [============================>.] - ETA: 0s - loss: 3.2683\n",
      "Epoch 00001: val_loss improved from inf to 0.14857, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "558/558 [==============================] - 3s 4ms/step - loss: 3.2307 - val_loss: 0.1486\n",
      "Epoch 2/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.6537\n",
      "Epoch 00002: val_loss improved from 0.14857 to 0.12913, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.6522 - val_loss: 0.1291\n",
      "Epoch 3/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.5365\n",
      "Epoch 00003: val_loss improved from 0.12913 to 0.11956, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.5359 - val_loss: 0.1196\n",
      "Epoch 4/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.4772\n",
      "Epoch 00004: val_loss improved from 0.11956 to 0.11874, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.4766 - val_loss: 0.1187\n",
      "Epoch 5/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.4272\n",
      "Epoch 00005: val_loss did not improve from 0.11874\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.4272 - val_loss: 0.1209\n",
      "Epoch 6/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.3800\n",
      "Epoch 00006: val_loss improved from 0.11874 to 0.11003, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes128_dropout0.5,dnodes32_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.3803 - val_loss: 0.1100\n",
      "Epoch 7/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.3337\n",
      "Epoch 00007: val_loss did not improve from 0.11003\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.3333 - val_loss: 0.1113\n",
      "Epoch 8/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.3011\n",
      "Epoch 00008: val_loss did not improve from 0.11003\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 0.3010 - val_loss: 0.1144\n",
      " ###9 fold : val acc1 0.575, acc3 0.979, mae 0.223###\n",
      "acc10.583_acc30.975\n",
      "random search 50/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/279 [============================>.] - ETA: 0s - loss: 2.3456\n",
      "Epoch 00001: val_loss improved from inf to 0.12173, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.3335 - val_loss: 0.1217\n",
      "Epoch 2/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.2349\n",
      "Epoch 00002: val_loss improved from 0.12173 to 0.10427, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2349 - val_loss: 0.1043\n",
      "Epoch 3/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.2104\n",
      "Epoch 00003: val_loss improved from 0.10427 to 0.10351, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2100 - val_loss: 0.1035\n",
      "Epoch 4/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1976\n",
      "Epoch 00004: val_loss improved from 0.10351 to 0.09902, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1974 - val_loss: 0.0990\n",
      "Epoch 5/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1891\n",
      "Epoch 00005: val_loss did not improve from 0.09902\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1889 - val_loss: 0.1015\n",
      "Epoch 6/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00006: val_loss did not improve from 0.09902\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1873 - val_loss: 0.1042\n",
      " ###0 fold : val acc1 0.600, acc3 0.978, mae 0.212###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/279 [============================>.] - ETA: 0s - loss: 2.4162\n",
      "Epoch 00001: val_loss improved from inf to 0.12099, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.3494 - val_loss: 0.1210\n",
      "Epoch 2/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.2343\n",
      "Epoch 00002: val_loss improved from 0.12099 to 0.10392, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2339 - val_loss: 0.1039\n",
      "Epoch 3/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.2088\n",
      "Epoch 00003: val_loss improved from 0.10392 to 0.10366, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2089 - val_loss: 0.1037\n",
      "Epoch 4/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1967\n",
      "Epoch 00004: val_loss improved from 0.10366 to 0.09913, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1963 - val_loss: 0.0991\n",
      "Epoch 5/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.1889\n",
      "Epoch 00005: val_loss did not improve from 0.09913\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1885 - val_loss: 0.1033\n",
      "Epoch 6/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1875\n",
      "Epoch 00006: val_loss did not improve from 0.09913\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1874 - val_loss: 0.1016\n",
      " ###1 fold : val acc1 0.587, acc3 0.983, mae 0.215###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/279 [============================>.] - ETA: 0s - loss: 2.3752\n",
      "Epoch 00001: val_loss improved from inf to 0.11961, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.3477 - val_loss: 0.1196\n",
      "Epoch 2/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.2323\n",
      "Epoch 00002: val_loss improved from 0.11961 to 0.10397, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2322 - val_loss: 0.1040\n",
      "Epoch 3/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.2078\n",
      "Epoch 00003: val_loss did not improve from 0.10397\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2078 - val_loss: 0.1045\n",
      "Epoch 4/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1939\n",
      "Epoch 00004: val_loss improved from 0.10397 to 0.09858, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1937 - val_loss: 0.0986\n",
      "Epoch 5/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00005: val_loss did not improve from 0.09858\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1875 - val_loss: 0.1049\n",
      "Epoch 6/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00006: val_loss did not improve from 0.09858\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1874 - val_loss: 0.1026\n",
      " ###2 fold : val acc1 0.587, acc3 0.979, mae 0.217###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/279 [============================>.] - ETA: 0s - loss: 2.3673\n",
      "Epoch 00001: val_loss improved from inf to 0.11820, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.3474 - val_loss: 0.1182\n",
      "Epoch 2/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.2340\n",
      "Epoch 00002: val_loss improved from 0.11820 to 0.10496, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2332 - val_loss: 0.1050\n",
      "Epoch 3/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2077\n",
      "Epoch 00003: val_loss improved from 0.10496 to 0.10293, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2081 - val_loss: 0.1029\n",
      "Epoch 4/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1951\n",
      "Epoch 00004: val_loss improved from 0.10293 to 0.09897, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1948 - val_loss: 0.0990\n",
      "Epoch 5/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1890\n",
      "Epoch 00005: val_loss did not improve from 0.09897\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1890 - val_loss: 0.1042\n",
      "Epoch 6/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00006: val_loss did not improve from 0.09897\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1877 - val_loss: 0.1038\n",
      " ###3 fold : val acc1 0.590, acc3 0.975, mae 0.218###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/279 [===========================>..] - ETA: 0s - loss: 2.5010\n",
      "Epoch 00001: val_loss improved from inf to 0.11811, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 2.3577 - val_loss: 0.1181\n",
      "Epoch 2/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.2295\n",
      "Epoch 00002: val_loss improved from 0.11811 to 0.10706, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2285 - val_loss: 0.1071\n",
      "Epoch 3/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.2103\n",
      "Epoch 00003: val_loss improved from 0.10706 to 0.10124, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2101 - val_loss: 0.1012\n",
      "Epoch 4/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1979\n",
      "Epoch 00004: val_loss improved from 0.10124 to 0.10001, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1972 - val_loss: 0.1000\n",
      "Epoch 5/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00005: val_loss did not improve from 0.10001\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1876 - val_loss: 0.1072\n",
      "Epoch 6/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1789\n",
      "Epoch 00006: val_loss improved from 0.10001 to 0.09723, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1792 - val_loss: 0.0972\n",
      "Epoch 7/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1765\n",
      "Epoch 00007: val_loss did not improve from 0.09723\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1764 - val_loss: 0.1051\n",
      "Epoch 8/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1772\n",
      "Epoch 00008: val_loss improved from 0.09723 to 0.09642, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1774 - val_loss: 0.0964\n",
      "Epoch 9/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1704\n",
      "Epoch 00009: val_loss did not improve from 0.09642\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1707 - val_loss: 0.0974\n",
      "Epoch 10/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1698\n",
      "Epoch 00010: val_loss did not improve from 0.09642\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1692 - val_loss: 0.1005\n",
      " ###4 fold : val acc1 0.578, acc3 0.984, mae 0.219###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/279 [============================>.] - ETA: 0s - loss: 2.3627\n",
      "Epoch 00001: val_loss improved from inf to 0.11797, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.3508 - val_loss: 0.1180\n",
      "Epoch 2/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.2313\n",
      "Epoch 00002: val_loss improved from 0.11797 to 0.10719, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2302 - val_loss: 0.1072\n",
      "Epoch 3/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.2111\n",
      "Epoch 00003: val_loss improved from 0.10719 to 0.09993, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2107 - val_loss: 0.0999\n",
      "Epoch 4/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1970\n",
      "Epoch 00004: val_loss improved from 0.09993 to 0.09951, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1963 - val_loss: 0.0995\n",
      "Epoch 5/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00005: val_loss did not improve from 0.09951\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1875 - val_loss: 0.1076\n",
      "Epoch 6/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1802\n",
      "Epoch 00006: val_loss improved from 0.09951 to 0.09715, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1804 - val_loss: 0.0972\n",
      "Epoch 7/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1768\n",
      "Epoch 00007: val_loss did not improve from 0.09715\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1768 - val_loss: 0.1077\n",
      "Epoch 8/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.1778\n",
      "Epoch 00008: val_loss did not improve from 0.09715\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1778 - val_loss: 0.0995\n",
      " ###5 fold : val acc1 0.591, acc3 0.980, mae 0.215###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264/279 [===========================>..] - ETA: 0s - loss: 2.4627\n",
      "Epoch 00001: val_loss improved from inf to 0.11675, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 2.3461 - val_loss: 0.1167\n",
      "Epoch 2/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.2314\n",
      "Epoch 00002: val_loss improved from 0.11675 to 0.10642, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2313 - val_loss: 0.1064\n",
      "Epoch 3/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2123\n",
      "Epoch 00003: val_loss improved from 0.10642 to 0.09892, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2119 - val_loss: 0.0989\n",
      "Epoch 4/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1969\n",
      "Epoch 00004: val_loss improved from 0.09892 to 0.09838, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1970 - val_loss: 0.0984\n",
      "Epoch 5/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00005: val_loss did not improve from 0.09838\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1877 - val_loss: 0.1051\n",
      "Epoch 6/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.1829\n",
      "Epoch 00006: val_loss improved from 0.09838 to 0.09752, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1830 - val_loss: 0.0975\n",
      "Epoch 7/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1786\n",
      "Epoch 00007: val_loss did not improve from 0.09752\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1786 - val_loss: 0.1036\n",
      "Epoch 8/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1785\n",
      "Epoch 00008: val_loss did not improve from 0.09752\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1785 - val_loss: 0.1010\n",
      " ###6 fold : val acc1 0.606, acc3 0.982, mae 0.206###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - ETA: 0s - loss: 2.3316\n",
      "Epoch 00001: val_loss improved from inf to 0.11625, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.3316 - val_loss: 0.1163\n",
      "Epoch 2/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.2311\n",
      "Epoch 00002: val_loss improved from 0.11625 to 0.10515, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2307 - val_loss: 0.1051\n",
      "Epoch 3/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.2125\n",
      "Epoch 00003: val_loss improved from 0.10515 to 0.09959, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2125 - val_loss: 0.0996\n",
      "Epoch 4/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1966\n",
      "Epoch 00004: val_loss improved from 0.09959 to 0.09924, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1967 - val_loss: 0.0992\n",
      "Epoch 5/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1862\n",
      "Epoch 00005: val_loss did not improve from 0.09924\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1868 - val_loss: 0.1129\n",
      "Epoch 6/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1832\n",
      "Epoch 00006: val_loss improved from 0.09924 to 0.09726, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1835 - val_loss: 0.0973\n",
      "Epoch 7/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1780\n",
      "Epoch 00007: val_loss did not improve from 0.09726\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1780 - val_loss: 0.1014\n",
      "Epoch 8/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1781\n",
      "Epoch 00008: val_loss did not improve from 0.09726\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1781 - val_loss: 0.0975\n",
      " ###7 fold : val acc1 0.588, acc3 0.971, mae 0.221###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/279 [===========================>..] - ETA: 0s - loss: 2.4166\n",
      "Epoch 00001: val_loss improved from inf to 0.11991, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.3416 - val_loss: 0.1199\n",
      "Epoch 2/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.2307\n",
      "Epoch 00002: val_loss improved from 0.11991 to 0.10952, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2307 - val_loss: 0.1095\n",
      "Epoch 3/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.2124\n",
      "Epoch 00003: val_loss improved from 0.10952 to 0.10309, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2122 - val_loss: 0.1031\n",
      "Epoch 4/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1967\n",
      "Epoch 00004: val_loss did not improve from 0.10309\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1966 - val_loss: 0.1033\n",
      "Epoch 5/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1880\n",
      "Epoch 00005: val_loss did not improve from 0.10309\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1880 - val_loss: 0.1150\n",
      " ###8 fold : val acc1 0.589, acc3 0.980, mae 0.216###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/279 [============================>.] - ETA: 0s - loss: 2.4084\n",
      "Epoch 00001: val_loss improved from inf to 0.12264, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.3416 - val_loss: 0.1226\n",
      "Epoch 2/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.2321\n",
      "Epoch 00002: val_loss improved from 0.12264 to 0.11218, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2307 - val_loss: 0.1122\n",
      "Epoch 3/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.2125\n",
      "Epoch 00003: val_loss improved from 0.11218 to 0.10507, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2122 - val_loss: 0.1051\n",
      "Epoch 4/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1966\n",
      "Epoch 00004: val_loss improved from 0.10507 to 0.10427, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.3,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1966 - val_loss: 0.1043\n",
      "Epoch 5/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1875\n",
      "Epoch 00005: val_loss did not improve from 0.10427\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1881 - val_loss: 0.1169\n",
      "Epoch 6/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1825\n",
      "Epoch 00006: val_loss did not improve from 0.10427\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1829 - val_loss: 0.1050\n",
      " ###9 fold : val acc1 0.574, acc3 0.983, mae 0.222###\n",
      "acc10.589_acc30.979\n",
      "random search 51/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/140 [===========================>..] - ETA: 0s - loss: 20.9805\n",
      "Epoch 00001: val_loss improved from inf to 16.09772, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 20.7500 - val_loss: 16.0977\n",
      "Epoch 2/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 12.9743\n",
      "Epoch 00002: val_loss improved from 16.09772 to 9.45116, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 12.9743 - val_loss: 9.4512\n",
      "Epoch 3/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 8.2651\n",
      "Epoch 00003: val_loss improved from 9.45116 to 5.57543, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 8.0794 - val_loss: 5.5754\n",
      "Epoch 4/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 5.4868\n",
      "Epoch 00004: val_loss improved from 5.57543 to 3.45146, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 5.4367 - val_loss: 3.4515\n",
      "Epoch 5/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 4.0946\n",
      "Epoch 00005: val_loss improved from 3.45146 to 2.20354, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 4.0491 - val_loss: 2.2035\n",
      "Epoch 6/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 3.1700\n",
      "Epoch 00006: val_loss improved from 2.20354 to 1.42181, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 3.1386 - val_loss: 1.4218\n",
      "Epoch 7/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 2.5238\n",
      "Epoch 00007: val_loss improved from 1.42181 to 0.91950, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 2.4851 - val_loss: 0.9195\n",
      "Epoch 8/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 2.0433\n",
      "Epoch 00008: val_loss improved from 0.91950 to 0.57966, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 2.0250 - val_loss: 0.5797\n",
      "Epoch 9/100\n",
      " 92/140 [==================>...........] - ETA: 0s - loss: 1.7492"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556/558 [============================>.] - ETA: 0s - loss: 0.5116\n",
      "Epoch 00009: val_loss improved from 0.12192 to 0.11024, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0.3,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.5115 - val_loss: 0.1102\n",
      "Epoch 10/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.4470\n",
      "Epoch 00010: val_loss did not improve from 0.11024\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.4469 - val_loss: 0.1198\n",
      "Epoch 11/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.4017\n",
      "Epoch 00011: val_loss did not improve from 0.11024\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.4011 - val_loss: 0.1120\n",
      " ###9 fold : val acc1 0.586, acc3 0.977, mae 0.218###\n",
      "acc10.586_acc30.973\n",
      "random search 53/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/140 [===========================>..] - ETA: 0s - loss: 3.1257\n",
      "Epoch 00001: val_loss improved from inf to 0.17778, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 3.0022 - val_loss: 0.1778\n",
      "Epoch 2/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.8277\n",
      "Epoch 00002: val_loss improved from 0.17778 to 0.13248, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8274 - val_loss: 0.1325\n",
      "Epoch 3/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.7860\n",
      "Epoch 00003: val_loss improved from 0.13248 to 0.12040, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7850 - val_loss: 0.1204\n",
      "Epoch 4/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.7907\n",
      "Epoch 00004: val_loss did not improve from 0.12040\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7901 - val_loss: 0.1361\n",
      "Epoch 5/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.7502\n",
      "Epoch 00005: val_loss did not improve from 0.12040\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7504 - val_loss: 0.1500\n",
      " ###0 fold : val acc1 0.550, acc3 0.967, mae 0.243###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/140 [============================>.] - ETA: 0s - loss: 3.0699\n",
      "Epoch 00001: val_loss improved from inf to 0.17687, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.0166 - val_loss: 0.1769\n",
      "Epoch 2/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.8262\n",
      "Epoch 00002: val_loss improved from 0.17687 to 0.12658, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8258 - val_loss: 0.1266\n",
      "Epoch 3/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.7857\n",
      "Epoch 00003: val_loss improved from 0.12658 to 0.12333, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7854 - val_loss: 0.1233\n",
      "Epoch 4/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.7935\n",
      "Epoch 00004: val_loss did not improve from 0.12333\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7914 - val_loss: 0.1269\n",
      "Epoch 5/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.7532\n",
      "Epoch 00005: val_loss did not improve from 0.12333\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7524 - val_loss: 0.1564\n",
      " ###1 fold : val acc1 0.543, acc3 0.965, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/140 [==========================>...] - ETA: 0s - loss: 3.2335\n",
      "Epoch 00001: val_loss improved from inf to 0.18087, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.0180 - val_loss: 0.1809\n",
      "Epoch 2/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.8245\n",
      "Epoch 00002: val_loss improved from 0.18087 to 0.12548, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8241 - val_loss: 0.1255\n",
      "Epoch 3/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.7861\n",
      "Epoch 00003: val_loss improved from 0.12548 to 0.12149, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7871 - val_loss: 0.1215\n",
      "Epoch 4/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.7880\n",
      "Epoch 00004: val_loss did not improve from 0.12149\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7884 - val_loss: 0.1265\n",
      "Epoch 5/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.7505\n",
      "Epoch 00005: val_loss did not improve from 0.12149\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7518 - val_loss: 0.1662\n",
      " ###2 fold : val acc1 0.548, acc3 0.962, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/140 [============================>.] - ETA: 0s - loss: 3.0877\n",
      "Epoch 00001: val_loss improved from inf to 0.16396, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.0339 - val_loss: 0.1640\n",
      "Epoch 2/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.8319\n",
      "Epoch 00002: val_loss improved from 0.16396 to 0.12686, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8297 - val_loss: 0.1269\n",
      "Epoch 3/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.7914\n",
      "Epoch 00003: val_loss improved from 0.12686 to 0.12166, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7918 - val_loss: 0.1217\n",
      "Epoch 4/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.7849\n",
      "Epoch 00004: val_loss did not improve from 0.12166\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7864 - val_loss: 0.1219\n",
      "Epoch 5/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.7548\n",
      "Epoch 00005: val_loss did not improve from 0.12166\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7556 - val_loss: 0.1633\n",
      " ###3 fold : val acc1 0.548, acc3 0.965, mae 0.244###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/140 [==========================>...] - ETA: 0s - loss: 3.2521\n",
      "Epoch 00001: val_loss improved from inf to 0.14860, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.0324 - val_loss: 0.1486\n",
      "Epoch 2/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.8096\n",
      "Epoch 00002: val_loss improved from 0.14860 to 0.12504, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8119 - val_loss: 0.1250\n",
      "Epoch 3/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.7909\n",
      "Epoch 00003: val_loss did not improve from 0.12504\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7939 - val_loss: 0.1282\n",
      "Epoch 4/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.7601\n",
      "Epoch 00004: val_loss improved from 0.12504 to 0.11889, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7620 - val_loss: 0.1189\n",
      "Epoch 5/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.7530\n",
      "Epoch 00005: val_loss did not improve from 0.11889\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.7530 - val_loss: 0.2022\n",
      "Epoch 6/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.7149\n",
      "Epoch 00006: val_loss did not improve from 0.11889\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7142 - val_loss: 0.1460\n",
      " ###4 fold : val acc1 0.544, acc3 0.964, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - ETA: 0s - loss: 3.0336\n",
      "Epoch 00001: val_loss improved from inf to 0.14866, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.0336 - val_loss: 0.1487\n",
      "Epoch 2/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.8127\n",
      "Epoch 00002: val_loss improved from 0.14866 to 0.12456, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8124 - val_loss: 0.1246\n",
      "Epoch 3/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.7921\n",
      "Epoch 00003: val_loss did not improve from 0.12456\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7941 - val_loss: 0.1260\n",
      "Epoch 4/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.7661\n",
      "Epoch 00004: val_loss improved from 0.12456 to 0.11195, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7661 - val_loss: 0.1119\n",
      "Epoch 5/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.7564\n",
      "Epoch 00005: val_loss did not improve from 0.11195\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7567 - val_loss: 0.2205\n",
      "Epoch 6/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.7168\n",
      "Epoch 00006: val_loss did not improve from 0.11195\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7154 - val_loss: 0.1606\n",
      " ###5 fold : val acc1 0.556, acc3 0.973, mae 0.236###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/140 [============================>.] - ETA: 0s - loss: 3.0329\n",
      "Epoch 00001: val_loss improved from inf to 0.14962, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 3.0280 - val_loss: 0.1496\n",
      "Epoch 2/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.8118\n",
      "Epoch 00002: val_loss improved from 0.14962 to 0.11894, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8135 - val_loss: 0.1189\n",
      "Epoch 3/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.7931\n",
      "Epoch 00003: val_loss did not improve from 0.11894\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7943 - val_loss: 0.1296\n",
      "Epoch 4/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.7693\n",
      "Epoch 00004: val_loss improved from 0.11894 to 0.11704, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7676 - val_loss: 0.1170\n",
      "Epoch 5/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.7588\n",
      "Epoch 00005: val_loss did not improve from 0.11704\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7589 - val_loss: 0.2075\n",
      "Epoch 6/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.7193\n",
      "Epoch 00006: val_loss did not improve from 0.11704\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7193 - val_loss: 0.1552\n",
      " ###6 fold : val acc1 0.546, acc3 0.969, mae 0.243###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/140 [=========================>....] - ETA: 0s - loss: 3.2759\n",
      "Epoch 00001: val_loss improved from inf to 0.14828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 3.0233 - val_loss: 0.1483\n",
      "Epoch 2/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.8094\n",
      "Epoch 00002: val_loss improved from 0.14828 to 0.12189, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.8106 - val_loss: 0.1219\n",
      "Epoch 3/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.7939\n",
      "Epoch 00003: val_loss did not improve from 0.12189\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7951 - val_loss: 0.1351\n",
      "Epoch 4/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.7669\n",
      "Epoch 00004: val_loss did not improve from 0.12189\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7660 - val_loss: 0.1235\n",
      " ###7 fold : val acc1 0.535, acc3 0.952, mae 0.257###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/140 [==========================>...] - ETA: 0s - loss: 3.2534\n",
      "Epoch 00001: val_loss improved from inf to 0.15258, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.0364 - val_loss: 0.1526\n",
      "Epoch 2/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.8091\n",
      "Epoch 00002: val_loss improved from 0.15258 to 0.12644, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8116 - val_loss: 0.1264\n",
      "Epoch 3/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.7932\n",
      "Epoch 00003: val_loss did not improve from 0.12644\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7947 - val_loss: 0.1397\n",
      "Epoch 4/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.7656\n",
      "Epoch 00004: val_loss did not improve from 0.12644\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.7646 - val_loss: 0.1296\n",
      " ###8 fold : val acc1 0.542, acc3 0.958, mae 0.250###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/140 [============================>.] - ETA: 0s - loss: 3.0726\n",
      "Epoch 00001: val_loss improved from inf to 0.15535, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.0364 - val_loss: 0.1553\n",
      "Epoch 2/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.8110\n",
      "Epoch 00002: val_loss improved from 0.15535 to 0.12975, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0,dnodes16_dropout0.2,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8116 - val_loss: 0.1297\n",
      "Epoch 3/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.7938\n",
      "Epoch 00003: val_loss did not improve from 0.12975\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7947 - val_loss: 0.1419\n",
      "Epoch 4/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.7675\n",
      "Epoch 00004: val_loss did not improve from 0.12975\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.7646 - val_loss: 0.1329\n",
      " ###9 fold : val acc1 0.553, acc3 0.963, mae 0.243###\n",
      "acc10.546_acc30.964\n",
      "random search 54/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/279 [===========================>..] - ETA: 0s - loss: 3.5333\n",
      "Epoch 00001: val_loss improved from inf to 0.23968, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.4120 - val_loss: 0.2397\n",
      "Epoch 2/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.6262\n",
      "Epoch 00002: val_loss did not improve from 0.23968\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.6257 - val_loss: 0.2438\n",
      "Epoch 3/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.4049\n",
      "Epoch 00003: val_loss improved from 0.23968 to 0.16572, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4048 - val_loss: 0.1657\n",
      "Epoch 4/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.3584\n",
      "Epoch 00004: val_loss improved from 0.16572 to 0.15991, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3582 - val_loss: 0.1599\n",
      "Epoch 5/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.3294\n",
      "Epoch 00005: val_loss improved from 0.15991 to 0.13509, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3298 - val_loss: 0.1351\n",
      "Epoch 6/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.3004\n",
      "Epoch 00006: val_loss did not improve from 0.13509\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3004 - val_loss: 0.1524\n",
      "Epoch 7/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.2877\n",
      "Epoch 00007: val_loss improved from 0.13509 to 0.12254, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2866 - val_loss: 0.1225\n",
      "Epoch 8/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.2643\n",
      "Epoch 00008: val_loss did not improve from 0.12254\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2647 - val_loss: 0.1336\n",
      "Epoch 9/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.2649\n",
      "Epoch 00009: val_loss improved from 0.12254 to 0.12159, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2648 - val_loss: 0.1216\n",
      "Epoch 10/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2555\n",
      "Epoch 00010: val_loss did not improve from 0.12159\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2554 - val_loss: 0.1237\n",
      "Epoch 11/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.2410\n",
      "Epoch 00011: val_loss did not improve from 0.12159\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2429 - val_loss: 0.1477\n",
      " ###0 fold : val acc1 0.538, acc3 0.979, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/279 [============================>.] - ETA: 0s - loss: 3.4540\n",
      "Epoch 00001: val_loss improved from inf to 0.23355, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.4106 - val_loss: 0.2335\n",
      "Epoch 2/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.6336\n",
      "Epoch 00002: val_loss did not improve from 0.23355\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.6263 - val_loss: 0.2684\n",
      "Epoch 3/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.4045\n",
      "Epoch 00003: val_loss improved from 0.23355 to 0.15316, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4041 - val_loss: 0.1532\n",
      "Epoch 4/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.3567\n",
      "Epoch 00004: val_loss improved from 0.15316 to 0.14311, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3559 - val_loss: 0.1431\n",
      "Epoch 5/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.3271\n",
      "Epoch 00005: val_loss improved from 0.14311 to 0.13488, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3268 - val_loss: 0.1349\n",
      "Epoch 6/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2996\n",
      "Epoch 00006: val_loss did not improve from 0.13488\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2997 - val_loss: 0.1737\n",
      "Epoch 7/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.2891\n",
      "Epoch 00007: val_loss did not improve from 0.13488\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2884 - val_loss: 0.1471\n",
      " ###1 fold : val acc1 0.509, acc3 0.963, mae 0.265###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/279 [============================>.] - ETA: 0s - loss: 3.4710\n",
      "Epoch 00001: val_loss improved from inf to 0.23420, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.4188 - val_loss: 0.2342\n",
      "Epoch 2/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.6275\n",
      "Epoch 00002: val_loss did not improve from 0.23420\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.6215 - val_loss: 0.2795\n",
      "Epoch 3/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.4066\n",
      "Epoch 00003: val_loss improved from 0.23420 to 0.15028, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4070 - val_loss: 0.1503\n",
      "Epoch 4/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.3615\n",
      "Epoch 00004: val_loss improved from 0.15028 to 0.14673, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3607 - val_loss: 0.1467\n",
      "Epoch 5/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.3319\n",
      "Epoch 00005: val_loss improved from 0.14673 to 0.13511, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3320 - val_loss: 0.1351\n",
      "Epoch 6/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.3009\n",
      "Epoch 00006: val_loss did not improve from 0.13511\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3009 - val_loss: 0.1490\n",
      "Epoch 7/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.2896\n",
      "Epoch 00007: val_loss improved from 0.13511 to 0.12394, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2887 - val_loss: 0.1239\n",
      "Epoch 8/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.2672\n",
      "Epoch 00008: val_loss improved from 0.12394 to 0.12278, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2677 - val_loss: 0.1228\n",
      "Epoch 9/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2654\n",
      "Epoch 00009: val_loss did not improve from 0.12278\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2652 - val_loss: 0.1233\n",
      "Epoch 10/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2546\n",
      "Epoch 00010: val_loss did not improve from 0.12278\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2548 - val_loss: 0.1254\n",
      " ###2 fold : val acc1 0.535, acc3 0.975, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/279 [============================>.] - ETA: 0s - loss: 3.4466\n",
      "Epoch 00001: val_loss improved from inf to 0.23035, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.4225 - val_loss: 0.2304\n",
      "Epoch 2/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.6285\n",
      "Epoch 00002: val_loss did not improve from 0.23035\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.6233 - val_loss: 0.2590\n",
      "Epoch 3/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.4064\n",
      "Epoch 00003: val_loss improved from 0.23035 to 0.15464, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4064 - val_loss: 0.1546\n",
      "Epoch 4/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.3620\n",
      "Epoch 00004: val_loss improved from 0.15464 to 0.13783, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3611 - val_loss: 0.1378\n",
      "Epoch 5/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.3328\n",
      "Epoch 00005: val_loss improved from 0.13783 to 0.13101, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3320 - val_loss: 0.1310\n",
      "Epoch 6/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.3028\n",
      "Epoch 00006: val_loss did not improve from 0.13101\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3023 - val_loss: 0.1447\n",
      "Epoch 7/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2895\n",
      "Epoch 00007: val_loss improved from 0.13101 to 0.12353, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2891 - val_loss: 0.1235\n",
      "Epoch 8/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.2687\n",
      "Epoch 00008: val_loss did not improve from 0.12353\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2686 - val_loss: 0.1236\n",
      "Epoch 9/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.2667\n",
      "Epoch 00009: val_loss did not improve from 0.12353\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2659 - val_loss: 0.1317\n",
      " ###3 fold : val acc1 0.522, acc3 0.966, mae 0.256###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/279 [===========================>..] - ETA: 0s - loss: 3.5103\n",
      "Epoch 00001: val_loss improved from inf to 0.22028, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.3878 - val_loss: 0.2203\n",
      "Epoch 2/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.6267\n",
      "Epoch 00002: val_loss did not improve from 0.22028\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.6195 - val_loss: 0.2360\n",
      "Epoch 3/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.4014\n",
      "Epoch 00003: val_loss improved from 0.22028 to 0.19715, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4013 - val_loss: 0.1971\n",
      "Epoch 4/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.3610\n",
      "Epoch 00004: val_loss improved from 0.19715 to 0.15090, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3611 - val_loss: 0.1509\n",
      "Epoch 5/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.3270\n",
      "Epoch 00005: val_loss did not improve from 0.15090\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3266 - val_loss: 0.2034\n",
      "Epoch 6/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.3006\n",
      "Epoch 00006: val_loss improved from 0.15090 to 0.12266, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3007 - val_loss: 0.1227\n",
      "Epoch 7/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.2896\n",
      "Epoch 00007: val_loss did not improve from 0.12266\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2896 - val_loss: 0.1324\n",
      "Epoch 8/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2721\n",
      "Epoch 00008: val_loss did not improve from 0.12266\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2715 - val_loss: 0.1522\n",
      " ###4 fold : val acc1 0.526, acc3 0.972, mae 0.251###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/279 [============================>.] - ETA: 0s - loss: 3.4729\n",
      "Epoch 00001: val_loss improved from inf to 0.23116, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.3906 - val_loss: 0.2312\n",
      "Epoch 2/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.6166\n",
      "Epoch 00002: val_loss improved from 0.23116 to 0.22996, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.6121 - val_loss: 0.2300\n",
      "Epoch 3/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.4014\n",
      "Epoch 00003: val_loss improved from 0.22996 to 0.18978, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4016 - val_loss: 0.1898\n",
      "Epoch 4/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.3605\n",
      "Epoch 00004: val_loss improved from 0.18978 to 0.15068, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3602 - val_loss: 0.1507\n",
      "Epoch 5/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.3257\n",
      "Epoch 00005: val_loss did not improve from 0.15068\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3254 - val_loss: 0.1948\n",
      "Epoch 6/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.3031\n",
      "Epoch 00006: val_loss improved from 0.15068 to 0.12815, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3029 - val_loss: 0.1281\n",
      "Epoch 7/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2913\n",
      "Epoch 00007: val_loss did not improve from 0.12815\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2910 - val_loss: 0.1395\n",
      "Epoch 8/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2715\n",
      "Epoch 00008: val_loss did not improve from 0.12815\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2709 - val_loss: 0.1589\n",
      " ###5 fold : val acc1 0.507, acc3 0.966, mae 0.264###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/279 [============================>.] - ETA: 0s - loss: 3.4405\n",
      "Epoch 00001: val_loss improved from inf to 0.26367, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.3780 - val_loss: 0.2637\n",
      "Epoch 2/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.6183\n",
      "Epoch 00002: val_loss improved from 0.26367 to 0.23231, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.6147 - val_loss: 0.2323\n",
      "Epoch 3/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.3988\n",
      "Epoch 00003: val_loss improved from 0.23231 to 0.18041, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3989 - val_loss: 0.1804\n",
      "Epoch 4/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.3606\n",
      "Epoch 00004: val_loss improved from 0.18041 to 0.14186, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3602 - val_loss: 0.1419\n",
      "Epoch 5/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.3262\n",
      "Epoch 00005: val_loss did not improve from 0.14186\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3266 - val_loss: 0.1970\n",
      "Epoch 6/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.3020\n",
      "Epoch 00006: val_loss improved from 0.14186 to 0.13338, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3022 - val_loss: 0.1334\n",
      "Epoch 7/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2862\n",
      "Epoch 00007: val_loss did not improve from 0.13338\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2864 - val_loss: 0.1335\n",
      "Epoch 8/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.2713\n",
      "Epoch 00008: val_loss did not improve from 0.13338\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2705 - val_loss: 0.1449\n",
      " ###6 fold : val acc1 0.491, acc3 0.971, mae 0.270###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/279 [===========================>..] - ETA: 0s - loss: 3.4707\n",
      "Epoch 00001: val_loss improved from inf to 0.28316, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.3702 - val_loss: 0.2832\n",
      "Epoch 2/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.6098\n",
      "Epoch 00002: val_loss improved from 0.28316 to 0.20827, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.6049 - val_loss: 0.2083\n",
      "Epoch 3/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.3936\n",
      "Epoch 00003: val_loss improved from 0.20827 to 0.19249, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3939 - val_loss: 0.1925\n",
      "Epoch 4/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.3538\n",
      "Epoch 00004: val_loss improved from 0.19249 to 0.15011, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3538 - val_loss: 0.1501\n",
      "Epoch 5/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.3223\n",
      "Epoch 00005: val_loss did not improve from 0.15011\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3231 - val_loss: 0.2027\n",
      "Epoch 6/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2979\n",
      "Epoch 00006: val_loss improved from 0.15011 to 0.13636, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2982 - val_loss: 0.1364\n",
      "Epoch 7/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2854\n",
      "Epoch 00007: val_loss improved from 0.13636 to 0.13310, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2852 - val_loss: 0.1331\n",
      "Epoch 8/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2728\n",
      "Epoch 00008: val_loss did not improve from 0.13310\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2720 - val_loss: 0.1457\n",
      "Epoch 9/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.2583\n",
      "Epoch 00009: val_loss improved from 0.13310 to 0.12125, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2582 - val_loss: 0.1212\n",
      "Epoch 10/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2550\n",
      "Epoch 00010: val_loss did not improve from 0.12125\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2542 - val_loss: 0.1414\n",
      "Epoch 11/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.2437\n",
      "Epoch 00011: val_loss improved from 0.12125 to 0.12020, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2432 - val_loss: 0.1202\n",
      "Epoch 12/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2374\n",
      "Epoch 00012: val_loss did not improve from 0.12020\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2368 - val_loss: 0.1484\n",
      "Epoch 13/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.2305\n",
      "Epoch 00013: val_loss did not improve from 0.12020\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2295 - val_loss: 0.1302\n",
      " ###7 fold : val acc1 0.520, acc3 0.965, mae 0.257###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267/279 [===========================>..] - ETA: 0s - loss: 3.4959\n",
      "Epoch 00001: val_loss improved from inf to 0.29914, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.3850 - val_loss: 0.2991\n",
      "Epoch 2/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.6093\n",
      "Epoch 00002: val_loss improved from 0.29914 to 0.21304, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.6067 - val_loss: 0.2130\n",
      "Epoch 3/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.3942\n",
      "Epoch 00003: val_loss improved from 0.21304 to 0.19136, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3947 - val_loss: 0.1914\n",
      "Epoch 4/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.3556\n",
      "Epoch 00004: val_loss improved from 0.19136 to 0.15486, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3558 - val_loss: 0.1549\n",
      "Epoch 5/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.3246\n",
      "Epoch 00005: val_loss did not improve from 0.15486\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3254 - val_loss: 0.2121\n",
      "Epoch 6/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2962\n",
      "Epoch 00006: val_loss improved from 0.15486 to 0.14323, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2973 - val_loss: 0.1432\n",
      "Epoch 7/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2854\n",
      "Epoch 00007: val_loss improved from 0.14323 to 0.13910, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2854 - val_loss: 0.1391\n",
      "Epoch 8/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.2728\n",
      "Epoch 00008: val_loss did not improve from 0.13910\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2714 - val_loss: 0.1471\n",
      "Epoch 9/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2577\n",
      "Epoch 00009: val_loss improved from 0.13910 to 0.12921, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2576 - val_loss: 0.1292\n",
      "Epoch 10/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.2559\n",
      "Epoch 00010: val_loss did not improve from 0.12921\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2548 - val_loss: 0.1497\n",
      "Epoch 11/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.2450\n",
      "Epoch 00011: val_loss did not improve from 0.12921\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2444 - val_loss: 0.1293\n",
      " ###8 fold : val acc1 0.496, acc3 0.963, mae 0.270###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267/279 [===========================>..] - ETA: 0s - loss: 3.4959\n",
      "Epoch 00001: val_loss improved from inf to 0.30486, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.3850 - val_loss: 0.3049\n",
      "Epoch 2/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.6083\n",
      "Epoch 00002: val_loss improved from 0.30486 to 0.21810, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.6067 - val_loss: 0.2181\n",
      "Epoch 3/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.3944\n",
      "Epoch 00003: val_loss improved from 0.21810 to 0.19535, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3947 - val_loss: 0.1953\n",
      "Epoch 4/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.3558\n",
      "Epoch 00004: val_loss improved from 0.19535 to 0.15807, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3558 - val_loss: 0.1581\n",
      "Epoch 5/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.3246\n",
      "Epoch 00005: val_loss did not improve from 0.15807\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3254 - val_loss: 0.2144\n",
      "Epoch 6/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.2964\n",
      "Epoch 00006: val_loss improved from 0.15807 to 0.14585, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2973 - val_loss: 0.1458\n",
      "Epoch 7/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.2857\n",
      "Epoch 00007: val_loss improved from 0.14585 to 0.14132, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2854 - val_loss: 0.1413\n",
      "Epoch 8/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.2721\n",
      "Epoch 00008: val_loss did not improve from 0.14132\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2714 - val_loss: 0.1496\n",
      "Epoch 9/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.2573\n",
      "Epoch 00009: val_loss improved from 0.14132 to 0.13078, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,dnodes32_dropout0.1,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2576 - val_loss: 0.1308\n",
      "Epoch 10/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.2564\n",
      "Epoch 00010: val_loss did not improve from 0.13078\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2548 - val_loss: 0.1514\n",
      "Epoch 11/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.2452\n",
      "Epoch 00011: val_loss did not improve from 0.13078\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2444 - val_loss: 0.1311\n",
      " ###9 fold : val acc1 0.510, acc3 0.962, mae 0.264###\n",
      "acc10.515_acc30.968\n",
      "random search 55/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260/279 [==========================>...] - ETA: 0s - loss: 2.8517\n",
      "Epoch 00001: val_loss improved from inf to 0.12383, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 2.6701 - val_loss: 0.1238\n",
      "Epoch 2/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00002: val_loss improved from 0.12383 to 0.10850, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1169 - val_loss: 0.1085\n",
      "Epoch 3/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1073\n",
      "Epoch 00003: val_loss improved from 0.10850 to 0.10238, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1070 - val_loss: 0.1024\n",
      "Epoch 4/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1026\n",
      "Epoch 00004: val_loss improved from 0.10238 to 0.09881, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1027 - val_loss: 0.0988\n",
      "Epoch 5/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.0995\n",
      "Epoch 00005: val_loss improved from 0.09881 to 0.09769, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.0997 - val_loss: 0.0977\n",
      "Epoch 6/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00006: val_loss did not improve from 0.09769\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.0992 - val_loss: 0.1025\n",
      "Epoch 7/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00007: val_loss improved from 0.09769 to 0.09551, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.0985 - val_loss: 0.0955\n",
      "Epoch 8/100\n",
      "261/279 [===========================>..] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00008: val_loss improved from 0.09551 to 0.09474, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.0977 - val_loss: 0.0947\n",
      "Epoch 9/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.0988\n",
      "Epoch 00009: val_loss did not improve from 0.09474\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.0986 - val_loss: 0.0954\n",
      "Epoch 10/100\n",
      "261/279 [===========================>..] - ETA: 0s - loss: 0.0983\n",
      "Epoch 00010: val_loss did not improve from 0.09474\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.0985 - val_loss: 0.0953\n",
      " ###0 fold : val acc1 0.608, acc3 0.981, mae 0.206###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/279 [============================>.] - ETA: 0s - loss: 2.7486\n",
      "Epoch 00001: val_loss improved from inf to 0.12456, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.6866 - val_loss: 0.1246\n",
      "Epoch 2/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00002: val_loss improved from 0.12456 to 0.10827, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1170 - val_loss: 0.1083\n",
      "Epoch 3/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.1070\n",
      "Epoch 00003: val_loss improved from 0.10827 to 0.10260, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1069 - val_loss: 0.1026\n",
      "Epoch 4/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1022\n",
      "Epoch 00004: val_loss improved from 0.10260 to 0.09807, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1023 - val_loss: 0.0981\n",
      "Epoch 5/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.0992\n",
      "Epoch 00005: val_loss improved from 0.09807 to 0.09755, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.0994 - val_loss: 0.0975\n",
      "Epoch 6/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00006: val_loss did not improve from 0.09755\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.0984 - val_loss: 0.0986\n",
      "Epoch 7/100\n",
      "262/279 [===========================>..] - ETA: 0s - loss: 0.0985\n",
      "Epoch 00007: val_loss improved from 0.09755 to 0.09477, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.0979 - val_loss: 0.0948\n",
      "Epoch 8/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 00008: val_loss improved from 0.09477 to 0.09444, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.0979 - val_loss: 0.0944\n",
      "Epoch 9/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.0979\n",
      "Epoch 00009: val_loss improved from 0.09444 to 0.09442, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.0978 - val_loss: 0.0944\n",
      "Epoch 10/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00010: val_loss did not improve from 0.09442\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.0977 - val_loss: 0.0950\n",
      "Epoch 11/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00011: val_loss did not improve from 0.09442\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.0968 - val_loss: 0.0955\n",
      " ###1 fold : val acc1 0.592, acc3 0.984, mae 0.212###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258/279 [==========================>...] - ETA: 0s - loss: 2.8911\n",
      "Epoch 00001: val_loss improved from inf to 0.12482, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 2.6872 - val_loss: 0.1248\n",
      "Epoch 2/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1171\n",
      "Epoch 00002: val_loss improved from 0.12482 to 0.10851, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1172 - val_loss: 0.1085\n",
      "Epoch 3/100\n",
      "259/279 [==========================>...] - ETA: 0s - loss: 0.1072\n",
      "Epoch 00003: val_loss improved from 0.10851 to 0.10289, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1073 - val_loss: 0.1029\n",
      "Epoch 4/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1027\n",
      "Epoch 00004: val_loss improved from 0.10289 to 0.09755, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1026 - val_loss: 0.0976\n",
      "Epoch 5/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00005: val_loss did not improve from 0.09755\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.0997 - val_loss: 0.0981\n",
      "Epoch 6/100\n",
      "142/279 [==============>...............] - ETA: 0s - loss: 0.0986"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 1.3224\n",
      "Epoch 00005: val_loss did not improve from 0.18908\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.3203 - val_loss: 0.1955\n",
      "Epoch 6/100\n",
      "261/279 [===========================>..] - ETA: 0s - loss: 1.1817\n",
      "Epoch 00006: val_loss improved from 0.18908 to 0.16977, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.1790 - val_loss: 0.1698\n",
      "Epoch 7/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 1.0710\n",
      "Epoch 00007: val_loss improved from 0.16977 to 0.15527, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.0706 - val_loss: 0.1553\n",
      "Epoch 8/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.9976\n",
      "Epoch 00008: val_loss did not improve from 0.15527\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.9949 - val_loss: 0.1588\n",
      "Epoch 9/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.9311\n",
      "Epoch 00009: val_loss improved from 0.15527 to 0.12844, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.9286 - val_loss: 0.1284\n",
      "Epoch 10/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.8709\n",
      "Epoch 00010: val_loss improved from 0.12844 to 0.12686, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8717 - val_loss: 0.1269\n",
      "Epoch 11/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.8331\n",
      "Epoch 00011: val_loss did not improve from 0.12686\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8347 - val_loss: 0.1313\n",
      "Epoch 12/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.7702\n",
      "Epoch 00012: val_loss improved from 0.12686 to 0.12137, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.7703 - val_loss: 0.1214\n",
      "Epoch 13/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.7546\n",
      "Epoch 00013: val_loss did not improve from 0.12137\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.7546 - val_loss: 0.1308\n",
      "Epoch 14/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.7097\n",
      "Epoch 00014: val_loss improved from 0.12137 to 0.11718, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.7089 - val_loss: 0.1172\n",
      "Epoch 15/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.6585\n",
      "Epoch 00015: val_loss did not improve from 0.11718\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.6571 - val_loss: 0.1180\n",
      "Epoch 16/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.6203\n",
      "Epoch 00016: val_loss did not improve from 0.11718\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.6194 - val_loss: 0.1292\n",
      " ###2 fold : val acc1 0.550, acc3 0.967, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/279 [===========================>..] - ETA: 0s - loss: 10.5569\n",
      "Epoch 00001: val_loss improved from inf to 1.96220, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 10.2139 - val_loss: 1.9622\n",
      "Epoch 2/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 2.6259\n",
      "Epoch 00002: val_loss improved from 1.96220 to 0.45685, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 2.6011 - val_loss: 0.4569\n",
      "Epoch 3/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 1.8215\n",
      "Epoch 00003: val_loss improved from 0.45685 to 0.22675, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.8209 - val_loss: 0.2267\n",
      "Epoch 4/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 1.5418\n",
      "Epoch 00004: val_loss improved from 0.22675 to 0.18266, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.5359 - val_loss: 0.1827\n",
      "Epoch 5/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 1.3247\n",
      "Epoch 00005: val_loss did not improve from 0.18266\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.3229 - val_loss: 0.1839\n",
      "Epoch 6/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 1.1735\n",
      "Epoch 00006: val_loss improved from 0.18266 to 0.17441, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.1741 - val_loss: 0.1744\n",
      "Epoch 7/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 1.0627\n",
      "Epoch 00007: val_loss improved from 0.17441 to 0.15196, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.0627 - val_loss: 0.1520\n",
      "Epoch 8/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.9956\n",
      "Epoch 00008: val_loss did not improve from 0.15196\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.9947 - val_loss: 0.1633\n",
      "Epoch 9/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.9418\n",
      "Epoch 00009: val_loss improved from 0.15196 to 0.13218, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.9379 - val_loss: 0.1322\n",
      "Epoch 10/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.8724\n",
      "Epoch 00010: val_loss improved from 0.13218 to 0.13087, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8728 - val_loss: 0.1309\n",
      "Epoch 11/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.8337\n",
      "Epoch 00011: val_loss improved from 0.13087 to 0.12988, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8344 - val_loss: 0.1299\n",
      "Epoch 12/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.7698\n",
      "Epoch 00012: val_loss improved from 0.12988 to 0.12226, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.7707 - val_loss: 0.1223\n",
      "Epoch 13/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.7583\n",
      "Epoch 00013: val_loss did not improve from 0.12226\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7578 - val_loss: 0.1276\n",
      "Epoch 14/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.7139\n",
      "Epoch 00014: val_loss improved from 0.12226 to 0.11665, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7142 - val_loss: 0.1166\n",
      "Epoch 15/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.6604\n",
      "Epoch 00015: val_loss did not improve from 0.11665\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.6605 - val_loss: 0.1168\n",
      "Epoch 16/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.6245\n",
      "Epoch 00016: val_loss did not improve from 0.11665\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.6246 - val_loss: 0.1327\n",
      " ###3 fold : val acc1 0.574, acc3 0.963, mae 0.232###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271/279 [============================>.] - ETA: 0s - loss: 10.4145\n",
      "Epoch 00001: val_loss improved from inf to 2.02262, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 10.2280 - val_loss: 2.0226\n",
      "Epoch 2/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 2.5995\n",
      "Epoch 00002: val_loss improved from 2.02262 to 0.45157, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.5998 - val_loss: 0.4516\n",
      "Epoch 3/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 1.7815\n",
      "Epoch 00003: val_loss improved from 0.45157 to 0.21397, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.7727 - val_loss: 0.2140\n",
      "Epoch 4/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 1.5139\n",
      "Epoch 00004: val_loss improved from 0.21397 to 0.20157, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.5110 - val_loss: 0.2016\n",
      "Epoch 5/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 1.3206\n",
      "Epoch 00005: val_loss improved from 0.20157 to 0.18686, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.3206 - val_loss: 0.1869\n",
      "Epoch 6/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 1.1781\n",
      "Epoch 00006: val_loss improved from 0.18686 to 0.15458, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.1781 - val_loss: 0.1546\n",
      "Epoch 7/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 1.0779\n",
      "Epoch 00007: val_loss improved from 0.15458 to 0.15078, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.0756 - val_loss: 0.1508\n",
      "Epoch 8/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 1.0126\n",
      "Epoch 00008: val_loss did not improve from 0.15078\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.0115 - val_loss: 0.1551\n",
      "Epoch 9/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.9443\n",
      "Epoch 00009: val_loss improved from 0.15078 to 0.14734, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.9445 - val_loss: 0.1473\n",
      "Epoch 10/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.8954\n",
      "Epoch 00010: val_loss improved from 0.14734 to 0.13629, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8923 - val_loss: 0.1363\n",
      "Epoch 11/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.8336\n",
      "Epoch 00011: val_loss did not improve from 0.13629\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8322 - val_loss: 0.1364\n",
      "Epoch 12/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.7831\n",
      "Epoch 00012: val_loss did not improve from 0.13629\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.7802 - val_loss: 0.1436\n",
      " ###4 fold : val acc1 0.514, acc3 0.947, mae 0.270###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262/279 [===========================>..] - ETA: 0s - loss: 10.6477\n",
      "Epoch 00001: val_loss improved from inf to 2.01169, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 10.2250 - val_loss: 2.0117\n",
      "Epoch 2/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 2.6019\n",
      "Epoch 00002: val_loss improved from 2.01169 to 0.44355, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 2.5858 - val_loss: 0.4435\n",
      "Epoch 3/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 1.7643\n",
      "Epoch 00003: val_loss improved from 0.44355 to 0.21757, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.7551 - val_loss: 0.2176\n",
      "Epoch 4/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 1.5003\n",
      "Epoch 00004: val_loss improved from 0.21757 to 0.20626, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.4975 - val_loss: 0.2063\n",
      "Epoch 5/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 1.3285\n",
      "Epoch 00005: val_loss improved from 0.20626 to 0.18695, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.3255 - val_loss: 0.1869\n",
      "Epoch 6/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 1.1883\n",
      "Epoch 00006: val_loss improved from 0.18695 to 0.15336, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.1870 - val_loss: 0.1534\n",
      "Epoch 7/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 1.0751\n",
      "Epoch 00007: val_loss improved from 0.15336 to 0.14953, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.0746 - val_loss: 0.1495\n",
      "Epoch 8/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 1.0119\n",
      "Epoch 00008: val_loss did not improve from 0.14953\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.0102 - val_loss: 0.1511\n",
      "Epoch 9/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.9292\n",
      "Epoch 00009: val_loss improved from 0.14953 to 0.13901, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.9270 - val_loss: 0.1390\n",
      "Epoch 10/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.8822\n",
      "Epoch 00010: val_loss improved from 0.13901 to 0.13483, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8822 - val_loss: 0.1348\n",
      "Epoch 11/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.8367\n",
      "Epoch 00011: val_loss did not improve from 0.13483\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8367 - val_loss: 0.1408\n",
      "Epoch 12/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.7855\n",
      "Epoch 00012: val_loss did not improve from 0.13483\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.7832 - val_loss: 0.1466\n",
      " ###5 fold : val acc1 0.523, acc3 0.949, mae 0.266###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/279 [===========================>..] - ETA: 0s - loss: 10.5719\n",
      "Epoch 00001: val_loss improved from inf to 2.00580, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 10.2286 - val_loss: 2.0058\n",
      "Epoch 2/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 2.6165\n",
      "Epoch 00002: val_loss improved from 2.00580 to 0.44112, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 2.5944 - val_loss: 0.4411\n",
      "Epoch 3/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 1.7558\n",
      "Epoch 00003: val_loss improved from 0.44112 to 0.22429, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.7552 - val_loss: 0.2243\n",
      "Epoch 4/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 1.5178\n",
      "Epoch 00004: val_loss improved from 0.22429 to 0.20897, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.5130 - val_loss: 0.2090\n",
      "Epoch 5/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 1.3403\n",
      "Epoch 00005: val_loss improved from 0.20897 to 0.18488, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.3388 - val_loss: 0.1849\n",
      "Epoch 6/100\n",
      "261/279 [===========================>..] - ETA: 0s - loss: 1.1905\n",
      "Epoch 00006: val_loss improved from 0.18488 to 0.15111, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.1848 - val_loss: 0.1511\n",
      "Epoch 7/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 1.0797\n",
      "Epoch 00007: val_loss improved from 0.15111 to 0.14916, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.0790 - val_loss: 0.1492\n",
      "Epoch 8/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 1.0138\n",
      "Epoch 00008: val_loss did not improve from 0.14916\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.0122 - val_loss: 0.1577\n",
      "Epoch 9/100\n",
      "262/279 [===========================>..] - ETA: 0s - loss: 0.9364\n",
      "Epoch 00009: val_loss improved from 0.14916 to 0.13864, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.9347 - val_loss: 0.1386\n",
      "Epoch 10/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.8966\n",
      "Epoch 00010: val_loss improved from 0.13864 to 0.13726, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8975 - val_loss: 0.1373\n",
      "Epoch 11/100\n",
      "260/279 [==========================>...] - ETA: 0s - loss: 0.8383\n",
      "Epoch 00011: val_loss did not improve from 0.13726\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8382 - val_loss: 0.1380\n",
      "Epoch 12/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.7872\n",
      "Epoch 00012: val_loss did not improve from 0.13726\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.7848 - val_loss: 0.1463\n",
      " ###6 fold : val acc1 0.514, acc3 0.958, mae 0.266###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/279 [============================>.] - ETA: 0s - loss: 10.3894\n",
      "Epoch 00001: val_loss improved from inf to 1.98066, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 10.1762 - val_loss: 1.9807\n",
      "Epoch 2/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 2.5857\n",
      "Epoch 00002: val_loss improved from 1.98066 to 0.45246, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 2.5733 - val_loss: 0.4525\n",
      "Epoch 3/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 1.7502\n",
      "Epoch 00003: val_loss improved from 0.45246 to 0.22761, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.7411 - val_loss: 0.2276\n",
      "Epoch 4/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 1.4998\n",
      "Epoch 00004: val_loss improved from 0.22761 to 0.20658, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.4941 - val_loss: 0.2066\n",
      "Epoch 5/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 1.3150\n",
      "Epoch 00005: val_loss improved from 0.20658 to 0.18190, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.3141 - val_loss: 0.1819\n",
      "Epoch 6/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 1.1821\n",
      "Epoch 00006: val_loss improved from 0.18190 to 0.15474, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.1816 - val_loss: 0.1547\n",
      "Epoch 7/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 1.0823\n",
      "Epoch 00007: val_loss improved from 0.15474 to 0.14927, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.0781 - val_loss: 0.1493\n",
      "Epoch 8/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 1.0213\n",
      "Epoch 00008: val_loss did not improve from 0.14927\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.0204 - val_loss: 0.1518\n",
      "Epoch 9/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.9383\n",
      "Epoch 00009: val_loss improved from 0.14927 to 0.13865, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.9353 - val_loss: 0.1387\n",
      "Epoch 10/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.8862\n",
      "Epoch 00010: val_loss improved from 0.13865 to 0.13113, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8858 - val_loss: 0.1311\n",
      "Epoch 11/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.8323\n",
      "Epoch 00011: val_loss did not improve from 0.13113\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8336 - val_loss: 0.1339\n",
      "Epoch 12/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.7819\n",
      "Epoch 00012: val_loss did not improve from 0.13113\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.7796 - val_loss: 0.1432\n",
      " ###7 fold : val acc1 0.524, acc3 0.945, mae 0.268###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/279 [============================>.] - ETA: 0s - loss: 10.2685\n",
      "Epoch 00001: val_loss improved from inf to 1.98799, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 10.2322 - val_loss: 1.9880\n",
      "Epoch 2/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 2.5634\n",
      "Epoch 00002: val_loss improved from 1.98799 to 0.45078, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 2.5622 - val_loss: 0.4508\n",
      "Epoch 3/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 1.7439\n",
      "Epoch 00003: val_loss improved from 0.45078 to 0.24392, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.7425 - val_loss: 0.2439\n",
      "Epoch 4/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 1.5070\n",
      "Epoch 00004: val_loss improved from 0.24392 to 0.21655, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.5045 - val_loss: 0.2166\n",
      "Epoch 5/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 1.3240\n",
      "Epoch 00005: val_loss improved from 0.21655 to 0.18772, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.3224 - val_loss: 0.1877\n",
      "Epoch 6/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 1.1874\n",
      "Epoch 00006: val_loss improved from 0.18772 to 0.16125, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.1875 - val_loss: 0.1612\n",
      "Epoch 7/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 1.0867\n",
      "Epoch 00007: val_loss improved from 0.16125 to 0.15482, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.0856 - val_loss: 0.1548\n",
      "Epoch 8/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 1.0282\n",
      "Epoch 00008: val_loss did not improve from 0.15482\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.0267 - val_loss: 0.1563\n",
      "Epoch 9/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.9474\n",
      "Epoch 00009: val_loss improved from 0.15482 to 0.14733, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.9477 - val_loss: 0.1473\n",
      "Epoch 10/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.8984\n",
      "Epoch 00010: val_loss improved from 0.14733 to 0.13477, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8959 - val_loss: 0.1348\n",
      "Epoch 11/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.8375\n",
      "Epoch 00011: val_loss did not improve from 0.13477\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8366 - val_loss: 0.1382\n",
      "Epoch 12/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.7822\n",
      "Epoch 00012: val_loss did not improve from 0.13477\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7800 - val_loss: 0.1452\n",
      " ###8 fold : val acc1 0.532, acc3 0.954, mae 0.258###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/279 [===========================>..] - ETA: 0s - loss: 10.5735\n",
      "Epoch 00001: val_loss improved from inf to 1.98855, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 10.2322 - val_loss: 1.9885\n",
      "Epoch 2/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 2.5622\n",
      "Epoch 00002: val_loss improved from 1.98855 to 0.45625, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 2.5622 - val_loss: 0.4562\n",
      "Epoch 3/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 1.7467\n",
      "Epoch 00003: val_loss improved from 0.45625 to 0.24931, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.7425 - val_loss: 0.2493\n",
      "Epoch 4/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 1.5060\n",
      "Epoch 00004: val_loss improved from 0.24931 to 0.22047, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.5045 - val_loss: 0.2205\n",
      "Epoch 5/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 1.3240\n",
      "Epoch 00005: val_loss improved from 0.22047 to 0.19110, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.3224 - val_loss: 0.1911\n",
      "Epoch 6/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 1.1892\n",
      "Epoch 00006: val_loss improved from 0.19110 to 0.16462, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.1875 - val_loss: 0.1646\n",
      "Epoch 7/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 1.0857\n",
      "Epoch 00007: val_loss improved from 0.16462 to 0.15837, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.0856 - val_loss: 0.1584\n",
      "Epoch 8/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 1.0294\n",
      "Epoch 00008: val_loss did not improve from 0.15837\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.0267 - val_loss: 0.1598\n",
      "Epoch 9/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.9512\n",
      "Epoch 00009: val_loss improved from 0.15837 to 0.15052, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.9477 - val_loss: 0.1505\n",
      "Epoch 10/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.8982\n",
      "Epoch 00010: val_loss improved from 0.15052 to 0.13828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.5,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8959 - val_loss: 0.1383\n",
      "Epoch 11/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.8374\n",
      "Epoch 00011: val_loss did not improve from 0.13828\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8366 - val_loss: 0.1414\n",
      "Epoch 12/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.7800\n",
      "Epoch 00012: val_loss did not improve from 0.13828\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.7800 - val_loss: 0.1481\n",
      " ###9 fold : val acc1 0.532, acc3 0.961, mae 0.255###\n",
      "acc10.535_acc30.957\n",
      "random search 61/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/279 [===========================>..] - ETA: 0s - loss: 5.8343\n",
      "Epoch 00001: val_loss improved from inf to 0.50339, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 5.6387 - val_loss: 0.5034\n",
      "Epoch 2/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2985\n",
      "Epoch 00002: val_loss improved from 0.50339 to 0.12637, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2977 - val_loss: 0.1264\n",
      "Epoch 3/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2077\n",
      "Epoch 00003: val_loss improved from 0.12637 to 0.11519, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2076 - val_loss: 0.1152\n",
      "Epoch 4/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1947\n",
      "Epoch 00004: val_loss improved from 0.11519 to 0.10900, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1954 - val_loss: 0.1090\n",
      "Epoch 5/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00005: val_loss improved from 0.10900 to 0.10353, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1869 - val_loss: 0.1035\n",
      "Epoch 6/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1763\n",
      "Epoch 00006: val_loss improved from 0.10353 to 0.10166, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1762 - val_loss: 0.1017\n",
      "Epoch 7/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1710\n",
      "Epoch 00007: val_loss improved from 0.10166 to 0.09978, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1711 - val_loss: 0.0998\n",
      "Epoch 8/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1638\n",
      "Epoch 00008: val_loss improved from 0.09978 to 0.09753, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1630 - val_loss: 0.0975\n",
      "Epoch 9/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1599\n",
      "Epoch 00009: val_loss improved from 0.09753 to 0.09593, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1595 - val_loss: 0.0959\n",
      "Epoch 10/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1570\n",
      "Epoch 00010: val_loss improved from 0.09593 to 0.09560, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1575 - val_loss: 0.0956\n",
      "Epoch 11/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1550\n",
      "Epoch 00011: val_loss did not improve from 0.09560\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1548 - val_loss: 0.0961\n",
      "Epoch 12/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1530\n",
      "Epoch 00012: val_loss improved from 0.09560 to 0.09474, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1530 - val_loss: 0.0947\n",
      "Epoch 13/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1508\n",
      "Epoch 00013: val_loss did not improve from 0.09474\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1508 - val_loss: 0.0966\n",
      "Epoch 14/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1460\n",
      "Epoch 00014: val_loss did not improve from 0.09474\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1462 - val_loss: 0.0964\n",
      " ###0 fold : val acc1 0.610, acc3 0.981, mae 0.205###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/279 [============================>.] - ETA: 0s - loss: 5.6604\n",
      "Epoch 00001: val_loss improved from inf to 0.50529, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 5.6502 - val_loss: 0.5053\n",
      "Epoch 2/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.2999\n",
      "Epoch 00002: val_loss improved from 0.50529 to 0.12690, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2981 - val_loss: 0.1269\n",
      "Epoch 3/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.2070\n",
      "Epoch 00003: val_loss improved from 0.12690 to 0.11560, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2069 - val_loss: 0.1156\n",
      "Epoch 4/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1950\n",
      "Epoch 00004: val_loss improved from 0.11560 to 0.10872, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1950 - val_loss: 0.1087\n",
      "Epoch 5/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00005: val_loss improved from 0.10872 to 0.10329, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1876 - val_loss: 0.1033\n",
      "Epoch 6/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1754\n",
      "Epoch 00006: val_loss improved from 0.10329 to 0.10226, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1754 - val_loss: 0.1023\n",
      "Epoch 7/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1709\n",
      "Epoch 00007: val_loss improved from 0.10226 to 0.09931, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1702 - val_loss: 0.0993\n",
      "Epoch 8/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1636\n",
      "Epoch 00008: val_loss improved from 0.09931 to 0.09738, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1629 - val_loss: 0.0974\n",
      "Epoch 9/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1593\n",
      "Epoch 00009: val_loss improved from 0.09738 to 0.09603, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1591 - val_loss: 0.0960\n",
      "Epoch 10/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1579\n",
      "Epoch 00010: val_loss improved from 0.09603 to 0.09561, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1580 - val_loss: 0.0956\n",
      "Epoch 11/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.1546\n",
      "Epoch 00011: val_loss improved from 0.09561 to 0.09559, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1543 - val_loss: 0.0956\n",
      "Epoch 12/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1531\n",
      "Epoch 00012: val_loss improved from 0.09559 to 0.09457, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1530 - val_loss: 0.0946\n",
      "Epoch 13/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1506\n",
      "Epoch 00013: val_loss did not improve from 0.09457\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1508 - val_loss: 0.0973\n",
      "Epoch 14/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.1463\n",
      "Epoch 00014: val_loss did not improve from 0.09457\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1464 - val_loss: 0.0962\n",
      " ###1 fold : val acc1 0.589, acc3 0.984, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/279 [===========================>..] - ETA: 0s - loss: 5.9000\n",
      "Epoch 00001: val_loss improved from inf to 0.49933, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 5.6462 - val_loss: 0.4993\n",
      "Epoch 2/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2980\n",
      "Epoch 00002: val_loss improved from 0.49933 to 0.12637, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2964 - val_loss: 0.1264\n",
      "Epoch 3/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2074\n",
      "Epoch 00003: val_loss improved from 0.12637 to 0.11592, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2077 - val_loss: 0.1159\n",
      "Epoch 4/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1956\n",
      "Epoch 00004: val_loss improved from 0.11592 to 0.10893, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1957 - val_loss: 0.1089\n",
      "Epoch 5/100\n",
      "262/279 [===========================>..] - ETA: 0s - loss: 0.1869\n",
      "Epoch 00005: val_loss improved from 0.10893 to 0.10376, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1867 - val_loss: 0.1038\n",
      "Epoch 6/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.1757\n",
      "Epoch 00006: val_loss did not improve from 0.10376\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1758 - val_loss: 0.1045\n",
      "Epoch 7/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1699\n",
      "Epoch 00007: val_loss improved from 0.10376 to 0.09941, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1696 - val_loss: 0.0994\n",
      "Epoch 8/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1643\n",
      "Epoch 00008: val_loss improved from 0.09941 to 0.09719, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1633 - val_loss: 0.0972\n",
      "Epoch 9/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1588\n",
      "Epoch 00009: val_loss improved from 0.09719 to 0.09629, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1588 - val_loss: 0.0963\n",
      "Epoch 10/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1584\n",
      "Epoch 00010: val_loss did not improve from 0.09629\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1585 - val_loss: 0.0964\n",
      "Epoch 11/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1544\n",
      "Epoch 00011: val_loss improved from 0.09629 to 0.09607, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1544 - val_loss: 0.0961\n",
      "Epoch 12/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1536\n",
      "Epoch 00012: val_loss improved from 0.09607 to 0.09540, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1539 - val_loss: 0.0954\n",
      "Epoch 13/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1510\n",
      "Epoch 00013: val_loss did not improve from 0.09540\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1507 - val_loss: 0.0957\n",
      "Epoch 14/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1466\n",
      "Epoch 00014: val_loss did not improve from 0.09540\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1470 - val_loss: 0.0969\n",
      " ###2 fold : val acc1 0.597, acc3 0.982, mae 0.210###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/279 [============================>.] - ETA: 0s - loss: 5.7578\n",
      "Epoch 00001: val_loss improved from inf to 0.49826, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 5.6554 - val_loss: 0.4983\n",
      "Epoch 2/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.3008\n",
      "Epoch 00002: val_loss improved from 0.49826 to 0.12638, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2971 - val_loss: 0.1264\n",
      "Epoch 3/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.2073\n",
      "Epoch 00003: val_loss improved from 0.12638 to 0.11636, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2077 - val_loss: 0.1164\n",
      "Epoch 4/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1962\n",
      "Epoch 00004: val_loss improved from 0.11636 to 0.10915, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1962 - val_loss: 0.1091\n",
      "Epoch 5/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1843\n",
      "Epoch 00005: val_loss improved from 0.10915 to 0.10369, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1847 - val_loss: 0.1037\n",
      "Epoch 6/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1766\n",
      "Epoch 00006: val_loss improved from 0.10369 to 0.10323, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1765 - val_loss: 0.1032\n",
      "Epoch 7/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1701\n",
      "Epoch 00007: val_loss improved from 0.10323 to 0.10055, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1698 - val_loss: 0.1005\n",
      "Epoch 8/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1644\n",
      "Epoch 00008: val_loss improved from 0.10055 to 0.09741, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1640 - val_loss: 0.0974\n",
      "Epoch 9/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1588\n",
      "Epoch 00009: val_loss improved from 0.09741 to 0.09643, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1587 - val_loss: 0.0964\n",
      "Epoch 10/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1590\n",
      "Epoch 00010: val_loss improved from 0.09643 to 0.09617, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1592 - val_loss: 0.0962\n",
      "Epoch 11/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1538\n",
      "Epoch 00011: val_loss improved from 0.09617 to 0.09526, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1534 - val_loss: 0.0953\n",
      "Epoch 12/100\n",
      "262/279 [===========================>..] - ETA: 0s - loss: 0.1533\n",
      "Epoch 00012: val_loss did not improve from 0.09526\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1536 - val_loss: 0.0954\n",
      "Epoch 13/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1497\n",
      "Epoch 00013: val_loss did not improve from 0.09526\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1497 - val_loss: 0.0966\n",
      " ###3 fold : val acc1 0.613, acc3 0.976, mae 0.205###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/279 [============================>.] - ETA: 0s - loss: 5.6347\n",
      "Epoch 00001: val_loss improved from inf to 0.49399, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 5.6239 - val_loss: 0.4940\n",
      "Epoch 2/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.3014\n",
      "Epoch 00002: val_loss improved from 0.49399 to 0.12737, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2968 - val_loss: 0.1274\n",
      "Epoch 3/100\n",
      "262/279 [===========================>..] - ETA: 0s - loss: 0.2089\n",
      "Epoch 00003: val_loss improved from 0.12737 to 0.11651, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2083 - val_loss: 0.1165\n",
      "Epoch 4/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1950\n",
      "Epoch 00004: val_loss improved from 0.11651 to 0.10868, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1947 - val_loss: 0.1087\n",
      "Epoch 5/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1850\n",
      "Epoch 00005: val_loss improved from 0.10868 to 0.10672, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1849 - val_loss: 0.1067\n",
      "Epoch 6/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1774\n",
      "Epoch 00006: val_loss improved from 0.10672 to 0.10131, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1773 - val_loss: 0.1013\n",
      "Epoch 7/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1723\n",
      "Epoch 00007: val_loss improved from 0.10131 to 0.10091, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1719 - val_loss: 0.1009\n",
      "Epoch 8/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1623\n",
      "Epoch 00008: val_loss improved from 0.10091 to 0.09706, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1621 - val_loss: 0.0971\n",
      "Epoch 9/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1620\n",
      "Epoch 00009: val_loss improved from 0.09706 to 0.09674, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1619 - val_loss: 0.0967\n",
      "Epoch 10/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1566\n",
      "Epoch 00010: val_loss improved from 0.09674 to 0.09610, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1562 - val_loss: 0.0961\n",
      "Epoch 11/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1561\n",
      "Epoch 00011: val_loss did not improve from 0.09610\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1562 - val_loss: 0.0999\n",
      "Epoch 12/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1513\n",
      "Epoch 00012: val_loss did not improve from 0.09610\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1513 - val_loss: 0.0970\n",
      " ###4 fold : val acc1 0.585, acc3 0.981, mae 0.217###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/279 [===========================>..] - ETA: 0s - loss: 5.8130\n",
      "Epoch 00001: val_loss improved from inf to 0.49723, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 5.6349 - val_loss: 0.4972\n",
      "Epoch 2/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2995\n",
      "Epoch 00002: val_loss improved from 0.49723 to 0.12832, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2980 - val_loss: 0.1283\n",
      "Epoch 3/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.2076\n",
      "Epoch 00003: val_loss improved from 0.12832 to 0.11691, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2074 - val_loss: 0.1169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/279 [============================>.] - ETA: 0s - loss: 7.1266\n",
      "Epoch 00001: val_loss improved from inf to 0.57698, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 6.9943 - val_loss: 0.5770\n",
      "Epoch 2/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 1.0665\n",
      "Epoch 00002: val_loss improved from 0.57698 to 0.15871, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.0665 - val_loss: 0.1587\n",
      "Epoch 3/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.8278\n",
      "Epoch 00003: val_loss improved from 0.15871 to 0.12878, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8278 - val_loss: 0.1288\n",
      "Epoch 4/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.7037\n",
      "Epoch 00004: val_loss improved from 0.12878 to 0.11544, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.7006 - val_loss: 0.1154\n",
      "Epoch 5/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.6256\n",
      "Epoch 00005: val_loss did not improve from 0.11544\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.6244 - val_loss: 0.1302\n",
      "Epoch 6/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.5792\n",
      "Epoch 00006: val_loss improved from 0.11544 to 0.11333, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.5784 - val_loss: 0.1133\n",
      "Epoch 7/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.5594\n",
      "Epoch 00007: val_loss did not improve from 0.11333\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.5589 - val_loss: 0.1178\n",
      "Epoch 8/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.5377\n",
      "Epoch 00008: val_loss improved from 0.11333 to 0.11173, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.5380 - val_loss: 0.1117\n",
      "Epoch 9/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.4919\n",
      "Epoch 00009: val_loss improved from 0.11173 to 0.11114, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.4906 - val_loss: 0.1111\n",
      "Epoch 10/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.4680\n",
      "Epoch 00010: val_loss improved from 0.11114 to 0.10668, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.4667 - val_loss: 0.1067\n",
      "Epoch 11/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.4215\n",
      "Epoch 00011: val_loss improved from 0.10668 to 0.10336, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.4203 - val_loss: 0.1034\n",
      "Epoch 12/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.3832\n",
      "Epoch 00012: val_loss did not improve from 0.10336\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.3816 - val_loss: 0.1083\n",
      "Epoch 13/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.3665\n",
      "Epoch 00013: val_loss did not improve from 0.10336\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.3663 - val_loss: 0.1074\n",
      " ###4 fold : val acc1 0.586, acc3 0.973, mae 0.221###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/279 [============================>.] - ETA: 0s - loss: 7.0095\n",
      "Epoch 00001: val_loss improved from inf to 0.54994, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 6.9971 - val_loss: 0.5499\n",
      "Epoch 2/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 1.0623\n",
      "Epoch 00002: val_loss improved from 0.54994 to 0.16526, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.0621 - val_loss: 0.1653\n",
      "Epoch 3/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.8418\n",
      "Epoch 00003: val_loss improved from 0.16526 to 0.12342, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8408 - val_loss: 0.1234\n",
      "Epoch 4/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.7106\n",
      "Epoch 00004: val_loss improved from 0.12342 to 0.11557, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.7097 - val_loss: 0.1156\n",
      "Epoch 5/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.6212\n",
      "Epoch 00005: val_loss did not improve from 0.11557\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.6208 - val_loss: 0.1310\n",
      "Epoch 6/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.5772\n",
      "Epoch 00006: val_loss improved from 0.11557 to 0.11269, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.5761 - val_loss: 0.1127\n",
      "Epoch 7/100\n",
      "262/279 [===========================>..] - ETA: 0s - loss: 0.5603\n",
      "Epoch 00007: val_loss did not improve from 0.11269\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.5598 - val_loss: 0.1158\n",
      "Epoch 8/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.5400\n",
      "Epoch 00008: val_loss improved from 0.11269 to 0.11176, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.5397 - val_loss: 0.1118\n",
      "Epoch 9/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.4919\n",
      "Epoch 00009: val_loss improved from 0.11176 to 0.11159, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.4906 - val_loss: 0.1116\n",
      "Epoch 10/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.4708\n",
      "Epoch 00010: val_loss improved from 0.11159 to 0.10849, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.4693 - val_loss: 0.1085\n",
      "Epoch 11/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.4276\n",
      "Epoch 00011: val_loss improved from 0.10849 to 0.10216, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.4270 - val_loss: 0.1022\n",
      "Epoch 12/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.3902\n",
      "Epoch 00012: val_loss did not improve from 0.10216\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.3880 - val_loss: 0.1101\n",
      "Epoch 13/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.3765\n",
      "Epoch 00013: val_loss did not improve from 0.10216\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.3756 - val_loss: 0.1034\n",
      " ###5 fold : val acc1 0.573, acc3 0.975, mae 0.227###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/279 [===========================>..] - ETA: 0s - loss: 7.2256\n",
      "Epoch 00001: val_loss improved from inf to 0.54265, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 7.0066 - val_loss: 0.5427\n",
      "Epoch 2/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 1.0609\n",
      "Epoch 00002: val_loss improved from 0.54265 to 0.16195, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.0605 - val_loss: 0.1619\n",
      "Epoch 3/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.8413\n",
      "Epoch 00003: val_loss improved from 0.16195 to 0.12474, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8391 - val_loss: 0.1247\n",
      "Epoch 4/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.7134\n",
      "Epoch 00004: val_loss improved from 0.12474 to 0.11735, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7134 - val_loss: 0.1173\n",
      "Epoch 5/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.6254\n",
      "Epoch 00005: val_loss did not improve from 0.11735\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.6235 - val_loss: 0.1271\n",
      "Epoch 6/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.5815\n",
      "Epoch 00006: val_loss improved from 0.11735 to 0.11334, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.5806 - val_loss: 0.1133\n",
      "Epoch 7/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.5587\n",
      "Epoch 00007: val_loss did not improve from 0.11334\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.5597 - val_loss: 0.1195\n",
      "Epoch 8/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.5411\n",
      "Epoch 00008: val_loss improved from 0.11334 to 0.11204, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.5414 - val_loss: 0.1120\n",
      "Epoch 9/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.4968\n",
      "Epoch 00009: val_loss improved from 0.11204 to 0.10991, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.4955 - val_loss: 0.1099\n",
      "Epoch 10/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.4739\n",
      "Epoch 00010: val_loss did not improve from 0.10991\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.4720 - val_loss: 0.1101\n",
      "Epoch 11/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.4271\n",
      "Epoch 00011: val_loss improved from 0.10991 to 0.10286, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4267 - val_loss: 0.1029\n",
      "Epoch 12/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.3866\n",
      "Epoch 00012: val_loss did not improve from 0.10286\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.3869 - val_loss: 0.1144\n",
      "Epoch 13/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.3761\n",
      "Epoch 00013: val_loss did not improve from 0.10286\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.3757 - val_loss: 0.1032\n",
      " ###6 fold : val acc1 0.592, acc3 0.979, mae 0.214###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/279 [===========================>..] - ETA: 0s - loss: 7.2330\n",
      "Epoch 00001: val_loss improved from inf to 0.55923, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 7.0146 - val_loss: 0.5592\n",
      "Epoch 2/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 1.0625\n",
      "Epoch 00002: val_loss improved from 0.55923 to 0.16493, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.0620 - val_loss: 0.1649\n",
      "Epoch 3/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.8435\n",
      "Epoch 00003: val_loss improved from 0.16493 to 0.12948, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8417 - val_loss: 0.1295\n",
      "Epoch 4/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.7112\n",
      "Epoch 00004: val_loss improved from 0.12948 to 0.11790, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.7088 - val_loss: 0.1179\n",
      "Epoch 5/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.6246\n",
      "Epoch 00005: val_loss did not improve from 0.11790\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.6230 - val_loss: 0.1316\n",
      "Epoch 6/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.5808\n",
      "Epoch 00006: val_loss improved from 0.11790 to 0.11781, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.5796 - val_loss: 0.1178\n",
      "Epoch 7/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.5599\n",
      "Epoch 00007: val_loss did not improve from 0.11781\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.5600 - val_loss: 0.1181\n",
      "Epoch 8/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.5380\n",
      "Epoch 00008: val_loss improved from 0.11781 to 0.11162, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.5379 - val_loss: 0.1116\n",
      "Epoch 9/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.4966\n",
      "Epoch 00009: val_loss did not improve from 0.11162\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.4953 - val_loss: 0.1120\n",
      "Epoch 10/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.4704\n",
      "Epoch 00010: val_loss improved from 0.11162 to 0.11027, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.4701 - val_loss: 0.1103\n",
      "Epoch 11/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.4261\n",
      "Epoch 00011: val_loss improved from 0.11027 to 0.10359, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.4253 - val_loss: 0.1036\n",
      "Epoch 12/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.3868\n",
      "Epoch 00012: val_loss did not improve from 0.10359\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.3854 - val_loss: 0.1122\n",
      "Epoch 13/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.3740\n",
      "Epoch 00013: val_loss improved from 0.10359 to 0.10148, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.3731 - val_loss: 0.1015\n",
      "Epoch 14/100\n",
      "261/279 [===========================>..] - ETA: 0s - loss: 0.3472\n",
      "Epoch 00014: val_loss did not improve from 0.10148\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.3453 - val_loss: 0.1103\n",
      "Epoch 15/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.3279\n",
      "Epoch 00015: val_loss did not improve from 0.10148\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.3263 - val_loss: 0.1064\n",
      " ###7 fold : val acc1 0.566, acc3 0.969, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/279 [============================>.] - ETA: 0s - loss: 7.1018\n",
      "Epoch 00001: val_loss improved from inf to 0.57012, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 7.0271 - val_loss: 0.5701\n",
      "Epoch 2/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 1.0622\n",
      "Epoch 00002: val_loss improved from 0.57012 to 0.16949, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.0601 - val_loss: 0.1695\n",
      "Epoch 3/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.8402\n",
      "Epoch 00003: val_loss improved from 0.16949 to 0.13468, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8414 - val_loss: 0.1347\n",
      "Epoch 4/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.7100\n",
      "Epoch 00004: val_loss improved from 0.13468 to 0.12044, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.7073 - val_loss: 0.1204\n",
      "Epoch 5/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.6235\n",
      "Epoch 00005: val_loss did not improve from 0.12044\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.6237 - val_loss: 0.1359\n",
      "Epoch 6/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.5833\n",
      "Epoch 00006: val_loss did not improve from 0.12044\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.5816 - val_loss: 0.1213\n",
      " ###8 fold : val acc1 0.557, acc3 0.964, mae 0.240###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/279 [===========================>..] - ETA: 0s - loss: 7.2912\n",
      "Epoch 00001: val_loss improved from inf to 0.57291, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 7.0271 - val_loss: 0.5729\n",
      "Epoch 2/100\n",
      "262/279 [===========================>..] - ETA: 0s - loss: 1.0702\n",
      "Epoch 00002: val_loss improved from 0.57291 to 0.17335, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.0601 - val_loss: 0.1733\n",
      "Epoch 3/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.8420\n",
      "Epoch 00003: val_loss improved from 0.17335 to 0.13884, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.8414 - val_loss: 0.1388\n",
      "Epoch 4/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.7069\n",
      "Epoch 00004: val_loss improved from 0.13884 to 0.12303, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,lr0.002/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.7073 - val_loss: 0.1230\n",
      "Epoch 5/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.6232\n",
      "Epoch 00005: val_loss did not improve from 0.12303\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.6237 - val_loss: 0.1398\n",
      "Epoch 6/100\n",
      "262/279 [===========================>..] - ETA: 0s - loss: 0.5864\n",
      "Epoch 00006: val_loss did not improve from 0.12303\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.5816 - val_loss: 0.1242\n",
      " ###9 fold : val acc1 0.559, acc3 0.970, mae 0.236###\n",
      "acc10.569_acc30.970\n",
      "random search 66/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 14.8005\n",
      "Epoch 00001: val_loss improved from inf to 5.52707, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 1s 6ms/step - loss: 14.8005 - val_loss: 5.5271\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 3.9805\n",
      "Epoch 00002: val_loss improved from 5.52707 to 1.77486, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.9805 - val_loss: 1.7749\n",
      "Epoch 3/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.9569\n",
      "Epoch 00003: val_loss improved from 1.77486 to 0.63091, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9354 - val_loss: 0.6309\n",
      "Epoch 4/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.2576\n",
      "Epoch 00004: val_loss improved from 0.63091 to 0.32474, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2547 - val_loss: 0.3247\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0142\n",
      "Epoch 00005: val_loss improved from 0.32474 to 0.22997, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0142 - val_loss: 0.2300\n",
      "Epoch 6/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8944\n",
      "Epoch 00006: val_loss improved from 0.22997 to 0.19653, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8938 - val_loss: 0.1965\n",
      "Epoch 7/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7855\n",
      "Epoch 00007: val_loss improved from 0.19653 to 0.17862, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7854 - val_loss: 0.1786\n",
      "Epoch 8/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.7102\n",
      "Epoch 00008: val_loss improved from 0.17862 to 0.15151, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.7066 - val_loss: 0.1515\n",
      "Epoch 9/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.6399\n",
      "Epoch 00009: val_loss improved from 0.15151 to 0.13961, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6336 - val_loss: 0.1396\n",
      "Epoch 10/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.5801\n",
      "Epoch 00010: val_loss improved from 0.13961 to 0.13918, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5768 - val_loss: 0.1392\n",
      "Epoch 11/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.5362\n",
      "Epoch 00011: val_loss improved from 0.13918 to 0.12897, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5350 - val_loss: 0.1290\n",
      "Epoch 12/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.4917\n",
      "Epoch 00012: val_loss improved from 0.12897 to 0.12582, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_0.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4904 - val_loss: 0.1258\n",
      "Epoch 13/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.4552\n",
      "Epoch 00013: val_loss did not improve from 0.12582\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4546 - val_loss: 0.1266\n",
      "Epoch 14/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.4287\n",
      "Epoch 00014: val_loss did not improve from 0.12582\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4268 - val_loss: 0.1269\n",
      " ###0 fold : val acc1 0.542, acc3 0.956, mae 0.253###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/70 [=======================>......] - ETA: 0s - loss: 16.4298\n",
      "Epoch 00001: val_loss improved from inf to 5.51701, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 1s 6ms/step - loss: 14.7746 - val_loss: 5.5170\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 3.9911\n",
      "Epoch 00002: val_loss improved from 5.51701 to 1.76531, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.9911 - val_loss: 1.7653\n",
      "Epoch 3/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.9536\n",
      "Epoch 00003: val_loss improved from 1.76531 to 0.62880, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 1.9386 - val_loss: 0.6288\n",
      "Epoch 4/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 1.2554\n",
      "Epoch 00004: val_loss improved from 0.62880 to 0.32352, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 1.2425 - val_loss: 0.3235\n",
      "Epoch 5/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 1.0145\n",
      "Epoch 00005: val_loss improved from 0.32352 to 0.23251, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 1.0132 - val_loss: 0.2325\n",
      "Epoch 6/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.9049\n",
      "Epoch 00006: val_loss improved from 0.23251 to 0.19813, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.9026 - val_loss: 0.1981\n",
      "Epoch 7/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.7876\n",
      "Epoch 00007: val_loss improved from 0.19813 to 0.18088, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.7855 - val_loss: 0.1809\n",
      "Epoch 8/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.7033\n",
      "Epoch 00008: val_loss improved from 0.18088 to 0.14991, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.7002 - val_loss: 0.1499\n",
      "Epoch 9/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.6403\n",
      "Epoch 00009: val_loss improved from 0.14991 to 0.14006, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6368 - val_loss: 0.1401\n",
      "Epoch 10/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.5810\n",
      "Epoch 00010: val_loss improved from 0.14006 to 0.13917, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5771 - val_loss: 0.1392\n",
      "Epoch 11/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.5346\n",
      "Epoch 00011: val_loss improved from 0.13917 to 0.12782, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5333 - val_loss: 0.1278\n",
      "Epoch 12/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.4865\n",
      "Epoch 00012: val_loss improved from 0.12782 to 0.12436, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_1.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4871 - val_loss: 0.1244\n",
      "Epoch 13/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4575\n",
      "Epoch 00013: val_loss did not improve from 0.12436\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4573 - val_loss: 0.1255\n",
      "Epoch 14/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.4286\n",
      "Epoch 00014: val_loss did not improve from 0.12436\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4266 - val_loss: 0.1283\n",
      " ###1 fold : val acc1 0.545, acc3 0.967, mae 0.245###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 14.8514\n",
      "Epoch 00001: val_loss improved from inf to 5.52294, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 1s 6ms/step - loss: 14.7667 - val_loss: 5.5229\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 3.9816\n",
      "Epoch 00002: val_loss improved from 5.52294 to 1.76048, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.9816 - val_loss: 1.7605\n",
      "Epoch 3/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 2.0308\n",
      "Epoch 00003: val_loss improved from 1.76048 to 0.62408, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9303 - val_loss: 0.6241\n",
      "Epoch 4/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 1.2822\n",
      "Epoch 00004: val_loss improved from 0.62408 to 0.31652, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2398 - val_loss: 0.3165\n",
      "Epoch 5/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0133\n",
      "Epoch 00005: val_loss improved from 0.31652 to 0.22964, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0118 - val_loss: 0.2296\n",
      "Epoch 6/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8970\n",
      "Epoch 00006: val_loss improved from 0.22964 to 0.19477, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.8957 - val_loss: 0.1948\n",
      "Epoch 7/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.7875\n",
      "Epoch 00007: val_loss improved from 0.19477 to 0.17529, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.7845 - val_loss: 0.1753\n",
      "Epoch 8/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.6987\n",
      "Epoch 00008: val_loss improved from 0.17529 to 0.14976, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6967 - val_loss: 0.1498\n",
      "Epoch 9/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.6398\n",
      "Epoch 00009: val_loss improved from 0.14976 to 0.14049, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6375 - val_loss: 0.1405\n",
      "Epoch 10/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.5758\n",
      "Epoch 00010: val_loss did not improve from 0.14049\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5735 - val_loss: 0.1421\n",
      "Epoch 11/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.5325\n",
      "Epoch 00011: val_loss improved from 0.14049 to 0.12704, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5326 - val_loss: 0.1270\n",
      "Epoch 12/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.4901\n",
      "Epoch 00012: val_loss improved from 0.12704 to 0.12356, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_2.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4885 - val_loss: 0.1236\n",
      "Epoch 13/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.4575\n",
      "Epoch 00013: val_loss did not improve from 0.12356\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4560 - val_loss: 0.1264\n",
      "Epoch 14/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.4278\n",
      "Epoch 00014: val_loss did not improve from 0.12356\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4274 - val_loss: 0.1258\n",
      " ###2 fold : val acc1 0.543, acc3 0.961, mae 0.249###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/70 [======================>.......] - ETA: 0s - loss: 16.7445\n",
      "Epoch 00001: val_loss improved from inf to 5.49656, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 1s 6ms/step - loss: 14.8052 - val_loss: 5.4966\n",
      "Epoch 2/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 4.2749\n",
      "Epoch 00002: val_loss improved from 5.49656 to 1.76436, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.9940 - val_loss: 1.7644\n",
      "Epoch 3/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 2.0270\n",
      "Epoch 00003: val_loss improved from 1.76436 to 0.62730, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9488 - val_loss: 0.6273\n",
      "Epoch 4/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 1.2852\n",
      "Epoch 00004: val_loss improved from 0.62730 to 0.31766, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2439 - val_loss: 0.3177\n",
      "Epoch 5/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0148\n",
      "Epoch 00005: val_loss improved from 0.31766 to 0.22579, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0135 - val_loss: 0.2258\n",
      "Epoch 6/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8866\n",
      "Epoch 00006: val_loss improved from 0.22579 to 0.19534, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.8857 - val_loss: 0.1953\n",
      "Epoch 7/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.7876\n",
      "Epoch 00007: val_loss improved from 0.19534 to 0.17381, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.7840 - val_loss: 0.1738\n",
      "Epoch 8/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.6958\n",
      "Epoch 00008: val_loss improved from 0.17381 to 0.14964, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6939 - val_loss: 0.1496\n",
      "Epoch 9/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.6405\n",
      "Epoch 00009: val_loss improved from 0.14964 to 0.14086, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6388 - val_loss: 0.1409\n",
      "Epoch 10/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.5753\n",
      "Epoch 00010: val_loss did not improve from 0.14086\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5722 - val_loss: 0.1448\n",
      "Epoch 11/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.5342\n",
      "Epoch 00011: val_loss improved from 0.14086 to 0.12803, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5328 - val_loss: 0.1280\n",
      "Epoch 12/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4901\n",
      "Epoch 00012: val_loss improved from 0.12803 to 0.12494, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_3.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4898 - val_loss: 0.1249\n",
      "Epoch 13/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4622\n",
      "Epoch 00013: val_loss did not improve from 0.12494\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4595 - val_loss: 0.1257\n",
      "Epoch 14/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.4295\n",
      "Epoch 00014: val_loss did not improve from 0.12494\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4303 - val_loss: 0.1258\n",
      " ###3 fold : val acc1 0.556, acc3 0.956, mae 0.245###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/70 [=======================>......] - ETA: 0s - loss: 16.4647\n",
      "Epoch 00001: val_loss improved from inf to 5.48192, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 14.6906 - val_loss: 5.4819\n",
      "Epoch 2/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 4.2133\n",
      "Epoch 00002: val_loss improved from 5.48192 to 1.75010, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.9679 - val_loss: 1.7501\n",
      "Epoch 3/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.9695\n",
      "Epoch 00003: val_loss improved from 1.75010 to 0.61858, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9536 - val_loss: 0.6186\n",
      "Epoch 4/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.2706\n",
      "Epoch 00004: val_loss improved from 0.61858 to 0.32142, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2637 - val_loss: 0.3214\n",
      "Epoch 5/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0021\n",
      "Epoch 00005: val_loss improved from 0.32142 to 0.23246, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0019 - val_loss: 0.2325\n",
      "Epoch 6/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.8958\n",
      "Epoch 00006: val_loss improved from 0.23246 to 0.20071, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8873 - val_loss: 0.2007\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.7868\n",
      "Epoch 00007: val_loss improved from 0.20071 to 0.17542, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7868 - val_loss: 0.1754\n",
      "Epoch 8/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.6996\n",
      "Epoch 00008: val_loss improved from 0.17542 to 0.16805, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6978 - val_loss: 0.1680\n",
      "Epoch 9/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.6386\n",
      "Epoch 00009: val_loss improved from 0.16805 to 0.14950, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6334 - val_loss: 0.1495\n",
      "Epoch 10/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.5902\n",
      "Epoch 00010: val_loss improved from 0.14950 to 0.13491, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5867 - val_loss: 0.1349\n",
      "Epoch 11/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.5241\n",
      "Epoch 00011: val_loss improved from 0.13491 to 0.12637, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5219 - val_loss: 0.1264\n",
      "Epoch 12/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.4900\n",
      "Epoch 00012: val_loss did not improve from 0.12637\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4896 - val_loss: 0.1275\n",
      "Epoch 13/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.4498\n",
      "Epoch 00013: val_loss improved from 0.12637 to 0.11979, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4537 - val_loss: 0.1198\n",
      "Epoch 14/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4308\n",
      "Epoch 00014: val_loss did not improve from 0.11979\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4292 - val_loss: 0.1223\n",
      "Epoch 15/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.4031\n",
      "Epoch 00015: val_loss improved from 0.11979 to 0.10994, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_4.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4004 - val_loss: 0.1099\n",
      "Epoch 16/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.3869\n",
      "Epoch 00016: val_loss did not improve from 0.10994\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3838 - val_loss: 0.1166\n",
      "Epoch 17/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.3705\n",
      "Epoch 00017: val_loss did not improve from 0.10994\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3683 - val_loss: 0.1166\n",
      " ###4 fold : val acc1 0.558, acc3 0.964, mae 0.239###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/70 [=======================>......] - ETA: 0s - loss: 16.4628\n",
      "Epoch 00001: val_loss improved from inf to 5.46239, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 14.6955 - val_loss: 5.4624\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 3.9762\n",
      "Epoch 00002: val_loss improved from 5.46239 to 1.73675, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.9762 - val_loss: 1.7367\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.9551\n",
      "Epoch 00003: val_loss improved from 1.73675 to 0.61227, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9551 - val_loss: 0.6123\n",
      "Epoch 4/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2565\n",
      "Epoch 00004: val_loss improved from 0.61227 to 0.31722, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2538 - val_loss: 0.3172\n",
      "Epoch 5/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0011\n",
      "Epoch 00005: val_loss improved from 0.31722 to 0.22865, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0014 - val_loss: 0.2286\n",
      "Epoch 6/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.8863\n",
      "Epoch 00006: val_loss improved from 0.22865 to 0.20276, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8868 - val_loss: 0.2028\n",
      "Epoch 7/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.7966\n",
      "Epoch 00007: val_loss improved from 0.20276 to 0.17519, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7876 - val_loss: 0.1752\n",
      "Epoch 8/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.7063\n",
      "Epoch 00008: val_loss improved from 0.17519 to 0.16563, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6979 - val_loss: 0.1656\n",
      "Epoch 9/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.6403\n",
      "Epoch 00009: val_loss improved from 0.16563 to 0.14938, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6287 - val_loss: 0.1494\n",
      "Epoch 10/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5870\n",
      "Epoch 00010: val_loss improved from 0.14938 to 0.13721, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5864 - val_loss: 0.1372\n",
      "Epoch 11/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5215\n",
      "Epoch 00011: val_loss improved from 0.13721 to 0.12777, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5209 - val_loss: 0.1278\n",
      "Epoch 12/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4897\n",
      "Epoch 00012: val_loss improved from 0.12777 to 0.12674, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4882 - val_loss: 0.1267\n",
      "Epoch 13/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4500\n",
      "Epoch 00013: val_loss improved from 0.12674 to 0.11822, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4520 - val_loss: 0.1182\n",
      "Epoch 14/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4279\n",
      "Epoch 00014: val_loss did not improve from 0.11822\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4261 - val_loss: 0.1215\n",
      "Epoch 15/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.4044\n",
      "Epoch 00015: val_loss improved from 0.11822 to 0.11037, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_5.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4019 - val_loss: 0.1104\n",
      "Epoch 16/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3858\n",
      "Epoch 00016: val_loss did not improve from 0.11037\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3837 - val_loss: 0.1199\n",
      "Epoch 17/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.3674\n",
      "Epoch 00017: val_loss did not improve from 0.11037\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3661 - val_loss: 0.1152\n",
      " ###5 fold : val acc1 0.548, acc3 0.967, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 14.7077\n",
      "Epoch 00001: val_loss improved from inf to 5.47061, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 1s 6ms/step - loss: 14.7077 - val_loss: 5.4706\n",
      "Epoch 2/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 4.2609\n",
      "Epoch 00002: val_loss improved from 5.47061 to 1.73580, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.9727 - val_loss: 1.7358\n",
      "Epoch 3/100\n",
      "55/70 [======================>.......] - ETA: 0s - loss: 2.0678\n",
      "Epoch 00003: val_loss improved from 1.73580 to 0.61207, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9635 - val_loss: 0.6121\n",
      "Epoch 4/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 1.2930\n",
      "Epoch 00004: val_loss improved from 0.61207 to 0.31458, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2548 - val_loss: 0.3146\n",
      "Epoch 5/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0083\n",
      "Epoch 00005: val_loss improved from 0.31458 to 0.23300, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0083 - val_loss: 0.2330\n",
      "Epoch 6/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.8960\n",
      "Epoch 00006: val_loss improved from 0.23300 to 0.20193, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8912 - val_loss: 0.2019\n",
      "Epoch 7/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.7967\n",
      "Epoch 00007: val_loss improved from 0.20193 to 0.17358, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7970 - val_loss: 0.1736\n",
      "Epoch 8/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.7011\n",
      "Epoch 00008: val_loss improved from 0.17358 to 0.16397, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6986 - val_loss: 0.1640\n",
      "Epoch 9/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.6354\n",
      "Epoch 00009: val_loss improved from 0.16397 to 0.14740, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6323 - val_loss: 0.1474\n",
      "Epoch 10/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.5906\n",
      "Epoch 00010: val_loss improved from 0.14740 to 0.13776, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5850 - val_loss: 0.1378\n",
      "Epoch 11/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.5221\n",
      "Epoch 00011: val_loss improved from 0.13776 to 0.13148, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5174 - val_loss: 0.1315\n",
      "Epoch 12/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.4939\n",
      "Epoch 00012: val_loss improved from 0.13148 to 0.12958, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4919 - val_loss: 0.1296\n",
      "Epoch 13/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.4515\n",
      "Epoch 00013: val_loss improved from 0.12958 to 0.11901, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4542 - val_loss: 0.1190\n",
      "Epoch 14/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.4307\n",
      "Epoch 00014: val_loss did not improve from 0.11901\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4282 - val_loss: 0.1205\n",
      "Epoch 15/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.4073\n",
      "Epoch 00015: val_loss improved from 0.11901 to 0.11114, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_6.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4038 - val_loss: 0.1111\n",
      "Epoch 16/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.3902\n",
      "Epoch 00016: val_loss did not improve from 0.11114\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3871 - val_loss: 0.1195\n",
      "Epoch 17/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.3708\n",
      "Epoch 00017: val_loss did not improve from 0.11114\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3692 - val_loss: 0.1128\n",
      " ###6 fold : val acc1 0.567, acc3 0.973, mae 0.230###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/70 [======================>.......] - ETA: 0s - loss: 16.6067\n",
      "Epoch 00001: val_loss improved from inf to 5.46483, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 14.7045 - val_loss: 5.4648\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 3.9752\n",
      "Epoch 00002: val_loss improved from 5.46483 to 1.72954, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.9752 - val_loss: 1.7295\n",
      "Epoch 3/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 1.9921\n",
      "Epoch 00003: val_loss improved from 1.72954 to 0.61992, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 1.9562 - val_loss: 0.6199\n",
      "Epoch 4/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 1.2654\n",
      "Epoch 00004: val_loss improved from 0.61992 to 0.31802, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 1.2508 - val_loss: 0.3180\n",
      "Epoch 5/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.0047\n",
      "Epoch 00005: val_loss improved from 0.31802 to 0.23196, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0030 - val_loss: 0.2320\n",
      "Epoch 6/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8892\n",
      "Epoch 00006: val_loss improved from 0.23196 to 0.20281, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8849 - val_loss: 0.2028\n",
      "Epoch 7/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8013\n",
      "Epoch 00007: val_loss improved from 0.20281 to 0.17613, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8022 - val_loss: 0.1761\n",
      "Epoch 8/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.6979\n",
      "Epoch 00008: val_loss improved from 0.17613 to 0.16410, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6924 - val_loss: 0.1641\n",
      "Epoch 9/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.6392\n",
      "Epoch 00009: val_loss improved from 0.16410 to 0.14616, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6331 - val_loss: 0.1462\n",
      "Epoch 10/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.5835\n",
      "Epoch 00010: val_loss improved from 0.14616 to 0.13863, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5798 - val_loss: 0.1386\n",
      "Epoch 11/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.5258\n",
      "Epoch 00011: val_loss improved from 0.13863 to 0.13010, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5180 - val_loss: 0.1301\n",
      "Epoch 12/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.4852\n",
      "Epoch 00012: val_loss improved from 0.13010 to 0.12711, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4821 - val_loss: 0.1271\n",
      "Epoch 13/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.4553\n",
      "Epoch 00013: val_loss improved from 0.12711 to 0.11570, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4557 - val_loss: 0.1157\n",
      "Epoch 14/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.4309\n",
      "Epoch 00014: val_loss did not improve from 0.11570\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4281 - val_loss: 0.1211\n",
      "Epoch 15/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.4043\n",
      "Epoch 00015: val_loss improved from 0.11570 to 0.11041, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_7.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4008 - val_loss: 0.1104\n",
      "Epoch 16/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.3889\n",
      "Epoch 00016: val_loss did not improve from 0.11041\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3867 - val_loss: 0.1193\n",
      "Epoch 17/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.3699\n",
      "Epoch 00017: val_loss did not improve from 0.11041\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3689 - val_loss: 0.1137\n",
      " ###7 fold : val acc1 0.550, acc3 0.962, mae 0.244###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/70 [============================>.] - ETA: 0s - loss: 14.8132\n",
      "Epoch 00001: val_loss improved from inf to 5.41939, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 1s 6ms/step - loss: 14.7392 - val_loss: 5.4194\n",
      "Epoch 2/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 4.2466\n",
      "Epoch 00002: val_loss improved from 5.41939 to 1.73207, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.9823 - val_loss: 1.7321\n",
      "Epoch 3/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 1.9981\n",
      "Epoch 00003: val_loss improved from 1.73207 to 0.62576, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 1.9574 - val_loss: 0.6258\n",
      "Epoch 4/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 1.2565\n",
      "Epoch 00004: val_loss improved from 0.62576 to 0.32274, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.2509 - val_loss: 0.3227\n",
      "Epoch 5/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.0081\n",
      "Epoch 00005: val_loss improved from 0.32274 to 0.23494, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 1.0054 - val_loss: 0.2349\n",
      "Epoch 6/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.8911\n",
      "Epoch 00006: val_loss improved from 0.23494 to 0.20535, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.8839 - val_loss: 0.2053\n",
      "Epoch 7/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.8024\n",
      "Epoch 00007: val_loss improved from 0.20535 to 0.18111, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.8030 - val_loss: 0.1811\n",
      "Epoch 8/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.6987\n",
      "Epoch 00008: val_loss improved from 0.18111 to 0.16899, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6938 - val_loss: 0.1690\n",
      "Epoch 9/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.6380\n",
      "Epoch 00009: val_loss improved from 0.16899 to 0.15079, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6355 - val_loss: 0.1508\n",
      "Epoch 10/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.5835\n",
      "Epoch 00010: val_loss improved from 0.15079 to 0.14237, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5805 - val_loss: 0.1424\n",
      "Epoch 11/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.5226\n",
      "Epoch 00011: val_loss improved from 0.14237 to 0.13508, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5206 - val_loss: 0.1351\n",
      "Epoch 12/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.4847\n",
      "Epoch 00012: val_loss improved from 0.13508 to 0.13129, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4823 - val_loss: 0.1313\n",
      "Epoch 13/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.4567\n",
      "Epoch 00013: val_loss improved from 0.13129 to 0.11932, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4570 - val_loss: 0.1193\n",
      "Epoch 14/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.4305\n",
      "Epoch 00014: val_loss did not improve from 0.11932\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4280 - val_loss: 0.1255\n",
      "Epoch 15/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.4036\n",
      "Epoch 00015: val_loss improved from 0.11932 to 0.11417, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4008 - val_loss: 0.1142\n",
      "Epoch 16/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.3887\n",
      "Epoch 00016: val_loss did not improve from 0.11417\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3867 - val_loss: 0.1234\n",
      "Epoch 17/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.3723\n",
      "Epoch 00017: val_loss did not improve from 0.11417\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3694 - val_loss: 0.1175\n",
      " ###8 fold : val acc1 0.551, acc3 0.969, mae 0.240###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/70 [======================>.......] - ETA: 0s - loss: 16.6518\n",
      "Epoch 00001: val_loss improved from inf to 5.44351, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 1s 5ms/step - loss: 14.7392 - val_loss: 5.4435\n",
      "Epoch 2/100\n",
      "56/70 [=======================>......] - ETA: 0s - loss: 4.2634\n",
      "Epoch 00002: val_loss improved from 5.44351 to 1.72713, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.9823 - val_loss: 1.7271\n",
      "Epoch 3/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.9609\n",
      "Epoch 00003: val_loss improved from 1.72713 to 0.63083, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.9574 - val_loss: 0.6308\n",
      "Epoch 4/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.2638\n",
      "Epoch 00004: val_loss improved from 0.63083 to 0.33010, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 1.2509 - val_loss: 0.3301\n",
      "Epoch 5/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 1.0114\n",
      "Epoch 00005: val_loss improved from 0.33010 to 0.23981, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.0054 - val_loss: 0.2398\n",
      "Epoch 6/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8883\n",
      "Epoch 00006: val_loss improved from 0.23981 to 0.21144, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8839 - val_loss: 0.2114\n",
      "Epoch 7/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.8018\n",
      "Epoch 00007: val_loss improved from 0.21144 to 0.18673, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8030 - val_loss: 0.1867\n",
      "Epoch 8/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.6987\n",
      "Epoch 00008: val_loss improved from 0.18673 to 0.17370, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6938 - val_loss: 0.1737\n",
      "Epoch 9/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.6390\n",
      "Epoch 00009: val_loss improved from 0.17370 to 0.15461, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6355 - val_loss: 0.1546\n",
      "Epoch 10/100\n",
      "54/70 [======================>.......] - ETA: 0s - loss: 0.5875\n",
      "Epoch 00010: val_loss improved from 0.15461 to 0.14683, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5805 - val_loss: 0.1468\n",
      "Epoch 11/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.5249\n",
      "Epoch 00011: val_loss improved from 0.14683 to 0.13917, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5206 - val_loss: 0.1392\n",
      "Epoch 12/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.4846\n",
      "Epoch 00012: val_loss improved from 0.13917 to 0.13549, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4823 - val_loss: 0.1355\n",
      "Epoch 13/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.4567\n",
      "Epoch 00013: val_loss improved from 0.13549 to 0.12237, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4570 - val_loss: 0.1224\n",
      "Epoch 14/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.4311\n",
      "Epoch 00014: val_loss did not improve from 0.12237\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4280 - val_loss: 0.1291\n",
      "Epoch 15/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.4044\n",
      "Epoch 00015: val_loss improved from 0.12237 to 0.11729, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0.2,dnodes64_dropout0.2,lr0.001/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4008 - val_loss: 0.1173\n",
      "Epoch 16/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.3887\n",
      "Epoch 00016: val_loss did not improve from 0.11729\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3867 - val_loss: 0.1271\n",
      "Epoch 17/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.3709\n",
      "Epoch 00017: val_loss did not improve from 0.11729\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3694 - val_loss: 0.1204\n",
      " ###9 fold : val acc1 0.567, acc3 0.975, mae 0.229###\n",
      "acc10.553_acc30.965\n",
      "random search 67/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 19.0984 \n",
      "Epoch 00001: val_loss improved from inf to 13.20161, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 17.5726 - val_loss: 13.2016\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 10.6460\n",
      "Epoch 00002: val_loss improved from 13.20161 to 4.18867, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 8.8423 - val_loss: 4.1887\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.3877\n",
      "Epoch 00003: val_loss improved from 4.18867 to 1.53113, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9425 - val_loss: 1.5311\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.9201\n",
      "Epoch 00004: val_loss improved from 1.53113 to 0.86932, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.7864 - val_loss: 0.8693\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.3445\n",
      "Epoch 00005: val_loss improved from 0.86932 to 0.50467, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2791 - val_loss: 0.5047\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0355\n",
      "Epoch 00006: val_loss improved from 0.50467 to 0.32364, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9992 - val_loss: 0.3236\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8651\n",
      "Epoch 00007: val_loss improved from 0.32364 to 0.24188, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8499 - val_loss: 0.2419\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8058\n",
      "Epoch 00008: val_loss improved from 0.24188 to 0.19568, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7929 - val_loss: 0.1957\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7518\n",
      "Epoch 00009: val_loss improved from 0.19568 to 0.18122, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7371 - val_loss: 0.1812\n",
      "Epoch 10/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7048\n",
      "Epoch 00010: val_loss improved from 0.18122 to 0.16508, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6942 - val_loss: 0.1651\n",
      "Epoch 11/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7000\n",
      "Epoch 00011: val_loss improved from 0.16508 to 0.16003, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6855 - val_loss: 0.1600\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6636\n",
      "Epoch 00012: val_loss improved from 0.16003 to 0.15481, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6586 - val_loss: 0.1548\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6417\n",
      "Epoch 00013: val_loss improved from 0.15481 to 0.15212, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6482 - val_loss: 0.1521\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6283\n",
      "Epoch 00014: val_loss improved from 0.15212 to 0.15117, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6273 - val_loss: 0.1512\n",
      "Epoch 15/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6289\n",
      "Epoch 00015: val_loss improved from 0.15117 to 0.14377, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6212 - val_loss: 0.1438\n",
      "Epoch 16/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6083\n",
      "Epoch 00016: val_loss improved from 0.14377 to 0.14079, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6130 - val_loss: 0.1408\n",
      "Epoch 17/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6079\n",
      "Epoch 00017: val_loss did not improve from 0.14079\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6010 - val_loss: 0.1425\n",
      "Epoch 18/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5951\n",
      "Epoch 00018: val_loss improved from 0.14079 to 0.13261, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5944 - val_loss: 0.1326\n",
      "Epoch 19/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5800\n",
      "Epoch 00019: val_loss improved from 0.13261 to 0.12949, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5809 - val_loss: 0.1295\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5823\n",
      "Epoch 00020: val_loss improved from 0.12949 to 0.12569, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5823 - val_loss: 0.1257\n",
      "Epoch 21/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.5771\n",
      "Epoch 00021: val_loss did not improve from 0.12569\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5791 - val_loss: 0.1257\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5741\n",
      "Epoch 00022: val_loss did not improve from 0.12569\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5741 - val_loss: 0.1262\n",
      " ###0 fold : val acc1 0.541, acc3 0.964, mae 0.248###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 19.1001 \n",
      "Epoch 00001: val_loss improved from inf to 13.19974, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 17.5730 - val_loss: 13.1997\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 10.6483\n",
      "Epoch 00002: val_loss improved from 13.19974 to 4.19654, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 8.8537 - val_loss: 4.1965\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.4005\n",
      "Epoch 00003: val_loss improved from 4.19654 to 1.52800, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9469 - val_loss: 1.5280\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.9149\n",
      "Epoch 00004: val_loss improved from 1.52800 to 0.86353, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.7786 - val_loss: 0.8635\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.3458\n",
      "Epoch 00005: val_loss improved from 0.86353 to 0.50221, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2732 - val_loss: 0.5022\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0331\n",
      "Epoch 00006: val_loss improved from 0.50221 to 0.32257, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9951 - val_loss: 0.3226\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8640\n",
      "Epoch 00007: val_loss improved from 0.32257 to 0.24342, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8443 - val_loss: 0.2434\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8030\n",
      "Epoch 00008: val_loss improved from 0.24342 to 0.19540, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7894 - val_loss: 0.1954\n",
      "Epoch 9/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7464\n",
      "Epoch 00009: val_loss improved from 0.19540 to 0.18157, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7388 - val_loss: 0.1816\n",
      "Epoch 10/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7005\n",
      "Epoch 00010: val_loss improved from 0.18157 to 0.16481, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6911 - val_loss: 0.1648\n",
      "Epoch 11/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6948\n",
      "Epoch 00011: val_loss improved from 0.16481 to 0.15992, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6825 - val_loss: 0.1599\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6729\n",
      "Epoch 00012: val_loss improved from 0.15992 to 0.15474, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6631 - val_loss: 0.1547\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6391\n",
      "Epoch 00013: val_loss improved from 0.15474 to 0.15170, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6449 - val_loss: 0.1517\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6254\n",
      "Epoch 00014: val_loss improved from 0.15170 to 0.14937, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6254 - val_loss: 0.1494\n",
      "Epoch 15/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6245\n",
      "Epoch 00015: val_loss improved from 0.14937 to 0.14380, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6182 - val_loss: 0.1438\n",
      "Epoch 16/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6021\n",
      "Epoch 00016: val_loss improved from 0.14380 to 0.14071, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6070 - val_loss: 0.1407\n",
      "Epoch 17/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5987\n",
      "Epoch 00017: val_loss did not improve from 0.14071\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5981 - val_loss: 0.1416\n",
      "Epoch 18/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5947\n",
      "Epoch 00018: val_loss improved from 0.14071 to 0.13155, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5937 - val_loss: 0.1316\n",
      "Epoch 19/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5734\n",
      "Epoch 00019: val_loss improved from 0.13155 to 0.13003, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5745 - val_loss: 0.1300\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5773\n",
      "Epoch 00020: val_loss improved from 0.13003 to 0.12561, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5773 - val_loss: 0.1256\n",
      "Epoch 21/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5751\n",
      "Epoch 00021: val_loss did not improve from 0.12561\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5763 - val_loss: 0.1266\n",
      "Epoch 22/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5771\n",
      "Epoch 00022: val_loss did not improve from 0.12561\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5759 - val_loss: 0.1271\n",
      " ###1 fold : val acc1 0.539, acc3 0.961, mae 0.251###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 19.1720 \n",
      "Epoch 00001: val_loss improved from inf to 13.19474, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 17.5481 - val_loss: 13.1947\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 10.7444\n",
      "Epoch 00002: val_loss improved from 13.19474 to 4.19290, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 8.8278 - val_loss: 4.1929\n",
      "Epoch 3/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 3.4431\n",
      "Epoch 00003: val_loss improved from 4.19290 to 1.52488, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9662 - val_loss: 1.5249\n",
      "Epoch 4/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.9167\n",
      "Epoch 00004: val_loss improved from 1.52488 to 0.86211, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7895 - val_loss: 0.8621\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.3439\n",
      "Epoch 00005: val_loss improved from 0.86211 to 0.49922, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2713 - val_loss: 0.4992\n",
      "Epoch 6/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.0390\n",
      "Epoch 00006: val_loss improved from 0.49922 to 0.31985, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9942 - val_loss: 0.3199\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8686\n",
      "Epoch 00007: val_loss improved from 0.31985 to 0.24273, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8476 - val_loss: 0.2427\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8010\n",
      "Epoch 00008: val_loss improved from 0.24273 to 0.19374, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7847 - val_loss: 0.1937\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7478\n",
      "Epoch 00009: val_loss improved from 0.19374 to 0.18017, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7345 - val_loss: 0.1802\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7062\n",
      "Epoch 00010: val_loss improved from 0.18017 to 0.16474, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6922 - val_loss: 0.1647\n",
      "Epoch 11/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6928\n",
      "Epoch 00011: val_loss improved from 0.16474 to 0.16086, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6813 - val_loss: 0.1609\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6717\n",
      "Epoch 00012: val_loss improved from 0.16086 to 0.15522, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6634 - val_loss: 0.1552\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6390\n",
      "Epoch 00013: val_loss improved from 0.15522 to 0.15149, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6451 - val_loss: 0.1515\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6228\n",
      "Epoch 00014: val_loss improved from 0.15149 to 0.15037, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6231 - val_loss: 0.1504\n",
      "Epoch 15/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6275\n",
      "Epoch 00015: val_loss improved from 0.15037 to 0.14411, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6199 - val_loss: 0.1441\n",
      "Epoch 16/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.6084\n",
      "Epoch 00016: val_loss improved from 0.14411 to 0.14347, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6077 - val_loss: 0.1435\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5955\n",
      "Epoch 00017: val_loss improved from 0.14347 to 0.14268, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5955 - val_loss: 0.1427\n",
      "Epoch 18/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5944\n",
      "Epoch 00018: val_loss improved from 0.14268 to 0.13160, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5932 - val_loss: 0.1316\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00019: val_loss did not improve from 0.13160\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5705 - val_loss: 0.1319\n",
      "Epoch 20/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5777\n",
      "Epoch 00020: val_loss improved from 0.13160 to 0.12538, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5782 - val_loss: 0.1254\n",
      "Epoch 21/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5748\n",
      "Epoch 00021: val_loss did not improve from 0.12538\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5764 - val_loss: 0.1264\n",
      "Epoch 22/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.5800\n",
      "Epoch 00022: val_loss did not improve from 0.12538\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5809 - val_loss: 0.1263\n",
      " ###2 fold : val acc1 0.542, acc3 0.961, mae 0.249###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 19.1908 \n",
      "Epoch 00001: val_loss improved from inf to 13.19114, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 17.6014 - val_loss: 13.1911\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 10.7731\n",
      "Epoch 00002: val_loss improved from 13.19114 to 4.18508, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 8.8595 - val_loss: 4.1851\n",
      "Epoch 3/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 3.4602\n",
      "Epoch 00003: val_loss improved from 4.18508 to 1.52219, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9766 - val_loss: 1.5222\n",
      "Epoch 4/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.9098\n",
      "Epoch 00004: val_loss improved from 1.52219 to 0.86159, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.7797 - val_loss: 0.8616\n",
      "Epoch 5/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.3468\n",
      "Epoch 00005: val_loss improved from 0.86159 to 0.49849, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2702 - val_loss: 0.4985\n",
      "Epoch 6/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.0538\n",
      "Epoch 00006: val_loss improved from 0.49849 to 0.31905, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0007 - val_loss: 0.3191\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8699\n",
      "Epoch 00007: val_loss improved from 0.31905 to 0.24152, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8493 - val_loss: 0.2415\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8026\n",
      "Epoch 00008: val_loss improved from 0.24152 to 0.19204, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7897 - val_loss: 0.1920\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7508\n",
      "Epoch 00009: val_loss improved from 0.19204 to 0.17951, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7365 - val_loss: 0.1795\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7081\n",
      "Epoch 00010: val_loss improved from 0.17951 to 0.16457, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6945 - val_loss: 0.1646\n",
      "Epoch 11/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6858\n",
      "Epoch 00011: val_loss improved from 0.16457 to 0.16003, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6845 - val_loss: 0.1600\n",
      "Epoch 12/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.6637\n",
      "Epoch 00012: val_loss improved from 0.16003 to 0.15406, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6628 - val_loss: 0.1541\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6469\n",
      "Epoch 00013: val_loss improved from 0.15406 to 0.15020, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6472 - val_loss: 0.1502\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6219\n",
      "Epoch 00014: val_loss improved from 0.15020 to 0.14884, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6211 - val_loss: 0.1488\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6216\n",
      "Epoch 00015: val_loss improved from 0.14884 to 0.14396, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6216 - val_loss: 0.1440\n",
      "Epoch 16/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.6086\n",
      "Epoch 00016: val_loss improved from 0.14396 to 0.14370, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6092 - val_loss: 0.1437\n",
      "Epoch 17/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5953\n",
      "Epoch 00017: val_loss improved from 0.14370 to 0.14165, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5947 - val_loss: 0.1417\n",
      "Epoch 18/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.5890\n",
      "Epoch 00018: val_loss improved from 0.14165 to 0.13116, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5876 - val_loss: 0.1312\n",
      "Epoch 19/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00019: val_loss did not improve from 0.13116\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5710 - val_loss: 0.1314\n",
      "Epoch 20/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.5809\n",
      "Epoch 00020: val_loss improved from 0.13116 to 0.12567, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5806 - val_loss: 0.1257\n",
      "Epoch 21/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.5760\n",
      "Epoch 00021: val_loss did not improve from 0.12567\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5785 - val_loss: 0.1273\n",
      "Epoch 22/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5786\n",
      "Epoch 00022: val_loss did not improve from 0.12567\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5774 - val_loss: 0.1269\n",
      " ###3 fold : val acc1 0.547, acc3 0.956, mae 0.249###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 18.9446 \n",
      "Epoch 00001: val_loss improved from inf to 13.19123, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 17.5551 - val_loss: 13.1912\n",
      "Epoch 2/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 10.5407\n",
      "Epoch 00002: val_loss improved from 13.19123 to 4.18527, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 8.8583 - val_loss: 4.1853\n",
      "Epoch 3/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 3.2968\n",
      "Epoch 00003: val_loss improved from 4.18527 to 1.53148, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.9259 - val_loss: 1.5315\n",
      "Epoch 4/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 1.9256\n",
      "Epoch 00004: val_loss improved from 1.53148 to 0.85670, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7888 - val_loss: 0.8567\n",
      "Epoch 5/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 1.3219\n",
      "Epoch 00005: val_loss improved from 0.85670 to 0.49780, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2658 - val_loss: 0.4978\n",
      "Epoch 6/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 1.0150\n",
      "Epoch 00006: val_loss improved from 0.49780 to 0.31950, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.9880 - val_loss: 0.3195\n",
      "Epoch 7/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8833\n",
      "Epoch 00007: val_loss improved from 0.31950 to 0.23521, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8535 - val_loss: 0.2352\n",
      "Epoch 8/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7918\n",
      "Epoch 00008: val_loss improved from 0.23521 to 0.20224, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7686 - val_loss: 0.2022\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7419\n",
      "Epoch 00009: val_loss improved from 0.20224 to 0.17554, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7434 - val_loss: 0.1755\n",
      "Epoch 10/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7008\n",
      "Epoch 00010: val_loss improved from 0.17554 to 0.17026, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6839 - val_loss: 0.1703\n",
      "Epoch 11/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.6812\n",
      "Epoch 00011: val_loss improved from 0.17026 to 0.15890, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6790 - val_loss: 0.1589\n",
      "Epoch 12/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6552\n",
      "Epoch 00012: val_loss improved from 0.15890 to 0.15523, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6537 - val_loss: 0.1552\n",
      "Epoch 13/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6534\n",
      "Epoch 00013: val_loss improved from 0.15523 to 0.14860, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6492 - val_loss: 0.1486\n",
      "Epoch 14/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6508\n",
      "Epoch 00014: val_loss improved from 0.14860 to 0.14679, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6403 - val_loss: 0.1468\n",
      "Epoch 15/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6218\n",
      "Epoch 00015: val_loss improved from 0.14679 to 0.14360, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6204 - val_loss: 0.1436\n",
      "Epoch 16/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6255\n",
      "Epoch 00016: val_loss improved from 0.14360 to 0.14256, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6207 - val_loss: 0.1426\n",
      "Epoch 17/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6047\n",
      "Epoch 00017: val_loss improved from 0.14256 to 0.13537, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6031 - val_loss: 0.1354\n",
      "Epoch 18/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5884\n",
      "Epoch 00018: val_loss improved from 0.13537 to 0.12847, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5913 - val_loss: 0.1285\n",
      "Epoch 19/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5871\n",
      "Epoch 00019: val_loss did not improve from 0.12847\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5889 - val_loss: 0.1299\n",
      "Epoch 20/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5718\n",
      "Epoch 00020: val_loss improved from 0.12847 to 0.12704, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5683 - val_loss: 0.1270\n",
      "Epoch 21/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5623\n",
      "Epoch 00021: val_loss improved from 0.12704 to 0.12570, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5694 - val_loss: 0.1257\n",
      "Epoch 22/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5772\n",
      "Epoch 00022: val_loss did not improve from 0.12570\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5643 - val_loss: 0.1293\n",
      "Epoch 23/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5531\n",
      "Epoch 00023: val_loss improved from 0.12570 to 0.12057, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5541 - val_loss: 0.1206\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5449\n",
      "Epoch 00024: val_loss improved from 0.12057 to 0.11769, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5620 - val_loss: 0.1177\n",
      "Epoch 25/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.5589\n",
      "Epoch 00025: val_loss did not improve from 0.11769\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5555 - val_loss: 0.1235\n",
      "Epoch 26/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5385\n",
      "Epoch 00026: val_loss did not improve from 0.11769\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5550 - val_loss: 0.1192\n",
      " ###4 fold : val acc1 0.554, acc3 0.961, mae 0.243###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 19.0284 \n",
      "Epoch 00001: val_loss improved from inf to 13.19048, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 17.5696 - val_loss: 13.1905\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 10.6538\n",
      "Epoch 00002: val_loss improved from 13.19048 to 4.17982, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 8.8729 - val_loss: 4.1798\n",
      "Epoch 3/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.3200\n",
      "Epoch 00003: val_loss improved from 4.17982 to 1.52313, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.9225 - val_loss: 1.5231\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.9450\n",
      "Epoch 00004: val_loss improved from 1.52313 to 0.85516, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.7855 - val_loss: 0.8552\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.3168\n",
      "Epoch 00005: val_loss improved from 0.85516 to 0.50004, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2596 - val_loss: 0.5000\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0277\n",
      "Epoch 00006: val_loss improved from 0.50004 to 0.32234, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9900 - val_loss: 0.3223\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8842\n",
      "Epoch 00007: val_loss improved from 0.32234 to 0.23673, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8551 - val_loss: 0.2367\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7944\n",
      "Epoch 00008: val_loss improved from 0.23673 to 0.20242, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7640 - val_loss: 0.2024\n",
      "Epoch 9/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7489\n",
      "Epoch 00009: val_loss improved from 0.20242 to 0.17515, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7453 - val_loss: 0.1752\n",
      "Epoch 10/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7048\n",
      "Epoch 00010: val_loss improved from 0.17515 to 0.16931, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6870 - val_loss: 0.1693\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6807\n",
      "Epoch 00011: val_loss improved from 0.16931 to 0.15855, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6807 - val_loss: 0.1585\n",
      "Epoch 12/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.6550\n",
      "Epoch 00012: val_loss improved from 0.15855 to 0.15384, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6550 - val_loss: 0.1538\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6508\n",
      "Epoch 00013: val_loss improved from 0.15384 to 0.14844, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6508 - val_loss: 0.1484\n",
      "Epoch 14/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.6419\n",
      "Epoch 00014: val_loss improved from 0.14844 to 0.14684, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6405 - val_loss: 0.1468\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6183\n",
      "Epoch 00015: val_loss improved from 0.14684 to 0.14307, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6183 - val_loss: 0.1431\n",
      "Epoch 16/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.6151\n",
      "Epoch 00016: val_loss improved from 0.14307 to 0.14092, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6151 - val_loss: 0.1409\n",
      "Epoch 17/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.6013\n",
      "Epoch 00017: val_loss improved from 0.14092 to 0.13548, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6017 - val_loss: 0.1355\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5926\n",
      "Epoch 00018: val_loss improved from 0.13548 to 0.12895, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5926 - val_loss: 0.1290\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5898\n",
      "Epoch 00019: val_loss did not improve from 0.12895\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5898 - val_loss: 0.1299\n",
      "Epoch 20/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.5691\n",
      "Epoch 00020: val_loss improved from 0.12895 to 0.12844, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5684 - val_loss: 0.1284\n",
      "Epoch 21/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5731\n",
      "Epoch 00021: val_loss improved from 0.12844 to 0.12584, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5721 - val_loss: 0.1258\n",
      "Epoch 22/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5624\n",
      "Epoch 00022: val_loss did not improve from 0.12584\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5631 - val_loss: 0.1307\n",
      "Epoch 23/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5511\n",
      "Epoch 00023: val_loss improved from 0.12584 to 0.12144, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5526 - val_loss: 0.1214\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5453\n",
      "Epoch 00024: val_loss improved from 0.12144 to 0.11804, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5624 - val_loss: 0.1180\n",
      "Epoch 25/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5538\n",
      "Epoch 00025: val_loss did not improve from 0.11804\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5567 - val_loss: 0.1217\n",
      "Epoch 26/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5335\n",
      "Epoch 00026: val_loss did not improve from 0.11804\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5542 - val_loss: 0.1202\n",
      " ###5 fold : val acc1 0.540, acc3 0.964, mae 0.249###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 19.0238 \n",
      "Epoch 00001: val_loss improved from inf to 13.19443, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 17.5567 - val_loss: 13.1944\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 10.6275\n",
      "Epoch 00002: val_loss improved from 13.19443 to 4.18639, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 8.8687 - val_loss: 4.1864\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.3681\n",
      "Epoch 00003: val_loss improved from 4.18639 to 1.52400, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9339 - val_loss: 1.5240\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.9500\n",
      "Epoch 00004: val_loss improved from 1.52400 to 0.85370, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.7930 - val_loss: 0.8537\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.3289\n",
      "Epoch 00005: val_loss improved from 0.85370 to 0.49775, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2628 - val_loss: 0.4977\n",
      "Epoch 6/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.0401\n",
      "Epoch 00006: val_loss improved from 0.49775 to 0.32167, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9930 - val_loss: 0.3217\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8923\n",
      "Epoch 00007: val_loss improved from 0.32167 to 0.23655, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8582 - val_loss: 0.2365\n",
      "Epoch 8/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7898\n",
      "Epoch 00008: val_loss improved from 0.23655 to 0.20351, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7650 - val_loss: 0.2035\n",
      "Epoch 9/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7493\n",
      "Epoch 00009: val_loss improved from 0.20351 to 0.17495, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7479 - val_loss: 0.1749\n",
      "Epoch 10/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7015\n",
      "Epoch 00010: val_loss improved from 0.17495 to 0.16884, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6878 - val_loss: 0.1688\n",
      "Epoch 11/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6893\n",
      "Epoch 00011: val_loss improved from 0.16884 to 0.15926, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6839 - val_loss: 0.1593\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6597\n",
      "Epoch 00012: val_loss improved from 0.15926 to 0.15337, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6566 - val_loss: 0.1534\n",
      "Epoch 13/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6501\n",
      "Epoch 00013: val_loss improved from 0.15337 to 0.14794, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6523 - val_loss: 0.1479\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6438\n",
      "Epoch 00014: val_loss improved from 0.14794 to 0.14705, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6438 - val_loss: 0.1471\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6209\n",
      "Epoch 00015: val_loss improved from 0.14705 to 0.14328, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6209 - val_loss: 0.1433\n",
      "Epoch 16/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6229\n",
      "Epoch 00016: val_loss improved from 0.14328 to 0.14050, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6161 - val_loss: 0.1405\n",
      "Epoch 17/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6001\n",
      "Epoch 00017: val_loss improved from 0.14050 to 0.13470, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6005 - val_loss: 0.1347\n",
      "Epoch 18/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5885\n",
      "Epoch 00018: val_loss improved from 0.13470 to 0.12917, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5952 - val_loss: 0.1292\n",
      "Epoch 19/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5942\n",
      "Epoch 00019: val_loss improved from 0.12917 to 0.12898, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5951 - val_loss: 0.1290\n",
      "Epoch 20/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5741\n",
      "Epoch 00020: val_loss improved from 0.12898 to 0.12784, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5742 - val_loss: 0.1278\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5750\n",
      "Epoch 00021: val_loss improved from 0.12784 to 0.12454, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5750 - val_loss: 0.1245\n",
      "Epoch 22/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5612\n",
      "Epoch 00022: val_loss did not improve from 0.12454\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5620 - val_loss: 0.1284\n",
      "Epoch 23/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 0.5476\n",
      "Epoch 00023: val_loss improved from 0.12454 to 0.12141, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5515 - val_loss: 0.1214\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5444\n",
      "Epoch 00024: val_loss improved from 0.12141 to 0.11764, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5589 - val_loss: 0.1176\n",
      "Epoch 25/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5546\n",
      "Epoch 00025: val_loss did not improve from 0.11764\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5553 - val_loss: 0.1212\n",
      "Epoch 26/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5519\n",
      "Epoch 00026: val_loss did not improve from 0.11764\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5519 - val_loss: 0.1205\n",
      " ###6 fold : val acc1 0.555, acc3 0.969, mae 0.238###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/35 [=================>............] - ETA: 0s - loss: 18.9492 \n",
      "Epoch 00001: val_loss improved from inf to 13.19162, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 17.5576 - val_loss: 13.1916\n",
      "Epoch 2/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 10.4904\n",
      "Epoch 00002: val_loss improved from 13.19162 to 4.19488, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 8.8645 - val_loss: 4.1949\n",
      "Epoch 3/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 3.3178\n",
      "Epoch 00003: val_loss improved from 4.19488 to 1.52599, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9419 - val_loss: 1.5260\n",
      "Epoch 4/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 1.9393\n",
      "Epoch 00004: val_loss improved from 1.52599 to 0.85372, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7977 - val_loss: 0.8537\n",
      "Epoch 5/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.3204\n",
      "Epoch 00005: val_loss improved from 0.85372 to 0.49640, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2538 - val_loss: 0.4964\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0304\n",
      "Epoch 00006: val_loss improved from 0.49640 to 0.32090, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9879 - val_loss: 0.3209\n",
      "Epoch 7/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.8853\n",
      "Epoch 00007: val_loss improved from 0.32090 to 0.23714, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8558 - val_loss: 0.2371\n",
      "Epoch 8/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7901\n",
      "Epoch 00008: val_loss improved from 0.23714 to 0.20193, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7694 - val_loss: 0.2019\n",
      "Epoch 9/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7491\n",
      "Epoch 00009: val_loss improved from 0.20193 to 0.17584, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7424 - val_loss: 0.1758\n",
      "Epoch 10/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6979\n",
      "Epoch 00010: val_loss improved from 0.17584 to 0.16837, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6857 - val_loss: 0.1684\n",
      "Epoch 11/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6881\n",
      "Epoch 00011: val_loss improved from 0.16837 to 0.15985, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6823 - val_loss: 0.1599\n",
      "Epoch 12/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6570\n",
      "Epoch 00012: val_loss improved from 0.15985 to 0.15289, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6544 - val_loss: 0.1529\n",
      "Epoch 13/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6437\n",
      "Epoch 00013: val_loss improved from 0.15289 to 0.14885, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6463 - val_loss: 0.1489\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6503\n",
      "Epoch 00014: val_loss improved from 0.14885 to 0.14785, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6410 - val_loss: 0.1479\n",
      "Epoch 15/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6162\n",
      "Epoch 00015: val_loss improved from 0.14785 to 0.14387, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6174 - val_loss: 0.1439\n",
      "Epoch 16/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6204\n",
      "Epoch 00016: val_loss improved from 0.14387 to 0.13938, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6133 - val_loss: 0.1394\n",
      "Epoch 17/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5991\n",
      "Epoch 00017: val_loss improved from 0.13938 to 0.13480, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5998 - val_loss: 0.1348\n",
      "Epoch 18/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5862\n",
      "Epoch 00018: val_loss improved from 0.13480 to 0.13025, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5949 - val_loss: 0.1303\n",
      "Epoch 19/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5914\n",
      "Epoch 00019: val_loss improved from 0.13025 to 0.12974, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5897 - val_loss: 0.1297\n",
      "Epoch 20/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5695\n",
      "Epoch 00020: val_loss improved from 0.12974 to 0.12962, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5670 - val_loss: 0.1296\n",
      "Epoch 21/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5643\n",
      "Epoch 00021: val_loss improved from 0.12962 to 0.12473, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5721 - val_loss: 0.1247\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5610\n",
      "Epoch 00022: val_loss did not improve from 0.12473\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5610 - val_loss: 0.1312\n",
      "Epoch 23/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 0.5475\n",
      "Epoch 00023: val_loss improved from 0.12473 to 0.12308, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5520 - val_loss: 0.1231\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5396\n",
      "Epoch 00024: val_loss improved from 0.12308 to 0.11895, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5549 - val_loss: 0.1189\n",
      "Epoch 25/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5539\n",
      "Epoch 00025: val_loss did not improve from 0.11895\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5559 - val_loss: 0.1213\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5483\n",
      "Epoch 00026: val_loss did not improve from 0.11895\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5483 - val_loss: 0.1217\n",
      " ###7 fold : val acc1 0.538, acc3 0.953, mae 0.255###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 19.0657 \n",
      "Epoch 00001: val_loss improved from inf to 13.21935, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 17.5819 - val_loss: 13.2193\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 10.7405\n",
      "Epoch 00002: val_loss improved from 13.21935 to 4.19808, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 8.8741 - val_loss: 4.1981\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.3769\n",
      "Epoch 00003: val_loss improved from 4.19808 to 1.52736, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9397 - val_loss: 1.5274\n",
      "Epoch 4/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.9715\n",
      "Epoch 00004: val_loss improved from 1.52736 to 0.86276, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.8021 - val_loss: 0.8628\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.3320\n",
      "Epoch 00005: val_loss improved from 0.86276 to 0.50473, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2624 - val_loss: 0.5047\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0371\n",
      "Epoch 00006: val_loss improved from 0.50473 to 0.32861, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9914 - val_loss: 0.3286\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8911\n",
      "Epoch 00007: val_loss improved from 0.32861 to 0.24355, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8568 - val_loss: 0.2436\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7945\n",
      "Epoch 00008: val_loss improved from 0.24355 to 0.20850, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7697 - val_loss: 0.2085\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7526\n",
      "Epoch 00009: val_loss improved from 0.20850 to 0.18146, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7433 - val_loss: 0.1815\n",
      "Epoch 10/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7042\n",
      "Epoch 00010: val_loss improved from 0.18146 to 0.17411, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6880 - val_loss: 0.1741\n",
      "Epoch 11/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6917\n",
      "Epoch 00011: val_loss improved from 0.17411 to 0.16504, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6834 - val_loss: 0.1650\n",
      "Epoch 12/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6600\n",
      "Epoch 00012: val_loss improved from 0.16504 to 0.15764, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6569 - val_loss: 0.1576\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6449\n",
      "Epoch 00013: val_loss improved from 0.15764 to 0.15291, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6473 - val_loss: 0.1529\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6500\n",
      "Epoch 00014: val_loss did not improve from 0.15291\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6405 - val_loss: 0.1530\n",
      "Epoch 15/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6181\n",
      "Epoch 00015: val_loss improved from 0.15291 to 0.14874, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6170 - val_loss: 0.1487\n",
      "Epoch 16/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6228\n",
      "Epoch 00016: val_loss improved from 0.14874 to 0.14428, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6148 - val_loss: 0.1443\n",
      "Epoch 17/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6007\n",
      "Epoch 00017: val_loss improved from 0.14428 to 0.13944, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6023 - val_loss: 0.1394\n",
      "Epoch 18/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5870\n",
      "Epoch 00018: val_loss improved from 0.13944 to 0.13456, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5958 - val_loss: 0.1346\n",
      "Epoch 19/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5916\n",
      "Epoch 00019: val_loss improved from 0.13456 to 0.13373, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5889 - val_loss: 0.1337\n",
      "Epoch 20/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5684\n",
      "Epoch 00020: val_loss did not improve from 0.13373\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5666 - val_loss: 0.1348\n",
      "Epoch 21/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00021: val_loss improved from 0.13373 to 0.12897, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5704 - val_loss: 0.1290\n",
      "Epoch 22/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5703\n",
      "Epoch 00022: val_loss did not improve from 0.12897\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5620 - val_loss: 0.1343\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5515\n",
      "Epoch 00023: val_loss improved from 0.12897 to 0.12716, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5515 - val_loss: 0.1272\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5577\n",
      "Epoch 00024: val_loss improved from 0.12716 to 0.12305, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5577 - val_loss: 0.1230\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5581\n",
      "Epoch 00025: val_loss did not improve from 0.12305\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5581 - val_loss: 0.1253\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5507\n",
      "Epoch 00026: val_loss did not improve from 0.12305\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5507 - val_loss: 0.1256\n",
      " ###8 fold : val acc1 0.546, acc3 0.964, mae 0.245###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 19.0657 \n",
      "Epoch 00001: val_loss improved from inf to 13.15606, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 17.5819 - val_loss: 13.1561\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 10.6144\n",
      "Epoch 00002: val_loss improved from 13.15606 to 4.19539, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 8.8741 - val_loss: 4.1954\n",
      "Epoch 3/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.3442\n",
      "Epoch 00003: val_loss improved from 4.19539 to 1.52785, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9397 - val_loss: 1.5279\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.9627\n",
      "Epoch 00004: val_loss improved from 1.52785 to 0.87194, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.8021 - val_loss: 0.8719\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.3320\n",
      "Epoch 00005: val_loss improved from 0.87194 to 0.51380, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2624 - val_loss: 0.5138\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0371\n",
      "Epoch 00006: val_loss improved from 0.51380 to 0.33684, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9914 - val_loss: 0.3368\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8911\n",
      "Epoch 00007: val_loss improved from 0.33684 to 0.25098, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8568 - val_loss: 0.2510\n",
      "Epoch 8/100\n",
      " 1/35 [..............................] - ETA: 0s - loss: 0.9328"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1027 - val_loss: 0.1057\n",
      "Epoch 63/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.1024\n",
      "Epoch 00063: val_loss improved from 0.10569 to 0.10532, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1023 - val_loss: 0.1053\n",
      "Epoch 64/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.1017\n",
      "Epoch 00064: val_loss improved from 0.10532 to 0.10499, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1021 - val_loss: 0.1050\n",
      "Epoch 65/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.1015\n",
      "Epoch 00065: val_loss improved from 0.10499 to 0.10472, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1018 - val_loss: 0.1047\n",
      "Epoch 66/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.1017\n",
      "Epoch 00066: val_loss improved from 0.10472 to 0.10467, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1015 - val_loss: 0.1047\n",
      "Epoch 67/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.1013\n",
      "Epoch 00067: val_loss improved from 0.10467 to 0.10422, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1012 - val_loss: 0.1042\n",
      "Epoch 68/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.1006\n",
      "Epoch 00068: val_loss improved from 0.10422 to 0.10394, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1010 - val_loss: 0.1039\n",
      "Epoch 69/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.1012\n",
      "Epoch 00069: val_loss improved from 0.10394 to 0.10370, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1008 - val_loss: 0.1037\n",
      "Epoch 70/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.1003\n",
      "Epoch 00070: val_loss improved from 0.10370 to 0.10347, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1005 - val_loss: 0.1035\n",
      "Epoch 71/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.1005\n",
      "Epoch 00071: val_loss improved from 0.10347 to 0.10335, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1003 - val_loss: 0.1034\n",
      "Epoch 72/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.1002\n",
      "Epoch 00072: val_loss improved from 0.10335 to 0.10308, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1001 - val_loss: 0.1031\n",
      "Epoch 73/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00073: val_loss improved from 0.10308 to 0.10284, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0999 - val_loss: 0.1028\n",
      "Epoch 74/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.1000\n",
      "Epoch 00074: val_loss improved from 0.10284 to 0.10274, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0996 - val_loss: 0.1027\n",
      "Epoch 75/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00075: val_loss improved from 0.10274 to 0.10245, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0995 - val_loss: 0.1024\n",
      "Epoch 76/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00076: val_loss improved from 0.10245 to 0.10229, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0992 - val_loss: 0.1023\n",
      "Epoch 77/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00077: val_loss improved from 0.10229 to 0.10205, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0990 - val_loss: 0.1021\n",
      "Epoch 78/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00078: val_loss improved from 0.10205 to 0.10191, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0989 - val_loss: 0.1019\n",
      "Epoch 79/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.0988\n",
      "Epoch 00079: val_loss improved from 0.10191 to 0.10176, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0987 - val_loss: 0.1018\n",
      "Epoch 80/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00080: val_loss improved from 0.10176 to 0.10164, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0986 - val_loss: 0.1016\n",
      "Epoch 81/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00081: val_loss improved from 0.10164 to 0.10160, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0985 - val_loss: 0.1016\n",
      "Epoch 82/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00082: val_loss improved from 0.10160 to 0.10142, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0983 - val_loss: 0.1014\n",
      "Epoch 83/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.0980\n",
      "Epoch 00083: val_loss improved from 0.10142 to 0.10111, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0981 - val_loss: 0.1011\n",
      "Epoch 84/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.0983\n",
      "Epoch 00084: val_loss improved from 0.10111 to 0.10104, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0981 - val_loss: 0.1010\n",
      "Epoch 85/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.0979\n",
      "Epoch 00085: val_loss improved from 0.10104 to 0.10090, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0979 - val_loss: 0.1009\n",
      "Epoch 86/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.0974\n",
      "Epoch 00086: val_loss improved from 0.10090 to 0.10067, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0978 - val_loss: 0.1007\n",
      "Epoch 87/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.0974\n",
      "Epoch 00087: val_loss improved from 0.10067 to 0.10057, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0977 - val_loss: 0.1006\n",
      "Epoch 88/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.0980\n",
      "Epoch 00088: val_loss improved from 0.10057 to 0.10053, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0976 - val_loss: 0.1005\n",
      "Epoch 89/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00089: val_loss did not improve from 0.10053\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0975 - val_loss: 0.1007\n",
      "Epoch 90/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00090: val_loss improved from 0.10053 to 0.10026, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0975 - val_loss: 0.1003\n",
      "Epoch 91/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00091: val_loss improved from 0.10026 to 0.10015, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0973 - val_loss: 0.1001\n",
      "Epoch 92/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00092: val_loss improved from 0.10015 to 0.10008, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0972 - val_loss: 0.1001\n",
      "Epoch 93/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00093: val_loss improved from 0.10008 to 0.09988, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0971 - val_loss: 0.0999\n",
      "Epoch 94/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00094: val_loss improved from 0.09988 to 0.09982, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0970 - val_loss: 0.0998\n",
      "Epoch 95/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.0968\n",
      "Epoch 00095: val_loss improved from 0.09982 to 0.09968, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0969 - val_loss: 0.0997\n",
      "Epoch 96/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00096: val_loss improved from 0.09968 to 0.09959, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0969 - val_loss: 0.0996\n",
      "Epoch 97/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00097: val_loss did not improve from 0.09959\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0968 - val_loss: 0.0996\n",
      "Epoch 98/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00098: val_loss improved from 0.09959 to 0.09950, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0968 - val_loss: 0.0995\n",
      "Epoch 99/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00099: val_loss improved from 0.09950 to 0.09943, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 0.0994\n",
      "Epoch 100/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00100: val_loss improved from 0.09943 to 0.09926, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0966 - val_loss: 0.0993\n",
      " ###8 fold : val acc1 0.592, acc3 0.983, mae 0.212###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/70 [====================>.........] - ETA: 0s - loss: 27.7560\n",
      "Epoch 00001: val_loss improved from inf to 24.73220, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 1s 4ms/step - loss: 27.1326 - val_loss: 24.7322\n",
      "Epoch 2/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 23.3119\n",
      "Epoch 00002: val_loss improved from 24.73220 to 21.29420, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 23.2654 - val_loss: 21.2942\n",
      "Epoch 3/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 20.1005\n",
      "Epoch 00003: val_loss improved from 21.29420 to 18.37016, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 20.0075 - val_loss: 18.3702\n",
      "Epoch 4/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 17.2276\n",
      "Epoch 00004: val_loss improved from 18.37016 to 15.81897, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 17.2015 - val_loss: 15.8190\n",
      "Epoch 5/100\n",
      "50/70 [====================>.........] - ETA: 0s - loss: 15.0831\n",
      "Epoch 00005: val_loss improved from 15.81897 to 13.61575, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 14.7702 - val_loss: 13.6158\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 12.6817\n",
      "Epoch 00006: val_loss improved from 13.61575 to 11.72281, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 12.6817 - val_loss: 11.7228\n",
      "Epoch 7/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 11.0053\n",
      "Epoch 00007: val_loss improved from 11.72281 to 10.10603, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 10.9047 - val_loss: 10.1060\n",
      "Epoch 8/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 9.4649\n",
      "Epoch 00008: val_loss improved from 10.10603 to 8.70603, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 9.3908 - val_loss: 8.7060\n",
      "Epoch 9/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 8.1207\n",
      "Epoch 00009: val_loss improved from 8.70603 to 7.29767, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 8.0156 - val_loss: 7.2977\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 6.5264\n",
      "Epoch 00010: val_loss improved from 7.29767 to 5.69332, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 6.5264 - val_loss: 5.6933\n",
      "Epoch 11/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 5.0022\n",
      "Epoch 00011: val_loss improved from 5.69332 to 4.11120, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 4.9317 - val_loss: 4.1112\n",
      "Epoch 12/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 3.5484\n",
      "Epoch 00012: val_loss improved from 4.11120 to 2.78124, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 3.4612 - val_loss: 2.7812\n",
      "Epoch 13/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 2.4050\n",
      "Epoch 00013: val_loss improved from 2.78124 to 1.96000, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 2.3820 - val_loss: 1.9600\n",
      "Epoch 14/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.7625\n",
      "Epoch 00014: val_loss improved from 1.96000 to 1.50976, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 1.7530 - val_loss: 1.5098\n",
      "Epoch 15/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 1.4011\n",
      "Epoch 00015: val_loss improved from 1.50976 to 1.24079, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 1.3958 - val_loss: 1.2408\n",
      "Epoch 16/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 1.1679\n",
      "Epoch 00016: val_loss improved from 1.24079 to 1.04501, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 1.1590 - val_loss: 1.0450\n",
      "Epoch 17/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.9888\n",
      "Epoch 00017: val_loss improved from 1.04501 to 0.89040, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.9787 - val_loss: 0.8904\n",
      "Epoch 18/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.8373\n",
      "Epoch 00018: val_loss improved from 0.89040 to 0.76351, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.8333 - val_loss: 0.7635\n",
      "Epoch 19/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.7172\n",
      "Epoch 00019: val_loss improved from 0.76351 to 0.65595, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7128 - val_loss: 0.6559\n",
      "Epoch 20/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.6110\n",
      "Epoch 00020: val_loss improved from 0.65595 to 0.56478, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6107 - val_loss: 0.5648\n",
      "Epoch 21/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.5296\n",
      "Epoch 00021: val_loss improved from 0.56478 to 0.48632, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5236 - val_loss: 0.4863\n",
      "Epoch 22/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.4528\n",
      "Epoch 00022: val_loss improved from 0.48632 to 0.42009, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4494 - val_loss: 0.4201\n",
      "Epoch 23/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.3887\n",
      "Epoch 00023: val_loss improved from 0.42009 to 0.36406, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3868 - val_loss: 0.3641\n",
      "Epoch 24/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.3360\n",
      "Epoch 00024: val_loss improved from 0.36406 to 0.31790, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.3346 - val_loss: 0.3179\n",
      "Epoch 25/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.2940\n",
      "Epoch 00025: val_loss improved from 0.31790 to 0.27989, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2916 - val_loss: 0.2799\n",
      "Epoch 26/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.2563\n",
      "Epoch 00026: val_loss improved from 0.27989 to 0.24877, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2564 - val_loss: 0.2488\n",
      "Epoch 27/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.2303\n",
      "Epoch 00027: val_loss improved from 0.24877 to 0.22331, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2279 - val_loss: 0.2233\n",
      "Epoch 28/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.2059\n",
      "Epoch 00028: val_loss improved from 0.22331 to 0.20243, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2047 - val_loss: 0.2024\n",
      "Epoch 29/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00029: val_loss improved from 0.20243 to 0.18537, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1859 - val_loss: 0.1854\n",
      "Epoch 30/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.1718\n",
      "Epoch 00030: val_loss improved from 0.18537 to 0.17115, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1705 - val_loss: 0.1711\n",
      "Epoch 31/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.1579\n",
      "Epoch 00031: val_loss improved from 0.17115 to 0.15926, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.1576 - val_loss: 0.1593\n",
      "Epoch 32/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.1478\n",
      "Epoch 00032: val_loss improved from 0.15926 to 0.14985, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.1471 - val_loss: 0.1499\n",
      "Epoch 33/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.1389\n",
      "Epoch 00033: val_loss improved from 0.14985 to 0.14265, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1389 - val_loss: 0.1427\n",
      "Epoch 34/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00034: val_loss improved from 0.14265 to 0.13711, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.1328 - val_loss: 0.1371\n",
      "Epoch 35/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.1282\n",
      "Epoch 00035: val_loss improved from 0.13711 to 0.13275, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.1281 - val_loss: 0.1327\n",
      "Epoch 36/100\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00036: val_loss improved from 0.13275 to 0.12934, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 0.1293\n",
      "Epoch 37/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.1218\n",
      "Epoch 00037: val_loss improved from 0.12934 to 0.12660, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.1214 - val_loss: 0.1266\n",
      "Epoch 38/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1192\n",
      "Epoch 00038: val_loss improved from 0.12660 to 0.12426, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.1191 - val_loss: 0.1243\n",
      "Epoch 39/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.1170\n",
      "Epoch 00039: val_loss improved from 0.12426 to 0.12235, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1171 - val_loss: 0.1223\n",
      "Epoch 40/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.1147\n",
      "Epoch 00040: val_loss improved from 0.12235 to 0.12079, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1154 - val_loss: 0.1208\n",
      "Epoch 41/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.1154\n",
      "Epoch 00041: val_loss improved from 0.12079 to 0.11940, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1141 - val_loss: 0.1194\n",
      "Epoch 42/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.1136\n",
      "Epoch 00042: val_loss improved from 0.11940 to 0.11832, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1129 - val_loss: 0.1183\n",
      "Epoch 43/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.1119\n",
      "Epoch 00043: val_loss improved from 0.11832 to 0.11738, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1119 - val_loss: 0.1174\n",
      "Epoch 44/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.1112\n",
      "Epoch 00044: val_loss improved from 0.11738 to 0.11643, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1110 - val_loss: 0.1164\n",
      "Epoch 45/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.1102\n",
      "Epoch 00045: val_loss improved from 0.11643 to 0.11570, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1102 - val_loss: 0.1157\n",
      "Epoch 46/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.1090\n",
      "Epoch 00046: val_loss improved from 0.11570 to 0.11501, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1095 - val_loss: 0.1150\n",
      "Epoch 47/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.1094\n",
      "Epoch 00047: val_loss improved from 0.11501 to 0.11440, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1088 - val_loss: 0.1144\n",
      "Epoch 48/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.1082\n",
      "Epoch 00048: val_loss improved from 0.11440 to 0.11380, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1082 - val_loss: 0.1138\n",
      "Epoch 49/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00049: val_loss improved from 0.11380 to 0.11324, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1077 - val_loss: 0.1132\n",
      "Epoch 50/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00050: val_loss improved from 0.11324 to 0.11277, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1072 - val_loss: 0.1128\n",
      "Epoch 51/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.1073\n",
      "Epoch 00051: val_loss improved from 0.11277 to 0.11225, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1067 - val_loss: 0.1122\n",
      "Epoch 52/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.1065\n",
      "Epoch 00052: val_loss improved from 0.11225 to 0.11179, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1062 - val_loss: 0.1118\n",
      "Epoch 53/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.1054\n",
      "Epoch 00053: val_loss improved from 0.11179 to 0.11134, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1058 - val_loss: 0.1113\n",
      "Epoch 54/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.1052\n",
      "Epoch 00054: val_loss improved from 0.11134 to 0.11087, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1054 - val_loss: 0.1109\n",
      "Epoch 55/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.1049\n",
      "Epoch 00055: val_loss improved from 0.11087 to 0.11049, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1051 - val_loss: 0.1105\n",
      "Epoch 56/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.1041\n",
      "Epoch 00056: val_loss improved from 0.11049 to 0.11025, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1046 - val_loss: 0.1103\n",
      "Epoch 57/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.1041\n",
      "Epoch 00057: val_loss improved from 0.11025 to 0.10981, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1043 - val_loss: 0.1098\n",
      "Epoch 58/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.1034\n",
      "Epoch 00058: val_loss improved from 0.10981 to 0.10944, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1039 - val_loss: 0.1094\n",
      "Epoch 59/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.1037\n",
      "Epoch 00059: val_loss improved from 0.10944 to 0.10905, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1037 - val_loss: 0.1091\n",
      "Epoch 60/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.1024\n",
      "Epoch 00060: val_loss improved from 0.10905 to 0.10868, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1033 - val_loss: 0.1087\n",
      "Epoch 61/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.1027\n",
      "Epoch 00061: val_loss improved from 0.10868 to 0.10841, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1030 - val_loss: 0.1084\n",
      "Epoch 62/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.1027\n",
      "Epoch 00062: val_loss improved from 0.10841 to 0.10806, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1027 - val_loss: 0.1081\n",
      "Epoch 63/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.1026\n",
      "Epoch 00063: val_loss improved from 0.10806 to 0.10785, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1023 - val_loss: 0.1078\n",
      "Epoch 64/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.1017\n",
      "Epoch 00064: val_loss improved from 0.10785 to 0.10738, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1021 - val_loss: 0.1074\n",
      "Epoch 65/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.1017\n",
      "Epoch 00065: val_loss improved from 0.10738 to 0.10729, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1018 - val_loss: 0.1073\n",
      "Epoch 66/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.1018\n",
      "Epoch 00066: val_loss improved from 0.10729 to 0.10691, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1015 - val_loss: 0.1069\n",
      "Epoch 67/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.1011\n",
      "Epoch 00067: val_loss improved from 0.10691 to 0.10662, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1012 - val_loss: 0.1066\n",
      "Epoch 68/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.1008\n",
      "Epoch 00068: val_loss improved from 0.10662 to 0.10631, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1010 - val_loss: 0.1063\n",
      "Epoch 69/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.1009\n",
      "Epoch 00069: val_loss improved from 0.10631 to 0.10610, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1008 - val_loss: 0.1061\n",
      "Epoch 70/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.1002\n",
      "Epoch 00070: val_loss improved from 0.10610 to 0.10581, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1005 - val_loss: 0.1058\n",
      "Epoch 71/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.1004\n",
      "Epoch 00071: val_loss improved from 0.10581 to 0.10554, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1003 - val_loss: 0.1055\n",
      "Epoch 72/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00072: val_loss improved from 0.10554 to 0.10538, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1001 - val_loss: 0.1054\n",
      "Epoch 73/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00073: val_loss improved from 0.10538 to 0.10509, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0999 - val_loss: 0.1051\n",
      "Epoch 74/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00074: val_loss improved from 0.10509 to 0.10479, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0996 - val_loss: 0.1048\n",
      "Epoch 75/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.0994\n",
      "Epoch 00075: val_loss improved from 0.10479 to 0.10460, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0995 - val_loss: 0.1046\n",
      "Epoch 76/100\n",
      "57/70 [=======================>......] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00076: val_loss improved from 0.10460 to 0.10436, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0992 - val_loss: 0.1044\n",
      "Epoch 77/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00077: val_loss improved from 0.10436 to 0.10419, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0990 - val_loss: 0.1042\n",
      "Epoch 78/100\n",
      "58/70 [=======================>......] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00078: val_loss improved from 0.10419 to 0.10393, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0989 - val_loss: 0.1039\n",
      "Epoch 79/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.0989\n",
      "Epoch 00079: val_loss improved from 0.10393 to 0.10374, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0987 - val_loss: 0.1037\n",
      "Epoch 80/100\n",
      "66/70 [===========================>..] - ETA: 0s - loss: 0.0986\n",
      "Epoch 00080: val_loss improved from 0.10374 to 0.10350, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0986 - val_loss: 0.1035\n",
      "Epoch 81/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.0985\n",
      "Epoch 00081: val_loss improved from 0.10350 to 0.10345, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0985 - val_loss: 0.1035\n",
      "Epoch 82/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00082: val_loss improved from 0.10345 to 0.10330, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0983 - val_loss: 0.1033\n",
      "Epoch 83/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.0980\n",
      "Epoch 00083: val_loss improved from 0.10330 to 0.10309, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0981 - val_loss: 0.1031\n",
      "Epoch 84/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.0983\n",
      "Epoch 00084: val_loss improved from 0.10309 to 0.10286, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0981 - val_loss: 0.1029\n",
      "Epoch 85/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.0978\n",
      "Epoch 00085: val_loss improved from 0.10286 to 0.10273, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0979 - val_loss: 0.1027\n",
      "Epoch 86/100\n",
      "62/70 [=========================>....] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00086: val_loss did not improve from 0.10273\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0978 - val_loss: 0.1028\n",
      "Epoch 87/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00087: val_loss improved from 0.10273 to 0.10248, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0977 - val_loss: 0.1025\n",
      "Epoch 88/100\n",
      "64/70 [==========================>...] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00088: val_loss improved from 0.10248 to 0.10238, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0976 - val_loss: 0.1024\n",
      "Epoch 89/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00089: val_loss did not improve from 0.10238\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0975 - val_loss: 0.1025\n",
      "Epoch 90/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.0977\n",
      "Epoch 00090: val_loss improved from 0.10238 to 0.10221, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0975 - val_loss: 0.1022\n",
      "Epoch 91/100\n",
      "61/70 [=========================>....] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00091: val_loss improved from 0.10221 to 0.10213, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0973 - val_loss: 0.1021\n",
      "Epoch 92/100\n",
      "60/70 [========================>.....] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00092: val_loss improved from 0.10213 to 0.10208, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0972 - val_loss: 0.1021\n",
      "Epoch 93/100\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00093: val_loss improved from 0.10208 to 0.10182, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0971 - val_loss: 0.1018\n",
      "Epoch 94/100\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00094: val_loss did not improve from 0.10182\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 0.1019\n",
      "Epoch 95/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00095: val_loss improved from 0.10182 to 0.10179, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0969 - val_loss: 0.1018\n",
      "Epoch 96/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00096: val_loss improved from 0.10179 to 0.10157, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0969 - val_loss: 0.1016\n",
      "Epoch 97/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.0964\n",
      "Epoch 00097: val_loss improved from 0.10157 to 0.10144, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0968 - val_loss: 0.1014\n",
      "Epoch 98/100\n",
      "63/70 [==========================>...] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00098: val_loss improved from 0.10144 to 0.10137, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0968 - val_loss: 0.1014\n",
      "Epoch 99/100\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0968\n",
      "Epoch 00099: val_loss improved from 0.10137 to 0.10136, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0967 - val_loss: 0.1014\n",
      "Epoch 100/100\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.0968\n",
      "Epoch 00100: val_loss improved from 0.10136 to 0.10114, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch256,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0966 - val_loss: 0.1011\n",
      " ###9 fold : val acc1 0.595, acc3 0.982, mae 0.211###\n",
      "acc10.594_acc30.980\n",
      "random search 70/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/140 [==========================>...] - ETA: 0s - loss: 7.3717\n",
      "Epoch 00001: val_loss improved from inf to 1.23055, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.9414 - val_loss: 1.2305\n",
      "Epoch 2/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.5605\n",
      "Epoch 00002: val_loss improved from 1.23055 to 0.14121, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5488 - val_loss: 0.1412\n",
      "Epoch 3/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2273\n",
      "Epoch 00003: val_loss improved from 0.14121 to 0.11665, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2265 - val_loss: 0.1167\n",
      "Epoch 4/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2072\n",
      "Epoch 00004: val_loss improved from 0.11665 to 0.11189, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2070 - val_loss: 0.1119\n",
      "Epoch 5/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2032\n",
      "Epoch 00005: val_loss improved from 0.11189 to 0.10818, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2029 - val_loss: 0.1082\n",
      "Epoch 6/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1978\n",
      "Epoch 00006: val_loss improved from 0.10818 to 0.10510, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1978 - val_loss: 0.1051\n",
      "Epoch 7/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1897\n",
      "Epoch 00007: val_loss improved from 0.10510 to 0.10365, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1893 - val_loss: 0.1036\n",
      "Epoch 8/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1819\n",
      "Epoch 00008: val_loss improved from 0.10365 to 0.10117, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1819 - val_loss: 0.1012\n",
      "Epoch 9/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1784\n",
      "Epoch 00009: val_loss improved from 0.10117 to 0.09917, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1780 - val_loss: 0.0992\n",
      "Epoch 10/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.1741\n",
      "Epoch 00010: val_loss improved from 0.09917 to 0.09899, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1742 - val_loss: 0.0990\n",
      "Epoch 11/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1698\n",
      "Epoch 00011: val_loss improved from 0.09899 to 0.09724, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1700 - val_loss: 0.0972\n",
      "Epoch 12/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.1686\n",
      "Epoch 00012: val_loss improved from 0.09724 to 0.09717, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1672 - val_loss: 0.0972\n",
      "Epoch 13/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1665\n",
      "Epoch 00013: val_loss improved from 0.09717 to 0.09714, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1665 - val_loss: 0.0971\n",
      "Epoch 14/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1633\n",
      "Epoch 00014: val_loss improved from 0.09714 to 0.09649, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1635 - val_loss: 0.0965\n",
      "Epoch 15/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1612\n",
      "Epoch 00015: val_loss improved from 0.09649 to 0.09510, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1616 - val_loss: 0.0951\n",
      "Epoch 16/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.1621\n",
      "Epoch 00016: val_loss did not improve from 0.09510\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1618 - val_loss: 0.0958\n",
      "Epoch 17/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1543\n",
      "Epoch 00017: val_loss did not improve from 0.09510\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1547 - val_loss: 0.0958\n",
      " ###0 fold : val acc1 0.610, acc3 0.981, mae 0.205###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/140 [=========================>....] - ETA: 0s - loss: 7.5634\n",
      "Epoch 00001: val_loss improved from inf to 1.22979, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.9456 - val_loss: 1.2298\n",
      "Epoch 2/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.5652\n",
      "Epoch 00002: val_loss improved from 1.22979 to 0.14104, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5484 - val_loss: 0.1410\n",
      "Epoch 3/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2245\n",
      "Epoch 00003: val_loss improved from 0.14104 to 0.11680, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2245 - val_loss: 0.1168\n",
      "Epoch 4/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00004: val_loss improved from 0.11680 to 0.11186, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2059 - val_loss: 0.1119\n",
      "Epoch 5/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2017\n",
      "Epoch 00005: val_loss improved from 0.11186 to 0.10837, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2009 - val_loss: 0.1084\n",
      "Epoch 6/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.1960\n",
      "Epoch 00006: val_loss improved from 0.10837 to 0.10578, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1956 - val_loss: 0.1058\n",
      "Epoch 7/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1910\n",
      "Epoch 00007: val_loss improved from 0.10578 to 0.10305, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1890 - val_loss: 0.1030\n",
      "Epoch 8/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1825\n",
      "Epoch 00008: val_loss improved from 0.10305 to 0.10128, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1825 - val_loss: 0.1013\n",
      "Epoch 9/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1769\n",
      "Epoch 00009: val_loss improved from 0.10128 to 0.09898, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1765 - val_loss: 0.0990\n",
      "Epoch 10/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1732\n",
      "Epoch 00010: val_loss improved from 0.09898 to 0.09877, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1734 - val_loss: 0.0988\n",
      "Epoch 11/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1704\n",
      "Epoch 00011: val_loss improved from 0.09877 to 0.09714, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1704 - val_loss: 0.0971\n",
      "Epoch 12/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1675\n",
      "Epoch 00012: val_loss did not improve from 0.09714\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1671 - val_loss: 0.0974\n",
      "Epoch 13/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.1655\n",
      "Epoch 00013: val_loss improved from 0.09714 to 0.09679, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1664 - val_loss: 0.0968\n",
      "Epoch 14/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1629\n",
      "Epoch 00014: val_loss did not improve from 0.09679\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1631 - val_loss: 0.0968\n",
      "Epoch 15/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1611\n",
      "Epoch 00015: val_loss improved from 0.09679 to 0.09498, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1610 - val_loss: 0.0950\n",
      "Epoch 16/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1609\n",
      "Epoch 00016: val_loss did not improve from 0.09498\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1608 - val_loss: 0.0961\n",
      "Epoch 17/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1550\n",
      "Epoch 00017: val_loss did not improve from 0.09498\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1551 - val_loss: 0.0962\n",
      " ###1 fold : val acc1 0.595, acc3 0.983, mae 0.211###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/140 [===========================>..] - ETA: 0s - loss: 7.1968\n",
      "Epoch 00001: val_loss improved from inf to 1.21931, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.9365 - val_loss: 1.2193\n",
      "Epoch 2/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.5686\n",
      "Epoch 00002: val_loss improved from 1.21931 to 0.14045, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.5451 - val_loss: 0.1405\n",
      "Epoch 3/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2251\n",
      "Epoch 00003: val_loss improved from 0.14045 to 0.11640, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2251 - val_loss: 0.1164\n",
      "Epoch 4/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.2068\n",
      "Epoch 00004: val_loss improved from 0.11640 to 0.11189, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2062 - val_loss: 0.1119\n",
      "Epoch 5/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.2015\n",
      "Epoch 00005: val_loss improved from 0.11189 to 0.10833, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2015 - val_loss: 0.1083\n",
      "Epoch 6/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1939\n",
      "Epoch 00006: val_loss improved from 0.10833 to 0.10521, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1939 - val_loss: 0.1052\n",
      "Epoch 7/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1865\n",
      "Epoch 00007: val_loss improved from 0.10521 to 0.10284, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1863 - val_loss: 0.1028\n",
      "Epoch 8/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1830\n",
      "Epoch 00008: val_loss improved from 0.10284 to 0.10112, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1826 - val_loss: 0.1011\n",
      "Epoch 9/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1773\n",
      "Epoch 00009: val_loss improved from 0.10112 to 0.09923, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1771 - val_loss: 0.0992\n",
      "Epoch 10/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1740\n",
      "Epoch 00010: val_loss did not improve from 0.09923\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1738 - val_loss: 0.0994\n",
      "Epoch 11/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1706\n",
      "Epoch 00011: val_loss improved from 0.09923 to 0.09806, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1708 - val_loss: 0.0981\n",
      "Epoch 12/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1701\n",
      "Epoch 00012: val_loss improved from 0.09806 to 0.09734, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1685 - val_loss: 0.0973\n",
      "Epoch 13/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1659\n",
      "Epoch 00013: val_loss improved from 0.09734 to 0.09731, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1669 - val_loss: 0.0973\n",
      "Epoch 14/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1637\n",
      "Epoch 00014: val_loss did not improve from 0.09731\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1646 - val_loss: 0.0979\n",
      "Epoch 15/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1609\n",
      "Epoch 00015: val_loss improved from 0.09731 to 0.09530, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1609 - val_loss: 0.0953\n",
      "Epoch 16/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1592\n",
      "Epoch 00016: val_loss did not improve from 0.09530\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1598 - val_loss: 0.0955\n",
      "Epoch 17/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1548\n",
      "Epoch 00017: val_loss did not improve from 0.09530\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1554 - val_loss: 0.0962\n",
      " ###2 fold : val acc1 0.602, acc3 0.980, mae 0.209###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/140 [==========================>...] - ETA: 0s - loss: 7.5323\n",
      "Epoch 00001: val_loss improved from inf to 1.21422, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.9558 - val_loss: 1.2142\n",
      "Epoch 2/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.5907\n",
      "Epoch 00002: val_loss improved from 1.21422 to 0.14046, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5475 - val_loss: 0.1405\n",
      "Epoch 3/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2239\n",
      "Epoch 00003: val_loss improved from 0.14046 to 0.11715, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2244 - val_loss: 0.1172\n",
      "Epoch 4/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.2078\n",
      "Epoch 00004: val_loss improved from 0.11715 to 0.11213, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2063 - val_loss: 0.1121\n",
      "Epoch 5/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00005: val_loss improved from 0.11213 to 0.10796, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2020 - val_loss: 0.1080\n",
      "Epoch 6/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1927\n",
      "Epoch 00006: val_loss improved from 0.10796 to 0.10546, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1927 - val_loss: 0.1055\n",
      "Epoch 7/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1869\n",
      "Epoch 00007: val_loss improved from 0.10546 to 0.10258, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1862 - val_loss: 0.1026\n",
      "Epoch 8/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1816\n",
      "Epoch 00008: val_loss improved from 0.10258 to 0.10094, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1813 - val_loss: 0.1009\n",
      "Epoch 9/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1756\n",
      "Epoch 00009: val_loss improved from 0.10094 to 0.09914, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1753 - val_loss: 0.0991\n",
      "Epoch 10/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.1738\n",
      "Epoch 00010: val_loss did not improve from 0.09914\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1736 - val_loss: 0.0994\n",
      "Epoch 11/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1712\n",
      "Epoch 00011: val_loss improved from 0.09914 to 0.09738, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1708 - val_loss: 0.0974\n",
      "Epoch 12/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1684\n",
      "Epoch 00012: val_loss improved from 0.09738 to 0.09727, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1683 - val_loss: 0.0973\n",
      "Epoch 13/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1658\n",
      "Epoch 00013: val_loss improved from 0.09727 to 0.09718, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1667 - val_loss: 0.0972\n",
      "Epoch 14/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1635\n",
      "Epoch 00014: val_loss did not improve from 0.09718\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1642 - val_loss: 0.0977\n",
      "Epoch 15/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1617\n",
      "Epoch 00015: val_loss improved from 0.09718 to 0.09525, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1614 - val_loss: 0.0952\n",
      "Epoch 16/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.1589\n",
      "Epoch 00016: val_loss did not improve from 0.09525\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1591 - val_loss: 0.0960\n",
      "Epoch 17/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1549\n",
      "Epoch 00017: val_loss improved from 0.09525 to 0.09509, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1556 - val_loss: 0.0951\n",
      "Epoch 18/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1558\n",
      "Epoch 00018: val_loss did not improve from 0.09509\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1559 - val_loss: 0.0955\n",
      "Epoch 19/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1557\n",
      "Epoch 00019: val_loss did not improve from 0.09509\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1556 - val_loss: 0.1019\n",
      " ###3 fold : val acc1 0.621, acc3 0.977, mae 0.201###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 7.4547\n",
      "Epoch 00001: val_loss improved from inf to 1.20661, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.9718 - val_loss: 1.2066\n",
      "Epoch 2/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.5510\n",
      "Epoch 00002: val_loss improved from 1.20661 to 0.14103, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.5326 - val_loss: 0.1410\n",
      "Epoch 3/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2256\n",
      "Epoch 00003: val_loss improved from 0.14103 to 0.11802, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2256 - val_loss: 0.1180\n",
      "Epoch 4/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2107\n",
      "Epoch 00004: val_loss improved from 0.11802 to 0.11261, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2109 - val_loss: 0.1126\n",
      "Epoch 5/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2021\n",
      "Epoch 00005: val_loss improved from 0.11261 to 0.10746, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2019 - val_loss: 0.1075\n",
      "Epoch 6/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1944\n",
      "Epoch 00006: val_loss improved from 0.10746 to 0.10566, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1941 - val_loss: 0.1057\n",
      "Epoch 7/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1890\n",
      "Epoch 00007: val_loss improved from 0.10566 to 0.10276, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1891 - val_loss: 0.1028\n",
      "Epoch 8/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1826\n",
      "Epoch 00008: val_loss improved from 0.10276 to 0.10050, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1820 - val_loss: 0.1005\n",
      "Epoch 9/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1762\n",
      "Epoch 00009: val_loss improved from 0.10050 to 0.09992, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1772 - val_loss: 0.0999\n",
      "Epoch 10/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1729\n",
      "Epoch 00010: val_loss improved from 0.09992 to 0.09918, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1730 - val_loss: 0.0992\n",
      "Epoch 11/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1681\n",
      "Epoch 00011: val_loss improved from 0.09918 to 0.09751, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1681 - val_loss: 0.0975\n",
      "Epoch 12/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1671\n",
      "Epoch 00012: val_loss improved from 0.09751 to 0.09734, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1671 - val_loss: 0.0973\n",
      "Epoch 13/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1646\n",
      "Epoch 00013: val_loss improved from 0.09734 to 0.09656, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1647 - val_loss: 0.0966\n",
      "Epoch 14/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1618\n",
      "Epoch 00014: val_loss improved from 0.09656 to 0.09647, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1617 - val_loss: 0.0965\n",
      "Epoch 15/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1615\n",
      "Epoch 00015: val_loss improved from 0.09647 to 0.09540, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1616 - val_loss: 0.0954\n",
      "Epoch 16/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1577\n",
      "Epoch 00016: val_loss did not improve from 0.09540\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1577 - val_loss: 0.0963\n",
      "Epoch 17/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1574\n",
      "Epoch 00017: val_loss did not improve from 0.09540\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1570 - val_loss: 0.0973\n",
      " ###4 fold : val acc1 0.591, acc3 0.978, mae 0.215###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/140 [=========================>....] - ETA: 0s - loss: 7.6052\n",
      "Epoch 00001: val_loss improved from inf to 1.20580, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.9795 - val_loss: 1.2058\n",
      "Epoch 2/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.5354\n",
      "Epoch 00002: val_loss improved from 1.20580 to 0.14128, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5326 - val_loss: 0.1413\n",
      "Epoch 3/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2247\n",
      "Epoch 00003: val_loss improved from 0.14128 to 0.11746, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2245 - val_loss: 0.1175\n",
      "Epoch 4/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2117\n",
      "Epoch 00004: val_loss improved from 0.11746 to 0.11330, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2116 - val_loss: 0.1133\n",
      "Epoch 5/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00005: val_loss improved from 0.11330 to 0.10783, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2024 - val_loss: 0.1078\n",
      "Epoch 6/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1934\n",
      "Epoch 00006: val_loss improved from 0.10783 to 0.10474, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1937 - val_loss: 0.1047\n",
      "Epoch 7/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.1903\n",
      "Epoch 00007: val_loss improved from 0.10474 to 0.10354, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1907 - val_loss: 0.1035\n",
      "Epoch 8/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1835\n",
      "Epoch 00008: val_loss improved from 0.10354 to 0.10039, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1824 - val_loss: 0.1004\n",
      "Epoch 9/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1764\n",
      "Epoch 00009: val_loss improved from 0.10039 to 0.09957, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1766 - val_loss: 0.0996\n",
      "Epoch 10/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1735\n",
      "Epoch 00010: val_loss improved from 0.09957 to 0.09870, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1735 - val_loss: 0.0987\n",
      "Epoch 11/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1687\n",
      "Epoch 00011: val_loss improved from 0.09870 to 0.09769, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1684 - val_loss: 0.0977\n",
      "Epoch 12/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1665\n",
      "Epoch 00012: val_loss improved from 0.09769 to 0.09724, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1667 - val_loss: 0.0972\n",
      "Epoch 13/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1654\n",
      "Epoch 00013: val_loss did not improve from 0.09724\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1649 - val_loss: 0.0976\n",
      "Epoch 14/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1608\n",
      "Epoch 00014: val_loss improved from 0.09724 to 0.09662, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1609 - val_loss: 0.0966\n",
      "Epoch 15/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1613\n",
      "Epoch 00015: val_loss improved from 0.09662 to 0.09536, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1618 - val_loss: 0.0954\n",
      "Epoch 16/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1577\n",
      "Epoch 00016: val_loss did not improve from 0.09536\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1576 - val_loss: 0.0959\n",
      "Epoch 17/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1575\n",
      "Epoch 00017: val_loss did not improve from 0.09536\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1573 - val_loss: 0.0971\n",
      " ###5 fold : val acc1 0.593, acc3 0.981, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/140 [=========================>....] - ETA: 0s - loss: 7.6489\n",
      "Epoch 00001: val_loss improved from inf to 1.20340, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.9795 - val_loss: 1.2034\n",
      "Epoch 2/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.5728\n",
      "Epoch 00002: val_loss improved from 1.20340 to 0.14020, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5383 - val_loss: 0.1402\n",
      "Epoch 3/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.2261\n",
      "Epoch 00003: val_loss improved from 0.14020 to 0.11752, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2260 - val_loss: 0.1175\n",
      "Epoch 4/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2104\n",
      "Epoch 00004: val_loss improved from 0.11752 to 0.11303, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2105 - val_loss: 0.1130\n",
      "Epoch 5/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.2036\n",
      "Epoch 00005: val_loss improved from 0.11303 to 0.10768, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2031 - val_loss: 0.1077\n",
      "Epoch 6/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1944\n",
      "Epoch 00006: val_loss improved from 0.10768 to 0.10438, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1944 - val_loss: 0.1044\n",
      "Epoch 7/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1902\n",
      "Epoch 00007: val_loss improved from 0.10438 to 0.10296, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1901 - val_loss: 0.1030\n",
      "Epoch 8/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1815\n",
      "Epoch 00008: val_loss improved from 0.10296 to 0.10040, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1813 - val_loss: 0.1004\n",
      "Epoch 9/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1768\n",
      "Epoch 00009: val_loss improved from 0.10040 to 0.09924, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1769 - val_loss: 0.0992\n",
      "Epoch 10/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1746\n",
      "Epoch 00010: val_loss improved from 0.09924 to 0.09813, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1744 - val_loss: 0.0981\n",
      "Epoch 11/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1673\n",
      "Epoch 00011: val_loss improved from 0.09813 to 0.09706, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1674 - val_loss: 0.0971\n",
      "Epoch 12/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1665\n",
      "Epoch 00012: val_loss did not improve from 0.09706\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1665 - val_loss: 0.0976\n",
      "Epoch 13/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1663\n",
      "Epoch 00013: val_loss did not improve from 0.09706\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1658 - val_loss: 0.0983\n",
      " ###6 fold : val acc1 0.601, acc3 0.984, mae 0.207###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/140 [==========================>...] - ETA: 0s - loss: 7.4439\n",
      "Epoch 00001: val_loss improved from inf to 1.20473, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.9649 - val_loss: 1.2047\n",
      "Epoch 2/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.5397\n",
      "Epoch 00002: val_loss improved from 1.20473 to 0.14080, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5346 - val_loss: 0.1408\n",
      "Epoch 3/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2244\n",
      "Epoch 00003: val_loss improved from 0.14080 to 0.11819, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2244 - val_loss: 0.1182\n",
      "Epoch 4/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2089\n",
      "Epoch 00004: val_loss improved from 0.11819 to 0.11318, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2091 - val_loss: 0.1132\n",
      "Epoch 5/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.2007\n",
      "Epoch 00005: val_loss improved from 0.11318 to 0.10821, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2010 - val_loss: 0.1082\n",
      "Epoch 6/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1911\n",
      "Epoch 00006: val_loss improved from 0.10821 to 0.10497, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1916 - val_loss: 0.1050\n",
      "Epoch 7/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1879\n",
      "Epoch 00007: val_loss improved from 0.10497 to 0.10251, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1873 - val_loss: 0.1025\n",
      "Epoch 8/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1805\n",
      "Epoch 00008: val_loss improved from 0.10251 to 0.10133, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1803 - val_loss: 0.1013\n",
      "Epoch 9/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1762\n",
      "Epoch 00009: val_loss improved from 0.10133 to 0.09963, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1763 - val_loss: 0.0996\n",
      "Epoch 10/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1738\n",
      "Epoch 00010: val_loss improved from 0.09963 to 0.09862, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1730 - val_loss: 0.0986\n",
      "Epoch 11/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.1653\n",
      "Epoch 00011: val_loss improved from 0.09862 to 0.09796, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1657 - val_loss: 0.0980\n",
      "Epoch 12/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1657\n",
      "Epoch 00012: val_loss improved from 0.09796 to 0.09732, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1657 - val_loss: 0.0973\n",
      "Epoch 13/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1656\n",
      "Epoch 00013: val_loss improved from 0.09732 to 0.09729, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1654 - val_loss: 0.0973\n",
      "Epoch 14/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1605\n",
      "Epoch 00014: val_loss did not improve from 0.09729\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1604 - val_loss: 0.0982\n",
      "Epoch 15/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.1621\n",
      "Epoch 00015: val_loss improved from 0.09729 to 0.09632, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1621 - val_loss: 0.0963\n",
      "Epoch 16/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1572\n",
      "Epoch 00016: val_loss improved from 0.09632 to 0.09589, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1569 - val_loss: 0.0959\n",
      "Epoch 17/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1558\n",
      "Epoch 00017: val_loss did not improve from 0.09589\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1558 - val_loss: 0.0964\n",
      "Epoch 18/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1551\n",
      "Epoch 00018: val_loss improved from 0.09589 to 0.09577, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1551 - val_loss: 0.0958\n",
      "Epoch 19/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1532\n",
      "Epoch 00019: val_loss did not improve from 0.09577\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1534 - val_loss: 0.0958\n",
      "Epoch 20/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1509\n",
      "Epoch 00020: val_loss improved from 0.09577 to 0.09505, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1500 - val_loss: 0.0950\n",
      "Epoch 21/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1526\n",
      "Epoch 00021: val_loss improved from 0.09505 to 0.09477, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1522 - val_loss: 0.0948\n",
      "Epoch 22/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1478\n",
      "Epoch 00022: val_loss did not improve from 0.09477\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1472 - val_loss: 0.0954\n",
      "Epoch 23/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1502\n",
      "Epoch 00023: val_loss improved from 0.09477 to 0.09450, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1498 - val_loss: 0.0945\n",
      "Epoch 24/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1486\n",
      "Epoch 00024: val_loss did not improve from 0.09450\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1488 - val_loss: 0.0971\n",
      "Epoch 25/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1493\n",
      "Epoch 00025: val_loss did not improve from 0.09450\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1487 - val_loss: 0.0956\n",
      " ###7 fold : val acc1 0.596, acc3 0.971, mae 0.216###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/140 [==========================>...] - ETA: 0s - loss: 7.5091\n",
      "Epoch 00001: val_loss improved from inf to 1.22139, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.9836 - val_loss: 1.2214\n",
      "Epoch 2/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.5773\n",
      "Epoch 00002: val_loss improved from 1.22139 to 0.14726, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5361 - val_loss: 0.1473\n",
      "Epoch 3/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2238\n",
      "Epoch 00003: val_loss improved from 0.14726 to 0.12211, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2237 - val_loss: 0.1221\n",
      "Epoch 4/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2083\n",
      "Epoch 00004: val_loss improved from 0.12211 to 0.11707, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2085 - val_loss: 0.1171\n",
      "Epoch 5/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2005\n",
      "Epoch 00005: val_loss improved from 0.11707 to 0.11224, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2010 - val_loss: 0.1122\n",
      "Epoch 6/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1918\n",
      "Epoch 00006: val_loss improved from 0.11224 to 0.10864, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1920 - val_loss: 0.1086\n",
      "Epoch 7/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1886\n",
      "Epoch 00007: val_loss improved from 0.10864 to 0.10603, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1872 - val_loss: 0.1060\n",
      "Epoch 8/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1809\n",
      "Epoch 00008: val_loss improved from 0.10603 to 0.10442, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1806 - val_loss: 0.1044\n",
      "Epoch 9/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1757\n",
      "Epoch 00009: val_loss improved from 0.10442 to 0.10270, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1758 - val_loss: 0.1027\n",
      "Epoch 10/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1727\n",
      "Epoch 00010: val_loss improved from 0.10270 to 0.10209, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1728 - val_loss: 0.1021\n",
      "Epoch 11/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1657\n",
      "Epoch 00011: val_loss improved from 0.10209 to 0.10098, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1657 - val_loss: 0.1010\n",
      "Epoch 12/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1659\n",
      "Epoch 00012: val_loss improved from 0.10098 to 0.10067, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1659 - val_loss: 0.1007\n",
      "Epoch 13/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1670\n",
      "Epoch 00013: val_loss improved from 0.10067 to 0.10008, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1664 - val_loss: 0.1001\n",
      "Epoch 14/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1602\n",
      "Epoch 00014: val_loss did not improve from 0.10008\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1603 - val_loss: 0.1004\n",
      "Epoch 15/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1617\n",
      "Epoch 00015: val_loss improved from 0.10008 to 0.09924, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1616 - val_loss: 0.0992\n",
      "Epoch 16/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1571\n",
      "Epoch 00016: val_loss improved from 0.09924 to 0.09889, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1570 - val_loss: 0.0989\n",
      "Epoch 17/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1553\n",
      "Epoch 00017: val_loss did not improve from 0.09889\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1553 - val_loss: 0.0996\n",
      "Epoch 18/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1554\n",
      "Epoch 00018: val_loss improved from 0.09889 to 0.09853, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1554 - val_loss: 0.0985\n",
      "Epoch 19/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1532\n",
      "Epoch 00019: val_loss did not improve from 0.09853\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1536 - val_loss: 0.0997\n",
      "Epoch 20/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1510\n",
      "Epoch 00020: val_loss improved from 0.09853 to 0.09832, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1499 - val_loss: 0.0983\n",
      "Epoch 21/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1532\n",
      "Epoch 00021: val_loss improved from 0.09832 to 0.09762, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1517 - val_loss: 0.0976\n",
      "Epoch 22/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1482\n",
      "Epoch 00022: val_loss did not improve from 0.09762\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1475 - val_loss: 0.0988\n",
      "Epoch 23/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1503\n",
      "Epoch 00023: val_loss did not improve from 0.09762\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1503 - val_loss: 0.0977\n",
      " ###8 fold : val acc1 0.603, acc3 0.981, mae 0.208###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/140 [==========================>...] - ETA: 0s - loss: 7.5091\n",
      "Epoch 00001: val_loss improved from inf to 1.22457, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.9836 - val_loss: 1.2246\n",
      "Epoch 2/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.5389\n",
      "Epoch 00002: val_loss improved from 1.22457 to 0.14963, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5361 - val_loss: 0.1496\n",
      "Epoch 3/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2241\n",
      "Epoch 00003: val_loss improved from 0.14963 to 0.12432, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2237 - val_loss: 0.1243\n",
      "Epoch 4/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.2083\n",
      "Epoch 00004: val_loss improved from 0.12432 to 0.12027, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2085 - val_loss: 0.1203\n",
      "Epoch 5/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.2010\n",
      "Epoch 00005: val_loss improved from 0.12027 to 0.11508, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2010 - val_loss: 0.1151\n",
      "Epoch 6/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1917\n",
      "Epoch 00006: val_loss improved from 0.11508 to 0.11138, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1920 - val_loss: 0.1114\n",
      "Epoch 7/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1887\n",
      "Epoch 00007: val_loss improved from 0.11138 to 0.10887, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1872 - val_loss: 0.1089\n",
      "Epoch 8/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1810\n",
      "Epoch 00008: val_loss improved from 0.10887 to 0.10666, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1806 - val_loss: 0.1067\n",
      "Epoch 9/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1756\n",
      "Epoch 00009: val_loss improved from 0.10666 to 0.10544, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1758 - val_loss: 0.1054\n",
      "Epoch 10/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1734\n",
      "Epoch 00010: val_loss improved from 0.10544 to 0.10423, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1728 - val_loss: 0.1042\n",
      "Epoch 11/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1657\n",
      "Epoch 00011: val_loss improved from 0.10423 to 0.10313, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1657 - val_loss: 0.1031\n",
      "Epoch 12/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1660\n",
      "Epoch 00012: val_loss did not improve from 0.10313\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1659 - val_loss: 0.1033\n",
      "Epoch 13/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1668\n",
      "Epoch 00013: val_loss improved from 0.10313 to 0.10231, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1664 - val_loss: 0.1023\n",
      "Epoch 14/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1601\n",
      "Epoch 00014: val_loss did not improve from 0.10231\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1603 - val_loss: 0.1030\n",
      "Epoch 15/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1617\n",
      "Epoch 00015: val_loss improved from 0.10231 to 0.10222, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1616 - val_loss: 0.1022\n",
      "Epoch 16/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.1570\n",
      "Epoch 00016: val_loss improved from 0.10222 to 0.10142, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1570 - val_loss: 0.1014\n",
      "Epoch 17/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1558\n",
      "Epoch 00017: val_loss did not improve from 0.10142\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1553 - val_loss: 0.1019\n",
      "Epoch 18/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1558\n",
      "Epoch 00018: val_loss improved from 0.10142 to 0.10076, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1554 - val_loss: 0.1008\n",
      "Epoch 19/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1533\n",
      "Epoch 00019: val_loss did not improve from 0.10076\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1536 - val_loss: 0.1011\n",
      "Epoch 20/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1506\n",
      "Epoch 00020: val_loss improved from 0.10076 to 0.10025, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1499 - val_loss: 0.1002\n",
      "Epoch 21/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1531\n",
      "Epoch 00021: val_loss improved from 0.10025 to 0.09974, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes256_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1517 - val_loss: 0.0997\n",
      "Epoch 22/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1482\n",
      "Epoch 00022: val_loss did not improve from 0.09974\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1475 - val_loss: 0.1017\n",
      "Epoch 23/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1502\n",
      "Epoch 00023: val_loss did not improve from 0.09974\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1503 - val_loss: 0.0999\n",
      " ###9 fold : val acc1 0.598, acc3 0.983, mae 0.210###\n",
      "acc10.601_acc30.980\n",
      "random search 71/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/279 [===========================>..] - ETA: 0s - loss: 1.5641\n",
      "Epoch 00001: val_loss improved from inf to 0.13073, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.5,dnodes128_dropout0.5,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 1.5247 - val_loss: 0.1307\n",
      "Epoch 2/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.6098\n",
      "Epoch 00002: val_loss improved from 0.13073 to 0.10849, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.5,dnodes128_dropout0.5,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.6104 - val_loss: 0.1085\n",
      "Epoch 3/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.5465\n",
      "Epoch 00003: val_loss did not improve from 0.10849\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.5465 - val_loss: 0.1159\n",
      "Epoch 4/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.4929\n",
      "Epoch 00004: val_loss improved from 0.10849 to 0.10485, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.5,dnodes128_dropout0.5,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4915 - val_loss: 0.1048\n",
      "Epoch 5/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.4470\n",
      "Epoch 00005: val_loss improved from 0.10485 to 0.10339, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.5,dnodes128_dropout0.5,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4473 - val_loss: 0.1034\n",
      "Epoch 6/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.4054\n",
      "Epoch 00006: val_loss did not improve from 0.10339\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4056 - val_loss: 0.1140\n",
      "Epoch 7/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.3718\n",
      "Epoch 00007: val_loss did not improve from 0.10339\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3710 - val_loss: 0.1041\n",
      " ###0 fold : val acc1 0.577, acc3 0.977, mae 0.223###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/279 [============================>.] - ETA: 0s - loss: 1.5168\n",
      "Epoch 00001: val_loss improved from inf to 0.12982, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.5,dnodes128_dropout0.5,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 1.5068 - val_loss: 0.1298\n",
      "Epoch 2/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.6124\n",
      "Epoch 00002: val_loss improved from 0.12982 to 0.10408, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.5,dnodes128_dropout0.5,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.6134 - val_loss: 0.1041\n",
      "Epoch 3/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.5478\n",
      "Epoch 00003: val_loss did not improve from 0.10408\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.5474 - val_loss: 0.1100\n",
      "Epoch 4/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.4928\n",
      "Epoch 00004: val_loss improved from 0.10408 to 0.10238, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.5,dnodes128_dropout0.5,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4918 - val_loss: 0.1024\n",
      "Epoch 5/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.4417\n",
      "Epoch 00005: val_loss did not improve from 0.10238\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4422 - val_loss: 0.1059\n",
      "Epoch 6/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.4029\n",
      "Epoch 00006: val_loss did not improve from 0.10238\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4026 - val_loss: 0.1192\n",
      " ###1 fold : val acc1 0.584, acc3 0.979, mae 0.219###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/279 [============================>.] - ETA: 0s - loss: 1.5241\n",
      "Epoch 00001: val_loss improved from inf to 0.12561, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.5,dnodes128_dropout0.5,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 1.5176 - val_loss: 0.1256\n",
      "Epoch 2/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.6208\n",
      "Epoch 00002: val_loss improved from 0.12561 to 0.10737, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.5,dnodes128_dropout0.5,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.6215 - val_loss: 0.1074\n",
      "Epoch 3/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.5592\n",
      "Epoch 00003: val_loss did not improve from 0.10737\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.5589 - val_loss: 0.1136\n",
      "Epoch 4/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.4910\n",
      "Epoch 00004: val_loss improved from 0.10737 to 0.10218, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes256_dropout0.5,dnodes128_dropout0.5,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4903 - val_loss: 0.1022\n",
      "Epoch 5/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.4417\n",
      "Epoch 00005: val_loss did not improve from 0.10218\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4428 - val_loss: 0.1032\n",
      "Epoch 6/100\n",
      "127/279 [============>.................] - ETA: 0s - loss: 0.4117"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551/558 [============================>.] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00008: val_loss improved from 0.09860 to 0.09736, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0986 - val_loss: 0.0974\n",
      "Epoch 9/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00009: val_loss did not improve from 0.09736\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0973 - val_loss: 0.1010\n",
      "Epoch 10/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00010: val_loss improved from 0.09736 to 0.09553, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0969 - val_loss: 0.0955\n",
      "Epoch 11/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.0963\n",
      "Epoch 00011: val_loss improved from 0.09553 to 0.09473, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0964 - val_loss: 0.0947\n",
      "Epoch 12/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.0963\n",
      "Epoch 00012: val_loss did not improve from 0.09473\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0962 - val_loss: 0.0954\n",
      "Epoch 13/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 0.0962\n",
      "Epoch 00013: val_loss did not improve from 0.09473\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0960 - val_loss: 0.0958\n",
      " ###7 fold : val acc1 0.588, acc3 0.972, mae 0.220###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541/558 [============================>.] - ETA: 0s - loss: 5.7342\n",
      "Epoch 00001: val_loss improved from inf to 0.55404, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 5.5841 - val_loss: 0.5540\n",
      "Epoch 2/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.2211\n",
      "Epoch 00002: val_loss improved from 0.55404 to 0.13319, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.2188 - val_loss: 0.1332\n",
      "Epoch 3/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.1211\n",
      "Epoch 00003: val_loss improved from 0.13319 to 0.11895, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1209 - val_loss: 0.1189\n",
      "Epoch 4/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.1108\n",
      "Epoch 00004: val_loss improved from 0.11895 to 0.11066, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1109 - val_loss: 0.1107\n",
      "Epoch 5/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.1049\n",
      "Epoch 00005: val_loss improved from 0.11066 to 0.10745, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1050 - val_loss: 0.1074\n",
      "Epoch 6/100\n",
      "541/558 [============================>.] - ETA: 0s - loss: 0.1013\n",
      "Epoch 00006: val_loss improved from 0.10745 to 0.10432, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1016 - val_loss: 0.1043\n",
      "Epoch 7/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00007: val_loss improved from 0.10432 to 0.10200, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0996 - val_loss: 0.1020\n",
      "Epoch 8/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.0988\n",
      "Epoch 00008: val_loss improved from 0.10200 to 0.10073, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0988 - val_loss: 0.1007\n",
      "Epoch 9/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.0972\n",
      "Epoch 00009: val_loss did not improve from 0.10073\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0973 - val_loss: 0.1035\n",
      "Epoch 10/100\n",
      "539/558 [===========================>..] - ETA: 0s - loss: 0.0972\n",
      "Epoch 00010: val_loss improved from 0.10073 to 0.09833, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0970 - val_loss: 0.0983\n",
      "Epoch 11/100\n",
      "539/558 [===========================>..] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00011: val_loss improved from 0.09833 to 0.09748, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0964 - val_loss: 0.0975\n",
      "Epoch 12/100\n",
      "541/558 [============================>.] - ETA: 0s - loss: 0.0964\n",
      "Epoch 00012: val_loss did not improve from 0.09748\n",
      "558/558 [==============================] - 1s 2ms/step - loss: 0.0963 - val_loss: 0.0980\n",
      "Epoch 13/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.0962\n",
      "Epoch 00013: val_loss did not improve from 0.09748\n",
      "558/558 [==============================] - 1s 3ms/step - loss: 0.0960 - val_loss: 0.0987\n",
      " ###8 fold : val acc1 0.599, acc3 0.982, mae 0.210###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/558 [============================>.] - ETA: 0s - loss: 5.6496\n",
      "Epoch 00001: val_loss improved from inf to 0.55746, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 5.5841 - val_loss: 0.5575\n",
      "Epoch 2/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.2211\n",
      "Epoch 00002: val_loss improved from 0.55746 to 0.13453, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.2188 - val_loss: 0.1345\n",
      "Epoch 3/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.1210\n",
      "Epoch 00003: val_loss improved from 0.13453 to 0.12090, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1209 - val_loss: 0.1209\n",
      "Epoch 4/100\n",
      "543/558 [============================>.] - ETA: 0s - loss: 0.1107\n",
      "Epoch 00004: val_loss improved from 0.12090 to 0.11281, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1109 - val_loss: 0.1128\n",
      "Epoch 5/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.1049\n",
      "Epoch 00005: val_loss improved from 0.11281 to 0.10937, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1050 - val_loss: 0.1094\n",
      "Epoch 6/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.1014\n",
      "Epoch 00006: val_loss improved from 0.10937 to 0.10645, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1016 - val_loss: 0.1064\n",
      "Epoch 7/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00007: val_loss improved from 0.10645 to 0.10441, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0996 - val_loss: 0.1044\n",
      "Epoch 8/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.0990\n",
      "Epoch 00008: val_loss improved from 0.10441 to 0.10291, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0988 - val_loss: 0.1029\n",
      "Epoch 9/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.0972\n",
      "Epoch 00009: val_loss did not improve from 0.10291\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0973 - val_loss: 0.1060\n",
      "Epoch 10/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00010: val_loss improved from 0.10291 to 0.10102, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0970 - val_loss: 0.1010\n",
      "Epoch 11/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00011: val_loss improved from 0.10102 to 0.09982, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes32_dropout0,lr0.001/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0964 - val_loss: 0.0998\n",
      "Epoch 12/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.0964\n",
      "Epoch 00012: val_loss did not improve from 0.09982\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0963 - val_loss: 0.1006\n",
      "Epoch 13/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00013: val_loss did not improve from 0.09982\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0960 - val_loss: 0.1010\n",
      " ###9 fold : val acc1 0.602, acc3 0.983, mae 0.208###\n",
      "acc10.596_acc30.981\n",
      "random search 74/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 16.3834 \n",
      "Epoch 00001: val_loss improved from inf to 3.14047, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 12.0303 - val_loss: 3.1405\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.8761\n",
      "Epoch 00002: val_loss improved from 3.14047 to 0.92602, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3901 - val_loss: 0.9260\n",
      "Epoch 3/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.0832\n",
      "Epoch 00003: val_loss improved from 0.92602 to 0.27312, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9690 - val_loss: 0.2731\n",
      "Epoch 4/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6442\n",
      "Epoch 00004: val_loss improved from 0.27312 to 0.14353, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6053 - val_loss: 0.1435\n",
      "Epoch 5/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.5376\n",
      "Epoch 00005: val_loss improved from 0.14353 to 0.13956, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5230 - val_loss: 0.1396\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5041\n",
      "Epoch 00006: val_loss improved from 0.13956 to 0.12820, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4960 - val_loss: 0.1282\n",
      "Epoch 7/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.4769\n",
      "Epoch 00007: val_loss improved from 0.12820 to 0.12796, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4696 - val_loss: 0.1280\n",
      "Epoch 8/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.4535\n",
      "Epoch 00008: val_loss improved from 0.12796 to 0.11899, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4479 - val_loss: 0.1190\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.4261\n",
      "Epoch 00009: val_loss did not improve from 0.11899\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4190 - val_loss: 0.1228\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4093\n",
      "Epoch 00010: val_loss improved from 0.11899 to 0.11749, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4061 - val_loss: 0.1175\n",
      "Epoch 11/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3718\n",
      "Epoch 00011: val_loss did not improve from 0.11749\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3714 - val_loss: 0.1207\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3381\n",
      "Epoch 00012: val_loss improved from 0.11749 to 0.11445, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3381 - val_loss: 0.1144\n",
      "Epoch 13/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3341\n",
      "Epoch 00013: val_loss improved from 0.11445 to 0.10899, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3290 - val_loss: 0.1090\n",
      "Epoch 14/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3051\n",
      "Epoch 00014: val_loss did not improve from 0.10899\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2990 - val_loss: 0.1334\n",
      "Epoch 15/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2903\n",
      "Epoch 00015: val_loss did not improve from 0.10899\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2863 - val_loss: 0.1122\n",
      " ###0 fold : val acc1 0.577, acc3 0.973, mae 0.226###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 16.0672 \n",
      "Epoch 00001: val_loss improved from inf to 3.14033, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 12.0298 - val_loss: 3.1403\n",
      "Epoch 2/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.9541\n",
      "Epoch 00002: val_loss improved from 3.14033 to 0.92689, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3819 - val_loss: 0.9269\n",
      "Epoch 3/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.1019\n",
      "Epoch 00003: val_loss improved from 0.92689 to 0.27845, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9726 - val_loss: 0.2784\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6475\n",
      "Epoch 00004: val_loss improved from 0.27845 to 0.14501, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6065 - val_loss: 0.1450\n",
      "Epoch 5/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5402\n",
      "Epoch 00005: val_loss improved from 0.14501 to 0.14019, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5249 - val_loss: 0.1402\n",
      "Epoch 6/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4972\n",
      "Epoch 00006: val_loss improved from 0.14019 to 0.12974, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4950 - val_loss: 0.1297\n",
      "Epoch 7/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4744\n",
      "Epoch 00007: val_loss did not improve from 0.12974\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4684 - val_loss: 0.1306\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4526\n",
      "Epoch 00008: val_loss improved from 0.12974 to 0.12025, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4483 - val_loss: 0.1202\n",
      "Epoch 9/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.4207\n",
      "Epoch 00009: val_loss did not improve from 0.12025\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4207 - val_loss: 0.1222\n",
      "Epoch 10/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4021\n",
      "Epoch 00010: val_loss improved from 0.12025 to 0.11802, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4041 - val_loss: 0.1180\n",
      "Epoch 11/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3706\n",
      "Epoch 00011: val_loss did not improve from 0.11802\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3706 - val_loss: 0.1202\n",
      "Epoch 12/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3356\n",
      "Epoch 00012: val_loss improved from 0.11802 to 0.11395, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3353 - val_loss: 0.1139\n",
      "Epoch 13/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.3297\n",
      "Epoch 00013: val_loss improved from 0.11395 to 0.10833, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3286 - val_loss: 0.1083\n",
      "Epoch 14/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3001\n",
      "Epoch 00014: val_loss did not improve from 0.10833\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2996 - val_loss: 0.1260\n",
      "Epoch 15/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2856\n",
      "Epoch 00015: val_loss did not improve from 0.10833\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2834 - val_loss: 0.1121\n",
      " ###1 fold : val acc1 0.573, acc3 0.977, mae 0.226###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 16.3509 \n",
      "Epoch 00001: val_loss improved from inf to 3.12865, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 12.0072 - val_loss: 3.1287\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.9244\n",
      "Epoch 00002: val_loss improved from 3.12865 to 0.91914, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3963 - val_loss: 0.9191\n",
      "Epoch 3/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.1120\n",
      "Epoch 00003: val_loss improved from 0.91914 to 0.27830, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9784 - val_loss: 0.2783\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6493\n",
      "Epoch 00004: val_loss improved from 0.27830 to 0.14360, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6069 - val_loss: 0.1436\n",
      "Epoch 5/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5440\n",
      "Epoch 00005: val_loss improved from 0.14360 to 0.13965, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5311 - val_loss: 0.1397\n",
      "Epoch 6/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4966\n",
      "Epoch 00006: val_loss improved from 0.13965 to 0.13149, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4927 - val_loss: 0.1315\n",
      "Epoch 7/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4761\n",
      "Epoch 00007: val_loss did not improve from 0.13149\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4691 - val_loss: 0.1329\n",
      "Epoch 8/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4469\n",
      "Epoch 00008: val_loss improved from 0.13149 to 0.12155, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4460 - val_loss: 0.1216\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4266\n",
      "Epoch 00009: val_loss did not improve from 0.12155\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4197 - val_loss: 0.1224\n",
      "Epoch 10/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4017\n",
      "Epoch 00010: val_loss improved from 0.12155 to 0.11929, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4026 - val_loss: 0.1193\n",
      "Epoch 11/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3696\n",
      "Epoch 00011: val_loss improved from 0.11929 to 0.11882, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3700 - val_loss: 0.1188\n",
      "Epoch 12/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.3389\n",
      "Epoch 00012: val_loss improved from 0.11882 to 0.11455, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3389 - val_loss: 0.1145\n",
      "Epoch 13/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3275\n",
      "Epoch 00013: val_loss improved from 0.11455 to 0.10972, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.3273 - val_loss: 0.1097\n",
      "Epoch 14/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2996\n",
      "Epoch 00014: val_loss did not improve from 0.10972\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2993 - val_loss: 0.1265\n",
      "Epoch 15/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2850\n",
      "Epoch 00015: val_loss did not improve from 0.10972\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2835 - val_loss: 0.1108\n",
      " ###2 fold : val acc1 0.567, acc3 0.973, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 16.3642 \n",
      "Epoch 00001: val_loss improved from inf to 3.12066, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 12.0303 - val_loss: 3.1207\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.9273\n",
      "Epoch 00002: val_loss improved from 3.12066 to 0.91747, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.4029 - val_loss: 0.9175\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.1118\n",
      "Epoch 00003: val_loss improved from 0.91747 to 0.28087, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9899 - val_loss: 0.2809\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6550\n",
      "Epoch 00004: val_loss improved from 0.28087 to 0.14529, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6119 - val_loss: 0.1453\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5458\n",
      "Epoch 00005: val_loss improved from 0.14529 to 0.13663, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5326 - val_loss: 0.1366\n",
      "Epoch 6/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.4978\n",
      "Epoch 00006: val_loss improved from 0.13663 to 0.13360, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4916 - val_loss: 0.1336\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4779\n",
      "Epoch 00007: val_loss improved from 0.13360 to 0.13348, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4675 - val_loss: 0.1335\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4578\n",
      "Epoch 00008: val_loss improved from 0.13348 to 0.12233, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4491 - val_loss: 0.1223\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4283\n",
      "Epoch 00009: val_loss did not improve from 0.12233\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4160 - val_loss: 0.1236\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4022\n",
      "Epoch 00010: val_loss improved from 0.12233 to 0.11974, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4024 - val_loss: 0.1197\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3731\n",
      "Epoch 00011: val_loss did not improve from 0.11974\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3731 - val_loss: 0.1205\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3395\n",
      "Epoch 00012: val_loss improved from 0.11974 to 0.11710, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3395 - val_loss: 0.1171\n",
      "Epoch 13/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 0.3363\n",
      "Epoch 00013: val_loss improved from 0.11710 to 0.10982, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3287 - val_loss: 0.1098\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3090\n",
      "Epoch 00014: val_loss did not improve from 0.10982\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3005 - val_loss: 0.1262\n",
      "Epoch 15/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2915\n",
      "Epoch 00015: val_loss did not improve from 0.10982\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2859 - val_loss: 0.1102\n",
      " ###3 fold : val acc1 0.581, acc3 0.964, mae 0.229###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 16.3643 \n",
      "Epoch 00001: val_loss improved from inf to 3.15194, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 11.9995 - val_loss: 3.1519\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.9423\n",
      "Epoch 00002: val_loss improved from 3.15194 to 0.89659, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3816 - val_loss: 0.8966\n",
      "Epoch 3/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.1235\n",
      "Epoch 00003: val_loss improved from 0.89659 to 0.25386, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9765 - val_loss: 0.2539\n",
      "Epoch 4/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6325\n",
      "Epoch 00004: val_loss improved from 0.25386 to 0.14514, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6071 - val_loss: 0.1451\n",
      "Epoch 5/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5303\n",
      "Epoch 00005: val_loss improved from 0.14514 to 0.13364, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5306 - val_loss: 0.1336\n",
      "Epoch 6/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 0.5077\n",
      "Epoch 00006: val_loss did not improve from 0.13364\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4992 - val_loss: 0.1344\n",
      "Epoch 7/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4682\n",
      "Epoch 00007: val_loss improved from 0.13364 to 0.12461, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4679 - val_loss: 0.1246\n",
      "Epoch 8/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4506\n",
      "Epoch 00008: val_loss did not improve from 0.12461\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4506 - val_loss: 0.1361\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4168\n",
      "Epoch 00009: val_loss did not improve from 0.12461\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4168 - val_loss: 0.1295\n",
      " ###4 fold : val acc1 0.536, acc3 0.953, mae 0.256###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 16.3477 \n",
      "Epoch 00001: val_loss improved from inf to 3.13086, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 11.9953 - val_loss: 3.1309\n",
      "Epoch 2/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.9760\n",
      "Epoch 00002: val_loss improved from 3.13086 to 0.90256, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3830 - val_loss: 0.9026\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.9841\n",
      "Epoch 00003: val_loss improved from 0.90256 to 0.25885, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9841 - val_loss: 0.2589\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6054\n",
      "Epoch 00004: val_loss improved from 0.25885 to 0.14552, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6054 - val_loss: 0.1455\n",
      "Epoch 5/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5261\n",
      "Epoch 00005: val_loss improved from 0.14552 to 0.13304, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5271 - val_loss: 0.1330\n",
      "Epoch 6/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4924\n",
      "Epoch 00006: val_loss did not improve from 0.13304\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4930 - val_loss: 0.1349\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4682\n",
      "Epoch 00007: val_loss improved from 0.13304 to 0.12523, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4682 - val_loss: 0.1252\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4600\n",
      "Epoch 00008: val_loss did not improve from 0.12523\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4528 - val_loss: 0.1345\n",
      "Epoch 9/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4168\n",
      "Epoch 00009: val_loss did not improve from 0.12523\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4154 - val_loss: 0.1268\n",
      " ###5 fold : val acc1 0.524, acc3 0.960, mae 0.259###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/35 [============================>.] - ETA: 0s - loss: 12.2162\n",
      "Epoch 00001: val_loss improved from inf to 3.12958, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 12.0084 - val_loss: 3.1296\n",
      "Epoch 2/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.9820\n",
      "Epoch 00002: val_loss improved from 3.12958 to 0.88597, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3751 - val_loss: 0.8860\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.9784\n",
      "Epoch 00003: val_loss improved from 0.88597 to 0.25876, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9784 - val_loss: 0.2588\n",
      "Epoch 4/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6321\n",
      "Epoch 00004: val_loss improved from 0.25876 to 0.14754, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6051 - val_loss: 0.1475\n",
      "Epoch 5/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5266\n",
      "Epoch 00005: val_loss improved from 0.14754 to 0.13492, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5282 - val_loss: 0.1349\n",
      "Epoch 6/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4963\n",
      "Epoch 00006: val_loss improved from 0.13492 to 0.13424, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4965 - val_loss: 0.1342\n",
      "Epoch 7/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4864\n",
      "Epoch 00007: val_loss improved from 0.13424 to 0.12445, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4697 - val_loss: 0.1245\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4594\n",
      "Epoch 00008: val_loss did not improve from 0.12445\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4516 - val_loss: 0.1369\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4204\n",
      "Epoch 00009: val_loss did not improve from 0.12445\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4204 - val_loss: 0.1267\n",
      " ###6 fold : val acc1 0.552, acc3 0.960, mae 0.244###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/35 [===============>..............] - ETA: 0s - loss: 16.6624 \n",
      "Epoch 00001: val_loss improved from inf to 3.13210, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 12.0045 - val_loss: 3.1321\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 2.3876\n",
      "Epoch 00002: val_loss improved from 3.13210 to 0.88770, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3876 - val_loss: 0.8877\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.9715\n",
      "Epoch 00003: val_loss improved from 0.88770 to 0.26300, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9715 - val_loss: 0.2630\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6004\n",
      "Epoch 00004: val_loss improved from 0.26300 to 0.14789, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6004 - val_loss: 0.1479\n",
      "Epoch 5/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5287\n",
      "Epoch 00005: val_loss improved from 0.14789 to 0.13546, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5301 - val_loss: 0.1355\n",
      "Epoch 6/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.4987\n",
      "Epoch 00006: val_loss did not improve from 0.13546\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4963 - val_loss: 0.1395\n",
      "Epoch 7/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.4754\n",
      "Epoch 00007: val_loss improved from 0.13546 to 0.12425, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4720 - val_loss: 0.1242\n",
      "Epoch 8/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.4503\n",
      "Epoch 00008: val_loss did not improve from 0.12425\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4498 - val_loss: 0.1377\n",
      "Epoch 9/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.4180\n",
      "Epoch 00009: val_loss did not improve from 0.12425\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4183 - val_loss: 0.1279\n",
      " ###7 fold : val acc1 0.527, acc3 0.948, mae 0.264###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 16.3775 \n",
      "Epoch 00001: val_loss improved from inf to 3.09097, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 12.0311 - val_loss: 3.0910\n",
      "Epoch 2/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 3.0002\n",
      "Epoch 00002: val_loss improved from 3.09097 to 0.89837, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3894 - val_loss: 0.8984\n",
      "Epoch 3/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.1195\n",
      "Epoch 00003: val_loss improved from 0.89837 to 0.26983, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9684 - val_loss: 0.2698\n",
      "Epoch 4/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6305\n",
      "Epoch 00004: val_loss improved from 0.26983 to 0.15426, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6026 - val_loss: 0.1543\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5312\n",
      "Epoch 00005: val_loss improved from 0.15426 to 0.14155, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5329 - val_loss: 0.1415\n",
      "Epoch 6/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5055\n",
      "Epoch 00006: val_loss did not improve from 0.14155\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4951 - val_loss: 0.1456\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4709\n",
      "Epoch 00007: val_loss improved from 0.14155 to 0.12874, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4709 - val_loss: 0.1287\n",
      "Epoch 8/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4495\n",
      "Epoch 00008: val_loss did not improve from 0.12874\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4489 - val_loss: 0.1440\n",
      "Epoch 9/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4214\n",
      "Epoch 00009: val_loss did not improve from 0.12874\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4195 - val_loss: 0.1326\n",
      " ###8 fold : val acc1 0.542, acc3 0.955, mae 0.252###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 16.3775 \n",
      "Epoch 00001: val_loss improved from inf to 3.12651, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 12.0311 - val_loss: 3.1265\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.8973\n",
      "Epoch 00002: val_loss improved from 3.12651 to 0.90431, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3894 - val_loss: 0.9043\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.1083\n",
      "Epoch 00003: val_loss improved from 0.90431 to 0.27359, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9684 - val_loss: 0.2736\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6026\n",
      "Epoch 00004: val_loss improved from 0.27359 to 0.15725, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6026 - val_loss: 0.1572\n",
      "Epoch 5/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5319\n",
      "Epoch 00005: val_loss improved from 0.15725 to 0.14412, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5329 - val_loss: 0.1441\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4951\n",
      "Epoch 00006: val_loss did not improve from 0.14412\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4951 - val_loss: 0.1482\n",
      "Epoch 7/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4721\n",
      "Epoch 00007: val_loss improved from 0.14412 to 0.13120, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,dnodes64_dropout0.1,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4709 - val_loss: 0.1312\n",
      "Epoch 8/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4495\n",
      "Epoch 00008: val_loss did not improve from 0.13120\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4489 - val_loss: 0.1474\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4195\n",
      "Epoch 00009: val_loss did not improve from 0.13120\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4195 - val_loss: 0.1358\n",
      " ###9 fold : val acc1 0.542, acc3 0.963, mae 0.248###\n",
      "acc10.552_acc30.963\n",
      "random search 75/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 22.8581 \n",
      "Epoch 00001: val_loss improved from inf to 17.73624, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 21.4108 - val_loss: 17.7362\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 16.2487\n",
      "Epoch 00002: val_loss improved from 17.73624 to 12.12940, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 15.0806 - val_loss: 12.1294\n",
      "Epoch 3/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 10.9231\n",
      "Epoch 00003: val_loss improved from 12.12940 to 7.74702, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0563 - val_loss: 7.7470\n",
      "Epoch 4/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 7.0017\n",
      "Epoch 00004: val_loss improved from 7.74702 to 4.87805, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 6.4176 - val_loss: 4.8781\n",
      "Epoch 5/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 4.5665\n",
      "Epoch 00005: val_loss improved from 4.87805 to 3.35551, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.2787 - val_loss: 3.3555\n",
      "Epoch 6/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.2818\n",
      "Epoch 00006: val_loss improved from 3.35551 to 2.46198, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1301 - val_loss: 2.4620\n",
      "Epoch 7/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.5138\n",
      "Epoch 00007: val_loss improved from 2.46198 to 1.79958, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3886 - val_loss: 1.7996\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.9101\n",
      "Epoch 00008: val_loss improved from 1.79958 to 1.28806, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.7997 - val_loss: 1.2881\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.4344\n",
      "Epoch 00009: val_loss improved from 1.28806 to 0.91187, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.3612 - val_loss: 0.9119\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.0857\n",
      "Epoch 00010: val_loss improved from 0.91187 to 0.64858, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0285 - val_loss: 0.6486\n",
      "Epoch 11/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8543\n",
      "Epoch 00011: val_loss improved from 0.64858 to 0.46787, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8155 - val_loss: 0.4679\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6825\n",
      "Epoch 00012: val_loss improved from 0.46787 to 0.34326, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6698 - val_loss: 0.3433\n",
      "Epoch 13/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5869\n",
      "Epoch 00013: val_loss improved from 0.34326 to 0.25991, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5621 - val_loss: 0.2599\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4953\n",
      "Epoch 00014: val_loss improved from 0.25991 to 0.20623, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4953 - val_loss: 0.2062\n",
      "Epoch 15/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4524\n",
      "Epoch 00015: val_loss improved from 0.20623 to 0.17318, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4520 - val_loss: 0.1732\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4297\n",
      "Epoch 00016: val_loss improved from 0.17318 to 0.15366, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4297 - val_loss: 0.1537\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4050\n",
      "Epoch 00017: val_loss improved from 0.15366 to 0.14143, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4050 - val_loss: 0.1414\n",
      "Epoch 18/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3871\n",
      "Epoch 00018: val_loss improved from 0.14143 to 0.13494, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3891 - val_loss: 0.1349\n",
      "Epoch 19/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3902\n",
      "Epoch 00019: val_loss improved from 0.13494 to 0.12976, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3860 - val_loss: 0.1298\n",
      "Epoch 20/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3866\n",
      "Epoch 00020: val_loss improved from 0.12976 to 0.12637, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3812 - val_loss: 0.1264\n",
      "Epoch 21/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3839\n",
      "Epoch 00021: val_loss improved from 0.12637 to 0.12465, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3842 - val_loss: 0.1247\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3759\n",
      "Epoch 00022: val_loss improved from 0.12465 to 0.12271, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3692 - val_loss: 0.1227\n",
      "Epoch 23/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3718\n",
      "Epoch 00023: val_loss improved from 0.12271 to 0.12128, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3700 - val_loss: 0.1213\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3619\n",
      "Epoch 00024: val_loss improved from 0.12128 to 0.12042, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3581 - val_loss: 0.1204\n",
      "Epoch 25/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3584\n",
      "Epoch 00025: val_loss improved from 0.12042 to 0.11964, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3574 - val_loss: 0.1196\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3548\n",
      "Epoch 00026: val_loss improved from 0.11964 to 0.11840, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3548 - val_loss: 0.1184\n",
      "Epoch 27/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3448\n",
      "Epoch 00027: val_loss improved from 0.11840 to 0.11789, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3432 - val_loss: 0.1179\n",
      "Epoch 28/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3518\n",
      "Epoch 00028: val_loss improved from 0.11789 to 0.11635, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3512 - val_loss: 0.1164\n",
      "Epoch 29/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3461\n",
      "Epoch 00029: val_loss improved from 0.11635 to 0.11597, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3449 - val_loss: 0.1160\n",
      "Epoch 30/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3366\n",
      "Epoch 00030: val_loss did not improve from 0.11597\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3393 - val_loss: 0.1161\n",
      "Epoch 31/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3348\n",
      "Epoch 00031: val_loss improved from 0.11597 to 0.11457, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3345 - val_loss: 0.1146\n",
      "Epoch 32/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3299\n",
      "Epoch 00032: val_loss improved from 0.11457 to 0.11426, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3322 - val_loss: 0.1143\n",
      "Epoch 33/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3274\n",
      "Epoch 00033: val_loss improved from 0.11426 to 0.11360, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3301 - val_loss: 0.1136\n",
      "Epoch 34/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3248\n",
      "Epoch 00034: val_loss improved from 0.11360 to 0.11277, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3219 - val_loss: 0.1128\n",
      "Epoch 35/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3291\n",
      "Epoch 00035: val_loss improved from 0.11277 to 0.11214, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3277 - val_loss: 0.1121\n",
      "Epoch 36/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3236\n",
      "Epoch 00036: val_loss improved from 0.11214 to 0.11164, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3276 - val_loss: 0.1116\n",
      "Epoch 37/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3196\n",
      "Epoch 00037: val_loss improved from 0.11164 to 0.11047, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3198 - val_loss: 0.1105\n",
      "Epoch 38/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3199\n",
      "Epoch 00038: val_loss did not improve from 0.11047\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3207 - val_loss: 0.1106\n",
      "Epoch 39/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3152\n",
      "Epoch 00039: val_loss improved from 0.11047 to 0.10974, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3136 - val_loss: 0.1097\n",
      "Epoch 40/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3076\n",
      "Epoch 00040: val_loss improved from 0.10974 to 0.10929, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3084 - val_loss: 0.1093\n",
      "Epoch 41/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3086\n",
      "Epoch 00041: val_loss improved from 0.10929 to 0.10834, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3069 - val_loss: 0.1083\n",
      "Epoch 42/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3053\n",
      "Epoch 00042: val_loss did not improve from 0.10834\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3081 - val_loss: 0.1088\n",
      "Epoch 43/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3034\n",
      "Epoch 00043: val_loss improved from 0.10834 to 0.10779, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3046 - val_loss: 0.1078\n",
      "Epoch 44/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3080\n",
      "Epoch 00044: val_loss improved from 0.10779 to 0.10725, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3078 - val_loss: 0.1072\n",
      "Epoch 45/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3116\n",
      "Epoch 00045: val_loss did not improve from 0.10725\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3079 - val_loss: 0.1077\n",
      "Epoch 46/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3034\n",
      "Epoch 00046: val_loss improved from 0.10725 to 0.10650, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3011 - val_loss: 0.1065\n",
      "Epoch 47/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2971\n",
      "Epoch 00047: val_loss improved from 0.10650 to 0.10598, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2989 - val_loss: 0.1060\n",
      "Epoch 48/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3004\n",
      "Epoch 00048: val_loss improved from 0.10598 to 0.10593, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2997 - val_loss: 0.1059\n",
      "Epoch 49/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2848\n",
      "Epoch 00049: val_loss improved from 0.10593 to 0.10504, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2848 - val_loss: 0.1050\n",
      "Epoch 50/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2957\n",
      "Epoch 00050: val_loss did not improve from 0.10504\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2939 - val_loss: 0.1051\n",
      "Epoch 51/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2936\n",
      "Epoch 00051: val_loss did not improve from 0.10504\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2944 - val_loss: 0.1051\n",
      " ###0 fold : val acc1 0.598, acc3 0.975, mae 0.214###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 22.8385 \n",
      "Epoch 00001: val_loss improved from inf to 17.73771, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 21.3933 - val_loss: 17.7377\n",
      "Epoch 2/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 16.1343\n",
      "Epoch 00002: val_loss improved from 17.73771 to 12.13183, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 15.0716 - val_loss: 12.1318\n",
      "Epoch 3/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 10.8809\n",
      "Epoch 00003: val_loss improved from 12.13183 to 7.75030, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0634 - val_loss: 7.7503\n",
      "Epoch 4/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 6.9895\n",
      "Epoch 00004: val_loss improved from 7.75030 to 4.88128, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 6.4264 - val_loss: 4.8813\n",
      "Epoch 5/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 4.5980\n",
      "Epoch 00005: val_loss improved from 4.88128 to 3.35812, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.2892 - val_loss: 3.3581\n",
      "Epoch 6/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.2938\n",
      "Epoch 00006: val_loss improved from 3.35812 to 2.46377, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1408 - val_loss: 2.4638\n",
      "Epoch 7/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.5241\n",
      "Epoch 00007: val_loss improved from 2.46377 to 1.79820, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3933 - val_loss: 1.7982\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.8959\n",
      "Epoch 00008: val_loss improved from 1.79820 to 1.28418, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.7964 - val_loss: 1.2842\n",
      "Epoch 9/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.4267\n",
      "Epoch 00009: val_loss improved from 1.28418 to 0.90535, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.3586 - val_loss: 0.9053\n",
      "Epoch 10/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0908\n",
      "Epoch 00010: val_loss improved from 0.90535 to 0.64109, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0280 - val_loss: 0.6411\n",
      "Epoch 11/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8435\n",
      "Epoch 00011: val_loss improved from 0.64109 to 0.46142, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8097 - val_loss: 0.4614\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6717\n",
      "Epoch 00012: val_loss improved from 0.46142 to 0.33811, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6558 - val_loss: 0.3381\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5580\n",
      "Epoch 00013: val_loss improved from 0.33811 to 0.25629, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5580 - val_loss: 0.2563\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4918\n",
      "Epoch 00014: val_loss improved from 0.25629 to 0.20400, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4918 - val_loss: 0.2040\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4477\n",
      "Epoch 00015: val_loss improved from 0.20400 to 0.17197, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4477 - val_loss: 0.1720\n",
      "Epoch 16/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4290\n",
      "Epoch 00016: val_loss improved from 0.17197 to 0.15282, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4285 - val_loss: 0.1528\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4056\n",
      "Epoch 00017: val_loss improved from 0.15282 to 0.14111, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4056 - val_loss: 0.1411\n",
      "Epoch 18/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3882\n",
      "Epoch 00018: val_loss improved from 0.14111 to 0.13476, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3883 - val_loss: 0.1348\n",
      "Epoch 19/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3868\n",
      "Epoch 00019: val_loss improved from 0.13476 to 0.12985, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3868 - val_loss: 0.1299\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3785\n",
      "Epoch 00020: val_loss improved from 0.12985 to 0.12686, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3785 - val_loss: 0.1269\n",
      "Epoch 21/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3816\n",
      "Epoch 00021: val_loss improved from 0.12686 to 0.12519, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3811 - val_loss: 0.1252\n",
      "Epoch 22/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3672\n",
      "Epoch 00022: val_loss improved from 0.12519 to 0.12303, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3662 - val_loss: 0.1230\n",
      "Epoch 23/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3686\n",
      "Epoch 00023: val_loss improved from 0.12303 to 0.12196, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3685 - val_loss: 0.1220\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3580\n",
      "Epoch 00024: val_loss improved from 0.12196 to 0.12089, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3580 - val_loss: 0.1209\n",
      "Epoch 25/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3580\n",
      "Epoch 00025: val_loss improved from 0.12089 to 0.12008, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3582 - val_loss: 0.1201\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3522\n",
      "Epoch 00026: val_loss improved from 0.12008 to 0.11878, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3522 - val_loss: 0.1188\n",
      "Epoch 27/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3435\n",
      "Epoch 00027: val_loss improved from 0.11878 to 0.11808, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3437 - val_loss: 0.1181\n",
      "Epoch 28/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 0.3482\n",
      "Epoch 00028: val_loss improved from 0.11808 to 0.11689, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3502 - val_loss: 0.1169\n",
      "Epoch 29/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3468\n",
      "Epoch 00029: val_loss improved from 0.11689 to 0.11634, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3478 - val_loss: 0.1163\n",
      "Epoch 30/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3345\n",
      "Epoch 00030: val_loss did not improve from 0.11634\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3374 - val_loss: 0.1166\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3342\n",
      "Epoch 00031: val_loss improved from 0.11634 to 0.11486, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3342 - val_loss: 0.1149\n",
      "Epoch 32/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3336\n",
      "Epoch 00032: val_loss improved from 0.11486 to 0.11438, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3337 - val_loss: 0.1144\n",
      "Epoch 33/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3273\n",
      "Epoch 00033: val_loss improved from 0.11438 to 0.11339, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3276 - val_loss: 0.1134\n",
      "Epoch 34/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3221\n",
      "Epoch 00034: val_loss improved from 0.11339 to 0.11289, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3191 - val_loss: 0.1129\n",
      "Epoch 35/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3272\n",
      "Epoch 00035: val_loss improved from 0.11289 to 0.11216, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3253 - val_loss: 0.1122\n",
      "Epoch 36/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3230\n",
      "Epoch 00036: val_loss improved from 0.11216 to 0.11149, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3255 - val_loss: 0.1115\n",
      "Epoch 37/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3164\n",
      "Epoch 00037: val_loss improved from 0.11149 to 0.11053, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3195 - val_loss: 0.1105\n",
      "Epoch 38/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3217\n",
      "Epoch 00038: val_loss improved from 0.11053 to 0.11023, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3217 - val_loss: 0.1102\n",
      "Epoch 39/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3144\n",
      "Epoch 00039: val_loss improved from 0.11023 to 0.10976, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3141 - val_loss: 0.1098\n",
      "Epoch 40/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3137\n",
      "Epoch 00040: val_loss improved from 0.10976 to 0.10923, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3109 - val_loss: 0.1092\n",
      "Epoch 41/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3079\n",
      "Epoch 00041: val_loss improved from 0.10923 to 0.10835, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3061 - val_loss: 0.1083\n",
      "Epoch 42/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3043\n",
      "Epoch 00042: val_loss improved from 0.10835 to 0.10830, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3058 - val_loss: 0.1083\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3066\n",
      "Epoch 00043: val_loss improved from 0.10830 to 0.10771, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3066 - val_loss: 0.1077\n",
      "Epoch 44/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3089\n",
      "Epoch 00044: val_loss improved from 0.10771 to 0.10722, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3060 - val_loss: 0.1072\n",
      "Epoch 45/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3103\n",
      "Epoch 00045: val_loss did not improve from 0.10722\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3075 - val_loss: 0.1075\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2981\n",
      "Epoch 00046: val_loss improved from 0.10722 to 0.10628, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2981 - val_loss: 0.1063\n",
      "Epoch 47/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2980\n",
      "Epoch 00047: val_loss improved from 0.10628 to 0.10574, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2986 - val_loss: 0.1057\n",
      "Epoch 48/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2988\n",
      "Epoch 00048: val_loss improved from 0.10574 to 0.10562, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2972 - val_loss: 0.1056\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2866\n",
      "Epoch 00049: val_loss improved from 0.10562 to 0.10482, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2866 - val_loss: 0.1048\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2930\n",
      "Epoch 00050: val_loss improved from 0.10482 to 0.10476, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2930 - val_loss: 0.1048\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2924\n",
      "Epoch 00051: val_loss did not improve from 0.10476\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2924 - val_loss: 0.1050\n",
      "Epoch 52/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2937\n",
      "Epoch 00052: val_loss improved from 0.10476 to 0.10415, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2947 - val_loss: 0.1041\n",
      "Epoch 53/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2844\n",
      "Epoch 00053: val_loss improved from 0.10415 to 0.10402, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2892 - val_loss: 0.1040\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2796\n",
      "Epoch 00054: val_loss improved from 0.10402 to 0.10305, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2796 - val_loss: 0.1030\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2839\n",
      "Epoch 00055: val_loss improved from 0.10305 to 0.10300, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2839 - val_loss: 0.1030\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2818\n",
      "Epoch 00056: val_loss improved from 0.10300 to 0.10270, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2818 - val_loss: 0.1027\n",
      "Epoch 57/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2820\n",
      "Epoch 00057: val_loss did not improve from 0.10270\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2817 - val_loss: 0.1028\n",
      "Epoch 58/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2809\n",
      "Epoch 00058: val_loss improved from 0.10270 to 0.10241, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2810 - val_loss: 0.1024\n",
      "Epoch 59/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 0.2749\n",
      "Epoch 00059: val_loss improved from 0.10241 to 0.10162, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2753 - val_loss: 0.1016\n",
      "Epoch 60/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2715\n",
      "Epoch 00060: val_loss improved from 0.10162 to 0.10137, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2752 - val_loss: 0.1014\n",
      "Epoch 61/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2775\n",
      "Epoch 00061: val_loss did not improve from 0.10137\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2745 - val_loss: 0.1020\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2740\n",
      "Epoch 00062: val_loss improved from 0.10137 to 0.10084, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2740 - val_loss: 0.1008\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2697\n",
      "Epoch 00063: val_loss improved from 0.10084 to 0.10067, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2697 - val_loss: 0.1007\n",
      "Epoch 64/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2798\n",
      "Epoch 00064: val_loss improved from 0.10067 to 0.10017, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2754 - val_loss: 0.1002\n",
      "Epoch 65/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2671\n",
      "Epoch 00065: val_loss did not improve from 0.10017\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2663 - val_loss: 0.1003\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2702\n",
      "Epoch 00066: val_loss did not improve from 0.10017\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2702 - val_loss: 0.1007\n",
      " ###1 fold : val acc1 0.595, acc3 0.980, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/35 [===================>..........] - ETA: 0s - loss: 22.4811 \n",
      "Epoch 00001: val_loss improved from inf to 17.73505, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 21.3769 - val_loss: 17.7350\n",
      "Epoch 2/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 16.0724\n",
      "Epoch 00002: val_loss improved from 17.73505 to 12.12815, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 15.0550 - val_loss: 12.1281\n",
      "Epoch 3/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 10.7767\n",
      "Epoch 00003: val_loss improved from 12.12815 to 7.75338, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0427 - val_loss: 7.7534\n",
      "Epoch 4/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 6.8408\n",
      "Epoch 00004: val_loss improved from 7.75338 to 4.88678, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 6.4204 - val_loss: 4.8868\n",
      "Epoch 5/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 4.5640\n",
      "Epoch 00005: val_loss improved from 4.88678 to 3.36252, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.2862 - val_loss: 3.3625\n",
      "Epoch 6/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 3.2791\n",
      "Epoch 00006: val_loss improved from 3.36252 to 2.46502, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.1449 - val_loss: 2.4650\n",
      "Epoch 7/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 2.5052\n",
      "Epoch 00007: val_loss improved from 2.46502 to 1.79590, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.3813 - val_loss: 1.7959\n",
      "Epoch 8/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 1.8758\n",
      "Epoch 00008: val_loss improved from 1.79590 to 1.28284, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7862 - val_loss: 1.2828\n",
      "Epoch 9/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 1.4074\n",
      "Epoch 00009: val_loss improved from 1.28284 to 0.90503, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.3547 - val_loss: 0.9050\n",
      "Epoch 10/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 1.0814\n",
      "Epoch 00010: val_loss improved from 0.90503 to 0.64030, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.0320 - val_loss: 0.6403\n",
      "Epoch 11/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.8477\n",
      "Epoch 00011: val_loss improved from 0.64030 to 0.45938, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.8105 - val_loss: 0.4594\n",
      "Epoch 12/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6779\n",
      "Epoch 00012: val_loss improved from 0.45938 to 0.33637, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6604 - val_loss: 0.3364\n",
      "Epoch 13/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.5661\n",
      "Epoch 00013: val_loss improved from 0.33637 to 0.25486, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5551 - val_loss: 0.2549\n",
      "Epoch 14/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.4969\n",
      "Epoch 00014: val_loss improved from 0.25486 to 0.20232, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4911 - val_loss: 0.2023\n",
      "Epoch 15/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.4428\n",
      "Epoch 00015: val_loss improved from 0.20232 to 0.17037, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4465 - val_loss: 0.1704\n",
      "Epoch 16/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.4181\n",
      "Epoch 00016: val_loss improved from 0.17037 to 0.15143, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4254 - val_loss: 0.1514\n",
      "Epoch 17/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.4106\n",
      "Epoch 00017: val_loss improved from 0.15143 to 0.14017, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4032 - val_loss: 0.1402\n",
      "Epoch 18/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3829\n",
      "Epoch 00018: val_loss improved from 0.14017 to 0.13412, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3848 - val_loss: 0.1341\n",
      "Epoch 19/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3942\n",
      "Epoch 00019: val_loss improved from 0.13412 to 0.12938, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3880 - val_loss: 0.1294\n",
      "Epoch 20/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3799\n",
      "Epoch 00020: val_loss improved from 0.12938 to 0.12652, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3784 - val_loss: 0.1265\n",
      "Epoch 21/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3711\n",
      "Epoch 00021: val_loss improved from 0.12652 to 0.12496, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3755 - val_loss: 0.1250\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3759\n",
      "Epoch 00022: val_loss improved from 0.12496 to 0.12313, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3700 - val_loss: 0.1231\n",
      "Epoch 23/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.3699\n",
      "Epoch 00023: val_loss improved from 0.12313 to 0.12194, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3695 - val_loss: 0.1219\n",
      "Epoch 24/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3643\n",
      "Epoch 00024: val_loss improved from 0.12194 to 0.12067, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3561 - val_loss: 0.1207\n",
      "Epoch 25/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3613\n",
      "Epoch 00025: val_loss improved from 0.12067 to 0.12010, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3588 - val_loss: 0.1201\n",
      "Epoch 26/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3543\n",
      "Epoch 00026: val_loss improved from 0.12010 to 0.11868, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3515 - val_loss: 0.1187\n",
      "Epoch 27/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3458\n",
      "Epoch 00027: val_loss improved from 0.11868 to 0.11831, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3459 - val_loss: 0.1183\n",
      "Epoch 28/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3473\n",
      "Epoch 00028: val_loss improved from 0.11831 to 0.11669, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3487 - val_loss: 0.1167\n",
      "Epoch 29/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3466\n",
      "Epoch 00029: val_loss improved from 0.11669 to 0.11630, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3460 - val_loss: 0.1163\n",
      "Epoch 30/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3370\n",
      "Epoch 00030: val_loss did not improve from 0.11630\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3351 - val_loss: 0.1167\n",
      "Epoch 31/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3368\n",
      "Epoch 00031: val_loss improved from 0.11630 to 0.11496, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3373 - val_loss: 0.1150\n",
      "Epoch 32/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3318\n",
      "Epoch 00032: val_loss improved from 0.11496 to 0.11439, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3323 - val_loss: 0.1144\n",
      "Epoch 33/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.3243\n",
      "Epoch 00033: val_loss improved from 0.11439 to 0.11339, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3266 - val_loss: 0.1134\n",
      "Epoch 34/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3212\n",
      "Epoch 00034: val_loss improved from 0.11339 to 0.11289, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3184 - val_loss: 0.1129\n",
      "Epoch 35/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3252\n",
      "Epoch 00035: val_loss improved from 0.11289 to 0.11225, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3243 - val_loss: 0.1123\n",
      "Epoch 36/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3211\n",
      "Epoch 00036: val_loss improved from 0.11225 to 0.11171, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3224 - val_loss: 0.1117\n",
      "Epoch 37/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3143\n",
      "Epoch 00037: val_loss improved from 0.11171 to 0.11100, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3192 - val_loss: 0.1110\n",
      "Epoch 38/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3174\n",
      "Epoch 00038: val_loss improved from 0.11100 to 0.11054, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3200 - val_loss: 0.1105\n",
      "Epoch 39/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3144\n",
      "Epoch 00039: val_loss improved from 0.11054 to 0.11008, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3145 - val_loss: 0.1101\n",
      "Epoch 40/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3157\n",
      "Epoch 00040: val_loss improved from 0.11008 to 0.10952, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3121 - val_loss: 0.1095\n",
      "Epoch 41/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3093\n",
      "Epoch 00041: val_loss improved from 0.10952 to 0.10867, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3064 - val_loss: 0.1087\n",
      "Epoch 42/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.3058\n",
      "Epoch 00042: val_loss improved from 0.10867 to 0.10846, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3050 - val_loss: 0.1085\n",
      "Epoch 43/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3030\n",
      "Epoch 00043: val_loss improved from 0.10846 to 0.10794, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3042 - val_loss: 0.1079\n",
      "Epoch 44/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3087\n",
      "Epoch 00044: val_loss improved from 0.10794 to 0.10747, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3047 - val_loss: 0.1075\n",
      "Epoch 45/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3085\n",
      "Epoch 00045: val_loss did not improve from 0.10747\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3050 - val_loss: 0.1076\n",
      "Epoch 46/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2947\n",
      "Epoch 00046: val_loss improved from 0.10747 to 0.10644, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2991 - val_loss: 0.1064\n",
      "Epoch 47/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.2947\n",
      "Epoch 00047: val_loss improved from 0.10644 to 0.10590, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2969 - val_loss: 0.1059\n",
      "Epoch 48/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2972\n",
      "Epoch 00048: val_loss improved from 0.10590 to 0.10567, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2950 - val_loss: 0.1057\n",
      "Epoch 49/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2861\n",
      "Epoch 00049: val_loss improved from 0.10567 to 0.10535, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2879 - val_loss: 0.1054\n",
      "Epoch 50/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2953\n",
      "Epoch 00050: val_loss improved from 0.10535 to 0.10527, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2958 - val_loss: 0.1053\n",
      "Epoch 51/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2918\n",
      "Epoch 00051: val_loss improved from 0.10527 to 0.10510, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2915 - val_loss: 0.1051\n",
      "Epoch 52/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2951\n",
      "Epoch 00052: val_loss improved from 0.10510 to 0.10411, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2924 - val_loss: 0.1041\n",
      "Epoch 53/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2814\n",
      "Epoch 00053: val_loss did not improve from 0.10411\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2877 - val_loss: 0.1043\n",
      "Epoch 54/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2783\n",
      "Epoch 00054: val_loss improved from 0.10411 to 0.10328, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2797 - val_loss: 0.1033\n",
      "Epoch 55/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2816\n",
      "Epoch 00055: val_loss did not improve from 0.10328\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2835 - val_loss: 0.1034\n",
      "Epoch 56/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2816\n",
      "Epoch 00056: val_loss improved from 0.10328 to 0.10297, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2810 - val_loss: 0.1030\n",
      "Epoch 57/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2821\n",
      "Epoch 00057: val_loss did not improve from 0.10297\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2807 - val_loss: 0.1030\n",
      "Epoch 58/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2825\n",
      "Epoch 00058: val_loss improved from 0.10297 to 0.10245, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2821 - val_loss: 0.1024\n",
      "Epoch 59/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2783\n",
      "Epoch 00059: val_loss improved from 0.10245 to 0.10206, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2758 - val_loss: 0.1021\n",
      "Epoch 60/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2689\n",
      "Epoch 00060: val_loss improved from 0.10206 to 0.10164, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2735 - val_loss: 0.1016\n",
      "Epoch 61/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2782\n",
      "Epoch 00061: val_loss did not improve from 0.10164\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2748 - val_loss: 0.1025\n",
      "Epoch 62/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2741\n",
      "Epoch 00062: val_loss improved from 0.10164 to 0.10118, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2735 - val_loss: 0.1012\n",
      "Epoch 63/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2714\n",
      "Epoch 00063: val_loss improved from 0.10118 to 0.10092, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2693 - val_loss: 0.1009\n",
      "Epoch 64/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2782\n",
      "Epoch 00064: val_loss improved from 0.10092 to 0.10058, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2762 - val_loss: 0.1006\n",
      "Epoch 65/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2646\n",
      "Epoch 00065: val_loss did not improve from 0.10058\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2656 - val_loss: 0.1006\n",
      "Epoch 66/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2732\n",
      "Epoch 00066: val_loss did not improve from 0.10058\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2713 - val_loss: 0.1013\n",
      " ###2 fold : val acc1 0.591, acc3 0.975, mae 0.218###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/35 [==================>...........] - ETA: 0s - loss: 22.6619 \n",
      "Epoch 00001: val_loss improved from inf to 17.72712, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 21.4481 - val_loss: 17.7271\n",
      "Epoch 2/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 16.0434\n",
      "Epoch 00002: val_loss improved from 17.72712 to 12.11850, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 15.1140 - val_loss: 12.1185\n",
      "Epoch 3/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 10.9617\n",
      "Epoch 00003: val_loss improved from 12.11850 to 7.74424, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0877 - val_loss: 7.7442\n",
      "Epoch 4/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 6.9809\n",
      "Epoch 00004: val_loss improved from 7.74424 to 4.88006, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 6.4586 - val_loss: 4.8801\n",
      "Epoch 5/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 4.6045\n",
      "Epoch 00005: val_loss improved from 4.88006 to 3.35949, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.3039 - val_loss: 3.3595\n",
      "Epoch 6/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.3159\n",
      "Epoch 00006: val_loss improved from 3.35949 to 2.46264, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1585 - val_loss: 2.4626\n",
      "Epoch 7/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.5361\n",
      "Epoch 00007: val_loss improved from 2.46264 to 1.79373, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.3902 - val_loss: 1.7937\n",
      "Epoch 8/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.8843\n",
      "Epoch 00008: val_loss improved from 1.79373 to 1.28054, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7859 - val_loss: 1.2805\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.4095\n",
      "Epoch 00009: val_loss improved from 1.28054 to 0.90358, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.3507 - val_loss: 0.9036\n",
      "Epoch 10/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.1070\n",
      "Epoch 00010: val_loss improved from 0.90358 to 0.63748, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0357 - val_loss: 0.6375\n",
      "Epoch 11/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8464\n",
      "Epoch 00011: val_loss improved from 0.63748 to 0.45718, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8111 - val_loss: 0.4572\n",
      "Epoch 12/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6786\n",
      "Epoch 00012: val_loss improved from 0.45718 to 0.33481, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6576 - val_loss: 0.3348\n",
      "Epoch 13/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5739\n",
      "Epoch 00013: val_loss improved from 0.33481 to 0.25361, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5522 - val_loss: 0.2536\n",
      "Epoch 14/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5064\n",
      "Epoch 00014: val_loss improved from 0.25361 to 0.20161, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4926 - val_loss: 0.2016\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4411\n",
      "Epoch 00015: val_loss improved from 0.20161 to 0.16995, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4411 - val_loss: 0.1699\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4243\n",
      "Epoch 00016: val_loss improved from 0.16995 to 0.15104, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4243 - val_loss: 0.1510\n",
      "Epoch 17/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4093\n",
      "Epoch 00017: val_loss improved from 0.15104 to 0.14029, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4022 - val_loss: 0.1403\n",
      "Epoch 18/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3838\n",
      "Epoch 00018: val_loss improved from 0.14029 to 0.13378, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3859 - val_loss: 0.1338\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3857\n",
      "Epoch 00019: val_loss improved from 0.13378 to 0.12902, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3857 - val_loss: 0.1290\n",
      "Epoch 20/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3810\n",
      "Epoch 00020: val_loss improved from 0.12902 to 0.12637, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3787 - val_loss: 0.1264\n",
      "Epoch 21/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3724\n",
      "Epoch 00021: val_loss improved from 0.12637 to 0.12459, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3735 - val_loss: 0.1246\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3684\n",
      "Epoch 00022: val_loss improved from 0.12459 to 0.12281, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3684 - val_loss: 0.1228\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3681\n",
      "Epoch 00023: val_loss improved from 0.12281 to 0.12168, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3681 - val_loss: 0.1217\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3570\n",
      "Epoch 00024: val_loss improved from 0.12168 to 0.12044, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3570 - val_loss: 0.1204\n",
      "Epoch 25/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3609\n",
      "Epoch 00025: val_loss improved from 0.12044 to 0.12001, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3602 - val_loss: 0.1200\n",
      "Epoch 26/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3541\n",
      "Epoch 00026: val_loss improved from 0.12001 to 0.11862, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3500 - val_loss: 0.1186\n",
      "Epoch 27/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3466\n",
      "Epoch 00027: val_loss improved from 0.11862 to 0.11826, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3469 - val_loss: 0.1183\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3475\n",
      "Epoch 00028: val_loss improved from 0.11826 to 0.11666, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3475 - val_loss: 0.1167\n",
      "Epoch 29/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3403\n",
      "Epoch 00029: val_loss improved from 0.11666 to 0.11640, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3448 - val_loss: 0.1164\n",
      "Epoch 30/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3368\n",
      "Epoch 00030: val_loss did not improve from 0.11640\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3350 - val_loss: 0.1166\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3360\n",
      "Epoch 00031: val_loss improved from 0.11640 to 0.11535, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3360 - val_loss: 0.1153\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3315\n",
      "Epoch 00032: val_loss improved from 0.11535 to 0.11447, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3315 - val_loss: 0.1145\n",
      "Epoch 33/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3266\n",
      "Epoch 00033: val_loss improved from 0.11447 to 0.11362, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3278 - val_loss: 0.1136\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3158\n",
      "Epoch 00034: val_loss improved from 0.11362 to 0.11279, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3158 - val_loss: 0.1128\n",
      "Epoch 35/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3178\n",
      "Epoch 00035: val_loss improved from 0.11279 to 0.11238, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3236 - val_loss: 0.1124\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3186\n",
      "Epoch 00036: val_loss improved from 0.11238 to 0.11177, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3186 - val_loss: 0.1118\n",
      "Epoch 37/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3210\n",
      "Epoch 00037: val_loss improved from 0.11177 to 0.11109, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3217 - val_loss: 0.1111\n",
      "Epoch 38/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3211\n",
      "Epoch 00038: val_loss improved from 0.11109 to 0.11045, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3218 - val_loss: 0.1105\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3148\n",
      "Epoch 00039: val_loss improved from 0.11045 to 0.11002, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3148 - val_loss: 0.1100\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3135\n",
      "Epoch 00040: val_loss improved from 0.11002 to 0.10963, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3135 - val_loss: 0.1096\n",
      "Epoch 41/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3076\n",
      "Epoch 00041: val_loss improved from 0.10963 to 0.10873, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3079 - val_loss: 0.1087\n",
      "Epoch 42/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3014\n",
      "Epoch 00042: val_loss improved from 0.10873 to 0.10841, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3034 - val_loss: 0.1084\n",
      "Epoch 43/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3005\n",
      "Epoch 00043: val_loss improved from 0.10841 to 0.10813, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3032 - val_loss: 0.1081\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3065\n",
      "Epoch 00044: val_loss improved from 0.10813 to 0.10763, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3065 - val_loss: 0.1076\n",
      "Epoch 45/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3067\n",
      "Epoch 00045: val_loss improved from 0.10763 to 0.10722, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3021 - val_loss: 0.1072\n",
      "Epoch 46/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2936\n",
      "Epoch 00046: val_loss improved from 0.10722 to 0.10649, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2985 - val_loss: 0.1065\n",
      "Epoch 47/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2908\n",
      "Epoch 00047: val_loss improved from 0.10649 to 0.10608, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2947 - val_loss: 0.1061\n",
      "Epoch 48/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2962\n",
      "Epoch 00048: val_loss improved from 0.10608 to 0.10575, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2942 - val_loss: 0.1058\n",
      "Epoch 49/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2874\n",
      "Epoch 00049: val_loss improved from 0.10575 to 0.10555, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2888 - val_loss: 0.1056\n",
      "Epoch 50/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2968\n",
      "Epoch 00050: val_loss improved from 0.10555 to 0.10500, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2969 - val_loss: 0.1050\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2933\n",
      "Epoch 00051: val_loss improved from 0.10500 to 0.10481, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2933 - val_loss: 0.1048\n",
      "Epoch 52/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2959\n",
      "Epoch 00052: val_loss improved from 0.10481 to 0.10410, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2943 - val_loss: 0.1041\n",
      "Epoch 53/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2823\n",
      "Epoch 00053: val_loss did not improve from 0.10410\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2882 - val_loss: 0.1044\n",
      "Epoch 54/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 0.2811\n",
      "Epoch 00054: val_loss improved from 0.10410 to 0.10331, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2800 - val_loss: 0.1033\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2833\n",
      "Epoch 00055: val_loss did not improve from 0.10331\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2833 - val_loss: 0.1035\n",
      "Epoch 56/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2837\n",
      "Epoch 00056: val_loss improved from 0.10331 to 0.10288, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2834 - val_loss: 0.1029\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2794\n",
      "Epoch 00057: val_loss did not improve from 0.10288\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2794 - val_loss: 0.1029\n",
      "Epoch 58/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2813\n",
      "Epoch 00058: val_loss improved from 0.10288 to 0.10253, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2817 - val_loss: 0.1025\n",
      "Epoch 59/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2801\n",
      "Epoch 00059: val_loss improved from 0.10253 to 0.10200, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2779 - val_loss: 0.1020\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2742\n",
      "Epoch 00060: val_loss improved from 0.10200 to 0.10160, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2742 - val_loss: 0.1016\n",
      "Epoch 61/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2797\n",
      "Epoch 00061: val_loss did not improve from 0.10160\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2750 - val_loss: 0.1027\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2742\n",
      "Epoch 00062: val_loss improved from 0.10160 to 0.10124, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2742 - val_loss: 0.1012\n",
      "Epoch 63/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2707\n",
      "Epoch 00063: val_loss improved from 0.10124 to 0.10103, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2685 - val_loss: 0.1010\n",
      "Epoch 64/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2772\n",
      "Epoch 00064: val_loss improved from 0.10103 to 0.10050, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2773 - val_loss: 0.1005\n",
      "Epoch 65/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2624\n",
      "Epoch 00065: val_loss did not improve from 0.10050\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2628 - val_loss: 0.1008\n",
      "Epoch 66/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2730\n",
      "Epoch 00066: val_loss did not improve from 0.10050\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2702 - val_loss: 0.1011\n",
      " ###3 fold : val acc1 0.606, acc3 0.971, mae 0.212###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 22.8959 \n",
      "Epoch 00001: val_loss improved from inf to 17.70554, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 21.3843 - val_loss: 17.7055\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 16.1780\n",
      "Epoch 00002: val_loss improved from 17.70554 to 12.09688, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 15.0652 - val_loss: 12.0969\n",
      "Epoch 3/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 10.8660\n",
      "Epoch 00003: val_loss improved from 12.09688 to 7.72954, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0419 - val_loss: 7.7295\n",
      "Epoch 4/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 6.9849\n",
      "Epoch 00004: val_loss improved from 7.72954 to 4.87116, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 6.4034 - val_loss: 4.8712\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 4.5401\n",
      "Epoch 00005: val_loss improved from 4.87116 to 3.34717, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.2634 - val_loss: 3.3472\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.2956\n",
      "Epoch 00006: val_loss improved from 3.34717 to 2.46487, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1357 - val_loss: 2.4649\n",
      "Epoch 7/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.5371\n",
      "Epoch 00007: val_loss improved from 2.46487 to 1.80005, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3778 - val_loss: 1.8001\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.9179\n",
      "Epoch 00008: val_loss improved from 1.80005 to 1.28946, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.8031 - val_loss: 1.2895\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.4428\n",
      "Epoch 00009: val_loss improved from 1.28946 to 0.91540, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.3644 - val_loss: 0.9154\n",
      "Epoch 10/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.0993\n",
      "Epoch 00010: val_loss improved from 0.91540 to 0.64988, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.0436 - val_loss: 0.6499\n",
      "Epoch 11/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8457\n",
      "Epoch 00011: val_loss improved from 0.64988 to 0.46785, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.8082 - val_loss: 0.4678\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6888\n",
      "Epoch 00012: val_loss improved from 0.46785 to 0.34456, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6598 - val_loss: 0.3446\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5700\n",
      "Epoch 00013: val_loss improved from 0.34456 to 0.26217, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5659 - val_loss: 0.2622\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4997\n",
      "Epoch 00014: val_loss improved from 0.26217 to 0.20830, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4978 - val_loss: 0.2083\n",
      "Epoch 15/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4414\n",
      "Epoch 00015: val_loss improved from 0.20830 to 0.17433, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4414 - val_loss: 0.1743\n",
      "Epoch 16/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4353\n",
      "Epoch 00016: val_loss improved from 0.17433 to 0.15401, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4216 - val_loss: 0.1540\n",
      "Epoch 17/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4135\n",
      "Epoch 00017: val_loss improved from 0.15401 to 0.14183, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4059 - val_loss: 0.1418\n",
      "Epoch 18/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3941\n",
      "Epoch 00018: val_loss improved from 0.14183 to 0.13462, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3912 - val_loss: 0.1346\n",
      "Epoch 19/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3924\n",
      "Epoch 00019: val_loss improved from 0.13462 to 0.12962, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3888 - val_loss: 0.1296\n",
      "Epoch 20/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3742\n",
      "Epoch 00020: val_loss improved from 0.12962 to 0.12629, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3759 - val_loss: 0.1263\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3733\n",
      "Epoch 00021: val_loss improved from 0.12629 to 0.12387, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3733 - val_loss: 0.1239\n",
      "Epoch 22/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3750\n",
      "Epoch 00022: val_loss improved from 0.12387 to 0.12240, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3669 - val_loss: 0.1224\n",
      "Epoch 23/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3647\n",
      "Epoch 00023: val_loss improved from 0.12240 to 0.12113, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3657 - val_loss: 0.1211\n",
      "Epoch 24/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3547\n",
      "Epoch 00024: val_loss improved from 0.12113 to 0.11997, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3553 - val_loss: 0.1200\n",
      "Epoch 25/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3483\n",
      "Epoch 00025: val_loss improved from 0.11997 to 0.11923, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3544 - val_loss: 0.1192\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3505\n",
      "Epoch 00026: val_loss improved from 0.11923 to 0.11837, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3505 - val_loss: 0.1184\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3499\n",
      "Epoch 00027: val_loss improved from 0.11837 to 0.11720, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3499 - val_loss: 0.1172\n",
      "Epoch 28/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3455\n",
      "Epoch 00028: val_loss improved from 0.11720 to 0.11641, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3487 - val_loss: 0.1164\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3457\n",
      "Epoch 00029: val_loss improved from 0.11641 to 0.11593, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3457 - val_loss: 0.1159\n",
      "Epoch 30/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3442\n",
      "Epoch 00030: val_loss improved from 0.11593 to 0.11531, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3432 - val_loss: 0.1153\n",
      "Epoch 31/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3360\n",
      "Epoch 00031: val_loss improved from 0.11531 to 0.11470, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3316 - val_loss: 0.1147\n",
      "Epoch 32/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3285\n",
      "Epoch 00032: val_loss improved from 0.11470 to 0.11373, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3337 - val_loss: 0.1137\n",
      "Epoch 33/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3280\n",
      "Epoch 00033: val_loss improved from 0.11373 to 0.11337, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3258 - val_loss: 0.1134\n",
      "Epoch 34/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3275\n",
      "Epoch 00034: val_loss improved from 0.11337 to 0.11249, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3271 - val_loss: 0.1125\n",
      "Epoch 35/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3185\n",
      "Epoch 00035: val_loss improved from 0.11249 to 0.11209, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3182 - val_loss: 0.1121\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3231\n",
      "Epoch 00036: val_loss improved from 0.11209 to 0.11120, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3231 - val_loss: 0.1112\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3192\n",
      "Epoch 00037: val_loss improved from 0.11120 to 0.11065, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3192 - val_loss: 0.1106\n",
      "Epoch 38/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3098\n",
      "Epoch 00038: val_loss improved from 0.11065 to 0.11005, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3148 - val_loss: 0.1101\n",
      "Epoch 39/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3208\n",
      "Epoch 00039: val_loss improved from 0.11005 to 0.10974, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3156 - val_loss: 0.1097\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3114\n",
      "Epoch 00040: val_loss improved from 0.10974 to 0.10910, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3114 - val_loss: 0.1091\n",
      "Epoch 41/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3041\n",
      "Epoch 00041: val_loss improved from 0.10910 to 0.10872, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3034 - val_loss: 0.1087\n",
      "Epoch 42/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 0.3096\n",
      "Epoch 00042: val_loss improved from 0.10872 to 0.10869, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3073 - val_loss: 0.1087\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3013\n",
      "Epoch 00043: val_loss improved from 0.10869 to 0.10795, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3013 - val_loss: 0.1079\n",
      "Epoch 44/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3066\n",
      "Epoch 00044: val_loss improved from 0.10795 to 0.10741, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3049 - val_loss: 0.1074\n",
      "Epoch 45/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2982\n",
      "Epoch 00045: val_loss improved from 0.10741 to 0.10711, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2976 - val_loss: 0.1071\n",
      "Epoch 46/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3060\n",
      "Epoch 00046: val_loss improved from 0.10711 to 0.10667, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3013 - val_loss: 0.1067\n",
      "Epoch 47/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2986\n",
      "Epoch 00047: val_loss improved from 0.10667 to 0.10643, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2969 - val_loss: 0.1064\n",
      "Epoch 48/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2907\n",
      "Epoch 00048: val_loss improved from 0.10643 to 0.10549, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2911 - val_loss: 0.1055\n",
      "Epoch 49/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2981\n",
      "Epoch 00049: val_loss did not improve from 0.10549\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2974 - val_loss: 0.1055\n",
      "Epoch 50/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2881\n",
      "Epoch 00050: val_loss improved from 0.10549 to 0.10547, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2887 - val_loss: 0.1055\n",
      "Epoch 51/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2920\n",
      "Epoch 00051: val_loss improved from 0.10547 to 0.10450, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2925 - val_loss: 0.1045\n",
      "Epoch 52/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2905\n",
      "Epoch 00052: val_loss improved from 0.10450 to 0.10371, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2902 - val_loss: 0.1037\n",
      "Epoch 53/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2869\n",
      "Epoch 00053: val_loss did not improve from 0.10371\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2838 - val_loss: 0.1038\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2845\n",
      "Epoch 00054: val_loss improved from 0.10371 to 0.10351, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2845 - val_loss: 0.1035\n",
      "Epoch 55/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2822\n",
      "Epoch 00055: val_loss improved from 0.10351 to 0.10320, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2854 - val_loss: 0.1032\n",
      "Epoch 56/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2823\n",
      "Epoch 00056: val_loss improved from 0.10320 to 0.10268, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2790 - val_loss: 0.1027\n",
      "Epoch 57/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2818\n",
      "Epoch 00057: val_loss improved from 0.10268 to 0.10219, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2813 - val_loss: 0.1022\n",
      "Epoch 58/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2776\n",
      "Epoch 00058: val_loss improved from 0.10219 to 0.10209, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2784 - val_loss: 0.1021\n",
      "Epoch 59/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2785\n",
      "Epoch 00059: val_loss did not improve from 0.10209\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2780 - val_loss: 0.1021\n",
      "Epoch 60/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2679\n",
      "Epoch 00060: val_loss improved from 0.10209 to 0.10178, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2739 - val_loss: 0.1018\n",
      "Epoch 61/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2726\n",
      "Epoch 00061: val_loss improved from 0.10178 to 0.10149, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2776 - val_loss: 0.1015\n",
      "Epoch 62/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2777\n",
      "Epoch 00062: val_loss did not improve from 0.10149\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2755 - val_loss: 0.1017\n",
      "Epoch 63/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2766\n",
      "Epoch 00063: val_loss improved from 0.10149 to 0.10125, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2750 - val_loss: 0.1013\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2691\n",
      "Epoch 00064: val_loss did not improve from 0.10125\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2691 - val_loss: 0.1013\n",
      "Epoch 65/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2659\n",
      "Epoch 00065: val_loss improved from 0.10125 to 0.10090, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2663 - val_loss: 0.1009\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2680\n",
      "Epoch 00066: val_loss improved from 0.10090 to 0.10020, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2680 - val_loss: 0.1002\n",
      "Epoch 67/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2686\n",
      "Epoch 00067: val_loss did not improve from 0.10020\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2680 - val_loss: 0.1012\n",
      "Epoch 68/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2660\n",
      "Epoch 00068: val_loss did not improve from 0.10020\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2662 - val_loss: 0.1005\n",
      " ###4 fold : val acc1 0.594, acc3 0.975, mae 0.215###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/35 [=================>............] - ETA: 0s - loss: 22.6837 \n",
      "Epoch 00001: val_loss improved from inf to 17.70838, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 21.4077 - val_loss: 17.7084\n",
      "Epoch 2/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 16.0396\n",
      "Epoch 00002: val_loss improved from 17.70838 to 12.09861, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 15.0957 - val_loss: 12.0986\n",
      "Epoch 3/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 10.8502\n",
      "Epoch 00003: val_loss improved from 12.09861 to 7.72794, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0604 - val_loss: 7.7279\n",
      "Epoch 4/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 6.9564\n",
      "Epoch 00004: val_loss improved from 7.72794 to 4.86360, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 6.4141 - val_loss: 4.8636\n",
      "Epoch 5/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 4.4713\n",
      "Epoch 00005: val_loss improved from 4.86360 to 3.33846, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.2591 - val_loss: 3.3385\n",
      "Epoch 6/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 3.2491\n",
      "Epoch 00006: val_loss improved from 3.33846 to 2.45873, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.1160 - val_loss: 2.4587\n",
      "Epoch 7/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.5363\n",
      "Epoch 00007: val_loss improved from 2.45873 to 1.79748, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.3809 - val_loss: 1.7975\n",
      "Epoch 8/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.9016\n",
      "Epoch 00008: val_loss improved from 1.79748 to 1.28826, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7985 - val_loss: 1.2883\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.4374\n",
      "Epoch 00009: val_loss improved from 1.28826 to 0.91216, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.3474 - val_loss: 0.9122\n",
      "Epoch 10/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.0965\n",
      "Epoch 00010: val_loss improved from 0.91216 to 0.64640, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.0375 - val_loss: 0.6464\n",
      "Epoch 11/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8283\n",
      "Epoch 00011: val_loss improved from 0.64640 to 0.46512, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7976 - val_loss: 0.4651\n",
      "Epoch 12/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6775\n",
      "Epoch 00012: val_loss improved from 0.46512 to 0.34185, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6514 - val_loss: 0.3418\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5623\n",
      "Epoch 00013: val_loss improved from 0.34185 to 0.26013, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5551 - val_loss: 0.2601\n",
      "Epoch 14/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5022\n",
      "Epoch 00014: val_loss improved from 0.26013 to 0.20675, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4921 - val_loss: 0.2067\n",
      "Epoch 15/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4358\n",
      "Epoch 00015: val_loss improved from 0.20675 to 0.17356, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4387 - val_loss: 0.1736\n",
      "Epoch 16/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4310\n",
      "Epoch 00016: val_loss improved from 0.17356 to 0.15371, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4188 - val_loss: 0.1537\n",
      "Epoch 17/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4150\n",
      "Epoch 00017: val_loss improved from 0.15371 to 0.14186, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4057 - val_loss: 0.1419\n",
      "Epoch 18/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3936\n",
      "Epoch 00018: val_loss improved from 0.14186 to 0.13514, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3916 - val_loss: 0.1351\n",
      "Epoch 19/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3910\n",
      "Epoch 00019: val_loss improved from 0.13514 to 0.13019, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3866 - val_loss: 0.1302\n",
      "Epoch 20/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3734\n",
      "Epoch 00020: val_loss improved from 0.13019 to 0.12711, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3749 - val_loss: 0.1271\n",
      "Epoch 21/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3831\n",
      "Epoch 00021: val_loss improved from 0.12711 to 0.12447, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3764 - val_loss: 0.1245\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3753\n",
      "Epoch 00022: val_loss improved from 0.12447 to 0.12272, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3695 - val_loss: 0.1227\n",
      "Epoch 23/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3643\n",
      "Epoch 00023: val_loss improved from 0.12272 to 0.12160, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3654 - val_loss: 0.1216\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3538\n",
      "Epoch 00024: val_loss improved from 0.12160 to 0.12061, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3563 - val_loss: 0.1206\n",
      "Epoch 25/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3490\n",
      "Epoch 00025: val_loss improved from 0.12061 to 0.11961, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3541 - val_loss: 0.1196\n",
      "Epoch 26/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3398\n",
      "Epoch 00026: val_loss improved from 0.11961 to 0.11914, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3519 - val_loss: 0.1191\n",
      "Epoch 27/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3437\n",
      "Epoch 00027: val_loss improved from 0.11914 to 0.11783, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3488 - val_loss: 0.1178\n",
      "Epoch 28/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3436\n",
      "Epoch 00028: val_loss improved from 0.11783 to 0.11698, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3453 - val_loss: 0.1170\n",
      "Epoch 29/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3418\n",
      "Epoch 00029: val_loss improved from 0.11698 to 0.11654, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3452 - val_loss: 0.1165\n",
      "Epoch 30/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3408\n",
      "Epoch 00030: val_loss improved from 0.11654 to 0.11588, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3418 - val_loss: 0.1159\n",
      "Epoch 31/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3389\n",
      "Epoch 00031: val_loss improved from 0.11588 to 0.11524, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3341 - val_loss: 0.1152\n",
      "Epoch 32/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3254\n",
      "Epoch 00032: val_loss improved from 0.11524 to 0.11443, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3320 - val_loss: 0.1144\n",
      "Epoch 33/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3320\n",
      "Epoch 00033: val_loss improved from 0.11443 to 0.11401, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3291 - val_loss: 0.1140\n",
      "Epoch 34/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3288\n",
      "Epoch 00034: val_loss improved from 0.11401 to 0.11321, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3257 - val_loss: 0.1132\n",
      "Epoch 35/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3153\n",
      "Epoch 00035: val_loss improved from 0.11321 to 0.11279, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3200 - val_loss: 0.1128\n",
      "Epoch 36/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3235\n",
      "Epoch 00036: val_loss improved from 0.11279 to 0.11188, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3239 - val_loss: 0.1119\n",
      "Epoch 37/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3284\n",
      "Epoch 00037: val_loss improved from 0.11188 to 0.11134, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3215 - val_loss: 0.1113\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3152\n",
      "Epoch 00038: val_loss improved from 0.11134 to 0.11040, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3152 - val_loss: 0.1104\n",
      "Epoch 39/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3223\n",
      "Epoch 00039: val_loss improved from 0.11040 to 0.11031, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3169 - val_loss: 0.1103\n",
      "Epoch 40/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3164\n",
      "Epoch 00040: val_loss improved from 0.11031 to 0.10967, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3118 - val_loss: 0.1097\n",
      "Epoch 41/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3045\n",
      "Epoch 00041: val_loss improved from 0.10967 to 0.10910, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3028 - val_loss: 0.1091\n",
      "Epoch 42/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3097\n",
      "Epoch 00042: val_loss improved from 0.10910 to 0.10881, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3081 - val_loss: 0.1088\n",
      "Epoch 43/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3007\n",
      "Epoch 00043: val_loss improved from 0.10881 to 0.10834, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3017 - val_loss: 0.1083\n",
      "Epoch 44/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3063\n",
      "Epoch 00044: val_loss improved from 0.10834 to 0.10785, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3042 - val_loss: 0.1079\n",
      "Epoch 45/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2962\n",
      "Epoch 00045: val_loss improved from 0.10785 to 0.10738, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2974 - val_loss: 0.1074\n",
      "Epoch 46/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3089\n",
      "Epoch 00046: val_loss improved from 0.10738 to 0.10709, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3027 - val_loss: 0.1071\n",
      "Epoch 47/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2983\n",
      "Epoch 00047: val_loss improved from 0.10709 to 0.10686, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2996 - val_loss: 0.1069\n",
      "Epoch 48/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2943\n",
      "Epoch 00048: val_loss improved from 0.10686 to 0.10586, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2921 - val_loss: 0.1059\n",
      "Epoch 49/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2966\n",
      "Epoch 00049: val_loss improved from 0.10586 to 0.10569, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2971 - val_loss: 0.1057\n",
      "Epoch 50/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2881\n",
      "Epoch 00050: val_loss improved from 0.10569 to 0.10567, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2895 - val_loss: 0.1057\n",
      "Epoch 51/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2877\n",
      "Epoch 00051: val_loss improved from 0.10567 to 0.10463, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2898 - val_loss: 0.1046\n",
      "Epoch 52/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2912\n",
      "Epoch 00052: val_loss improved from 0.10463 to 0.10415, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2904 - val_loss: 0.1041\n",
      "Epoch 53/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2882\n",
      "Epoch 00053: val_loss improved from 0.10415 to 0.10387, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2844 - val_loss: 0.1039\n",
      "Epoch 54/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2840\n",
      "Epoch 00054: val_loss improved from 0.10387 to 0.10377, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2854 - val_loss: 0.1038\n",
      "Epoch 55/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2812\n",
      "Epoch 00055: val_loss improved from 0.10377 to 0.10350, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2843 - val_loss: 0.1035\n",
      "Epoch 56/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2849\n",
      "Epoch 00056: val_loss improved from 0.10350 to 0.10291, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2804 - val_loss: 0.1029\n",
      "Epoch 57/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2774\n",
      "Epoch 00057: val_loss improved from 0.10291 to 0.10246, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2783 - val_loss: 0.1025\n",
      "Epoch 58/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2800\n",
      "Epoch 00058: val_loss improved from 0.10246 to 0.10225, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2806 - val_loss: 0.1022\n",
      "Epoch 59/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2781\n",
      "Epoch 00059: val_loss improved from 0.10225 to 0.10215, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2779 - val_loss: 0.1022\n",
      "Epoch 60/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2691\n",
      "Epoch 00060: val_loss improved from 0.10215 to 0.10195, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2734 - val_loss: 0.1020\n",
      "Epoch 61/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2710\n",
      "Epoch 00061: val_loss improved from 0.10195 to 0.10180, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2762 - val_loss: 0.1018\n",
      "Epoch 62/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2767\n",
      "Epoch 00062: val_loss improved from 0.10180 to 0.10169, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2756 - val_loss: 0.1017\n",
      "Epoch 63/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2731\n",
      "Epoch 00063: val_loss improved from 0.10169 to 0.10148, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2722 - val_loss: 0.1015\n",
      "Epoch 64/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2645\n",
      "Epoch 00064: val_loss improved from 0.10148 to 0.10139, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2662 - val_loss: 0.1014\n",
      "Epoch 65/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2657\n",
      "Epoch 00065: val_loss improved from 0.10139 to 0.10126, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2646 - val_loss: 0.1013\n",
      "Epoch 66/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2709\n",
      "Epoch 00066: val_loss improved from 0.10126 to 0.10051, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2667 - val_loss: 0.1005\n",
      "Epoch 67/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2687\n",
      "Epoch 00067: val_loss did not improve from 0.10051\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2681 - val_loss: 0.1014\n",
      "Epoch 68/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2626\n",
      "Epoch 00068: val_loss did not improve from 0.10051\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2635 - val_loss: 0.1008\n",
      " ###5 fold : val acc1 0.582, acc3 0.977, mae 0.222###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/35 [===================>..........] - ETA: 0s - loss: 22.4893 \n",
      "Epoch 00001: val_loss improved from inf to 17.71294, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 1s 6ms/step - loss: 21.4085 - val_loss: 17.7129\n",
      "Epoch 2/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 16.0292\n",
      "Epoch 00002: val_loss improved from 17.71294 to 12.10210, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 15.0852 - val_loss: 12.1021\n",
      "Epoch 3/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 10.8085\n",
      "Epoch 00003: val_loss improved from 12.10210 to 7.73638, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0628 - val_loss: 7.7364\n",
      "Epoch 4/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 6.8611\n",
      "Epoch 00004: val_loss improved from 7.73638 to 4.87311, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 6.4140 - val_loss: 4.8731\n",
      "Epoch 5/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 4.4983\n",
      "Epoch 00005: val_loss improved from 4.87311 to 3.34127, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.2684 - val_loss: 3.3413\n",
      "Epoch 6/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 3.2390\n",
      "Epoch 00006: val_loss improved from 3.34127 to 2.45785, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.1202 - val_loss: 2.4579\n",
      "Epoch 7/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 2.5112\n",
      "Epoch 00007: val_loss improved from 2.45785 to 1.79280, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.3922 - val_loss: 1.7928\n",
      "Epoch 8/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 1.8854\n",
      "Epoch 00008: val_loss improved from 1.79280 to 1.28032, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7970 - val_loss: 1.2803\n",
      "Epoch 9/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 1.4115\n",
      "Epoch 00009: val_loss improved from 1.28032 to 0.90524, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.3435 - val_loss: 0.9052\n",
      "Epoch 10/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 1.0878\n",
      "Epoch 00010: val_loss improved from 0.90524 to 0.64034, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.0353 - val_loss: 0.6403\n",
      "Epoch 11/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.8129\n",
      "Epoch 00011: val_loss improved from 0.64034 to 0.46111, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7922 - val_loss: 0.4611\n",
      "Epoch 12/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6841\n",
      "Epoch 00012: val_loss improved from 0.46111 to 0.33848, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6547 - val_loss: 0.3385\n",
      "Epoch 13/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.5585\n",
      "Epoch 00013: val_loss improved from 0.33848 to 0.25832, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5551 - val_loss: 0.2583\n",
      "Epoch 14/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.4934\n",
      "Epoch 00014: val_loss improved from 0.25832 to 0.20526, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4898 - val_loss: 0.2053\n",
      "Epoch 15/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.4438\n",
      "Epoch 00015: val_loss improved from 0.20526 to 0.17230, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4436 - val_loss: 0.1723\n",
      "Epoch 16/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.4325\n",
      "Epoch 00016: val_loss improved from 0.17230 to 0.15250, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4215 - val_loss: 0.1525\n",
      "Epoch 17/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.4116\n",
      "Epoch 00017: val_loss improved from 0.15250 to 0.14099, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4079 - val_loss: 0.1410\n",
      "Epoch 18/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.3903\n",
      "Epoch 00018: val_loss improved from 0.14099 to 0.13439, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3941 - val_loss: 0.1344\n",
      "Epoch 19/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3938\n",
      "Epoch 00019: val_loss improved from 0.13439 to 0.12954, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3878 - val_loss: 0.1295\n",
      "Epoch 20/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3746\n",
      "Epoch 00020: val_loss improved from 0.12954 to 0.12661, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3753 - val_loss: 0.1266\n",
      "Epoch 21/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3818\n",
      "Epoch 00021: val_loss improved from 0.12661 to 0.12390, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3787 - val_loss: 0.1239\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3740\n",
      "Epoch 00022: val_loss improved from 0.12390 to 0.12224, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3704 - val_loss: 0.1222\n",
      "Epoch 23/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3627\n",
      "Epoch 00023: val_loss improved from 0.12224 to 0.12076, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3666 - val_loss: 0.1208\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3559\n",
      "Epoch 00024: val_loss improved from 0.12076 to 0.11982, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3584 - val_loss: 0.1198\n",
      "Epoch 25/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3486\n",
      "Epoch 00025: val_loss improved from 0.11982 to 0.11890, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3561 - val_loss: 0.1189\n",
      "Epoch 26/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3442\n",
      "Epoch 00026: val_loss improved from 0.11890 to 0.11833, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3539 - val_loss: 0.1183\n",
      "Epoch 27/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3444\n",
      "Epoch 00027: val_loss improved from 0.11833 to 0.11705, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3502 - val_loss: 0.1170\n",
      "Epoch 28/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3478\n",
      "Epoch 00028: val_loss improved from 0.11705 to 0.11619, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3453 - val_loss: 0.1162\n",
      "Epoch 29/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3439\n",
      "Epoch 00029: val_loss improved from 0.11619 to 0.11596, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3481 - val_loss: 0.1160\n",
      "Epoch 30/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3430\n",
      "Epoch 00030: val_loss improved from 0.11596 to 0.11521, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3441 - val_loss: 0.1152\n",
      "Epoch 31/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3392\n",
      "Epoch 00031: val_loss improved from 0.11521 to 0.11449, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3347 - val_loss: 0.1145\n",
      "Epoch 32/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3289\n",
      "Epoch 00032: val_loss improved from 0.11449 to 0.11357, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3327 - val_loss: 0.1136\n",
      "Epoch 33/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3324\n",
      "Epoch 00033: val_loss improved from 0.11357 to 0.11324, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3295 - val_loss: 0.1132\n",
      "Epoch 34/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3339\n",
      "Epoch 00034: val_loss improved from 0.11324 to 0.11246, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3275 - val_loss: 0.1125\n",
      "Epoch 35/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3188\n",
      "Epoch 00035: val_loss improved from 0.11246 to 0.11207, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3231 - val_loss: 0.1121\n",
      "Epoch 36/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3239\n",
      "Epoch 00036: val_loss improved from 0.11207 to 0.11102, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3250 - val_loss: 0.1110\n",
      "Epoch 37/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3283\n",
      "Epoch 00037: val_loss improved from 0.11102 to 0.11064, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3212 - val_loss: 0.1106\n",
      "Epoch 38/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3102\n",
      "Epoch 00038: val_loss improved from 0.11064 to 0.10973, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3157 - val_loss: 0.1097\n",
      "Epoch 39/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3220\n",
      "Epoch 00039: val_loss improved from 0.10973 to 0.10958, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3167 - val_loss: 0.1096\n",
      "Epoch 40/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3152\n",
      "Epoch 00040: val_loss improved from 0.10958 to 0.10891, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3106 - val_loss: 0.1089\n",
      "Epoch 41/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3043\n",
      "Epoch 00041: val_loss improved from 0.10891 to 0.10841, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3052 - val_loss: 0.1084\n",
      "Epoch 42/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3088\n",
      "Epoch 00042: val_loss improved from 0.10841 to 0.10800, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3077 - val_loss: 0.1080\n",
      "Epoch 43/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3003\n",
      "Epoch 00043: val_loss improved from 0.10800 to 0.10736, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3034 - val_loss: 0.1074\n",
      "Epoch 44/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3067\n",
      "Epoch 00044: val_loss improved from 0.10736 to 0.10697, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3051 - val_loss: 0.1070\n",
      "Epoch 45/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2978\n",
      "Epoch 00045: val_loss improved from 0.10697 to 0.10673, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2990 - val_loss: 0.1067\n",
      "Epoch 46/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3120\n",
      "Epoch 00046: val_loss improved from 0.10673 to 0.10641, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3050 - val_loss: 0.1064\n",
      "Epoch 47/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3023\n",
      "Epoch 00047: val_loss improved from 0.10641 to 0.10610, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3002 - val_loss: 0.1061\n",
      "Epoch 48/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2968\n",
      "Epoch 00048: val_loss improved from 0.10610 to 0.10519, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2949 - val_loss: 0.1052\n",
      "Epoch 49/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2971\n",
      "Epoch 00049: val_loss improved from 0.10519 to 0.10510, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2991 - val_loss: 0.1051\n",
      "Epoch 50/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2908\n",
      "Epoch 00050: val_loss did not improve from 0.10510\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2938 - val_loss: 0.1052\n",
      "Epoch 51/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2901\n",
      "Epoch 00051: val_loss improved from 0.10510 to 0.10395, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2918 - val_loss: 0.1040\n",
      "Epoch 52/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2943\n",
      "Epoch 00052: val_loss improved from 0.10395 to 0.10368, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2934 - val_loss: 0.1037\n",
      "Epoch 53/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2875\n",
      "Epoch 00053: val_loss improved from 0.10368 to 0.10320, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2871 - val_loss: 0.1032\n",
      "Epoch 54/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2843\n",
      "Epoch 00054: val_loss improved from 0.10320 to 0.10318, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2865 - val_loss: 0.1032\n",
      "Epoch 55/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2831\n",
      "Epoch 00055: val_loss improved from 0.10318 to 0.10291, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2858 - val_loss: 0.1029\n",
      "Epoch 56/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2837\n",
      "Epoch 00056: val_loss improved from 0.10291 to 0.10222, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2797 - val_loss: 0.1022\n",
      "Epoch 57/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2826\n",
      "Epoch 00057: val_loss improved from 0.10222 to 0.10195, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2812 - val_loss: 0.1020\n",
      "Epoch 58/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2820\n",
      "Epoch 00058: val_loss improved from 0.10195 to 0.10181, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2822 - val_loss: 0.1018\n",
      "Epoch 59/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.2815\n",
      "Epoch 00059: val_loss improved from 0.10181 to 0.10138, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2790 - val_loss: 0.1014\n",
      "Epoch 60/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2685\n",
      "Epoch 00060: val_loss did not improve from 0.10138\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2740 - val_loss: 0.1016\n",
      "Epoch 61/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2706\n",
      "Epoch 00061: val_loss did not improve from 0.10138\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2761 - val_loss: 0.1015\n",
      " ###6 fold : val acc1 0.608, acc3 0.977, mae 0.207###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/35 [====================>.........] - ETA: 0s - loss: 22.4030 \n",
      "Epoch 00001: val_loss improved from inf to 17.70593, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 1s 6ms/step - loss: 21.4137 - val_loss: 17.7059\n",
      "Epoch 2/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 15.9358\n",
      "Epoch 00002: val_loss improved from 17.70593 to 12.09774, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 15.0895 - val_loss: 12.0977\n",
      "Epoch 3/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 10.8257\n",
      "Epoch 00003: val_loss improved from 12.09774 to 7.73554, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0684 - val_loss: 7.7355\n",
      "Epoch 4/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 6.9480\n",
      "Epoch 00004: val_loss improved from 7.73554 to 4.87209, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 6.4084 - val_loss: 4.8721\n",
      "Epoch 5/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 4.5198\n",
      "Epoch 00005: val_loss improved from 4.87209 to 3.33966, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.2779 - val_loss: 3.3397\n",
      "Epoch 6/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 3.2697\n",
      "Epoch 00006: val_loss improved from 3.33966 to 2.45735, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.1239 - val_loss: 2.4574\n",
      "Epoch 7/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 2.5243\n",
      "Epoch 00007: val_loss improved from 2.45735 to 1.79268, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.3909 - val_loss: 1.7927\n",
      "Epoch 8/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.9076\n",
      "Epoch 00008: val_loss improved from 1.79268 to 1.28268, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7967 - val_loss: 1.2827\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.4431\n",
      "Epoch 00009: val_loss improved from 1.28268 to 0.91013, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.3548 - val_loss: 0.9101\n",
      "Epoch 10/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.0917\n",
      "Epoch 00010: val_loss improved from 0.91013 to 0.64641, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.0334 - val_loss: 0.6464\n",
      "Epoch 11/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8179\n",
      "Epoch 00011: val_loss improved from 0.64641 to 0.46731, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7947 - val_loss: 0.4673\n",
      "Epoch 12/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6917\n",
      "Epoch 00012: val_loss improved from 0.46731 to 0.34357, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6583 - val_loss: 0.3436\n",
      "Epoch 13/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.5621\n",
      "Epoch 00013: val_loss improved from 0.34357 to 0.26253, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5556 - val_loss: 0.2625\n",
      "Epoch 14/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.4959\n",
      "Epoch 00014: val_loss improved from 0.26253 to 0.20843, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4896 - val_loss: 0.2084\n",
      "Epoch 15/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.4449\n",
      "Epoch 00015: val_loss improved from 0.20843 to 0.17427, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4448 - val_loss: 0.1743\n",
      "Epoch 16/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.4370\n",
      "Epoch 00016: val_loss improved from 0.17427 to 0.15378, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4219 - val_loss: 0.1538\n",
      "Epoch 17/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.4121\n",
      "Epoch 00017: val_loss improved from 0.15378 to 0.14168, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4045 - val_loss: 0.1417\n",
      "Epoch 18/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.3851\n",
      "Epoch 00018: val_loss improved from 0.14168 to 0.13504, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3914 - val_loss: 0.1350\n",
      "Epoch 19/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3903\n",
      "Epoch 00019: val_loss improved from 0.13504 to 0.13016, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3853 - val_loss: 0.1302\n",
      "Epoch 20/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3736\n",
      "Epoch 00020: val_loss improved from 0.13016 to 0.12702, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3759 - val_loss: 0.1270\n",
      "Epoch 21/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3793\n",
      "Epoch 00021: val_loss improved from 0.12702 to 0.12467, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3762 - val_loss: 0.1247\n",
      "Epoch 22/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3753\n",
      "Epoch 00022: val_loss improved from 0.12467 to 0.12277, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3707 - val_loss: 0.1228\n",
      "Epoch 23/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3567\n",
      "Epoch 00023: val_loss improved from 0.12277 to 0.12118, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3629 - val_loss: 0.1212\n",
      "Epoch 24/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3537\n",
      "Epoch 00024: val_loss improved from 0.12118 to 0.12034, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3580 - val_loss: 0.1203\n",
      "Epoch 25/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3473\n",
      "Epoch 00025: val_loss improved from 0.12034 to 0.11934, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3544 - val_loss: 0.1193\n",
      "Epoch 26/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3436\n",
      "Epoch 00026: val_loss improved from 0.11934 to 0.11869, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3543 - val_loss: 0.1187\n",
      "Epoch 27/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3391\n",
      "Epoch 00027: val_loss improved from 0.11869 to 0.11755, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3455 - val_loss: 0.1175\n",
      "Epoch 28/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3463\n",
      "Epoch 00028: val_loss improved from 0.11755 to 0.11671, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3446 - val_loss: 0.1167\n",
      "Epoch 29/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3439\n",
      "Epoch 00029: val_loss improved from 0.11671 to 0.11638, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3469 - val_loss: 0.1164\n",
      "Epoch 30/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3382\n",
      "Epoch 00030: val_loss improved from 0.11638 to 0.11574, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3401 - val_loss: 0.1157\n",
      "Epoch 31/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3374\n",
      "Epoch 00031: val_loss improved from 0.11574 to 0.11505, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3302 - val_loss: 0.1151\n",
      "Epoch 32/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3258\n",
      "Epoch 00032: val_loss improved from 0.11505 to 0.11443, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3309 - val_loss: 0.1144\n",
      "Epoch 33/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3278\n",
      "Epoch 00033: val_loss improved from 0.11443 to 0.11375, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3248 - val_loss: 0.1137\n",
      "Epoch 34/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3263\n",
      "Epoch 00034: val_loss improved from 0.11375 to 0.11293, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3270 - val_loss: 0.1129\n",
      "Epoch 35/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3150\n",
      "Epoch 00035: val_loss improved from 0.11293 to 0.11258, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes256_dropout0.4,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3187 - val_loss: 0.1126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2021 - val_loss: 0.1017\n",
      "Epoch 36/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2039\n",
      "Epoch 00036: val_loss improved from 0.10175 to 0.10105, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2031 - val_loss: 0.1011\n",
      "Epoch 37/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1958\n",
      "Epoch 00037: val_loss improved from 0.10105 to 0.10072, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1961 - val_loss: 0.1007\n",
      "Epoch 38/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1986\n",
      "Epoch 00038: val_loss improved from 0.10072 to 0.10061, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1989 - val_loss: 0.1006\n",
      "Epoch 39/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1966\n",
      "Epoch 00039: val_loss improved from 0.10061 to 0.10052, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1967 - val_loss: 0.1005\n",
      "Epoch 40/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1980\n",
      "Epoch 00040: val_loss improved from 0.10052 to 0.10037, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1975 - val_loss: 0.1004\n",
      "Epoch 41/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1956\n",
      "Epoch 00041: val_loss improved from 0.10037 to 0.10020, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1958 - val_loss: 0.1002\n",
      "Epoch 42/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1934\n",
      "Epoch 00042: val_loss improved from 0.10020 to 0.09973, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1933 - val_loss: 0.0997\n",
      "Epoch 43/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1922\n",
      "Epoch 00043: val_loss did not improve from 0.09973\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1920 - val_loss: 0.0999\n",
      "Epoch 44/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1949\n",
      "Epoch 00044: val_loss improved from 0.09973 to 0.09929, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1946 - val_loss: 0.0993\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1911\n",
      "Epoch 00045: val_loss did not improve from 0.09929\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1911 - val_loss: 0.0994\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1915\n",
      "Epoch 00046: val_loss improved from 0.09929 to 0.09879, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1915 - val_loss: 0.0988\n",
      "Epoch 47/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00047: val_loss did not improve from 0.09879\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1876 - val_loss: 0.0989\n",
      "Epoch 48/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1925\n",
      "Epoch 00048: val_loss improved from 0.09879 to 0.09827, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1910 - val_loss: 0.0983\n",
      "Epoch 49/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1897\n",
      "Epoch 00049: val_loss did not improve from 0.09827\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1905 - val_loss: 0.0997\n",
      "Epoch 50/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1870\n",
      "Epoch 00050: val_loss did not improve from 0.09827\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1869 - val_loss: 0.0991\n",
      " ###2 fold : val acc1 0.594, acc3 0.978, mae 0.214###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 20.7534 \n",
      "Epoch 00001: val_loss improved from inf to 11.72822, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 18.1856 - val_loss: 11.7282\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 8.8559 \n",
      "Epoch 00002: val_loss improved from 11.72822 to 3.64653, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 7.2623 - val_loss: 3.6465\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.1539\n",
      "Epoch 00003: val_loss improved from 3.64653 to 1.73245, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.7671 - val_loss: 1.7324\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.5991\n",
      "Epoch 00004: val_loss improved from 1.73245 to 0.85150, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4201 - val_loss: 0.8515\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8701\n",
      "Epoch 00005: val_loss improved from 0.85150 to 0.44070, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7859 - val_loss: 0.4407\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5177\n",
      "Epoch 00006: val_loss improved from 0.44070 to 0.25673, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4834 - val_loss: 0.2567\n",
      "Epoch 7/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3789\n",
      "Epoch 00007: val_loss improved from 0.25673 to 0.17791, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3609 - val_loss: 0.1779\n",
      "Epoch 8/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3079\n",
      "Epoch 00008: val_loss improved from 0.17791 to 0.14668, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3073 - val_loss: 0.1467\n",
      "Epoch 9/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2917\n",
      "Epoch 00009: val_loss improved from 0.14668 to 0.13386, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2914 - val_loss: 0.1339\n",
      "Epoch 10/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2783\n",
      "Epoch 00010: val_loss improved from 0.13386 to 0.12779, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2779 - val_loss: 0.1278\n",
      "Epoch 11/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2651\n",
      "Epoch 00011: val_loss improved from 0.12779 to 0.12531, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2651 - val_loss: 0.1253\n",
      "Epoch 12/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2676\n",
      "Epoch 00012: val_loss improved from 0.12531 to 0.12282, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2684 - val_loss: 0.1228\n",
      "Epoch 13/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2595\n",
      "Epoch 00013: val_loss improved from 0.12282 to 0.12071, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2590 - val_loss: 0.1207\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2491\n",
      "Epoch 00014: val_loss improved from 0.12071 to 0.11920, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2491 - val_loss: 0.1192\n",
      "Epoch 15/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2455\n",
      "Epoch 00015: val_loss improved from 0.11920 to 0.11777, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2453 - val_loss: 0.1178\n",
      "Epoch 16/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2450\n",
      "Epoch 00016: val_loss improved from 0.11777 to 0.11633, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2460 - val_loss: 0.1163\n",
      "Epoch 17/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2361\n",
      "Epoch 00017: val_loss improved from 0.11633 to 0.11496, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2366 - val_loss: 0.1150\n",
      "Epoch 18/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2409\n",
      "Epoch 00018: val_loss improved from 0.11496 to 0.11395, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2404 - val_loss: 0.1140\n",
      "Epoch 19/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 0.2457\n",
      "Epoch 00019: val_loss improved from 0.11395 to 0.11381, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2399 - val_loss: 0.1138\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2280\n",
      "Epoch 00020: val_loss improved from 0.11381 to 0.11168, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2280 - val_loss: 0.1117\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2312\n",
      "Epoch 00021: val_loss improved from 0.11168 to 0.11039, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2312 - val_loss: 0.1104\n",
      "Epoch 22/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2257\n",
      "Epoch 00022: val_loss improved from 0.11039 to 0.10941, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2256 - val_loss: 0.1094\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2223\n",
      "Epoch 00023: val_loss improved from 0.10941 to 0.10815, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2223 - val_loss: 0.1081\n",
      "Epoch 24/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2216\n",
      "Epoch 00024: val_loss improved from 0.10815 to 0.10781, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2219 - val_loss: 0.1078\n",
      "Epoch 25/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2230\n",
      "Epoch 00025: val_loss improved from 0.10781 to 0.10710, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2226 - val_loss: 0.1071\n",
      "Epoch 26/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2165\n",
      "Epoch 00026: val_loss improved from 0.10710 to 0.10627, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2167 - val_loss: 0.1063\n",
      "Epoch 27/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2125\n",
      "Epoch 00027: val_loss improved from 0.10627 to 0.10600, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2126 - val_loss: 0.1060\n",
      "Epoch 28/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2096\n",
      "Epoch 00028: val_loss improved from 0.10600 to 0.10500, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2100 - val_loss: 0.1050\n",
      "Epoch 29/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2127\n",
      "Epoch 00029: val_loss improved from 0.10500 to 0.10406, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2124 - val_loss: 0.1041\n",
      "Epoch 30/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2078\n",
      "Epoch 00030: val_loss improved from 0.10406 to 0.10400, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2075 - val_loss: 0.1040\n",
      "Epoch 31/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00031: val_loss did not improve from 0.10400\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2061 - val_loss: 0.1041\n",
      "Epoch 32/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2070\n",
      "Epoch 00032: val_loss improved from 0.10400 to 0.10270, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2056 - val_loss: 0.1027\n",
      "Epoch 33/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2077\n",
      "Epoch 00033: val_loss improved from 0.10270 to 0.10231, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2078 - val_loss: 0.1023\n",
      "Epoch 34/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00034: val_loss improved from 0.10231 to 0.10208, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2060 - val_loss: 0.1021\n",
      "Epoch 35/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2025\n",
      "Epoch 00035: val_loss improved from 0.10208 to 0.10195, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2020 - val_loss: 0.1020\n",
      "Epoch 36/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2043\n",
      "Epoch 00036: val_loss improved from 0.10195 to 0.10149, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2035 - val_loss: 0.1015\n",
      "Epoch 37/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1958\n",
      "Epoch 00037: val_loss improved from 0.10149 to 0.10078, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1962 - val_loss: 0.1008\n",
      "Epoch 38/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1982\n",
      "Epoch 00038: val_loss improved from 0.10078 to 0.10043, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1985 - val_loss: 0.1004\n",
      "Epoch 39/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1979\n",
      "Epoch 00039: val_loss did not improve from 0.10043\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1979 - val_loss: 0.1006\n",
      "Epoch 40/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1978\n",
      "Epoch 00040: val_loss improved from 0.10043 to 0.10001, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1972 - val_loss: 0.1000\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1955\n",
      "Epoch 00041: val_loss did not improve from 0.10001\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1955 - val_loss: 0.1004\n",
      "Epoch 42/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1923\n",
      "Epoch 00042: val_loss did not improve from 0.10001\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1924 - val_loss: 0.1000\n",
      " ###3 fold : val acc1 0.599, acc3 0.972, mae 0.215###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 20.5254 \n",
      "Epoch 00001: val_loss improved from inf to 11.69861, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 18.0949 - val_loss: 11.6986\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 8.8075 \n",
      "Epoch 00002: val_loss improved from 11.69861 to 3.64822, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 7.2301 - val_loss: 3.6482\n",
      "Epoch 3/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 3.1561\n",
      "Epoch 00003: val_loss improved from 3.64822 to 1.73944, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.7411 - val_loss: 1.7394\n",
      "Epoch 4/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.6139\n",
      "Epoch 00004: val_loss improved from 1.73944 to 0.85489, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4178 - val_loss: 0.8549\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8870\n",
      "Epoch 00005: val_loss improved from 0.85489 to 0.43986, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7906 - val_loss: 0.4399\n",
      "Epoch 6/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.5280\n",
      "Epoch 00006: val_loss improved from 0.43986 to 0.25464, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4906 - val_loss: 0.2546\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3810\n",
      "Epoch 00007: val_loss improved from 0.25464 to 0.17647, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3605 - val_loss: 0.1765\n",
      "Epoch 8/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3187\n",
      "Epoch 00008: val_loss improved from 0.17647 to 0.14562, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3103 - val_loss: 0.1456\n",
      "Epoch 9/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2915\n",
      "Epoch 00009: val_loss improved from 0.14562 to 0.13359, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2891 - val_loss: 0.1336\n",
      "Epoch 10/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2774\n",
      "Epoch 00010: val_loss improved from 0.13359 to 0.12792, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2736 - val_loss: 0.1279\n",
      "Epoch 11/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2674\n",
      "Epoch 00011: val_loss improved from 0.12792 to 0.12495, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2656 - val_loss: 0.1249\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2619\n",
      "Epoch 00012: val_loss improved from 0.12495 to 0.12290, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2628 - val_loss: 0.1229\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2549\n",
      "Epoch 00013: val_loss improved from 0.12290 to 0.12110, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2561 - val_loss: 0.1211\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2514\n",
      "Epoch 00014: val_loss improved from 0.12110 to 0.11928, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2528 - val_loss: 0.1193\n",
      "Epoch 15/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2486\n",
      "Epoch 00015: val_loss improved from 0.11928 to 0.11780, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2479 - val_loss: 0.1178\n",
      "Epoch 16/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2464\n",
      "Epoch 00016: val_loss improved from 0.11780 to 0.11676, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2453 - val_loss: 0.1168\n",
      "Epoch 17/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2407\n",
      "Epoch 00017: val_loss improved from 0.11676 to 0.11548, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2410 - val_loss: 0.1155\n",
      "Epoch 18/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2370\n",
      "Epoch 00018: val_loss improved from 0.11548 to 0.11439, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2366 - val_loss: 0.1144\n",
      "Epoch 19/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2347\n",
      "Epoch 00019: val_loss improved from 0.11439 to 0.11293, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2347 - val_loss: 0.1129\n",
      "Epoch 20/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2264\n",
      "Epoch 00020: val_loss improved from 0.11293 to 0.11182, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2259 - val_loss: 0.1118\n",
      "Epoch 21/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2289\n",
      "Epoch 00021: val_loss improved from 0.11182 to 0.11056, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2284 - val_loss: 0.1106\n",
      "Epoch 22/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2257\n",
      "Epoch 00022: val_loss improved from 0.11056 to 0.10943, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2256 - val_loss: 0.1094\n",
      "Epoch 23/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2192\n",
      "Epoch 00023: val_loss improved from 0.10943 to 0.10896, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2207 - val_loss: 0.1090\n",
      "Epoch 24/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2215\n",
      "Epoch 00024: val_loss improved from 0.10896 to 0.10799, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2218 - val_loss: 0.1080\n",
      "Epoch 25/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2139\n",
      "Epoch 00025: val_loss improved from 0.10799 to 0.10725, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2137 - val_loss: 0.1072\n",
      "Epoch 26/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2136\n",
      "Epoch 00026: val_loss improved from 0.10725 to 0.10610, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2137 - val_loss: 0.1061\n",
      "Epoch 27/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2190\n",
      "Epoch 00027: val_loss improved from 0.10610 to 0.10562, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2186 - val_loss: 0.1056\n",
      "Epoch 28/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2126\n",
      "Epoch 00028: val_loss improved from 0.10562 to 0.10464, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2125 - val_loss: 0.1046\n",
      "Epoch 29/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2127\n",
      "Epoch 00029: val_loss improved from 0.10464 to 0.10405, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2131 - val_loss: 0.1040\n",
      "Epoch 30/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2074\n",
      "Epoch 00030: val_loss improved from 0.10405 to 0.10334, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2068 - val_loss: 0.1033\n",
      "Epoch 31/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2110\n",
      "Epoch 00031: val_loss improved from 0.10334 to 0.10291, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2106 - val_loss: 0.1029\n",
      "Epoch 32/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2058\n",
      "Epoch 00032: val_loss improved from 0.10291 to 0.10267, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2055 - val_loss: 0.1027\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2049\n",
      "Epoch 00033: val_loss improved from 0.10267 to 0.10246, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2049 - val_loss: 0.1025\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2007\n",
      "Epoch 00034: val_loss improved from 0.10246 to 0.10163, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2007 - val_loss: 0.1016\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2039\n",
      "Epoch 00035: val_loss did not improve from 0.10163\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2039 - val_loss: 0.1018\n",
      "Epoch 36/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2009\n",
      "Epoch 00036: val_loss improved from 0.10163 to 0.10092, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2008 - val_loss: 0.1009\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1987\n",
      "Epoch 00037: val_loss did not improve from 0.10092\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1987 - val_loss: 0.1009\n",
      "Epoch 38/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1987\n",
      "Epoch 00038: val_loss improved from 0.10092 to 0.10031, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1985 - val_loss: 0.1003\n",
      "Epoch 39/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1954\n",
      "Epoch 00039: val_loss did not improve from 0.10031\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1956 - val_loss: 0.1004\n",
      "Epoch 40/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1903\n",
      "Epoch 00040: val_loss improved from 0.10031 to 0.10002, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1911 - val_loss: 0.1000\n",
      "Epoch 41/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1941\n",
      "Epoch 00041: val_loss improved from 0.10002 to 0.09969, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1937 - val_loss: 0.0997\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1955\n",
      "Epoch 00042: val_loss did not improve from 0.09969\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1955 - val_loss: 0.0999\n",
      "Epoch 43/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1930\n",
      "Epoch 00043: val_loss did not improve from 0.09969\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1930 - val_loss: 0.1000\n",
      " ###4 fold : val acc1 0.583, acc3 0.977, mae 0.220###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/35 [===============>..............] - ETA: 0s - loss: 20.8690 \n",
      "Epoch 00001: val_loss improved from inf to 11.69496, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 18.1094 - val_loss: 11.6950\n",
      "Epoch 2/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 9.0555 \n",
      "Epoch 00002: val_loss improved from 11.69496 to 3.64460, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 7.2350 - val_loss: 3.6446\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.1182\n",
      "Epoch 00003: val_loss improved from 3.64460 to 1.74601, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.7374 - val_loss: 1.7460\n",
      "Epoch 4/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.6186\n",
      "Epoch 00004: val_loss improved from 1.74601 to 0.85951, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4174 - val_loss: 0.8595\n",
      "Epoch 5/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.8720\n",
      "Epoch 00005: val_loss improved from 0.85951 to 0.44402, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7932 - val_loss: 0.4440\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5413\n",
      "Epoch 00006: val_loss improved from 0.44402 to 0.25665, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4954 - val_loss: 0.2566\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3785\n",
      "Epoch 00007: val_loss improved from 0.25665 to 0.17797, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3603 - val_loss: 0.1780\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3195\n",
      "Epoch 00008: val_loss improved from 0.17797 to 0.14653, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3075 - val_loss: 0.1465\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2891\n",
      "Epoch 00009: val_loss improved from 0.14653 to 0.13428, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2881 - val_loss: 0.1343\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2830\n",
      "Epoch 00010: val_loss improved from 0.13428 to 0.12818, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2762 - val_loss: 0.1282\n",
      "Epoch 11/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2681\n",
      "Epoch 00011: val_loss improved from 0.12818 to 0.12535, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2659 - val_loss: 0.1253\n",
      "Epoch 12/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2619\n",
      "Epoch 00012: val_loss improved from 0.12535 to 0.12340, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2624 - val_loss: 0.1234\n",
      "Epoch 13/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2546\n",
      "Epoch 00013: val_loss improved from 0.12340 to 0.12145, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2568 - val_loss: 0.1214\n",
      "Epoch 14/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2518\n",
      "Epoch 00014: val_loss improved from 0.12145 to 0.11966, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2526 - val_loss: 0.1197\n",
      "Epoch 15/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2487\n",
      "Epoch 00015: val_loss improved from 0.11966 to 0.11810, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2486 - val_loss: 0.1181\n",
      "Epoch 16/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2446\n",
      "Epoch 00016: val_loss improved from 0.11810 to 0.11683, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2445 - val_loss: 0.1168\n",
      "Epoch 17/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2390\n",
      "Epoch 00017: val_loss improved from 0.11683 to 0.11576, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2397 - val_loss: 0.1158\n",
      "Epoch 18/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2367\n",
      "Epoch 00018: val_loss improved from 0.11576 to 0.11446, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2361 - val_loss: 0.1145\n",
      "Epoch 19/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2356\n",
      "Epoch 00019: val_loss improved from 0.11446 to 0.11296, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2355 - val_loss: 0.1130\n",
      "Epoch 20/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.2277\n",
      "Epoch 00020: val_loss improved from 0.11296 to 0.11241, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2282 - val_loss: 0.1124\n",
      "Epoch 21/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2315\n",
      "Epoch 00021: val_loss improved from 0.11241 to 0.11072, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2296 - val_loss: 0.1107\n",
      "Epoch 22/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2274\n",
      "Epoch 00022: val_loss improved from 0.11072 to 0.10972, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2272 - val_loss: 0.1097\n",
      "Epoch 23/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2194\n",
      "Epoch 00023: val_loss improved from 0.10972 to 0.10899, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2209 - val_loss: 0.1090\n",
      "Epoch 24/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2203\n",
      "Epoch 00024: val_loss improved from 0.10899 to 0.10804, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2210 - val_loss: 0.1080\n",
      "Epoch 25/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.2172\n",
      "Epoch 00025: val_loss improved from 0.10804 to 0.10718, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2155 - val_loss: 0.1072\n",
      "Epoch 26/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2110\n",
      "Epoch 00026: val_loss improved from 0.10718 to 0.10617, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2113 - val_loss: 0.1062\n",
      "Epoch 27/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2195\n",
      "Epoch 00027: val_loss improved from 0.10617 to 0.10545, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2187 - val_loss: 0.1054\n",
      "Epoch 28/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2144\n",
      "Epoch 00028: val_loss improved from 0.10545 to 0.10474, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2146 - val_loss: 0.1047\n",
      "Epoch 29/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2117\n",
      "Epoch 00029: val_loss improved from 0.10474 to 0.10410, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2121 - val_loss: 0.1041\n",
      "Epoch 30/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2067\n",
      "Epoch 00030: val_loss improved from 0.10410 to 0.10341, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2059 - val_loss: 0.1034\n",
      "Epoch 31/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2109\n",
      "Epoch 00031: val_loss improved from 0.10341 to 0.10325, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2115 - val_loss: 0.1032\n",
      "Epoch 32/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2071\n",
      "Epoch 00032: val_loss improved from 0.10325 to 0.10267, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2067 - val_loss: 0.1027\n",
      "Epoch 33/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2043\n",
      "Epoch 00033: val_loss improved from 0.10267 to 0.10232, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2052 - val_loss: 0.1023\n",
      "Epoch 34/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2023\n",
      "Epoch 00034: val_loss improved from 0.10232 to 0.10181, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2022 - val_loss: 0.1018\n",
      "Epoch 35/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2032\n",
      "Epoch 00035: val_loss did not improve from 0.10181\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2028 - val_loss: 0.1022\n",
      "Epoch 36/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.2006\n",
      "Epoch 00036: val_loss improved from 0.10181 to 0.10111, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2009 - val_loss: 0.1011\n",
      "Epoch 37/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2001\n",
      "Epoch 00037: val_loss improved from 0.10111 to 0.10088, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1992 - val_loss: 0.1009\n",
      "Epoch 38/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1979\n",
      "Epoch 00038: val_loss improved from 0.10088 to 0.10044, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1980 - val_loss: 0.1004\n",
      "Epoch 39/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1948\n",
      "Epoch 00039: val_loss improved from 0.10044 to 0.10043, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1949 - val_loss: 0.1004\n",
      "Epoch 40/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1915\n",
      "Epoch 00040: val_loss improved from 0.10043 to 0.10022, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1925 - val_loss: 0.1002\n",
      "Epoch 41/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1939\n",
      "Epoch 00041: val_loss improved from 0.10022 to 0.09994, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1933 - val_loss: 0.0999\n",
      "Epoch 42/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1970\n",
      "Epoch 00042: val_loss did not improve from 0.09994\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1955 - val_loss: 0.1000\n",
      "Epoch 43/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.1946\n",
      "Epoch 00043: val_loss improved from 0.09994 to 0.09981, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1944 - val_loss: 0.0998\n",
      "Epoch 44/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1912\n",
      "Epoch 00044: val_loss improved from 0.09981 to 0.09917, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1915 - val_loss: 0.0992\n",
      "Epoch 45/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1863\n",
      "Epoch 00045: val_loss improved from 0.09917 to 0.09906, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1870 - val_loss: 0.0991\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1891\n",
      "Epoch 00046: val_loss improved from 0.09906 to 0.09877, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1891 - val_loss: 0.0988\n",
      "Epoch 47/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1908\n",
      "Epoch 00047: val_loss improved from 0.09877 to 0.09873, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1898 - val_loss: 0.0987\n",
      "Epoch 48/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.1882\n",
      "Epoch 00048: val_loss improved from 0.09873 to 0.09827, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1881 - val_loss: 0.0983\n",
      "Epoch 49/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1897\n",
      "Epoch 00049: val_loss did not improve from 0.09827\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1881 - val_loss: 0.0984\n",
      "Epoch 50/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1893\n",
      "Epoch 00050: val_loss did not improve from 0.09827\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1891 - val_loss: 0.0984\n",
      " ###5 fold : val acc1 0.588, acc3 0.979, mae 0.217###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 20.5246 \n",
      "Epoch 00001: val_loss improved from inf to 11.69517, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 18.1144 - val_loss: 11.6952\n",
      "Epoch 2/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 9.0198 \n",
      "Epoch 00002: val_loss improved from 11.69517 to 3.64498, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 7.2245 - val_loss: 3.6450\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.1265\n",
      "Epoch 00003: val_loss improved from 3.64498 to 1.73789, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.7388 - val_loss: 1.7379\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 1.4112\n",
      "Epoch 00004: val_loss improved from 1.73789 to 0.85378, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4112 - val_loss: 0.8538\n",
      "Epoch 5/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8918\n",
      "Epoch 00005: val_loss improved from 0.85378 to 0.44128, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7921 - val_loss: 0.4413\n",
      "Epoch 6/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5457\n",
      "Epoch 00006: val_loss improved from 0.44128 to 0.25464, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4959 - val_loss: 0.2546\n",
      "Epoch 7/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3817\n",
      "Epoch 00007: val_loss improved from 0.25464 to 0.17714, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3611 - val_loss: 0.1771\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3202\n",
      "Epoch 00008: val_loss improved from 0.17714 to 0.14607, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3095 - val_loss: 0.1461\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2914\n",
      "Epoch 00009: val_loss improved from 0.14607 to 0.13395, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2888 - val_loss: 0.1339\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2836\n",
      "Epoch 00010: val_loss improved from 0.13395 to 0.12767, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2763 - val_loss: 0.1277\n",
      "Epoch 11/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2693\n",
      "Epoch 00011: val_loss improved from 0.12767 to 0.12459, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2671 - val_loss: 0.1246\n",
      "Epoch 12/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2588\n",
      "Epoch 00012: val_loss improved from 0.12459 to 0.12245, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2614 - val_loss: 0.1225\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2589\n",
      "Epoch 00013: val_loss improved from 0.12245 to 0.12070, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2589 - val_loss: 0.1207\n",
      "Epoch 14/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2549\n",
      "Epoch 00014: val_loss improved from 0.12070 to 0.11886, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2542 - val_loss: 0.1189\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2482\n",
      "Epoch 00015: val_loss improved from 0.11886 to 0.11737, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2482 - val_loss: 0.1174\n",
      "Epoch 16/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2463\n",
      "Epoch 00016: val_loss improved from 0.11737 to 0.11597, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2465 - val_loss: 0.1160\n",
      "Epoch 17/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2404\n",
      "Epoch 00017: val_loss improved from 0.11597 to 0.11484, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2410 - val_loss: 0.1148\n",
      "Epoch 18/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2364\n",
      "Epoch 00018: val_loss improved from 0.11484 to 0.11355, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2358 - val_loss: 0.1136\n",
      "Epoch 19/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2366\n",
      "Epoch 00019: val_loss improved from 0.11355 to 0.11225, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2358 - val_loss: 0.1123\n",
      "Epoch 20/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2308\n",
      "Epoch 00020: val_loss improved from 0.11225 to 0.11190, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2305 - val_loss: 0.1119\n",
      "Epoch 21/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2341\n",
      "Epoch 00021: val_loss improved from 0.11190 to 0.11006, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2334 - val_loss: 0.1101\n",
      "Epoch 22/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2265\n",
      "Epoch 00022: val_loss improved from 0.11006 to 0.10912, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2271 - val_loss: 0.1091\n",
      "Epoch 23/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2214\n",
      "Epoch 00023: val_loss improved from 0.10912 to 0.10819, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2224 - val_loss: 0.1082\n",
      "Epoch 24/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2227\n",
      "Epoch 00024: val_loss improved from 0.10819 to 0.10740, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2230 - val_loss: 0.1074\n",
      "Epoch 25/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2185\n",
      "Epoch 00025: val_loss improved from 0.10740 to 0.10636, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2167 - val_loss: 0.1064\n",
      "Epoch 26/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2127\n",
      "Epoch 00026: val_loss improved from 0.10636 to 0.10557, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2125 - val_loss: 0.1056\n",
      "Epoch 27/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2196\n",
      "Epoch 00027: val_loss improved from 0.10557 to 0.10482, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2189 - val_loss: 0.1048\n",
      "Epoch 28/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2148\n",
      "Epoch 00028: val_loss improved from 0.10482 to 0.10420, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2150 - val_loss: 0.1042\n",
      "Epoch 29/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2125\n",
      "Epoch 00029: val_loss improved from 0.10420 to 0.10386, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2132 - val_loss: 0.1039\n",
      "Epoch 30/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2077\n",
      "Epoch 00030: val_loss improved from 0.10386 to 0.10303, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2068 - val_loss: 0.1030\n",
      "Epoch 31/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2106\n",
      "Epoch 00031: val_loss improved from 0.10303 to 0.10272, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2116 - val_loss: 0.1027\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2072\n",
      "Epoch 00032: val_loss improved from 0.10272 to 0.10218, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2072 - val_loss: 0.1022\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2067\n",
      "Epoch 00033: val_loss improved from 0.10218 to 0.10202, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2067 - val_loss: 0.1020\n",
      "Epoch 34/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2022\n",
      "Epoch 00034: val_loss improved from 0.10202 to 0.10125, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2027 - val_loss: 0.1012\n",
      "Epoch 35/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2045\n",
      "Epoch 00035: val_loss did not improve from 0.10125\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2038 - val_loss: 0.1018\n",
      "Epoch 36/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2009\n",
      "Epoch 00036: val_loss improved from 0.10125 to 0.10091, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2010 - val_loss: 0.1009\n",
      "Epoch 37/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00037: val_loss improved from 0.10091 to 0.10047, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2013 - val_loss: 0.1005\n",
      "Epoch 38/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1991\n",
      "Epoch 00038: val_loss improved from 0.10047 to 0.10000, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1997 - val_loss: 0.1000\n",
      "Epoch 39/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1960\n",
      "Epoch 00039: val_loss improved from 0.10000 to 0.09987, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1959 - val_loss: 0.0999\n",
      "Epoch 40/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1922\n",
      "Epoch 00040: val_loss did not improve from 0.09987\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1937 - val_loss: 0.0999\n",
      "Epoch 41/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1945\n",
      "Epoch 00041: val_loss did not improve from 0.09987\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1935 - val_loss: 0.1001\n",
      " ###6 fold : val acc1 0.602, acc3 0.982, mae 0.208###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/35 [===============>..............] - ETA: 0s - loss: 20.8696 \n",
      "Epoch 00001: val_loss improved from inf to 11.69079, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 18.1227 - val_loss: 11.6908\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 8.8880 \n",
      "Epoch 00002: val_loss improved from 11.69079 to 3.63655, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 7.2355 - val_loss: 3.6365\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.1303\n",
      "Epoch 00003: val_loss improved from 3.63655 to 1.73584, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.7398 - val_loss: 1.7358\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 1.4077\n",
      "Epoch 00004: val_loss improved from 1.73584 to 0.85297, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4077 - val_loss: 0.8530\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.7865\n",
      "Epoch 00005: val_loss improved from 0.85297 to 0.44111, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7865 - val_loss: 0.4411\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4930\n",
      "Epoch 00006: val_loss improved from 0.44111 to 0.25394, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4930 - val_loss: 0.2539\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3567\n",
      "Epoch 00007: val_loss improved from 0.25394 to 0.17689, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3567 - val_loss: 0.1769\n",
      "Epoch 8/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3110\n",
      "Epoch 00008: val_loss improved from 0.17689 to 0.14604, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3101 - val_loss: 0.1460\n",
      "Epoch 9/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2880\n",
      "Epoch 00009: val_loss improved from 0.14604 to 0.13396, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2879 - val_loss: 0.1340\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2748\n",
      "Epoch 00010: val_loss improved from 0.13396 to 0.12783, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2748 - val_loss: 0.1278\n",
      "Epoch 11/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2670\n",
      "Epoch 00011: val_loss improved from 0.12783 to 0.12512, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2666 - val_loss: 0.1251\n",
      "Epoch 12/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2599\n",
      "Epoch 00012: val_loss improved from 0.12512 to 0.12295, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2596 - val_loss: 0.1229\n",
      "Epoch 13/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2586\n",
      "Epoch 00013: val_loss improved from 0.12295 to 0.12127, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2573 - val_loss: 0.1213\n",
      "Epoch 14/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2531\n",
      "Epoch 00014: val_loss improved from 0.12127 to 0.11942, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2531 - val_loss: 0.1194\n",
      "Epoch 15/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2453\n",
      "Epoch 00015: val_loss improved from 0.11942 to 0.11780, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2460 - val_loss: 0.1178\n",
      "Epoch 16/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2436\n",
      "Epoch 00016: val_loss improved from 0.11780 to 0.11664, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2433 - val_loss: 0.1166\n",
      "Epoch 17/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2393\n",
      "Epoch 00017: val_loss improved from 0.11664 to 0.11509, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2393 - val_loss: 0.1151\n",
      "Epoch 18/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2344\n",
      "Epoch 00018: val_loss improved from 0.11509 to 0.11376, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2334 - val_loss: 0.1138\n",
      "Epoch 19/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2334\n",
      "Epoch 00019: val_loss improved from 0.11376 to 0.11260, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2331 - val_loss: 0.1126\n",
      "Epoch 20/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2285\n",
      "Epoch 00020: val_loss improved from 0.11260 to 0.11226, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2296 - val_loss: 0.1123\n",
      "Epoch 21/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2330\n",
      "Epoch 00021: val_loss improved from 0.11226 to 0.11030, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2310 - val_loss: 0.1103\n",
      "Epoch 22/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2272\n",
      "Epoch 00022: val_loss improved from 0.11030 to 0.10972, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2277 - val_loss: 0.1097\n",
      "Epoch 23/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2216\n",
      "Epoch 00023: val_loss improved from 0.10972 to 0.10848, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2218 - val_loss: 0.1085\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2197\n",
      "Epoch 00024: val_loss improved from 0.10848 to 0.10786, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2197 - val_loss: 0.1079\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2163\n",
      "Epoch 00025: val_loss improved from 0.10786 to 0.10662, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2163 - val_loss: 0.1066\n",
      "Epoch 26/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2120\n",
      "Epoch 00026: val_loss improved from 0.10662 to 0.10603, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2118 - val_loss: 0.1060\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2168\n",
      "Epoch 00027: val_loss improved from 0.10603 to 0.10517, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2168 - val_loss: 0.1052\n",
      "Epoch 28/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2132\n",
      "Epoch 00028: val_loss improved from 0.10517 to 0.10474, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2129 - val_loss: 0.1047\n",
      "Epoch 29/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2100\n",
      "Epoch 00029: val_loss improved from 0.10474 to 0.10431, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2106 - val_loss: 0.1043\n",
      "Epoch 30/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2075\n",
      "Epoch 00030: val_loss improved from 0.10431 to 0.10345, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2068 - val_loss: 0.1034\n",
      "Epoch 31/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2081\n",
      "Epoch 00031: val_loss did not improve from 0.10345\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2087 - val_loss: 0.1035\n",
      "Epoch 32/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.2046\n",
      "Epoch 00032: val_loss improved from 0.10345 to 0.10261, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2046 - val_loss: 0.1026\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2054\n",
      "Epoch 00033: val_loss improved from 0.10261 to 0.10258, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2054 - val_loss: 0.1026\n",
      "Epoch 34/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2011\n",
      "Epoch 00034: val_loss improved from 0.10258 to 0.10162, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2017 - val_loss: 0.1016\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2022\n",
      "Epoch 00035: val_loss did not improve from 0.10162\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2022 - val_loss: 0.1017\n",
      "Epoch 36/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1977\n",
      "Epoch 00036: val_loss improved from 0.10162 to 0.10106, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1968 - val_loss: 0.1011\n",
      "Epoch 37/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1989\n",
      "Epoch 00037: val_loss improved from 0.10106 to 0.10065, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1987 - val_loss: 0.1006\n",
      "Epoch 38/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1977\n",
      "Epoch 00038: val_loss improved from 0.10065 to 0.10019, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1985 - val_loss: 0.1002\n",
      "Epoch 39/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1943\n",
      "Epoch 00039: val_loss improved from 0.10019 to 0.10000, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1942 - val_loss: 0.1000\n",
      "Epoch 40/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1908\n",
      "Epoch 00040: val_loss improved from 0.10000 to 0.09982, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1917 - val_loss: 0.0998\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1920\n",
      "Epoch 00041: val_loss did not improve from 0.09982\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1920 - val_loss: 0.1010\n",
      "Epoch 42/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.1946\n",
      "Epoch 00042: val_loss did not improve from 0.09982\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1933 - val_loss: 0.0999\n",
      " ###7 fold : val acc1 0.579, acc3 0.970, mae 0.226###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 20.5699 \n",
      "Epoch 00001: val_loss improved from inf to 11.65311, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 18.1491 - val_loss: 11.6531\n",
      "Epoch 2/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 8.6744 \n",
      "Epoch 00002: val_loss improved from 11.65311 to 3.59153, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 7.2454 - val_loss: 3.5915\n",
      "Epoch 3/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.1120\n",
      "Epoch 00003: val_loss improved from 3.59153 to 1.72536, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.7365 - val_loss: 1.7254\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.6088\n",
      "Epoch 00004: val_loss improved from 1.72536 to 0.85661, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4075 - val_loss: 0.8566\n",
      "Epoch 5/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8821\n",
      "Epoch 00005: val_loss improved from 0.85661 to 0.44597, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7852 - val_loss: 0.4460\n",
      "Epoch 6/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5409\n",
      "Epoch 00006: val_loss improved from 0.44597 to 0.25989, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4911 - val_loss: 0.2599\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3769\n",
      "Epoch 00007: val_loss improved from 0.25989 to 0.18274, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3571 - val_loss: 0.1827\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3209\n",
      "Epoch 00008: val_loss improved from 0.18274 to 0.15137, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3105 - val_loss: 0.1514\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2898\n",
      "Epoch 00009: val_loss improved from 0.15137 to 0.13903, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2883 - val_loss: 0.1390\n",
      "Epoch 10/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2801\n",
      "Epoch 00010: val_loss improved from 0.13903 to 0.13261, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2750 - val_loss: 0.1326\n",
      "Epoch 11/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2676\n",
      "Epoch 00011: val_loss improved from 0.13261 to 0.12962, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2675 - val_loss: 0.1296\n",
      "Epoch 12/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2613\n",
      "Epoch 00012: val_loss improved from 0.12962 to 0.12726, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2609 - val_loss: 0.1273\n",
      "Epoch 13/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2578\n",
      "Epoch 00013: val_loss improved from 0.12726 to 0.12553, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2571 - val_loss: 0.1255\n",
      "Epoch 14/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2541\n",
      "Epoch 00014: val_loss improved from 0.12553 to 0.12373, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2537 - val_loss: 0.1237\n",
      "Epoch 15/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2464\n",
      "Epoch 00015: val_loss improved from 0.12373 to 0.12189, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2465 - val_loss: 0.1219\n",
      "Epoch 16/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2440\n",
      "Epoch 00016: val_loss improved from 0.12189 to 0.12064, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2441 - val_loss: 0.1206\n",
      "Epoch 17/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2398\n",
      "Epoch 00017: val_loss improved from 0.12064 to 0.11900, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2397 - val_loss: 0.1190\n",
      "Epoch 18/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2346\n",
      "Epoch 00018: val_loss improved from 0.11900 to 0.11769, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2342 - val_loss: 0.1177\n",
      "Epoch 19/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2325\n",
      "Epoch 00019: val_loss improved from 0.11769 to 0.11640, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2330 - val_loss: 0.1164\n",
      "Epoch 20/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2291\n",
      "Epoch 00020: val_loss did not improve from 0.11640\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2295 - val_loss: 0.1164\n",
      "Epoch 21/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2324\n",
      "Epoch 00021: val_loss improved from 0.11640 to 0.11416, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2315 - val_loss: 0.1142\n",
      "Epoch 22/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2271\n",
      "Epoch 00022: val_loss improved from 0.11416 to 0.11360, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2277 - val_loss: 0.1136\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2223\n",
      "Epoch 00023: val_loss improved from 0.11360 to 0.11227, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2223 - val_loss: 0.1123\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2171\n",
      "Epoch 00024: val_loss improved from 0.11227 to 0.11174, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2198 - val_loss: 0.1117\n",
      "Epoch 25/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2171\n",
      "Epoch 00025: val_loss improved from 0.11174 to 0.11046, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2165 - val_loss: 0.1105\n",
      "Epoch 26/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2122\n",
      "Epoch 00026: val_loss improved from 0.11046 to 0.10975, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2120 - val_loss: 0.1097\n",
      "Epoch 27/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2183\n",
      "Epoch 00027: val_loss improved from 0.10975 to 0.10882, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2167 - val_loss: 0.1088\n",
      "Epoch 28/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2139\n",
      "Epoch 00028: val_loss improved from 0.10882 to 0.10866, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2135 - val_loss: 0.1087\n",
      "Epoch 29/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2106\n",
      "Epoch 00029: val_loss improved from 0.10866 to 0.10815, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2110 - val_loss: 0.1081\n",
      "Epoch 30/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2083\n",
      "Epoch 00030: val_loss improved from 0.10815 to 0.10711, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2073 - val_loss: 0.1071\n",
      "Epoch 31/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2100\n",
      "Epoch 00031: val_loss did not improve from 0.10711\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2099 - val_loss: 0.1071\n",
      "Epoch 32/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2047\n",
      "Epoch 00032: val_loss improved from 0.10711 to 0.10620, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2045 - val_loss: 0.1062\n",
      "Epoch 33/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2046\n",
      "Epoch 00033: val_loss did not improve from 0.10620\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2054 - val_loss: 0.1062\n",
      "Epoch 34/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2013\n",
      "Epoch 00034: val_loss improved from 0.10620 to 0.10532, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2015 - val_loss: 0.1053\n",
      "Epoch 35/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2006\n",
      "Epoch 00035: val_loss did not improve from 0.10532\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2025 - val_loss: 0.1054\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1977\n",
      "Epoch 00036: val_loss improved from 0.10532 to 0.10465, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1977 - val_loss: 0.1046\n",
      "Epoch 37/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1968\n",
      "Epoch 00037: val_loss improved from 0.10465 to 0.10424, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1982 - val_loss: 0.1042\n",
      "Epoch 38/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1979\n",
      "Epoch 00038: val_loss improved from 0.10424 to 0.10368, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1986 - val_loss: 0.1037\n",
      "Epoch 39/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1911\n",
      "Epoch 00039: val_loss improved from 0.10368 to 0.10344, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1944 - val_loss: 0.1034\n",
      "Epoch 40/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1888\n",
      "Epoch 00040: val_loss improved from 0.10344 to 0.10334, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1918 - val_loss: 0.1033\n",
      "Epoch 41/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1939\n",
      "Epoch 00041: val_loss did not improve from 0.10334\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1920 - val_loss: 0.1044\n",
      "Epoch 42/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1945\n",
      "Epoch 00042: val_loss did not improve from 0.10334\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1928 - val_loss: 0.1034\n",
      " ###8 fold : val acc1 0.587, acc3 0.979, mae 0.217###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 20.5699 \n",
      "Epoch 00001: val_loss improved from inf to 11.62028, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 18.1491 - val_loss: 11.6203\n",
      "Epoch 2/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 8.6744 \n",
      "Epoch 00002: val_loss improved from 11.62028 to 3.60577, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 7.2454 - val_loss: 3.6058\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.1294\n",
      "Epoch 00003: val_loss improved from 3.60577 to 1.73604, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.7365 - val_loss: 1.7360\n",
      "Epoch 4/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.6253\n",
      "Epoch 00004: val_loss improved from 1.73604 to 0.86253, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4075 - val_loss: 0.8625\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8786\n",
      "Epoch 00005: val_loss improved from 0.86253 to 0.45089, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7852 - val_loss: 0.4509\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5393\n",
      "Epoch 00006: val_loss improved from 0.45089 to 0.26344, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4911 - val_loss: 0.2634\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3769\n",
      "Epoch 00007: val_loss improved from 0.26344 to 0.18527, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3571 - val_loss: 0.1853\n",
      "Epoch 8/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3186\n",
      "Epoch 00008: val_loss improved from 0.18527 to 0.15400, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3105 - val_loss: 0.1540\n",
      "Epoch 9/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2909\n",
      "Epoch 00009: val_loss improved from 0.15400 to 0.14096, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2883 - val_loss: 0.1410\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2807\n",
      "Epoch 00010: val_loss improved from 0.14096 to 0.13492, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2750 - val_loss: 0.1349\n",
      "Epoch 11/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2704\n",
      "Epoch 00011: val_loss improved from 0.13492 to 0.13213, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2675 - val_loss: 0.1321\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2606\n",
      "Epoch 00012: val_loss improved from 0.13213 to 0.12990, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2609 - val_loss: 0.1299\n",
      "Epoch 13/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2569\n",
      "Epoch 00013: val_loss improved from 0.12990 to 0.12769, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2571 - val_loss: 0.1277\n",
      "Epoch 14/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2523\n",
      "Epoch 00014: val_loss improved from 0.12769 to 0.12598, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2537 - val_loss: 0.1260\n",
      "Epoch 15/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2462\n",
      "Epoch 00015: val_loss improved from 0.12598 to 0.12437, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2465 - val_loss: 0.1244\n",
      "Epoch 16/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2445\n",
      "Epoch 00016: val_loss improved from 0.12437 to 0.12323, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2441 - val_loss: 0.1232\n",
      "Epoch 17/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2392\n",
      "Epoch 00017: val_loss improved from 0.12323 to 0.12153, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2397 - val_loss: 0.1215\n",
      "Epoch 18/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2346\n",
      "Epoch 00018: val_loss improved from 0.12153 to 0.11982, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2342 - val_loss: 0.1198\n",
      "Epoch 19/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2325\n",
      "Epoch 00019: val_loss improved from 0.11982 to 0.11876, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2330 - val_loss: 0.1188\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2295\n",
      "Epoch 00020: val_loss improved from 0.11876 to 0.11874, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2295 - val_loss: 0.1187\n",
      "Epoch 21/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2321\n",
      "Epoch 00021: val_loss improved from 0.11874 to 0.11616, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2315 - val_loss: 0.1162\n",
      "Epoch 22/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2270\n",
      "Epoch 00022: val_loss improved from 0.11616 to 0.11558, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2277 - val_loss: 0.1156\n",
      "Epoch 23/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2223\n",
      "Epoch 00023: val_loss improved from 0.11558 to 0.11454, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2223 - val_loss: 0.1145\n",
      "Epoch 24/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2192\n",
      "Epoch 00024: val_loss improved from 0.11454 to 0.11421, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2198 - val_loss: 0.1142\n",
      "Epoch 25/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2169\n",
      "Epoch 00025: val_loss improved from 0.11421 to 0.11275, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2165 - val_loss: 0.1128\n",
      "Epoch 26/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.2129\n",
      "Epoch 00026: val_loss improved from 0.11275 to 0.11214, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2120 - val_loss: 0.1121\n",
      "Epoch 27/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2174\n",
      "Epoch 00027: val_loss improved from 0.11214 to 0.11116, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2167 - val_loss: 0.1112\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2135\n",
      "Epoch 00028: val_loss improved from 0.11116 to 0.11076, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2135 - val_loss: 0.1108\n",
      "Epoch 29/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2107\n",
      "Epoch 00029: val_loss improved from 0.11076 to 0.11034, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2110 - val_loss: 0.1103\n",
      "Epoch 30/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2082\n",
      "Epoch 00030: val_loss improved from 0.11034 to 0.10941, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2073 - val_loss: 0.1094\n",
      "Epoch 31/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2095\n",
      "Epoch 00031: val_loss did not improve from 0.10941\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2099 - val_loss: 0.1096\n",
      "Epoch 32/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2047\n",
      "Epoch 00032: val_loss improved from 0.10941 to 0.10845, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2045 - val_loss: 0.1085\n",
      "Epoch 33/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2046\n",
      "Epoch 00033: val_loss did not improve from 0.10845\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2054 - val_loss: 0.1087\n",
      "Epoch 34/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2010\n",
      "Epoch 00034: val_loss improved from 0.10845 to 0.10735, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2015 - val_loss: 0.1073\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2025\n",
      "Epoch 00035: val_loss did not improve from 0.10735\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2025 - val_loss: 0.1078\n",
      "Epoch 36/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1976\n",
      "Epoch 00036: val_loss improved from 0.10735 to 0.10669, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1977 - val_loss: 0.1067\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1982\n",
      "Epoch 00037: val_loss improved from 0.10669 to 0.10642, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1982 - val_loss: 0.1064\n",
      "Epoch 38/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1982\n",
      "Epoch 00038: val_loss improved from 0.10642 to 0.10598, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1986 - val_loss: 0.1060\n",
      "Epoch 39/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1939\n",
      "Epoch 00039: val_loss improved from 0.10598 to 0.10568, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1944 - val_loss: 0.1057\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1918\n",
      "Epoch 00040: val_loss improved from 0.10568 to 0.10539, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes512_dropout0.5,dnodes16_dropout0,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1918 - val_loss: 0.1054\n",
      "Epoch 41/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1923\n",
      "Epoch 00041: val_loss did not improve from 0.10539\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1920 - val_loss: 0.1062\n",
      "Epoch 42/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1937\n",
      "Epoch 00042: val_loss did not improve from 0.10539\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1928 - val_loss: 0.1058\n",
      " ###9 fold : val acc1 0.586, acc3 0.981, mae 0.216###\n",
      "acc10.591_acc30.978\n",
      "random search 80/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557/558 [============================>.] - ETA: 0s - loss: 5.0972\n",
      "Epoch 00001: val_loss improved from inf to 0.25512, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 5.0962 - val_loss: 0.2551\n",
      "Epoch 2/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 1.0434\n",
      "Epoch 00002: val_loss improved from 0.25512 to 0.15779, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 1.0426 - val_loss: 0.1578\n",
      "Epoch 3/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.8592\n",
      "Epoch 00003: val_loss improved from 0.15779 to 0.12539, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.8592 - val_loss: 0.1254\n",
      "Epoch 4/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.7449\n",
      "Epoch 00004: val_loss did not improve from 0.12539\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.7450 - val_loss: 0.1373\n",
      "Epoch 5/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.6789\n",
      "Epoch 00005: val_loss did not improve from 0.12539\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.6786 - val_loss: 0.1323\n",
      " ###0 fold : val acc1 0.535, acc3 0.961, mae 0.253###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/558 [============================>.] - ETA: 0s - loss: 5.1882\n",
      "Epoch 00001: val_loss improved from inf to 0.26207, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 4ms/step - loss: 5.1103 - val_loss: 0.2621\n",
      "Epoch 2/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 1.0431\n",
      "Epoch 00002: val_loss improved from 0.26207 to 0.15275, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 1.0431 - val_loss: 0.1528\n",
      "Epoch 3/100\n",
      "543/558 [============================>.] - ETA: 0s - loss: 0.8604\n",
      "Epoch 00003: val_loss improved from 0.15275 to 0.12091, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.8582 - val_loss: 0.1209\n",
      "Epoch 4/100\n",
      "541/558 [============================>.] - ETA: 0s - loss: 0.7535\n",
      "Epoch 00004: val_loss did not improve from 0.12091\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.7522 - val_loss: 0.1350\n",
      "Epoch 5/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.6814\n",
      "Epoch 00005: val_loss did not improve from 0.12091\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.6813 - val_loss: 0.1314\n",
      " ###1 fold : val acc1 0.544, acc3 0.969, mae 0.244###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549/558 [============================>.] - ETA: 0s - loss: 5.1666\n",
      "Epoch 00001: val_loss improved from inf to 0.26167, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 5.1095 - val_loss: 0.2617\n",
      "Epoch 2/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 1.0358\n",
      "Epoch 00002: val_loss improved from 0.26167 to 0.14970, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 1.0330 - val_loss: 0.1497\n",
      "Epoch 3/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.8497\n",
      "Epoch 00003: val_loss improved from 0.14970 to 0.11951, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.8489 - val_loss: 0.1195\n",
      "Epoch 4/100\n",
      "543/558 [============================>.] - ETA: 0s - loss: 0.7525\n",
      "Epoch 00004: val_loss did not improve from 0.11951\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.7517 - val_loss: 0.1337\n",
      "Epoch 5/100\n",
      "543/558 [============================>.] - ETA: 0s - loss: 0.6767\n",
      "Epoch 00005: val_loss did not improve from 0.11951\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.6755 - val_loss: 0.1290\n",
      " ###2 fold : val acc1 0.564, acc3 0.967, mae 0.236###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/558 [============================>.] - ETA: 0s - loss: 5.1924\n",
      "Epoch 00001: val_loss improved from inf to 0.25297, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 5.1128 - val_loss: 0.2530\n",
      "Epoch 2/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 1.0424\n",
      "Epoch 00002: val_loss improved from 0.25297 to 0.14845, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 1.0398 - val_loss: 0.1485\n",
      "Epoch 3/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.8516\n",
      "Epoch 00003: val_loss improved from 0.14845 to 0.12187, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.8513 - val_loss: 0.1219\n",
      "Epoch 4/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.7553\n",
      "Epoch 00004: val_loss did not improve from 0.12187\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.7554 - val_loss: 0.1341\n",
      "Epoch 5/100\n",
      "543/558 [============================>.] - ETA: 0s - loss: 0.6782\n",
      "Epoch 00005: val_loss did not improve from 0.12187\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.6759 - val_loss: 0.1289\n",
      " ###3 fold : val acc1 0.570, acc3 0.956, mae 0.238###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543/558 [============================>.] - ETA: 0s - loss: 5.2191\n",
      "Epoch 00001: val_loss improved from inf to 0.24472, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 5.1157 - val_loss: 0.2447\n",
      "Epoch 2/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 1.0303\n",
      "Epoch 00002: val_loss improved from 0.24472 to 0.14097, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 1.0282 - val_loss: 0.1410\n",
      "Epoch 3/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.8492\n",
      "Epoch 00003: val_loss improved from 0.14097 to 0.12782, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.8498 - val_loss: 0.1278\n",
      "Epoch 4/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.7313\n",
      "Epoch 00004: val_loss improved from 0.12782 to 0.12655, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.7318 - val_loss: 0.1265\n",
      "Epoch 5/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.6635\n",
      "Epoch 00005: val_loss improved from 0.12655 to 0.12481, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.6638 - val_loss: 0.1248\n",
      "Epoch 6/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.6026\n",
      "Epoch 00006: val_loss did not improve from 0.12481\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.5997 - val_loss: 0.1266\n",
      "Epoch 7/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.5574\n",
      "Epoch 00007: val_loss improved from 0.12481 to 0.12068, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.5573 - val_loss: 0.1207\n",
      "Epoch 8/100\n",
      "541/558 [============================>.] - ETA: 0s - loss: 0.5060\n",
      "Epoch 00008: val_loss improved from 0.12068 to 0.10755, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.5061 - val_loss: 0.1075\n",
      "Epoch 9/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.4508\n",
      "Epoch 00009: val_loss improved from 0.10755 to 0.10309, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.4503 - val_loss: 0.1031\n",
      "Epoch 10/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.4042\n",
      "Epoch 00010: val_loss did not improve from 0.10309\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.4042 - val_loss: 0.1041\n",
      "Epoch 11/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.3540\n",
      "Epoch 00011: val_loss did not improve from 0.10309\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.3546 - val_loss: 0.1090\n",
      " ###4 fold : val acc1 0.590, acc3 0.972, mae 0.219###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558/558 [==============================] - ETA: 0s - loss: 5.0966\n",
      "Epoch 00001: val_loss improved from inf to 0.24521, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 5.0966 - val_loss: 0.2452\n",
      "Epoch 2/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 1.0396\n",
      "Epoch 00002: val_loss improved from 0.24521 to 0.14428, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 1.0355 - val_loss: 0.1443\n",
      "Epoch 3/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.8488\n",
      "Epoch 00003: val_loss improved from 0.14428 to 0.12890, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.8503 - val_loss: 0.1289\n",
      "Epoch 4/100\n",
      "554/558 [============================>.] - ETA: 0s - loss: 0.7391\n",
      "Epoch 00004: val_loss did not improve from 0.12890\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.7395 - val_loss: 0.1305\n",
      "Epoch 5/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.6720\n",
      "Epoch 00005: val_loss improved from 0.12890 to 0.12186, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.6712 - val_loss: 0.1219\n",
      "Epoch 6/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.5991\n",
      "Epoch 00006: val_loss did not improve from 0.12186\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.5990 - val_loss: 0.1222\n",
      "Epoch 7/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.5573\n",
      "Epoch 00007: val_loss improved from 0.12186 to 0.11761, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.5571 - val_loss: 0.1176\n",
      "Epoch 8/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.5042\n",
      "Epoch 00008: val_loss improved from 0.11761 to 0.10655, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.5039 - val_loss: 0.1066\n",
      "Epoch 9/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.4483\n",
      "Epoch 00009: val_loss improved from 0.10655 to 0.10308, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.4472 - val_loss: 0.1031\n",
      "Epoch 10/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.3999\n",
      "Epoch 00010: val_loss did not improve from 0.10308\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.3998 - val_loss: 0.1073\n",
      "Epoch 11/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.3514\n",
      "Epoch 00011: val_loss did not improve from 0.10308\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.3513 - val_loss: 0.1137\n",
      " ###5 fold : val acc1 0.583, acc3 0.975, mae 0.222###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544/558 [============================>.] - ETA: 0s - loss: 5.2006\n",
      "Epoch 00001: val_loss improved from inf to 0.24694, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 5.1052 - val_loss: 0.2469\n",
      "Epoch 2/100\n",
      "543/558 [============================>.] - ETA: 0s - loss: 1.0458\n",
      "Epoch 00002: val_loss improved from 0.24694 to 0.13872, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 1.0418 - val_loss: 0.1387\n",
      "Epoch 3/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.8478\n",
      "Epoch 00003: val_loss improved from 0.13872 to 0.12701, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes64_dropout0.5,lr0.001/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.8483 - val_loss: 0.1270\n",
      "Epoch 4/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.7386\n",
      "Epoch 00004: val_loss did not improve from 0.12701\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.7390 - val_loss: 0.1384\n",
      "Epoch 5/100\n",
      "488/558 [=========================>....] - ETA: 0s - loss: 0.6725"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/279 [============================>.] - ETA: 0s - loss: 0.1754\n",
      "Epoch 00008: val_loss improved from 0.10667 to 0.10410, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1750 - val_loss: 0.1041\n",
      "Epoch 9/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1719\n",
      "Epoch 00009: val_loss improved from 0.10410 to 0.10286, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1712 - val_loss: 0.1029\n",
      "Epoch 10/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1692\n",
      "Epoch 00010: val_loss improved from 0.10286 to 0.10059, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1692 - val_loss: 0.1006\n",
      "Epoch 11/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1615\n",
      "Epoch 00011: val_loss improved from 0.10059 to 0.09958, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1617 - val_loss: 0.0996\n",
      "Epoch 12/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1621\n",
      "Epoch 00012: val_loss improved from 0.09958 to 0.09850, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1621 - val_loss: 0.0985\n",
      "Epoch 13/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1561\n",
      "Epoch 00013: val_loss improved from 0.09850 to 0.09764, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1559 - val_loss: 0.0976\n",
      "Epoch 14/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1529\n",
      "Epoch 00014: val_loss improved from 0.09764 to 0.09752, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1531 - val_loss: 0.0975\n",
      "Epoch 15/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1526\n",
      "Epoch 00015: val_loss improved from 0.09752 to 0.09598, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1525 - val_loss: 0.0960\n",
      "Epoch 16/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1487\n",
      "Epoch 00016: val_loss improved from 0.09598 to 0.09566, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1487 - val_loss: 0.0957\n",
      "Epoch 17/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1484\n",
      "Epoch 00017: val_loss did not improve from 0.09566\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1480 - val_loss: 0.0966\n",
      "Epoch 18/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1468\n",
      "Epoch 00018: val_loss improved from 0.09566 to 0.09556, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1466 - val_loss: 0.0956\n",
      "Epoch 19/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1445\n",
      "Epoch 00019: val_loss improved from 0.09556 to 0.09470, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1444 - val_loss: 0.0947\n",
      "Epoch 20/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1444\n",
      "Epoch 00020: val_loss did not improve from 0.09470\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1443 - val_loss: 0.0964\n",
      "Epoch 21/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1424\n",
      "Epoch 00021: val_loss did not improve from 0.09470\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1422 - val_loss: 0.0952\n",
      " ###7 fold : val acc1 0.585, acc3 0.974, mae 0.220###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/279 [============================>.] - ETA: 0s - loss: 10.0116\n",
      "Epoch 00001: val_loss improved from inf to 2.34292, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 9.8611 - val_loss: 2.3429\n",
      "Epoch 2/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 1.2254\n",
      "Epoch 00002: val_loss improved from 2.34292 to 0.44204, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 1.2238 - val_loss: 0.4420\n",
      "Epoch 3/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.3182\n",
      "Epoch 00003: val_loss improved from 0.44204 to 0.14941, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.3163 - val_loss: 0.1494\n",
      "Epoch 4/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2065\n",
      "Epoch 00004: val_loss improved from 0.14941 to 0.12220, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.2064 - val_loss: 0.1222\n",
      "Epoch 5/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1909\n",
      "Epoch 00005: val_loss improved from 0.12220 to 0.11639, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1910 - val_loss: 0.1164\n",
      "Epoch 6/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1841\n",
      "Epoch 00006: val_loss improved from 0.11639 to 0.11300, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1838 - val_loss: 0.1130\n",
      "Epoch 7/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1795\n",
      "Epoch 00007: val_loss improved from 0.11300 to 0.11027, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1795 - val_loss: 0.1103\n",
      "Epoch 8/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1747\n",
      "Epoch 00008: val_loss improved from 0.11027 to 0.10766, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1747 - val_loss: 0.1077\n",
      "Epoch 9/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1715\n",
      "Epoch 00009: val_loss improved from 0.10766 to 0.10623, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1713 - val_loss: 0.1062\n",
      "Epoch 10/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.1692\n",
      "Epoch 00010: val_loss improved from 0.10623 to 0.10407, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1688 - val_loss: 0.1041\n",
      "Epoch 11/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.1616\n",
      "Epoch 00011: val_loss improved from 0.10407 to 0.10302, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1618 - val_loss: 0.1030\n",
      "Epoch 12/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1616\n",
      "Epoch 00012: val_loss improved from 0.10302 to 0.10181, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1615 - val_loss: 0.1018\n",
      "Epoch 13/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.1565\n",
      "Epoch 00013: val_loss improved from 0.10181 to 0.10059, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1558 - val_loss: 0.1006\n",
      "Epoch 14/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1536\n",
      "Epoch 00014: val_loss improved from 0.10059 to 0.10041, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1538 - val_loss: 0.1004\n",
      "Epoch 15/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1528\n",
      "Epoch 00015: val_loss improved from 0.10041 to 0.09911, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1529 - val_loss: 0.0991\n",
      "Epoch 16/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1494\n",
      "Epoch 00016: val_loss improved from 0.09911 to 0.09878, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1492 - val_loss: 0.0988\n",
      "Epoch 17/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.1484\n",
      "Epoch 00017: val_loss did not improve from 0.09878\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1481 - val_loss: 0.0998\n",
      "Epoch 18/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1468\n",
      "Epoch 00018: val_loss improved from 0.09878 to 0.09857, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1468 - val_loss: 0.0986\n",
      "Epoch 19/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.1437\n",
      "Epoch 00019: val_loss improved from 0.09857 to 0.09802, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1442 - val_loss: 0.0980\n",
      "Epoch 20/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1440\n",
      "Epoch 00020: val_loss did not improve from 0.09802\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1441 - val_loss: 0.0991\n",
      "Epoch 21/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1426\n",
      "Epoch 00021: val_loss did not improve from 0.09802\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1424 - val_loss: 0.0981\n",
      " ###8 fold : val acc1 0.590, acc3 0.982, mae 0.214###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271/279 [============================>.] - ETA: 0s - loss: 10.0677\n",
      "Epoch 00001: val_loss improved from inf to 2.34801, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 9.8611 - val_loss: 2.3480\n",
      "Epoch 2/100\n",
      "262/279 [===========================>..] - ETA: 0s - loss: 1.2678\n",
      "Epoch 00002: val_loss improved from 2.34801 to 0.44548, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 1.2238 - val_loss: 0.4455\n",
      "Epoch 3/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.3192\n",
      "Epoch 00003: val_loss improved from 0.44548 to 0.15238, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.3163 - val_loss: 0.1524\n",
      "Epoch 4/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.2070\n",
      "Epoch 00004: val_loss improved from 0.15238 to 0.12430, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2064 - val_loss: 0.1243\n",
      "Epoch 5/100\n",
      "263/279 [===========================>..] - ETA: 0s - loss: 0.1913\n",
      "Epoch 00005: val_loss improved from 0.12430 to 0.11864, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1910 - val_loss: 0.1186\n",
      "Epoch 6/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1841\n",
      "Epoch 00006: val_loss improved from 0.11864 to 0.11554, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1838 - val_loss: 0.1155\n",
      "Epoch 7/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1795\n",
      "Epoch 00007: val_loss improved from 0.11554 to 0.11299, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1795 - val_loss: 0.1130\n",
      "Epoch 8/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1750\n",
      "Epoch 00008: val_loss improved from 0.11299 to 0.11047, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1747 - val_loss: 0.1105\n",
      "Epoch 9/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.1720\n",
      "Epoch 00009: val_loss improved from 0.11047 to 0.10917, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1713 - val_loss: 0.1092\n",
      "Epoch 10/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1690\n",
      "Epoch 00010: val_loss improved from 0.10917 to 0.10658, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1688 - val_loss: 0.1066\n",
      "Epoch 11/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.1614\n",
      "Epoch 00011: val_loss improved from 0.10658 to 0.10520, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1618 - val_loss: 0.1052\n",
      "Epoch 12/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.1617\n",
      "Epoch 00012: val_loss improved from 0.10520 to 0.10456, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1615 - val_loss: 0.1046\n",
      "Epoch 13/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1559\n",
      "Epoch 00013: val_loss improved from 0.10456 to 0.10338, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1558 - val_loss: 0.1034\n",
      "Epoch 14/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1536\n",
      "Epoch 00014: val_loss improved from 0.10338 to 0.10292, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1538 - val_loss: 0.1029\n",
      "Epoch 15/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.1531\n",
      "Epoch 00015: val_loss improved from 0.10292 to 0.10136, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1529 - val_loss: 0.1014\n",
      "Epoch 16/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1491\n",
      "Epoch 00016: val_loss improved from 0.10136 to 0.10105, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1492 - val_loss: 0.1011\n",
      "Epoch 17/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1485\n",
      "Epoch 00017: val_loss did not improve from 0.10105\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1481 - val_loss: 0.1017\n",
      "Epoch 18/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.1470\n",
      "Epoch 00018: val_loss improved from 0.10105 to 0.10083, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1468 - val_loss: 0.1008\n",
      "Epoch 19/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1437\n",
      "Epoch 00019: val_loss improved from 0.10083 to 0.09975, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes128_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1442 - val_loss: 0.0998\n",
      "Epoch 20/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.1446\n",
      "Epoch 00020: val_loss did not improve from 0.09975\n",
      "279/279 [==============================] - 1s 3ms/step - loss: 0.1441 - val_loss: 0.1006\n",
      "Epoch 21/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1428\n",
      "Epoch 00021: val_loss did not improve from 0.09975\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1424 - val_loss: 0.1006\n",
      " ###9 fold : val acc1 0.593, acc3 0.984, mae 0.211###\n",
      "acc10.598_acc30.981\n",
      "random search 84/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/140 [============================>.] - ETA: 0s - loss: 3.4529\n",
      "Epoch 00001: val_loss improved from inf to 0.17576, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 3.4464 - val_loss: 0.1758\n",
      "Epoch 2/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.4331\n",
      "Epoch 00002: val_loss improved from 0.17576 to 0.13703, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4327 - val_loss: 0.1370\n",
      "Epoch 3/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3828\n",
      "Epoch 00003: val_loss improved from 0.13703 to 0.11755, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3804 - val_loss: 0.1176\n",
      "Epoch 4/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3480\n",
      "Epoch 00004: val_loss did not improve from 0.11755\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3474 - val_loss: 0.1232\n",
      "Epoch 5/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3114\n",
      "Epoch 00005: val_loss improved from 0.11755 to 0.11655, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3111 - val_loss: 0.1165\n",
      "Epoch 6/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.2973\n",
      "Epoch 00006: val_loss improved from 0.11655 to 0.10459, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2968 - val_loss: 0.1046\n",
      "Epoch 7/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.2811\n",
      "Epoch 00007: val_loss did not improve from 0.10459\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2802 - val_loss: 0.1111\n",
      "Epoch 8/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2630\n",
      "Epoch 00008: val_loss improved from 0.10459 to 0.10124, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2632 - val_loss: 0.1012\n",
      "Epoch 9/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2539\n",
      "Epoch 00009: val_loss improved from 0.10124 to 0.10098, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2547 - val_loss: 0.1010\n",
      "Epoch 10/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.2479\n",
      "Epoch 00010: val_loss did not improve from 0.10098\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2485 - val_loss: 0.1060\n",
      "Epoch 11/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2308\n",
      "Epoch 00011: val_loss did not improve from 0.10098\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2310 - val_loss: 0.1011\n",
      " ###0 fold : val acc1 0.607, acc3 0.974, mae 0.210###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - ETA: 0s - loss: 3.4508\n",
      "Epoch 00001: val_loss improved from inf to 0.17435, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.4508 - val_loss: 0.1743\n",
      "Epoch 2/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.4289\n",
      "Epoch 00002: val_loss improved from 0.17435 to 0.13505, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4290 - val_loss: 0.1350\n",
      "Epoch 3/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3799\n",
      "Epoch 00003: val_loss improved from 0.13505 to 0.11738, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3778 - val_loss: 0.1174\n",
      "Epoch 4/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3465\n",
      "Epoch 00004: val_loss improved from 0.11738 to 0.11676, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3458 - val_loss: 0.1168\n",
      "Epoch 5/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3092\n",
      "Epoch 00005: val_loss improved from 0.11676 to 0.11624, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3094 - val_loss: 0.1162\n",
      "Epoch 6/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2967\n",
      "Epoch 00006: val_loss improved from 0.11624 to 0.10396, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2961 - val_loss: 0.1040\n",
      "Epoch 7/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2793\n",
      "Epoch 00007: val_loss did not improve from 0.10396\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2788 - val_loss: 0.1103\n",
      "Epoch 8/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2629\n",
      "Epoch 00008: val_loss improved from 0.10396 to 0.10340, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2623 - val_loss: 0.1034\n",
      "Epoch 9/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.2540\n",
      "Epoch 00009: val_loss improved from 0.10340 to 0.10080, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2539 - val_loss: 0.1008\n",
      "Epoch 10/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.2463\n",
      "Epoch 00010: val_loss did not improve from 0.10080\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2464 - val_loss: 0.1075\n",
      "Epoch 11/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2306\n",
      "Epoch 00011: val_loss improved from 0.10080 to 0.09924, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2303 - val_loss: 0.0992\n",
      "Epoch 12/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2227\n",
      "Epoch 00012: val_loss did not improve from 0.09924\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2223 - val_loss: 0.1025\n",
      "Epoch 13/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2155\n",
      "Epoch 00013: val_loss did not improve from 0.09924\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2157 - val_loss: 0.0998\n",
      " ###1 fold : val acc1 0.589, acc3 0.978, mae 0.217###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/140 [==========================>...] - ETA: 0s - loss: 3.7175\n",
      "Epoch 00001: val_loss improved from inf to 0.17363, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.4542 - val_loss: 0.1736\n",
      "Epoch 2/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.4297\n",
      "Epoch 00002: val_loss improved from 0.17363 to 0.13483, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4297 - val_loss: 0.1348\n",
      "Epoch 3/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.3806\n",
      "Epoch 00003: val_loss improved from 0.13483 to 0.11855, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3804 - val_loss: 0.1185\n",
      "Epoch 4/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.3480\n",
      "Epoch 00004: val_loss improved from 0.11855 to 0.11810, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3477 - val_loss: 0.1181\n",
      "Epoch 5/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.3093\n",
      "Epoch 00005: val_loss did not improve from 0.11810\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3098 - val_loss: 0.1205\n",
      "Epoch 6/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2966\n",
      "Epoch 00006: val_loss improved from 0.11810 to 0.10389, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2965 - val_loss: 0.1039\n",
      "Epoch 7/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2805\n",
      "Epoch 00007: val_loss did not improve from 0.10389\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2793 - val_loss: 0.1076\n",
      "Epoch 8/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2634\n",
      "Epoch 00008: val_loss did not improve from 0.10389\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2630 - val_loss: 0.1041\n",
      " ###2 fold : val acc1 0.572, acc3 0.974, mae 0.227###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - ETA: 0s - loss: 3.4511\n",
      "Epoch 00001: val_loss improved from inf to 0.17060, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.4511 - val_loss: 0.1706\n",
      "Epoch 2/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.4319\n",
      "Epoch 00002: val_loss improved from 0.17060 to 0.13537, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4305 - val_loss: 0.1354\n",
      "Epoch 3/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3802\n",
      "Epoch 00003: val_loss improved from 0.13537 to 0.11658, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3782 - val_loss: 0.1166\n",
      "Epoch 4/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3448\n",
      "Epoch 00004: val_loss did not improve from 0.11658\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3444 - val_loss: 0.1179\n",
      "Epoch 5/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3102\n",
      "Epoch 00005: val_loss did not improve from 0.11658\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3100 - val_loss: 0.1171\n",
      " ###3 fold : val acc1 0.583, acc3 0.958, mae 0.230###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/140 [============================>.] - ETA: 0s - loss: 3.5074\n",
      "Epoch 00001: val_loss improved from inf to 0.17679, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.4571 - val_loss: 0.1768\n",
      "Epoch 2/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.4400\n",
      "Epoch 00002: val_loss improved from 0.17679 to 0.13519, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.4383 - val_loss: 0.1352\n",
      "Epoch 3/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3829\n",
      "Epoch 00003: val_loss improved from 0.13519 to 0.11990, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3807 - val_loss: 0.1199\n",
      "Epoch 4/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3425\n",
      "Epoch 00004: val_loss improved from 0.11990 to 0.11413, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3423 - val_loss: 0.1141\n",
      "Epoch 5/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.3145\n",
      "Epoch 00005: val_loss improved from 0.11413 to 0.11075, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3134 - val_loss: 0.1107\n",
      "Epoch 6/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2952\n",
      "Epoch 00006: val_loss did not improve from 0.11075\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2954 - val_loss: 0.1112\n",
      "Epoch 7/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2834\n",
      "Epoch 00007: val_loss improved from 0.11075 to 0.10586, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2822 - val_loss: 0.1059\n",
      "Epoch 8/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2643\n",
      "Epoch 00008: val_loss improved from 0.10586 to 0.10300, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2633 - val_loss: 0.1030\n",
      "Epoch 9/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2479\n",
      "Epoch 00009: val_loss improved from 0.10300 to 0.09909, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2469 - val_loss: 0.0991\n",
      "Epoch 10/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2368\n",
      "Epoch 00010: val_loss did not improve from 0.09909\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2376 - val_loss: 0.1055\n",
      "Epoch 11/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2322\n",
      "Epoch 00011: val_loss did not improve from 0.09909\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2315 - val_loss: 0.1071\n",
      " ###4 fold : val acc1 0.585, acc3 0.977, mae 0.219###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - ETA: 0s - loss: 3.4721\n",
      "Epoch 00001: val_loss improved from inf to 0.17453, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.4721 - val_loss: 0.1745\n",
      "Epoch 2/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.4401\n",
      "Epoch 00002: val_loss improved from 0.17453 to 0.13665, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4387 - val_loss: 0.1367\n",
      "Epoch 3/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3813\n",
      "Epoch 00003: val_loss improved from 0.13665 to 0.12127, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3770 - val_loss: 0.1213\n",
      "Epoch 4/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3418\n",
      "Epoch 00004: val_loss improved from 0.12127 to 0.11544, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3414 - val_loss: 0.1154\n",
      "Epoch 5/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.3160\n",
      "Epoch 00005: val_loss improved from 0.11544 to 0.11297, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3154 - val_loss: 0.1130\n",
      "Epoch 6/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2955\n",
      "Epoch 00006: val_loss improved from 0.11297 to 0.10997, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2961 - val_loss: 0.1100\n",
      "Epoch 7/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2805\n",
      "Epoch 00007: val_loss improved from 0.10997 to 0.10568, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2805 - val_loss: 0.1057\n",
      "Epoch 8/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2674\n",
      "Epoch 00008: val_loss improved from 0.10568 to 0.10367, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2659 - val_loss: 0.1037\n",
      "Epoch 9/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.2477\n",
      "Epoch 00009: val_loss improved from 0.10367 to 0.09892, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2478 - val_loss: 0.0989\n",
      "Epoch 10/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2386\n",
      "Epoch 00010: val_loss did not improve from 0.09892\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2393 - val_loss: 0.1043\n",
      "Epoch 11/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2318\n",
      "Epoch 00011: val_loss did not improve from 0.09892\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2312 - val_loss: 0.1041\n",
      " ###5 fold : val acc1 0.575, acc3 0.979, mae 0.224###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/140 [============================>.] - ETA: 0s - loss: 3.4751\n",
      "Epoch 00001: val_loss improved from inf to 0.17321, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.4686 - val_loss: 0.1732\n",
      "Epoch 2/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.4432\n",
      "Epoch 00002: val_loss improved from 0.17321 to 0.13339, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4398 - val_loss: 0.1334\n",
      "Epoch 3/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.3806\n",
      "Epoch 00003: val_loss improved from 0.13339 to 0.11926, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3779 - val_loss: 0.1193\n",
      "Epoch 4/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.3458\n",
      "Epoch 00004: val_loss improved from 0.11926 to 0.11463, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3457 - val_loss: 0.1146\n",
      "Epoch 5/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3194\n",
      "Epoch 00005: val_loss improved from 0.11463 to 0.11328, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3184 - val_loss: 0.1133\n",
      "Epoch 6/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2965\n",
      "Epoch 00006: val_loss improved from 0.11328 to 0.10752, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2970 - val_loss: 0.1075\n",
      "Epoch 7/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2836\n",
      "Epoch 00007: val_loss improved from 0.10752 to 0.10675, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2815 - val_loss: 0.1067\n",
      "Epoch 8/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2668\n",
      "Epoch 00008: val_loss improved from 0.10675 to 0.10106, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2656 - val_loss: 0.1011\n",
      "Epoch 9/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.2479\n",
      "Epoch 00009: val_loss improved from 0.10106 to 0.09924, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2481 - val_loss: 0.0992\n",
      "Epoch 10/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2401\n",
      "Epoch 00010: val_loss did not improve from 0.09924\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2407 - val_loss: 0.1035\n",
      "Epoch 11/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2304\n",
      "Epoch 00011: val_loss did not improve from 0.09924\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2297 - val_loss: 0.1013\n",
      " ###6 fold : val acc1 0.588, acc3 0.981, mae 0.215###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/140 [============================>.] - ETA: 0s - loss: 3.4682\n",
      "Epoch 00001: val_loss improved from inf to 0.17376, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.4617 - val_loss: 0.1738\n",
      "Epoch 2/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.4397\n",
      "Epoch 00002: val_loss improved from 0.17376 to 0.13360, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.4368 - val_loss: 0.1336\n",
      "Epoch 3/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.3819\n",
      "Epoch 00003: val_loss improved from 0.13360 to 0.11832, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3790 - val_loss: 0.1183\n",
      "Epoch 4/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3405\n",
      "Epoch 00004: val_loss did not improve from 0.11832\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3409 - val_loss: 0.1251\n",
      "Epoch 5/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.3151\n",
      "Epoch 00005: val_loss improved from 0.11832 to 0.11633, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3156 - val_loss: 0.1163\n",
      "Epoch 6/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2946\n",
      "Epoch 00006: val_loss improved from 0.11633 to 0.11253, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2955 - val_loss: 0.1125\n",
      "Epoch 7/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2822\n",
      "Epoch 00007: val_loss improved from 0.11253 to 0.10479, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2799 - val_loss: 0.1048\n",
      "Epoch 8/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.2625\n",
      "Epoch 00008: val_loss improved from 0.10479 to 0.10375, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2625 - val_loss: 0.1038\n",
      "Epoch 9/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2482\n",
      "Epoch 00009: val_loss improved from 0.10375 to 0.10232, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2485 - val_loss: 0.1023\n",
      "Epoch 10/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.2430\n",
      "Epoch 00010: val_loss did not improve from 0.10232\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2428 - val_loss: 0.1032\n",
      "Epoch 11/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2299\n",
      "Epoch 00011: val_loss did not improve from 0.10232\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2295 - val_loss: 0.1028\n",
      " ###7 fold : val acc1 0.571, acc3 0.968, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/140 [============================>.] - ETA: 0s - loss: 3.5257\n",
      "Epoch 00001: val_loss improved from inf to 0.17960, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.4746 - val_loss: 0.1796\n",
      "Epoch 2/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.4413\n",
      "Epoch 00002: val_loss improved from 0.17960 to 0.13858, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4382 - val_loss: 0.1386\n",
      "Epoch 3/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3831\n",
      "Epoch 00003: val_loss improved from 0.13858 to 0.12315, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3793 - val_loss: 0.1232\n",
      "Epoch 4/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3412\n",
      "Epoch 00004: val_loss did not improve from 0.12315\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3409 - val_loss: 0.1279\n",
      "Epoch 5/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3179\n",
      "Epoch 00005: val_loss improved from 0.12315 to 0.12045, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3165 - val_loss: 0.1205\n",
      "Epoch 6/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2955\n",
      "Epoch 00006: val_loss improved from 0.12045 to 0.11597, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2953 - val_loss: 0.1160\n",
      "Epoch 7/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2819\n",
      "Epoch 00007: val_loss improved from 0.11597 to 0.10817, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2796 - val_loss: 0.1082\n",
      "Epoch 8/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2638\n",
      "Epoch 00008: val_loss did not improve from 0.10817\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2633 - val_loss: 0.1094\n",
      "Epoch 9/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2481\n",
      "Epoch 00009: val_loss improved from 0.10817 to 0.10455, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2482 - val_loss: 0.1046\n",
      "Epoch 10/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2424\n",
      "Epoch 00010: val_loss did not improve from 0.10455\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2428 - val_loss: 0.1070\n",
      "Epoch 11/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2306\n",
      "Epoch 00011: val_loss did not improve from 0.10455\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2301 - val_loss: 0.1057\n",
      " ###8 fold : val acc1 0.586, acc3 0.976, mae 0.219###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/140 [==========================>...] - ETA: 0s - loss: 3.7668\n",
      "Epoch 00001: val_loss improved from inf to 0.18417, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.4746 - val_loss: 0.1842\n",
      "Epoch 2/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.4413\n",
      "Epoch 00002: val_loss improved from 0.18417 to 0.14215, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4382 - val_loss: 0.1422\n",
      "Epoch 3/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3823\n",
      "Epoch 00003: val_loss improved from 0.14215 to 0.12689, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3793 - val_loss: 0.1269\n",
      "Epoch 4/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3412\n",
      "Epoch 00004: val_loss did not improve from 0.12689\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3409 - val_loss: 0.1317\n",
      "Epoch 5/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3181\n",
      "Epoch 00005: val_loss improved from 0.12689 to 0.12346, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3165 - val_loss: 0.1235\n",
      "Epoch 6/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.2948\n",
      "Epoch 00006: val_loss improved from 0.12346 to 0.12011, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2953 - val_loss: 0.1201\n",
      "Epoch 7/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2800\n",
      "Epoch 00007: val_loss improved from 0.12011 to 0.11148, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2796 - val_loss: 0.1115\n",
      "Epoch 8/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2638\n",
      "Epoch 00008: val_loss did not improve from 0.11148\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2633 - val_loss: 0.1125\n",
      "Epoch 9/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2480\n",
      "Epoch 00009: val_loss improved from 0.11148 to 0.10672, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0.3,dnodes128_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2482 - val_loss: 0.1067\n",
      "Epoch 10/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2422\n",
      "Epoch 00010: val_loss did not improve from 0.10672\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2428 - val_loss: 0.1090\n",
      "Epoch 11/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2310\n",
      "Epoch 00011: val_loss did not improve from 0.10672\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2301 - val_loss: 0.1075\n",
      " ###9 fold : val acc1 0.576, acc3 0.982, mae 0.221###\n",
      "acc10.583_acc30.975\n",
      "random search 85/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544/558 [============================>.] - ETA: 0s - loss: 11.1192\n",
      "Epoch 00001: val_loss improved from inf to 3.36133, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 10.9476 - val_loss: 3.3613\n",
      "Epoch 2/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 2.3378\n",
      "Epoch 00002: val_loss improved from 3.36133 to 0.99763, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 2.3283 - val_loss: 0.9976\n",
      "Epoch 3/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.9642\n",
      "Epoch 00003: val_loss improved from 0.99763 to 0.26008, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.9568 - val_loss: 0.2601\n",
      "Epoch 4/100\n",
      "540/558 [============================>.] - ETA: 0s - loss: 0.5814\n",
      "Epoch 00004: val_loss improved from 0.26008 to 0.13495, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.5794 - val_loss: 0.1350\n",
      "Epoch 5/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.4751\n",
      "Epoch 00005: val_loss improved from 0.13495 to 0.11789, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.4753 - val_loss: 0.1179\n",
      "Epoch 6/100\n",
      "541/558 [============================>.] - ETA: 0s - loss: 0.4624\n",
      "Epoch 00006: val_loss improved from 0.11789 to 0.11248, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.4611 - val_loss: 0.1125\n",
      "Epoch 7/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.4324\n",
      "Epoch 00007: val_loss improved from 0.11248 to 0.11120, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.4325 - val_loss: 0.1112\n",
      "Epoch 8/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.4056\n",
      "Epoch 00008: val_loss improved from 0.11120 to 0.10876, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.4048 - val_loss: 0.1088\n",
      "Epoch 9/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.3849\n",
      "Epoch 00009: val_loss did not improve from 0.10876\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.3850 - val_loss: 0.1101\n",
      "Epoch 10/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.3621\n",
      "Epoch 00010: val_loss improved from 0.10876 to 0.10662, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.3629 - val_loss: 0.1066\n",
      "Epoch 11/100\n",
      "554/558 [============================>.] - ETA: 0s - loss: 0.3518\n",
      "Epoch 00011: val_loss did not improve from 0.10662\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.3516 - val_loss: 0.1067\n",
      "Epoch 12/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.3356\n",
      "Epoch 00012: val_loss improved from 0.10662 to 0.10311, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.3355 - val_loss: 0.1031\n",
      "Epoch 13/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.3225\n",
      "Epoch 00013: val_loss did not improve from 0.10311\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.3225 - val_loss: 0.1056\n",
      "Epoch 14/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.2966\n",
      "Epoch 00014: val_loss did not improve from 0.10311\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.2959 - val_loss: 0.1065\n",
      " ###0 fold : val acc1 0.597, acc3 0.974, mae 0.215###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/558 [============================>.] - ETA: 0s - loss: 11.0268\n",
      "Epoch 00001: val_loss improved from inf to 3.35089, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 10.9378 - val_loss: 3.3509\n",
      "Epoch 2/100\n",
      "543/558 [============================>.] - ETA: 0s - loss: 2.3413\n",
      "Epoch 00002: val_loss improved from 3.35089 to 0.97658, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 2.3169 - val_loss: 0.9766\n",
      "Epoch 3/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.9401\n",
      "Epoch 00003: val_loss improved from 0.97658 to 0.25577, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.9356 - val_loss: 0.2558\n",
      "Epoch 4/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.5764\n",
      "Epoch 00004: val_loss improved from 0.25577 to 0.13413, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.5740 - val_loss: 0.1341\n",
      "Epoch 5/100\n",
      "543/558 [============================>.] - ETA: 0s - loss: 0.4735\n",
      "Epoch 00005: val_loss improved from 0.13413 to 0.11842, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.4734 - val_loss: 0.1184\n",
      "Epoch 6/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.4591\n",
      "Epoch 00006: val_loss improved from 0.11842 to 0.11233, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.4586 - val_loss: 0.1123\n",
      "Epoch 7/100\n",
      "541/558 [============================>.] - ETA: 0s - loss: 0.4304\n",
      "Epoch 00007: val_loss improved from 0.11233 to 0.11090, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.4281 - val_loss: 0.1109\n",
      "Epoch 8/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.4068\n",
      "Epoch 00008: val_loss improved from 0.11090 to 0.10952, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.4064 - val_loss: 0.1095\n",
      "Epoch 9/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.3827\n",
      "Epoch 00009: val_loss improved from 0.10952 to 0.10888, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.3826 - val_loss: 0.1089\n",
      "Epoch 10/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.3660\n",
      "Epoch 00010: val_loss improved from 0.10888 to 0.10646, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.3662 - val_loss: 0.1065\n",
      "Epoch 11/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.3516\n",
      "Epoch 00011: val_loss improved from 0.10646 to 0.10630, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.3516 - val_loss: 0.1063\n",
      "Epoch 12/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.3370\n",
      "Epoch 00012: val_loss improved from 0.10630 to 0.10324, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch32,dnodes16_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.3371 - val_loss: 0.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/279 [============================>.] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00004: val_loss improved from 0.10406 to 0.09987, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes64_dropout0,dnodes256_dropout0.1,lr0.001/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1250 - val_loss: 0.0999\n",
      "Epoch 5/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1222\n",
      "Epoch 00005: val_loss did not improve from 0.09987\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1222 - val_loss: 0.1009\n",
      "Epoch 6/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1227\n",
      "Epoch 00006: val_loss did not improve from 0.09987\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1228 - val_loss: 0.1006\n",
      " ###8 fold : val acc1 0.586, acc3 0.981, mae 0.216###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/279 [===========================>..] - ETA: 0s - loss: 1.8661\n",
      "Epoch 00001: val_loss improved from inf to 0.12893, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes64_dropout0,dnodes256_dropout0.1,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 1.7822 - val_loss: 0.1289\n",
      "Epoch 2/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1420\n",
      "Epoch 00002: val_loss improved from 0.12893 to 0.11233, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes64_dropout0,dnodes256_dropout0.1,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1420 - val_loss: 0.1123\n",
      "Epoch 3/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1321\n",
      "Epoch 00003: val_loss improved from 0.11233 to 0.10536, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes64_dropout0,dnodes256_dropout0.1,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1320 - val_loss: 0.1054\n",
      "Epoch 4/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00004: val_loss improved from 0.10536 to 0.10222, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes64_dropout0,dnodes256_dropout0.1,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1250 - val_loss: 0.1022\n",
      "Epoch 5/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.1221\n",
      "Epoch 00005: val_loss improved from 0.10222 to 0.10151, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes64_dropout0,dnodes256_dropout0.1,lr0.001/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1222 - val_loss: 0.1015\n",
      "Epoch 6/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1226\n",
      "Epoch 00006: val_loss did not improve from 0.10151\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1228 - val_loss: 0.1019\n",
      "Epoch 7/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1213\n",
      "Epoch 00007: val_loss did not improve from 0.10151\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1215 - val_loss: 0.1163\n",
      " ###9 fold : val acc1 0.574, acc3 0.984, mae 0.221###\n",
      "acc10.590_acc30.980\n",
      "random search 87/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/35 [===================>..........] - ETA: 0s - loss: 25.2305 \n",
      "Epoch 00001: val_loss improved from inf to 23.08999, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 1s 6ms/step - loss: 24.7108 - val_loss: 23.0900\n",
      "Epoch 2/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 22.4160\n",
      "Epoch 00002: val_loss improved from 23.08999 to 20.49208, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 21.9847 - val_loss: 20.4921\n",
      "Epoch 3/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 19.8550\n",
      "Epoch 00003: val_loss improved from 20.49208 to 18.03849, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 19.4432 - val_loss: 18.0385\n",
      "Epoch 4/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 17.5157\n",
      "Epoch 00004: val_loss improved from 18.03849 to 15.72343, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 17.0729 - val_loss: 15.7234\n",
      "Epoch 5/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 15.1803\n",
      "Epoch 00005: val_loss improved from 15.72343 to 13.56182, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 14.7990 - val_loss: 13.5618\n",
      "Epoch 6/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 13.0644\n",
      "Epoch 00006: val_loss improved from 13.56182 to 11.56480, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 12.7411 - val_loss: 11.5648\n",
      "Epoch 7/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 11.1686\n",
      "Epoch 00007: val_loss improved from 11.56480 to 9.76665, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.8619 - val_loss: 9.7667\n",
      "Epoch 8/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 9.4381\n",
      "Epoch 00008: val_loss improved from 9.76665 to 8.19323, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 9.1848 - val_loss: 8.1932\n",
      "Epoch 9/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 7.9874\n",
      "Epoch 00009: val_loss improved from 8.19323 to 6.85144, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 7.7791 - val_loss: 6.8514\n",
      "Epoch 10/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 6.7670\n",
      "Epoch 00010: val_loss improved from 6.85144 to 5.73678, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 6.5584 - val_loss: 5.7368\n",
      "Epoch 11/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 5.7525\n",
      "Epoch 00011: val_loss improved from 5.73678 to 4.83901, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.6027 - val_loss: 4.8390\n",
      "Epoch 12/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 4.9648\n",
      "Epoch 00012: val_loss improved from 4.83901 to 4.12335, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.8366 - val_loss: 4.1233\n",
      "Epoch 13/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 4.3383\n",
      "Epoch 00013: val_loss improved from 4.12335 to 3.56824, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.2399 - val_loss: 3.5682\n",
      "Epoch 14/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 3.8389\n",
      "Epoch 00014: val_loss improved from 3.56824 to 3.13186, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.7700 - val_loss: 3.1319\n",
      "Epoch 15/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 3.4844\n",
      "Epoch 00015: val_loss improved from 3.13186 to 2.78102, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.4352 - val_loss: 2.7810\n",
      "Epoch 16/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.1798\n",
      "Epoch 00016: val_loss improved from 2.78102 to 2.49757, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.1494 - val_loss: 2.4976\n",
      "Epoch 17/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 2.9844\n",
      "Epoch 00017: val_loss improved from 2.49757 to 2.25792, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.9462 - val_loss: 2.2579\n",
      "Epoch 18/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.7703\n",
      "Epoch 00018: val_loss improved from 2.25792 to 2.05066, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.7424 - val_loss: 2.0507\n",
      "Epoch 19/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 2.5868\n",
      "Epoch 00019: val_loss improved from 2.05066 to 1.86809, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.5408 - val_loss: 1.8681\n",
      "Epoch 20/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 2.4091\n",
      "Epoch 00020: val_loss improved from 1.86809 to 1.70297, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.3895 - val_loss: 1.7030\n",
      "Epoch 21/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 2.2504\n",
      "Epoch 00021: val_loss improved from 1.70297 to 1.55032, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.2152 - val_loss: 1.5503\n",
      "Epoch 22/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.1052\n",
      "Epoch 00022: val_loss improved from 1.55032 to 1.41030, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.0650 - val_loss: 1.4103\n",
      "Epoch 23/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.0036\n",
      "Epoch 00023: val_loss improved from 1.41030 to 1.28262, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9560 - val_loss: 1.2826\n",
      "Epoch 24/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.8927\n",
      "Epoch 00024: val_loss improved from 1.28262 to 1.16225, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.8597 - val_loss: 1.1623\n",
      "Epoch 25/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 1.7536\n",
      "Epoch 00025: val_loss improved from 1.16225 to 1.05144, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7385 - val_loss: 1.0514\n",
      "Epoch 26/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 1.6750\n",
      "Epoch 00026: val_loss improved from 1.05144 to 0.94814, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.6363 - val_loss: 0.9481\n",
      "Epoch 27/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 1.5848\n",
      "Epoch 00027: val_loss improved from 0.94814 to 0.85495, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.5538 - val_loss: 0.8550\n",
      "Epoch 28/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 1.4576\n",
      "Epoch 00028: val_loss improved from 0.85495 to 0.77034, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4561 - val_loss: 0.7703\n",
      "Epoch 29/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 1.3467\n",
      "Epoch 00029: val_loss improved from 0.77034 to 0.69435, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.3422 - val_loss: 0.6943\n",
      "Epoch 30/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.3260\n",
      "Epoch 00030: val_loss improved from 0.69435 to 0.62422, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2972 - val_loss: 0.6242\n",
      "Epoch 31/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.2601\n",
      "Epoch 00031: val_loss improved from 0.62422 to 0.56142, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.2545 - val_loss: 0.5614\n",
      "Epoch 32/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.1876\n",
      "Epoch 00032: val_loss improved from 0.56142 to 0.50501, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1841 - val_loss: 0.5050\n",
      "Epoch 33/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.1197\n",
      "Epoch 00033: val_loss improved from 0.50501 to 0.45654, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1185 - val_loss: 0.4565\n",
      "Epoch 34/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.0635\n",
      "Epoch 00034: val_loss improved from 0.45654 to 0.41331, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0666 - val_loss: 0.4133\n",
      "Epoch 35/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.0264\n",
      "Epoch 00035: val_loss improved from 0.41331 to 0.37562, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0165 - val_loss: 0.3756\n",
      "Epoch 36/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.0094\n",
      "Epoch 00036: val_loss improved from 0.37562 to 0.34277, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0088 - val_loss: 0.3428\n",
      "Epoch 37/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.9847\n",
      "Epoch 00037: val_loss improved from 0.34277 to 0.31441, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9604 - val_loss: 0.3144\n",
      "Epoch 38/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.9410\n",
      "Epoch 00038: val_loss improved from 0.31441 to 0.29116, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9417 - val_loss: 0.2912\n",
      "Epoch 39/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.9100\n",
      "Epoch 00039: val_loss improved from 0.29116 to 0.26967, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9150 - val_loss: 0.2697\n",
      "Epoch 40/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.9023\n",
      "Epoch 00040: val_loss improved from 0.26967 to 0.25223, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8934 - val_loss: 0.2522\n",
      "Epoch 41/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8745\n",
      "Epoch 00041: val_loss improved from 0.25223 to 0.23631, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8789 - val_loss: 0.2363\n",
      "Epoch 42/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8236\n",
      "Epoch 00042: val_loss improved from 0.23631 to 0.22207, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8365 - val_loss: 0.2221\n",
      "Epoch 43/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8489\n",
      "Epoch 00043: val_loss improved from 0.22207 to 0.21121, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8398 - val_loss: 0.2112\n",
      "Epoch 44/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.8171\n",
      "Epoch 00044: val_loss improved from 0.21121 to 0.20170, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.8120 - val_loss: 0.2017\n",
      "Epoch 45/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7997\n",
      "Epoch 00045: val_loss improved from 0.20170 to 0.19358, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7918 - val_loss: 0.1936\n",
      "Epoch 46/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7914\n",
      "Epoch 00046: val_loss improved from 0.19358 to 0.18718, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7920 - val_loss: 0.1872\n",
      "Epoch 47/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7819\n",
      "Epoch 00047: val_loss improved from 0.18718 to 0.18073, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7795 - val_loss: 0.1807\n",
      "Epoch 48/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7956\n",
      "Epoch 00048: val_loss improved from 0.18073 to 0.17619, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7904 - val_loss: 0.1762\n",
      "Epoch 49/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7822\n",
      "Epoch 00049: val_loss improved from 0.17619 to 0.17041, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7723 - val_loss: 0.1704\n",
      "Epoch 50/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7702\n",
      "Epoch 00050: val_loss improved from 0.17041 to 0.16633, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7612 - val_loss: 0.1663\n",
      "Epoch 51/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7330\n",
      "Epoch 00051: val_loss improved from 0.16633 to 0.16271, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7356 - val_loss: 0.1627\n",
      "Epoch 52/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7453\n",
      "Epoch 00052: val_loss improved from 0.16271 to 0.15961, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7444 - val_loss: 0.1596\n",
      "Epoch 53/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7358\n",
      "Epoch 00053: val_loss improved from 0.15961 to 0.15652, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7324 - val_loss: 0.1565\n",
      "Epoch 54/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.7464\n",
      "Epoch 00054: val_loss improved from 0.15652 to 0.15469, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7470 - val_loss: 0.1547\n",
      "Epoch 55/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7277\n",
      "Epoch 00055: val_loss improved from 0.15469 to 0.15226, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7225 - val_loss: 0.1523\n",
      "Epoch 56/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7291\n",
      "Epoch 00056: val_loss improved from 0.15226 to 0.15042, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7291 - val_loss: 0.1504\n",
      "Epoch 57/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7243\n",
      "Epoch 00057: val_loss improved from 0.15042 to 0.14980, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7213 - val_loss: 0.1498\n",
      "Epoch 58/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7132\n",
      "Epoch 00058: val_loss improved from 0.14980 to 0.14769, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7046 - val_loss: 0.1477\n",
      "Epoch 59/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6894\n",
      "Epoch 00059: val_loss improved from 0.14769 to 0.14666, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6904 - val_loss: 0.1467\n",
      "Epoch 60/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7037\n",
      "Epoch 00060: val_loss improved from 0.14666 to 0.14501, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6911 - val_loss: 0.1450\n",
      "Epoch 61/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6909\n",
      "Epoch 00061: val_loss improved from 0.14501 to 0.14382, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6822 - val_loss: 0.1438\n",
      "Epoch 62/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6983\n",
      "Epoch 00062: val_loss improved from 0.14382 to 0.14302, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6942 - val_loss: 0.1430\n",
      "Epoch 63/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6853\n",
      "Epoch 00063: val_loss improved from 0.14302 to 0.14098, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6851 - val_loss: 0.1410\n",
      "Epoch 64/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6811\n",
      "Epoch 00064: val_loss improved from 0.14098 to 0.14019, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6739 - val_loss: 0.1402\n",
      "Epoch 65/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6756\n",
      "Epoch 00065: val_loss improved from 0.14019 to 0.13891, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6708 - val_loss: 0.1389\n",
      "Epoch 66/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6440\n",
      "Epoch 00066: val_loss improved from 0.13891 to 0.13839, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6515 - val_loss: 0.1384\n",
      "Epoch 67/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6534\n",
      "Epoch 00067: val_loss did not improve from 0.13839\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6552 - val_loss: 0.1388\n",
      "Epoch 68/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6587\n",
      "Epoch 00068: val_loss improved from 0.13839 to 0.13702, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6540 - val_loss: 0.1370\n",
      "Epoch 69/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6492\n",
      "Epoch 00069: val_loss improved from 0.13702 to 0.13635, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6487 - val_loss: 0.1363\n",
      "Epoch 70/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6212\n",
      "Epoch 00070: val_loss improved from 0.13635 to 0.13464, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6254 - val_loss: 0.1346\n",
      "Epoch 71/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6243\n",
      "Epoch 00071: val_loss improved from 0.13464 to 0.13373, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6313 - val_loss: 0.1337\n",
      "Epoch 72/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6413\n",
      "Epoch 00072: val_loss improved from 0.13373 to 0.13275, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6332 - val_loss: 0.1328\n",
      "Epoch 73/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6087\n",
      "Epoch 00073: val_loss improved from 0.13275 to 0.13152, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6108 - val_loss: 0.1315\n",
      "Epoch 74/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.6174\n",
      "Epoch 00074: val_loss improved from 0.13152 to 0.13142, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6206 - val_loss: 0.1314\n",
      "Epoch 75/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6058\n",
      "Epoch 00075: val_loss improved from 0.13142 to 0.12980, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6088 - val_loss: 0.1298\n",
      "Epoch 76/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.5965\n",
      "Epoch 00076: val_loss improved from 0.12980 to 0.12912, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6028 - val_loss: 0.1291\n",
      "Epoch 77/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6030\n",
      "Epoch 00077: val_loss improved from 0.12912 to 0.12804, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5859 - val_loss: 0.1280\n",
      "Epoch 78/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.5964\n",
      "Epoch 00078: val_loss did not improve from 0.12804\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6010 - val_loss: 0.1280\n",
      "Epoch 79/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.5914\n",
      "Epoch 00079: val_loss improved from 0.12804 to 0.12752, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5905 - val_loss: 0.1275\n",
      "Epoch 80/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.5772\n",
      "Epoch 00080: val_loss improved from 0.12752 to 0.12651, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5830 - val_loss: 0.1265\n",
      "Epoch 81/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.5854\n",
      "Epoch 00081: val_loss improved from 0.12651 to 0.12469, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5771 - val_loss: 0.1247\n",
      "Epoch 82/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.5772\n",
      "Epoch 00082: val_loss improved from 0.12469 to 0.12455, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5842 - val_loss: 0.1245\n",
      "Epoch 83/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.5769\n",
      "Epoch 00083: val_loss did not improve from 0.12455\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5741 - val_loss: 0.1258\n",
      "Epoch 84/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5632\n",
      "Epoch 00084: val_loss did not improve from 0.12455\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5733 - val_loss: 0.1249\n",
      " ###0 fold : val acc1 0.554, acc3 0.960, mae 0.244###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/35 [=================>............] - ETA: 0s - loss: 25.3031 \n",
      "Epoch 00001: val_loss improved from inf to 23.09041, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 24.7029 - val_loss: 23.0904\n",
      "Epoch 2/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 22.4823\n",
      "Epoch 00002: val_loss improved from 23.09041 to 20.49295, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 21.9716 - val_loss: 20.4929\n",
      "Epoch 3/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 19.8794\n",
      "Epoch 00003: val_loss improved from 20.49295 to 18.04180, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 19.4373 - val_loss: 18.0418\n",
      "Epoch 4/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 17.5627\n",
      "Epoch 00004: val_loss improved from 18.04180 to 15.72530, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 17.0820 - val_loss: 15.7253\n",
      "Epoch 5/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 15.2425\n",
      "Epoch 00005: val_loss improved from 15.72530 to 13.56533, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 14.8071 - val_loss: 13.5653\n",
      "Epoch 6/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 13.0788\n",
      "Epoch 00006: val_loss improved from 13.56533 to 11.56501, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 12.7517 - val_loss: 11.5650\n",
      "Epoch 7/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 11.2152\n",
      "Epoch 00007: val_loss improved from 11.56501 to 9.76688, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.8745 - val_loss: 9.7669\n",
      "Epoch 8/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 9.5011 \n",
      "Epoch 00008: val_loss improved from 9.76688 to 8.18979, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 9.1975 - val_loss: 8.1898\n",
      "Epoch 9/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 8.0255\n",
      "Epoch 00009: val_loss improved from 8.18979 to 6.84591, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 7.7891 - val_loss: 6.8459\n",
      "Epoch 10/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 6.8133\n",
      "Epoch 00010: val_loss improved from 6.84591 to 5.72796, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 6.5669 - val_loss: 5.7280\n",
      "Epoch 11/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 5.7762\n",
      "Epoch 00011: val_loss improved from 5.72796 to 4.82856, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.6068 - val_loss: 4.8286\n",
      "Epoch 12/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 4.9884\n",
      "Epoch 00012: val_loss improved from 4.82856 to 4.11620, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.8349 - val_loss: 4.1162\n",
      "Epoch 13/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 4.3571\n",
      "Epoch 00013: val_loss improved from 4.11620 to 3.56000, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.2436 - val_loss: 3.5600\n",
      "Epoch 14/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 3.8577\n",
      "Epoch 00014: val_loss improved from 3.56000 to 3.12274, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.7797 - val_loss: 3.1227\n",
      "Epoch 15/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 3.5038\n",
      "Epoch 00015: val_loss improved from 3.12274 to 2.77301, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.4520 - val_loss: 2.7730\n",
      "Epoch 16/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 3.2018\n",
      "Epoch 00016: val_loss improved from 2.77301 to 2.48804, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.1514 - val_loss: 2.4880\n",
      "Epoch 17/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 2.9962\n",
      "Epoch 00017: val_loss improved from 2.48804 to 2.24895, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.9457 - val_loss: 2.2490\n",
      "Epoch 18/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 2.7596\n",
      "Epoch 00018: val_loss improved from 2.24895 to 2.04165, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.7372 - val_loss: 2.0417\n",
      "Epoch 19/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.5855\n",
      "Epoch 00019: val_loss improved from 2.04165 to 1.85884, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.5306 - val_loss: 1.8588\n",
      "Epoch 20/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.4316\n",
      "Epoch 00020: val_loss improved from 1.85884 to 1.69357, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3847 - val_loss: 1.6936\n",
      "Epoch 21/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.2485\n",
      "Epoch 00021: val_loss improved from 1.69357 to 1.53962, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.2118 - val_loss: 1.5396\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.0910\n",
      "Epoch 00022: val_loss improved from 1.53962 to 1.39835, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0623 - val_loss: 1.3983\n",
      "Epoch 23/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.9794\n",
      "Epoch 00023: val_loss improved from 1.39835 to 1.27102, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9374 - val_loss: 1.2710\n",
      "Epoch 24/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.8893\n",
      "Epoch 00024: val_loss improved from 1.27102 to 1.15120, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.8513 - val_loss: 1.1512\n",
      "Epoch 25/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.7266\n",
      "Epoch 00025: val_loss improved from 1.15120 to 1.04102, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.7201 - val_loss: 1.0410\n",
      "Epoch 26/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.6670\n",
      "Epoch 00026: val_loss improved from 1.04102 to 0.93857, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.6360 - val_loss: 0.9386\n",
      "Epoch 27/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.5763\n",
      "Epoch 00027: val_loss improved from 0.93857 to 0.84577, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.5483 - val_loss: 0.8458\n",
      "Epoch 28/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.4230\n",
      "Epoch 00028: val_loss improved from 0.84577 to 0.76174, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4351 - val_loss: 0.7617\n",
      "Epoch 29/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.3389\n",
      "Epoch 00029: val_loss improved from 0.76174 to 0.68642, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.3294 - val_loss: 0.6864\n",
      "Epoch 30/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.3119\n",
      "Epoch 00030: val_loss improved from 0.68642 to 0.61719, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2820 - val_loss: 0.6172\n",
      "Epoch 31/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.2529\n",
      "Epoch 00031: val_loss improved from 0.61719 to 0.55530, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2449 - val_loss: 0.5553\n",
      "Epoch 32/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.1856\n",
      "Epoch 00032: val_loss improved from 0.55530 to 0.49976, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1797 - val_loss: 0.4998\n",
      "Epoch 33/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.1218\n",
      "Epoch 00033: val_loss improved from 0.49976 to 0.45205, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1138 - val_loss: 0.4520\n",
      "Epoch 34/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.0630\n",
      "Epoch 00034: val_loss improved from 0.45205 to 0.40955, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.0669 - val_loss: 0.4095\n",
      "Epoch 35/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.0362\n",
      "Epoch 00035: val_loss improved from 0.40955 to 0.37204, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0219 - val_loss: 0.3720\n",
      "Epoch 36/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0049\n",
      "Epoch 00036: val_loss improved from 0.37204 to 0.33996, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0046 - val_loss: 0.3400\n",
      "Epoch 37/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9762\n",
      "Epoch 00037: val_loss improved from 0.33996 to 0.31215, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9525 - val_loss: 0.3121\n",
      "Epoch 38/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9399\n",
      "Epoch 00038: val_loss improved from 0.31215 to 0.28903, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9383 - val_loss: 0.2890\n",
      "Epoch 39/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.9142\n",
      "Epoch 00039: val_loss improved from 0.28903 to 0.26800, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9215 - val_loss: 0.2680\n",
      "Epoch 40/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8992\n",
      "Epoch 00040: val_loss improved from 0.26800 to 0.25108, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8904 - val_loss: 0.2511\n",
      "Epoch 41/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8835\n",
      "Epoch 00041: val_loss improved from 0.25108 to 0.23587, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.8818 - val_loss: 0.2359\n",
      "Epoch 42/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8342\n",
      "Epoch 00042: val_loss improved from 0.23587 to 0.22193, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.8404 - val_loss: 0.2219\n",
      "Epoch 43/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8520\n",
      "Epoch 00043: val_loss improved from 0.22193 to 0.21109, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8449 - val_loss: 0.2111\n",
      "Epoch 44/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8217\n",
      "Epoch 00044: val_loss improved from 0.21109 to 0.20185, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8097 - val_loss: 0.2018\n",
      "Epoch 45/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8046\n",
      "Epoch 00045: val_loss improved from 0.20185 to 0.19351, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7988 - val_loss: 0.1935\n",
      "Epoch 46/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7820\n",
      "Epoch 00046: val_loss improved from 0.19351 to 0.18721, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7901 - val_loss: 0.1872\n",
      "Epoch 47/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7853\n",
      "Epoch 00047: val_loss improved from 0.18721 to 0.18117, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7841 - val_loss: 0.1812\n",
      "Epoch 48/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8020\n",
      "Epoch 00048: val_loss improved from 0.18117 to 0.17661, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7915 - val_loss: 0.1766\n",
      "Epoch 49/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7814\n",
      "Epoch 00049: val_loss improved from 0.17661 to 0.17050, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7730 - val_loss: 0.1705\n",
      "Epoch 50/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7546\n",
      "Epoch 00050: val_loss improved from 0.17050 to 0.16637, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7552 - val_loss: 0.1664\n",
      "Epoch 51/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7331\n",
      "Epoch 00051: val_loss improved from 0.16637 to 0.16271, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7359 - val_loss: 0.1627\n",
      "Epoch 52/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7472\n",
      "Epoch 00052: val_loss improved from 0.16271 to 0.15972, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7439 - val_loss: 0.1597\n",
      "Epoch 53/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7426\n",
      "Epoch 00053: val_loss improved from 0.15972 to 0.15657, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7356 - val_loss: 0.1566\n",
      "Epoch 54/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7606\n",
      "Epoch 00054: val_loss improved from 0.15657 to 0.15460, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7493 - val_loss: 0.1546\n",
      "Epoch 55/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7323\n",
      "Epoch 00055: val_loss improved from 0.15460 to 0.15245, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7294 - val_loss: 0.1525\n",
      "Epoch 56/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7313\n",
      "Epoch 00056: val_loss improved from 0.15245 to 0.15068, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7313 - val_loss: 0.1507\n",
      "Epoch 57/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7195\n",
      "Epoch 00057: val_loss improved from 0.15068 to 0.14968, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7198 - val_loss: 0.1497\n",
      "Epoch 58/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7093\n",
      "Epoch 00058: val_loss improved from 0.14968 to 0.14774, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7030 - val_loss: 0.1477\n",
      "Epoch 59/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6948\n",
      "Epoch 00059: val_loss improved from 0.14774 to 0.14651, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6945 - val_loss: 0.1465\n",
      "Epoch 60/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7153\n",
      "Epoch 00060: val_loss improved from 0.14651 to 0.14509, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6977 - val_loss: 0.1451\n",
      "Epoch 61/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6848\n",
      "Epoch 00061: val_loss improved from 0.14509 to 0.14357, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6791 - val_loss: 0.1436\n",
      "Epoch 62/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6880\n",
      "Epoch 00062: val_loss improved from 0.14357 to 0.14302, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6844 - val_loss: 0.1430\n",
      "Epoch 63/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6816\n",
      "Epoch 00063: val_loss improved from 0.14302 to 0.14104, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6877 - val_loss: 0.1410\n",
      "Epoch 64/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6787\n",
      "Epoch 00064: val_loss improved from 0.14104 to 0.14042, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6743 - val_loss: 0.1404\n",
      "Epoch 65/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6787\n",
      "Epoch 00065: val_loss improved from 0.14042 to 0.13890, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6754 - val_loss: 0.1389\n",
      "Epoch 66/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6456\n",
      "Epoch 00066: val_loss improved from 0.13890 to 0.13852, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6536 - val_loss: 0.1385\n",
      "Epoch 67/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6574\n",
      "Epoch 00067: val_loss did not improve from 0.13852\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6560 - val_loss: 0.1394\n",
      "Epoch 68/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6554\n",
      "Epoch 00068: val_loss improved from 0.13852 to 0.13763, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6515 - val_loss: 0.1376\n",
      "Epoch 69/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6520\n",
      "Epoch 00069: val_loss improved from 0.13763 to 0.13671, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6498 - val_loss: 0.1367\n",
      "Epoch 70/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6294\n",
      "Epoch 00070: val_loss improved from 0.13671 to 0.13479, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6332 - val_loss: 0.1348\n",
      "Epoch 71/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6348\n",
      "Epoch 00071: val_loss improved from 0.13479 to 0.13361, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6409 - val_loss: 0.1336\n",
      "Epoch 72/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6434\n",
      "Epoch 00072: val_loss improved from 0.13361 to 0.13196, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6354 - val_loss: 0.1320\n",
      "Epoch 73/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6087\n",
      "Epoch 00073: val_loss improved from 0.13196 to 0.13105, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6104 - val_loss: 0.1311\n",
      "Epoch 74/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6179\n",
      "Epoch 00074: val_loss did not improve from 0.13105\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6166 - val_loss: 0.1312\n",
      "Epoch 75/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6147\n",
      "Epoch 00075: val_loss improved from 0.13105 to 0.12948, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6107 - val_loss: 0.1295\n",
      "Epoch 76/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5848\n",
      "Epoch 00076: val_loss improved from 0.12948 to 0.12900, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5984 - val_loss: 0.1290\n",
      "Epoch 77/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6029\n",
      "Epoch 00077: val_loss improved from 0.12900 to 0.12777, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5899 - val_loss: 0.1278\n",
      "Epoch 78/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5919\n",
      "Epoch 00078: val_loss improved from 0.12777 to 0.12751, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6031 - val_loss: 0.1275\n",
      "Epoch 79/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5990\n",
      "Epoch 00079: val_loss improved from 0.12751 to 0.12700, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5972 - val_loss: 0.1270\n",
      "Epoch 80/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5881\n",
      "Epoch 00080: val_loss improved from 0.12700 to 0.12621, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5851 - val_loss: 0.1262\n",
      "Epoch 81/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.5840\n",
      "Epoch 00081: val_loss improved from 0.12621 to 0.12405, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5763 - val_loss: 0.1241\n",
      "Epoch 82/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.5689\n",
      "Epoch 00082: val_loss did not improve from 0.12405\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5781 - val_loss: 0.1241\n",
      "Epoch 83/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5723\n",
      "Epoch 00083: val_loss did not improve from 0.12405\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5722 - val_loss: 0.1253\n",
      " ###1 fold : val acc1 0.551, acc3 0.966, mae 0.242###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 25.2363 \n",
      "Epoch 00001: val_loss improved from inf to 23.08935, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 24.6909 - val_loss: 23.0894\n",
      "Epoch 2/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 22.4449\n",
      "Epoch 00002: val_loss improved from 23.08935 to 20.49211, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 21.9500 - val_loss: 20.4921\n",
      "Epoch 3/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 19.8579\n",
      "Epoch 00003: val_loss improved from 20.49211 to 18.04535, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 19.4299 - val_loss: 18.0454\n",
      "Epoch 4/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 17.5430\n",
      "Epoch 00004: val_loss improved from 18.04535 to 15.72936, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 17.0672 - val_loss: 15.7294\n",
      "Epoch 5/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 15.2839\n",
      "Epoch 00005: val_loss improved from 15.72936 to 13.57102, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 14.7888 - val_loss: 13.5710\n",
      "Epoch 6/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 13.0554\n",
      "Epoch 00006: val_loss improved from 13.57102 to 11.57124, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 12.7357 - val_loss: 11.5712\n",
      "Epoch 7/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 11.2413\n",
      "Epoch 00007: val_loss improved from 11.57124 to 9.77300, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 10.8624 - val_loss: 9.7730\n",
      "Epoch 8/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 9.5187\n",
      "Epoch 00008: val_loss improved from 9.77300 to 8.19642, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 9.1827 - val_loss: 8.1964\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 8.0765\n",
      "Epoch 00009: val_loss improved from 8.19642 to 6.85132, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 7.7854 - val_loss: 6.8513\n",
      "Epoch 10/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 6.8048\n",
      "Epoch 00010: val_loss improved from 6.85132 to 5.73528, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 6.5658 - val_loss: 5.7353\n",
      "Epoch 11/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 5.7969\n",
      "Epoch 00011: val_loss improved from 5.73528 to 4.83462, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.6015 - val_loss: 4.8346\n",
      "Epoch 12/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 4.9793\n",
      "Epoch 00012: val_loss improved from 4.83462 to 4.12045, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.8243 - val_loss: 4.1205\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 4.3849\n",
      "Epoch 00013: val_loss improved from 4.12045 to 3.56452, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.2245 - val_loss: 3.5645\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.8672\n",
      "Epoch 00014: val_loss improved from 3.56452 to 3.12740, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.7913 - val_loss: 3.1274\n",
      "Epoch 15/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 3.5371\n",
      "Epoch 00015: val_loss improved from 3.12740 to 2.77714, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.4611 - val_loss: 2.7771\n",
      "Epoch 16/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.2019\n",
      "Epoch 00016: val_loss improved from 2.77714 to 2.49206, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1442 - val_loss: 2.4921\n",
      "Epoch 17/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.0258\n",
      "Epoch 00017: val_loss improved from 2.49206 to 2.25183, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9544 - val_loss: 2.2518\n",
      "Epoch 18/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.7585\n",
      "Epoch 00018: val_loss improved from 2.25183 to 2.04476, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.7364 - val_loss: 2.0448\n",
      "Epoch 19/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.5704\n",
      "Epoch 00019: val_loss improved from 2.04476 to 1.86117, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.5191 - val_loss: 1.8612\n",
      "Epoch 20/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.4103\n",
      "Epoch 00020: val_loss improved from 1.86117 to 1.69527, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3791 - val_loss: 1.6953\n",
      "Epoch 21/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.2378\n",
      "Epoch 00021: val_loss improved from 1.69527 to 1.54167, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.2104 - val_loss: 1.5417\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.0708\n",
      "Epoch 00022: val_loss improved from 1.54167 to 1.40082, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0461 - val_loss: 1.4008\n",
      "Epoch 23/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.9776\n",
      "Epoch 00023: val_loss improved from 1.40082 to 1.27388, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9352 - val_loss: 1.2739\n",
      "Epoch 24/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 1.8348\n",
      "Epoch 00024: val_loss improved from 1.27388 to 1.15377, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.8379 - val_loss: 1.1538\n",
      "Epoch 25/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.7427\n",
      "Epoch 00025: val_loss improved from 1.15377 to 1.04380, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.7245 - val_loss: 1.0438\n",
      "Epoch 26/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.6537\n",
      "Epoch 00026: val_loss improved from 1.04380 to 0.94157, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.6291 - val_loss: 0.9416\n",
      "Epoch 27/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.5686\n",
      "Epoch 00027: val_loss improved from 0.94157 to 0.84824, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.5470 - val_loss: 0.8482\n",
      "Epoch 28/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.4222\n",
      "Epoch 00028: val_loss improved from 0.84824 to 0.76385, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4330 - val_loss: 0.7639\n",
      "Epoch 29/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.3577\n",
      "Epoch 00029: val_loss improved from 0.76385 to 0.68797, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.3363 - val_loss: 0.6880\n",
      "Epoch 30/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 1.2920\n",
      "Epoch 00030: val_loss improved from 0.68797 to 0.61962, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2678 - val_loss: 0.6196\n",
      "Epoch 31/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.2442\n",
      "Epoch 00031: val_loss improved from 0.61962 to 0.55754, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2404 - val_loss: 0.5575\n",
      "Epoch 32/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.1871\n",
      "Epoch 00032: val_loss improved from 0.55754 to 0.50195, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1863 - val_loss: 0.5019\n",
      "Epoch 33/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.1196\n",
      "Epoch 00033: val_loss improved from 0.50195 to 0.45397, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1150 - val_loss: 0.4540\n",
      "Epoch 34/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.0627\n",
      "Epoch 00034: val_loss improved from 0.45397 to 0.41103, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0650 - val_loss: 0.4110\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 1.0212\n",
      "Epoch 00035: val_loss improved from 0.41103 to 0.37344, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0212 - val_loss: 0.3734\n",
      "Epoch 36/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.0087\n",
      "Epoch 00036: val_loss improved from 0.37344 to 0.34124, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0017 - val_loss: 0.3412\n",
      "Epoch 37/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.9725\n",
      "Epoch 00037: val_loss improved from 0.34124 to 0.31301, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9518 - val_loss: 0.3130\n",
      "Epoch 38/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9375\n",
      "Epoch 00038: val_loss improved from 0.31301 to 0.28975, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9407 - val_loss: 0.2898\n",
      "Epoch 39/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.9247\n",
      "Epoch 00039: val_loss improved from 0.28975 to 0.26819, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9279 - val_loss: 0.2682\n",
      "Epoch 40/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8967\n",
      "Epoch 00040: val_loss improved from 0.26819 to 0.25133, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8932 - val_loss: 0.2513\n",
      "Epoch 41/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8883\n",
      "Epoch 00041: val_loss improved from 0.25133 to 0.23569, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8879 - val_loss: 0.2357\n",
      "Epoch 42/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8252\n",
      "Epoch 00042: val_loss improved from 0.23569 to 0.22199, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8403 - val_loss: 0.2220\n",
      "Epoch 43/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8541\n",
      "Epoch 00043: val_loss improved from 0.22199 to 0.21119, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8508 - val_loss: 0.2112\n",
      "Epoch 44/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8195\n",
      "Epoch 00044: val_loss improved from 0.21119 to 0.20190, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8130 - val_loss: 0.2019\n",
      "Epoch 45/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7976\n",
      "Epoch 00045: val_loss improved from 0.20190 to 0.19341, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7965 - val_loss: 0.1934\n",
      "Epoch 46/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7784\n",
      "Epoch 00046: val_loss improved from 0.19341 to 0.18702, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7894 - val_loss: 0.1870\n",
      "Epoch 47/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7873\n",
      "Epoch 00047: val_loss improved from 0.18702 to 0.18082, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7821 - val_loss: 0.1808\n",
      "Epoch 48/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8019\n",
      "Epoch 00048: val_loss improved from 0.18082 to 0.17645, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7924 - val_loss: 0.1765\n",
      "Epoch 49/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7859\n",
      "Epoch 00049: val_loss improved from 0.17645 to 0.17051, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7783 - val_loss: 0.1705\n",
      "Epoch 50/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7528\n",
      "Epoch 00050: val_loss improved from 0.17051 to 0.16658, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7538 - val_loss: 0.1666\n",
      "Epoch 51/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7366\n",
      "Epoch 00051: val_loss improved from 0.16658 to 0.16272, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7326 - val_loss: 0.1627\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.7342\n",
      "Epoch 00052: val_loss improved from 0.16272 to 0.15994, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7342 - val_loss: 0.1599\n",
      "Epoch 53/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7358\n",
      "Epoch 00053: val_loss improved from 0.15994 to 0.15689, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7298 - val_loss: 0.1569\n",
      "Epoch 54/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7518\n",
      "Epoch 00054: val_loss improved from 0.15689 to 0.15482, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7495 - val_loss: 0.1548\n",
      "Epoch 55/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7288\n",
      "Epoch 00055: val_loss improved from 0.15482 to 0.15306, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7309 - val_loss: 0.1531\n",
      "Epoch 56/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7296\n",
      "Epoch 00056: val_loss improved from 0.15306 to 0.15098, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7277 - val_loss: 0.1510\n",
      "Epoch 57/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7149\n",
      "Epoch 00057: val_loss improved from 0.15098 to 0.15047, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7155 - val_loss: 0.1505\n",
      "Epoch 58/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7019\n",
      "Epoch 00058: val_loss improved from 0.15047 to 0.14810, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6990 - val_loss: 0.1481\n",
      "Epoch 59/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6878\n",
      "Epoch 00059: val_loss improved from 0.14810 to 0.14728, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6913 - val_loss: 0.1473\n",
      "Epoch 60/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7185\n",
      "Epoch 00060: val_loss improved from 0.14728 to 0.14578, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6996 - val_loss: 0.1458\n",
      "Epoch 61/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6971\n",
      "Epoch 00061: val_loss improved from 0.14578 to 0.14393, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6840 - val_loss: 0.1439\n",
      "Epoch 62/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6933\n",
      "Epoch 00062: val_loss improved from 0.14393 to 0.14375, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6845 - val_loss: 0.1438\n",
      "Epoch 63/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6785\n",
      "Epoch 00063: val_loss improved from 0.14375 to 0.14126, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6840 - val_loss: 0.1413\n",
      "Epoch 64/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6731\n",
      "Epoch 00064: val_loss improved from 0.14126 to 0.14072, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6673 - val_loss: 0.1407\n",
      "Epoch 65/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6705\n",
      "Epoch 00065: val_loss improved from 0.14072 to 0.13980, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6682 - val_loss: 0.1398\n",
      "Epoch 66/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6469\n",
      "Epoch 00066: val_loss improved from 0.13980 to 0.13902, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6575 - val_loss: 0.1390\n",
      "Epoch 67/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6498\n",
      "Epoch 00067: val_loss did not improve from 0.13902\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6558 - val_loss: 0.1393\n",
      "Epoch 68/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6550\n",
      "Epoch 00068: val_loss improved from 0.13902 to 0.13786, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6495 - val_loss: 0.1379\n",
      "Epoch 69/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6547\n",
      "Epoch 00069: val_loss improved from 0.13786 to 0.13666, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6523 - val_loss: 0.1367\n",
      "Epoch 70/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6365\n",
      "Epoch 00070: val_loss improved from 0.13666 to 0.13487, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6315 - val_loss: 0.1349\n",
      "Epoch 71/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6315\n",
      "Epoch 00071: val_loss improved from 0.13487 to 0.13361, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6413 - val_loss: 0.1336\n",
      "Epoch 72/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6472\n",
      "Epoch 00072: val_loss improved from 0.13361 to 0.13205, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6375 - val_loss: 0.1320\n",
      "Epoch 73/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6105\n",
      "Epoch 00073: val_loss improved from 0.13205 to 0.13137, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6145 - val_loss: 0.1314\n",
      "Epoch 74/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6158\n",
      "Epoch 00074: val_loss did not improve from 0.13137\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6138 - val_loss: 0.1319\n",
      "Epoch 75/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6119\n",
      "Epoch 00075: val_loss improved from 0.13137 to 0.13064, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6090 - val_loss: 0.1306\n",
      "Epoch 76/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5924\n",
      "Epoch 00076: val_loss improved from 0.13064 to 0.12952, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6050 - val_loss: 0.1295\n",
      "Epoch 77/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6048\n",
      "Epoch 00077: val_loss improved from 0.12952 to 0.12854, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5880 - val_loss: 0.1285\n",
      "Epoch 78/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5886\n",
      "Epoch 00078: val_loss improved from 0.12854 to 0.12830, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6007 - val_loss: 0.1283\n",
      "Epoch 79/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5983\n",
      "Epoch 00079: val_loss improved from 0.12830 to 0.12798, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5960 - val_loss: 0.1280\n",
      "Epoch 80/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5885\n",
      "Epoch 00080: val_loss improved from 0.12798 to 0.12770, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5888 - val_loss: 0.1277\n",
      "Epoch 81/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5826\n",
      "Epoch 00081: val_loss improved from 0.12770 to 0.12505, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5746 - val_loss: 0.1251\n",
      "Epoch 82/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5651\n",
      "Epoch 00082: val_loss improved from 0.12505 to 0.12484, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5757 - val_loss: 0.1248\n",
      "Epoch 83/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5742\n",
      "Epoch 00083: val_loss did not improve from 0.12484\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5751 - val_loss: 0.1258\n",
      "Epoch 84/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5620\n",
      "Epoch 00084: val_loss improved from 0.12484 to 0.12356, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5702 - val_loss: 0.1236\n",
      "Epoch 85/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5553\n",
      "Epoch 00085: val_loss did not improve from 0.12356\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5558 - val_loss: 0.1245\n",
      "Epoch 86/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5626\n",
      "Epoch 00086: val_loss did not improve from 0.12356\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5615 - val_loss: 0.1249\n",
      " ###2 fold : val acc1 0.548, acc3 0.963, mae 0.245###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/35 [=================>............] - ETA: 0s - loss: 25.3081 \n",
      "Epoch 00001: val_loss improved from inf to 23.08230, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 24.7470 - val_loss: 23.0823\n",
      "Epoch 2/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 22.4512\n",
      "Epoch 00002: val_loss improved from 23.08230 to 20.48289, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 21.9918 - val_loss: 20.4829\n",
      "Epoch 3/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 19.9127\n",
      "Epoch 00003: val_loss improved from 20.48289 to 18.03400, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 19.4877 - val_loss: 18.0340\n",
      "Epoch 4/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 17.6049\n",
      "Epoch 00004: val_loss improved from 18.03400 to 15.71746, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 17.1041 - val_loss: 15.7175\n",
      "Epoch 5/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 15.2541\n",
      "Epoch 00005: val_loss improved from 15.71746 to 13.55966, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 14.8268 - val_loss: 13.5597\n",
      "Epoch 6/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 13.0937\n",
      "Epoch 00006: val_loss improved from 13.55966 to 11.55982, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 12.7699 - val_loss: 11.5598\n",
      "Epoch 7/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 11.2407\n",
      "Epoch 00007: val_loss improved from 11.55982 to 9.76391, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.8853 - val_loss: 9.7639\n",
      "Epoch 8/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 9.5188 \n",
      "Epoch 00008: val_loss improved from 9.76391 to 8.18606, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 9.2073 - val_loss: 8.1861\n",
      "Epoch 9/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 8.0745\n",
      "Epoch 00009: val_loss improved from 8.18606 to 6.83920, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 7.8110 - val_loss: 6.8392\n",
      "Epoch 10/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 6.8121\n",
      "Epoch 00010: val_loss improved from 6.83920 to 5.72476, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 6.5735 - val_loss: 5.7248\n",
      "Epoch 11/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 5.8207\n",
      "Epoch 00011: val_loss improved from 5.72476 to 4.82379, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 5.6171 - val_loss: 4.8238\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 5.0032\n",
      "Epoch 00012: val_loss improved from 4.82379 to 4.10912, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.8477 - val_loss: 4.1091\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 4.4115\n",
      "Epoch 00013: val_loss improved from 4.10912 to 3.55433, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.2554 - val_loss: 3.5543\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.8966\n",
      "Epoch 00014: val_loss improved from 3.55433 to 3.11937, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.7884 - val_loss: 3.1194\n",
      "Epoch 15/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.5402\n",
      "Epoch 00015: val_loss improved from 3.11937 to 2.77063, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.4705 - val_loss: 2.7706\n",
      "Epoch 16/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.2162\n",
      "Epoch 00016: val_loss improved from 2.77063 to 2.48643, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1562 - val_loss: 2.4864\n",
      "Epoch 17/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 3.0146\n",
      "Epoch 00017: val_loss improved from 2.48643 to 2.24761, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9461 - val_loss: 2.2476\n",
      "Epoch 18/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.7776\n",
      "Epoch 00018: val_loss improved from 2.24761 to 2.04052, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.7476 - val_loss: 2.0405\n",
      "Epoch 19/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.5825\n",
      "Epoch 00019: val_loss improved from 2.04052 to 1.85685, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.5342 - val_loss: 1.8568\n",
      "Epoch 20/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.4202\n",
      "Epoch 00020: val_loss improved from 1.85685 to 1.69109, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3794 - val_loss: 1.6911\n",
      "Epoch 21/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.2463\n",
      "Epoch 00021: val_loss improved from 1.69109 to 1.53776, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.2142 - val_loss: 1.5378\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.0746\n",
      "Epoch 00022: val_loss improved from 1.53776 to 1.39636, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0462 - val_loss: 1.3964\n",
      "Epoch 23/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.9709\n",
      "Epoch 00023: val_loss improved from 1.39636 to 1.26919, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9384 - val_loss: 1.2692\n",
      "Epoch 24/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.8733\n",
      "Epoch 00024: val_loss improved from 1.26919 to 1.14896, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.8362 - val_loss: 1.1490\n",
      "Epoch 25/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.7505\n",
      "Epoch 00025: val_loss improved from 1.14896 to 1.03860, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.7261 - val_loss: 1.0386\n",
      "Epoch 26/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.6731\n",
      "Epoch 00026: val_loss improved from 1.03860 to 0.93614, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.6331 - val_loss: 0.9361\n",
      "Epoch 27/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.5617\n",
      "Epoch 00027: val_loss improved from 0.93614 to 0.84276, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.5440 - val_loss: 0.8428\n",
      "Epoch 28/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.4166\n",
      "Epoch 00028: val_loss improved from 0.84276 to 0.75847, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4381 - val_loss: 0.7585\n",
      "Epoch 29/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.3658\n",
      "Epoch 00029: val_loss improved from 0.75847 to 0.68264, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.3453 - val_loss: 0.6826\n",
      "Epoch 30/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.3058\n",
      "Epoch 00030: val_loss improved from 0.68264 to 0.61400, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2740 - val_loss: 0.6140\n",
      "Epoch 31/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.2435\n",
      "Epoch 00031: val_loss improved from 0.61400 to 0.55275, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2340 - val_loss: 0.5528\n",
      "Epoch 32/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.1767\n",
      "Epoch 00032: val_loss improved from 0.55275 to 0.49757, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1868 - val_loss: 0.4976\n",
      "Epoch 33/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.1310\n",
      "Epoch 00033: val_loss improved from 0.49757 to 0.44936, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1220 - val_loss: 0.4494\n",
      "Epoch 34/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0592\n",
      "Epoch 00034: val_loss improved from 0.44936 to 0.40631, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0620 - val_loss: 0.4063\n",
      "Epoch 35/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 1.0218\n",
      "Epoch 00035: val_loss improved from 0.40631 to 0.36931, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0173 - val_loss: 0.3693\n",
      "Epoch 36/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.9917\n",
      "Epoch 00036: val_loss improved from 0.36931 to 0.33729, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9992 - val_loss: 0.3373\n",
      "Epoch 37/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9681\n",
      "Epoch 00037: val_loss improved from 0.33729 to 0.30960, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9482 - val_loss: 0.3096\n",
      "Epoch 38/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9336\n",
      "Epoch 00038: val_loss improved from 0.30960 to 0.28641, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9379 - val_loss: 0.2864\n",
      "Epoch 39/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9312\n",
      "Epoch 00039: val_loss improved from 0.28641 to 0.26513, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9289 - val_loss: 0.2651\n",
      "Epoch 40/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8864\n",
      "Epoch 00040: val_loss improved from 0.26513 to 0.24807, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8879 - val_loss: 0.2481\n",
      "Epoch 41/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8827\n",
      "Epoch 00041: val_loss improved from 0.24807 to 0.23259, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8812 - val_loss: 0.2326\n",
      "Epoch 42/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8396\n",
      "Epoch 00042: val_loss improved from 0.23259 to 0.21924, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8497 - val_loss: 0.2192\n",
      "Epoch 43/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8549\n",
      "Epoch 00043: val_loss improved from 0.21924 to 0.20875, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8544 - val_loss: 0.2087\n",
      "Epoch 44/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8209\n",
      "Epoch 00044: val_loss improved from 0.20875 to 0.19968, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8186 - val_loss: 0.1997\n",
      "Epoch 45/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7952\n",
      "Epoch 00045: val_loss improved from 0.19968 to 0.19155, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7953 - val_loss: 0.1916\n",
      "Epoch 46/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7682\n",
      "Epoch 00046: val_loss improved from 0.19155 to 0.18554, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7828 - val_loss: 0.1855\n",
      "Epoch 47/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7842\n",
      "Epoch 00047: val_loss improved from 0.18554 to 0.17928, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7886 - val_loss: 0.1793\n",
      "Epoch 48/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7977\n",
      "Epoch 00048: val_loss improved from 0.17928 to 0.17498, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7877 - val_loss: 0.1750\n",
      "Epoch 49/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7923\n",
      "Epoch 00049: val_loss improved from 0.17498 to 0.16894, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7813 - val_loss: 0.1689\n",
      "Epoch 50/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7478\n",
      "Epoch 00050: val_loss improved from 0.16894 to 0.16536, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7499 - val_loss: 0.1654\n",
      "Epoch 51/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7343\n",
      "Epoch 00051: val_loss improved from 0.16536 to 0.16155, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7303 - val_loss: 0.1616\n",
      "Epoch 52/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7427\n",
      "Epoch 00052: val_loss improved from 0.16155 to 0.15919, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7403 - val_loss: 0.1592\n",
      "Epoch 53/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7368\n",
      "Epoch 00053: val_loss improved from 0.15919 to 0.15628, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7319 - val_loss: 0.1563\n",
      "Epoch 54/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7610\n",
      "Epoch 00054: val_loss improved from 0.15628 to 0.15425, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7487 - val_loss: 0.1543\n",
      "Epoch 55/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7304\n",
      "Epoch 00055: val_loss improved from 0.15425 to 0.15216, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7306 - val_loss: 0.1522\n",
      "Epoch 56/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7252\n",
      "Epoch 00056: val_loss improved from 0.15216 to 0.14992, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7297 - val_loss: 0.1499\n",
      "Epoch 57/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7076\n",
      "Epoch 00057: val_loss improved from 0.14992 to 0.14953, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7112 - val_loss: 0.1495\n",
      "Epoch 58/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7099\n",
      "Epoch 00058: val_loss improved from 0.14953 to 0.14746, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7056 - val_loss: 0.1475\n",
      "Epoch 59/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6920\n",
      "Epoch 00059: val_loss improved from 0.14746 to 0.14609, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6901 - val_loss: 0.1461\n",
      "Epoch 60/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7226\n",
      "Epoch 00060: val_loss improved from 0.14609 to 0.14489, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6999 - val_loss: 0.1449\n",
      "Epoch 61/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6925\n",
      "Epoch 00061: val_loss improved from 0.14489 to 0.14319, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6802 - val_loss: 0.1432\n",
      "Epoch 62/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6915\n",
      "Epoch 00062: val_loss improved from 0.14319 to 0.14285, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6843 - val_loss: 0.1429\n",
      "Epoch 63/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6806\n",
      "Epoch 00063: val_loss improved from 0.14285 to 0.14100, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6870 - val_loss: 0.1410\n",
      "Epoch 64/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6775\n",
      "Epoch 00064: val_loss improved from 0.14100 to 0.14034, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6733 - val_loss: 0.1403\n",
      "Epoch 65/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6808\n",
      "Epoch 00065: val_loss improved from 0.14034 to 0.13881, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6723 - val_loss: 0.1388\n",
      "Epoch 66/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6382\n",
      "Epoch 00066: val_loss improved from 0.13881 to 0.13863, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6512 - val_loss: 0.1386\n",
      "Epoch 67/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6544\n",
      "Epoch 00067: val_loss did not improve from 0.13863\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6574 - val_loss: 0.1387\n",
      "Epoch 68/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6522\n",
      "Epoch 00068: val_loss improved from 0.13863 to 0.13717, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6484 - val_loss: 0.1372\n",
      "Epoch 69/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6571\n",
      "Epoch 00069: val_loss improved from 0.13717 to 0.13634, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6533 - val_loss: 0.1363\n",
      "Epoch 70/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.6436\n",
      "Epoch 00070: val_loss improved from 0.13634 to 0.13526, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6406 - val_loss: 0.1353\n",
      "Epoch 71/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.6362\n",
      "Epoch 00071: val_loss improved from 0.13526 to 0.13334, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6408 - val_loss: 0.1333\n",
      "Epoch 72/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.6461\n",
      "Epoch 00072: val_loss improved from 0.13334 to 0.13161, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6382 - val_loss: 0.1316\n",
      "Epoch 73/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.6170\n",
      "Epoch 00073: val_loss improved from 0.13161 to 0.13096, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6185 - val_loss: 0.1310\n",
      "Epoch 74/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6141\n",
      "Epoch 00074: val_loss did not improve from 0.13096\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6166 - val_loss: 0.1318\n",
      "Epoch 75/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.6080\n",
      "Epoch 00075: val_loss improved from 0.13096 to 0.13051, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6076 - val_loss: 0.1305\n",
      "Epoch 76/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.5931\n",
      "Epoch 00076: val_loss improved from 0.13051 to 0.12912, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6035 - val_loss: 0.1291\n",
      "Epoch 77/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6042\n",
      "Epoch 00077: val_loss improved from 0.12912 to 0.12838, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5907 - val_loss: 0.1284\n",
      "Epoch 78/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.5956\n",
      "Epoch 00078: val_loss improved from 0.12838 to 0.12826, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6036 - val_loss: 0.1283\n",
      "Epoch 79/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.5939\n",
      "Epoch 00079: val_loss improved from 0.12826 to 0.12774, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5973 - val_loss: 0.1277\n",
      "Epoch 80/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.5901\n",
      "Epoch 00080: val_loss improved from 0.12774 to 0.12751, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5931 - val_loss: 0.1275\n",
      "Epoch 81/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.5856\n",
      "Epoch 00081: val_loss improved from 0.12751 to 0.12507, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5803 - val_loss: 0.1251\n",
      "Epoch 82/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.5751\n",
      "Epoch 00082: val_loss improved from 0.12507 to 0.12460, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5782 - val_loss: 0.1246\n",
      "Epoch 83/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.5736\n",
      "Epoch 00083: val_loss did not improve from 0.12460\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5758 - val_loss: 0.1253\n",
      "Epoch 84/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5675\n",
      "Epoch 00084: val_loss improved from 0.12460 to 0.12306, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5736 - val_loss: 0.1231\n",
      "Epoch 85/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5547\n",
      "Epoch 00085: val_loss did not improve from 0.12306\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5610 - val_loss: 0.1242\n",
      "Epoch 86/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5675\n",
      "Epoch 00086: val_loss did not improve from 0.12306\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.5661 - val_loss: 0.1251\n",
      " ###3 fold : val acc1 0.564, acc3 0.956, mae 0.241###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/35 [==================>...........] - ETA: 0s - loss: 25.2068 \n",
      "Epoch 00001: val_loss improved from inf to 23.07998, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 24.7068 - val_loss: 23.0800\n",
      "Epoch 2/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 22.4419\n",
      "Epoch 00002: val_loss improved from 23.07998 to 20.47407, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 21.9919 - val_loss: 20.4741\n",
      "Epoch 3/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 19.8088\n",
      "Epoch 00003: val_loss improved from 20.47407 to 18.02626, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 19.4030 - val_loss: 18.0263\n",
      "Epoch 4/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 17.4718\n",
      "Epoch 00004: val_loss improved from 18.02626 to 15.71788, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 17.0587 - val_loss: 15.7179\n",
      "Epoch 5/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 15.1974\n",
      "Epoch 00005: val_loss improved from 15.71788 to 13.55483, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 14.8291 - val_loss: 13.5548\n",
      "Epoch 6/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 13.0588\n",
      "Epoch 00006: val_loss improved from 13.55483 to 11.55628, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 12.7194 - val_loss: 11.5563\n",
      "Epoch 7/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 11.2055\n",
      "Epoch 00007: val_loss improved from 11.55628 to 9.75378, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.8651 - val_loss: 9.7538\n",
      "Epoch 8/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 9.5394 \n",
      "Epoch 00008: val_loss improved from 9.75378 to 8.17696, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 9.2215 - val_loss: 8.1770\n",
      "Epoch 9/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 8.0138\n",
      "Epoch 00009: val_loss improved from 8.17696 to 6.83343, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 7.7783 - val_loss: 6.8334\n",
      "Epoch 10/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 6.7987\n",
      "Epoch 00010: val_loss improved from 6.83343 to 5.72320, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 6.5972 - val_loss: 5.7232\n",
      "Epoch 11/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 5.7448\n",
      "Epoch 00011: val_loss improved from 5.72320 to 4.82409, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.6312 - val_loss: 4.8241\n",
      "Epoch 12/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 4.9633\n",
      "Epoch 00012: val_loss improved from 4.82409 to 4.11293, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.8670 - val_loss: 4.1129\n",
      "Epoch 13/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 4.3561\n",
      "Epoch 00013: val_loss improved from 4.11293 to 3.55506, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.2813 - val_loss: 3.5551\n",
      "Epoch 14/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 3.8617\n",
      "Epoch 00014: val_loss improved from 3.55506 to 3.11820, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.8112 - val_loss: 3.1182\n",
      "Epoch 15/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 3.4665\n",
      "Epoch 00015: val_loss improved from 3.11820 to 2.76842, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.4274 - val_loss: 2.7684\n",
      "Epoch 16/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.1981\n",
      "Epoch 00016: val_loss improved from 2.76842 to 2.48538, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1398 - val_loss: 2.4854\n",
      "Epoch 17/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.9864\n",
      "Epoch 00017: val_loss improved from 2.48538 to 2.24672, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9494 - val_loss: 2.2467\n",
      "Epoch 18/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.7856\n",
      "Epoch 00018: val_loss improved from 2.24672 to 2.03737, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.7426 - val_loss: 2.0374\n",
      "Epoch 19/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.5867\n",
      "Epoch 00019: val_loss improved from 2.03737 to 1.85401, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.5221 - val_loss: 1.8540\n",
      "Epoch 20/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.3887\n",
      "Epoch 00020: val_loss improved from 1.85401 to 1.68675, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3653 - val_loss: 1.6868\n",
      "Epoch 21/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.2446\n",
      "Epoch 00021: val_loss improved from 1.68675 to 1.53425, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.2308 - val_loss: 1.5342\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.1028\n",
      "Epoch 00022: val_loss improved from 1.53425 to 1.39190, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0762 - val_loss: 1.3919\n",
      "Epoch 23/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.9953\n",
      "Epoch 00023: val_loss improved from 1.39190 to 1.26369, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9539 - val_loss: 1.2637\n",
      "Epoch 24/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.8582\n",
      "Epoch 00024: val_loss improved from 1.26369 to 1.14307, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.8530 - val_loss: 1.1431\n",
      "Epoch 25/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.6926\n",
      "Epoch 00025: val_loss improved from 1.14307 to 1.03273, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.7096 - val_loss: 1.0327\n",
      "Epoch 26/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.6378\n",
      "Epoch 00026: val_loss improved from 1.03273 to 0.93064, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.6115 - val_loss: 0.9306\n",
      "Epoch 27/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.5091\n",
      "Epoch 00027: val_loss improved from 0.93064 to 0.83591, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4937 - val_loss: 0.8359\n",
      "Epoch 28/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.4333\n",
      "Epoch 00028: val_loss improved from 0.83591 to 0.75238, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4244 - val_loss: 0.7524\n",
      "Epoch 29/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.3416\n",
      "Epoch 00029: val_loss improved from 0.75238 to 0.67614, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.3319 - val_loss: 0.6761\n",
      "Epoch 30/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.2717\n",
      "Epoch 00030: val_loss improved from 0.67614 to 0.60708, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2855 - val_loss: 0.6071\n",
      "Epoch 31/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.2228\n",
      "Epoch 00031: val_loss improved from 0.60708 to 0.54651, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2123 - val_loss: 0.5465\n",
      "Epoch 32/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.1964\n",
      "Epoch 00032: val_loss improved from 0.54651 to 0.49248, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1831 - val_loss: 0.4925\n",
      "Epoch 33/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.1168\n",
      "Epoch 00033: val_loss improved from 0.49248 to 0.44406, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1175 - val_loss: 0.4441\n",
      "Epoch 34/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0920\n",
      "Epoch 00034: val_loss improved from 0.44406 to 0.40233, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0778 - val_loss: 0.4023\n",
      "Epoch 35/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0298\n",
      "Epoch 00035: val_loss improved from 0.40233 to 0.36636, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0155 - val_loss: 0.3664\n",
      "Epoch 36/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9665\n",
      "Epoch 00036: val_loss improved from 0.36636 to 0.33530, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9662 - val_loss: 0.3353\n",
      "Epoch 37/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9638\n",
      "Epoch 00037: val_loss improved from 0.33530 to 0.30787, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9610 - val_loss: 0.3079\n",
      "Epoch 38/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9169\n",
      "Epoch 00038: val_loss improved from 0.30787 to 0.28509, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9218 - val_loss: 0.2851\n",
      "Epoch 39/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.9252\n",
      "Epoch 00039: val_loss improved from 0.28509 to 0.26505, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9135 - val_loss: 0.2651\n",
      "Epoch 40/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.8904\n",
      "Epoch 00040: val_loss improved from 0.26505 to 0.24716, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.8822 - val_loss: 0.2472\n",
      "Epoch 41/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8748\n",
      "Epoch 00041: val_loss improved from 0.24716 to 0.23210, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8696 - val_loss: 0.2321\n",
      "Epoch 42/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8491\n",
      "Epoch 00042: val_loss improved from 0.23210 to 0.22018, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8435 - val_loss: 0.2202\n",
      "Epoch 43/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8245\n",
      "Epoch 00043: val_loss improved from 0.22018 to 0.20961, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8292 - val_loss: 0.2096\n",
      "Epoch 44/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8198\n",
      "Epoch 00044: val_loss improved from 0.20961 to 0.19988, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8288 - val_loss: 0.1999\n",
      "Epoch 45/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8247\n",
      "Epoch 00045: val_loss improved from 0.19988 to 0.19057, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8307 - val_loss: 0.1906\n",
      "Epoch 46/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8027\n",
      "Epoch 00046: val_loss improved from 0.19057 to 0.18374, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7957 - val_loss: 0.1837\n",
      "Epoch 47/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.7678\n",
      "Epoch 00047: val_loss improved from 0.18374 to 0.17857, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7730 - val_loss: 0.1786\n",
      "Epoch 48/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7775\n",
      "Epoch 00048: val_loss improved from 0.17857 to 0.17382, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7762 - val_loss: 0.1738\n",
      "Epoch 49/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7769\n",
      "Epoch 00049: val_loss improved from 0.17382 to 0.16954, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7683 - val_loss: 0.1695\n",
      "Epoch 50/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.7512\n",
      "Epoch 00050: val_loss improved from 0.16954 to 0.16563, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7512 - val_loss: 0.1656\n",
      "Epoch 51/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7663\n",
      "Epoch 00051: val_loss improved from 0.16563 to 0.16197, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7597 - val_loss: 0.1620\n",
      "Epoch 52/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7250\n",
      "Epoch 00052: val_loss improved from 0.16197 to 0.15808, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7286 - val_loss: 0.1581\n",
      "Epoch 53/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7460\n",
      "Epoch 00053: val_loss improved from 0.15808 to 0.15633, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7532 - val_loss: 0.1563\n",
      "Epoch 54/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7165\n",
      "Epoch 00054: val_loss improved from 0.15633 to 0.15384, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7208 - val_loss: 0.1538\n",
      "Epoch 55/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7065\n",
      "Epoch 00055: val_loss improved from 0.15384 to 0.15155, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7162 - val_loss: 0.1515\n",
      "Epoch 56/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7377\n",
      "Epoch 00056: val_loss improved from 0.15155 to 0.15069, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7146 - val_loss: 0.1507\n",
      "Epoch 57/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7117\n",
      "Epoch 00057: val_loss improved from 0.15069 to 0.14767, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7118 - val_loss: 0.1477\n",
      "Epoch 58/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6972\n",
      "Epoch 00058: val_loss improved from 0.14767 to 0.14715, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6978 - val_loss: 0.1472\n",
      "Epoch 59/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6967\n",
      "Epoch 00059: val_loss improved from 0.14715 to 0.14512, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7045 - val_loss: 0.1451\n",
      "Epoch 60/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6814\n",
      "Epoch 00060: val_loss improved from 0.14512 to 0.14460, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6886 - val_loss: 0.1446\n",
      "Epoch 61/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7000\n",
      "Epoch 00061: val_loss improved from 0.14460 to 0.14220, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6970 - val_loss: 0.1422\n",
      "Epoch 62/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6768\n",
      "Epoch 00062: val_loss improved from 0.14220 to 0.14122, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6813 - val_loss: 0.1412\n",
      "Epoch 63/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6609\n",
      "Epoch 00063: val_loss improved from 0.14122 to 0.13978, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6705 - val_loss: 0.1398\n",
      "Epoch 64/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6627\n",
      "Epoch 00064: val_loss improved from 0.13978 to 0.13853, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6665 - val_loss: 0.1385\n",
      "Epoch 65/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6638\n",
      "Epoch 00065: val_loss did not improve from 0.13853\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6591 - val_loss: 0.1388\n",
      "Epoch 66/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6568\n",
      "Epoch 00066: val_loss did not improve from 0.13853\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6598 - val_loss: 0.1394\n",
      " ###4 fold : val acc1 0.519, acc3 0.943, mae 0.270###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/35 [===================>..........] - ETA: 0s - loss: 25.1636 \n",
      "Epoch 00001: val_loss improved from inf to 23.07799, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 24.7174 - val_loss: 23.0780\n",
      "Epoch 2/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 22.4329\n",
      "Epoch 00002: val_loss improved from 23.07799 to 20.47283, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 22.0203 - val_loss: 20.4728\n",
      "Epoch 3/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 19.8270\n",
      "Epoch 00003: val_loss improved from 20.47283 to 18.02907, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 19.4276 - val_loss: 18.0291\n",
      "Epoch 4/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 17.4602\n",
      "Epoch 00004: val_loss improved from 18.02907 to 15.71981, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 17.0737 - val_loss: 15.7198\n",
      "Epoch 5/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 15.1597\n",
      "Epoch 00005: val_loss improved from 15.71981 to 13.55797, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 14.8596 - val_loss: 13.5580\n",
      "Epoch 6/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 13.0540\n",
      "Epoch 00006: val_loss improved from 13.55797 to 11.55838, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 12.7371 - val_loss: 11.5584\n",
      "Epoch 7/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 11.1696\n",
      "Epoch 00007: val_loss improved from 11.55838 to 9.75823, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.8847 - val_loss: 9.7582\n",
      "Epoch 8/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 9.5481 \n",
      "Epoch 00008: val_loss improved from 9.75823 to 8.18011, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 9.2405 - val_loss: 8.1801\n",
      "Epoch 9/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 8.0236\n",
      "Epoch 00009: val_loss improved from 8.18011 to 6.83356, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 7.7909 - val_loss: 6.8336\n",
      "Epoch 10/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 6.7971\n",
      "Epoch 00010: val_loss improved from 6.83356 to 5.72022, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 6.6067 - val_loss: 5.7202\n",
      "Epoch 11/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 5.7405\n",
      "Epoch 00011: val_loss improved from 5.72022 to 4.81857, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.6278 - val_loss: 4.8186\n",
      "Epoch 12/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 4.9680\n",
      "Epoch 00012: val_loss improved from 4.81857 to 4.10621, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.8668 - val_loss: 4.1062\n",
      "Epoch 13/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 4.3374\n",
      "Epoch 00013: val_loss improved from 4.10621 to 3.54808, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.2709 - val_loss: 3.5481\n",
      "Epoch 14/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 3.8806\n",
      "Epoch 00014: val_loss improved from 3.54808 to 3.11153, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.8137 - val_loss: 3.1115\n",
      "Epoch 15/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 3.4643\n",
      "Epoch 00015: val_loss improved from 3.11153 to 2.76511, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.4183 - val_loss: 2.7651\n",
      "Epoch 16/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.1895\n",
      "Epoch 00016: val_loss improved from 2.76511 to 2.48322, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1298 - val_loss: 2.4832\n",
      "Epoch 17/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.0061\n",
      "Epoch 00017: val_loss improved from 2.48322 to 2.24635, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9498 - val_loss: 2.2463\n",
      "Epoch 18/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.7831\n",
      "Epoch 00018: val_loss improved from 2.24635 to 2.03919, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.7460 - val_loss: 2.0392\n",
      "Epoch 19/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.5702\n",
      "Epoch 00019: val_loss improved from 2.03919 to 1.85710, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.5145 - val_loss: 1.8571\n",
      "Epoch 20/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.4037\n",
      "Epoch 00020: val_loss improved from 1.85710 to 1.69045, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3723 - val_loss: 1.6905\n",
      "Epoch 21/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.2277\n",
      "Epoch 00021: val_loss improved from 1.69045 to 1.53915, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.2169 - val_loss: 1.5392\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.1120\n",
      "Epoch 00022: val_loss improved from 1.53915 to 1.39742, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0807 - val_loss: 1.3974\n",
      "Epoch 23/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.9961\n",
      "Epoch 00023: val_loss improved from 1.39742 to 1.26959, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9469 - val_loss: 1.2696\n",
      "Epoch 24/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.8608\n",
      "Epoch 00024: val_loss improved from 1.26959 to 1.15015, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.8470 - val_loss: 1.1502\n",
      "Epoch 25/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.7047\n",
      "Epoch 00025: val_loss improved from 1.15015 to 1.03996, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.7149 - val_loss: 1.0400\n",
      "Epoch 26/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.6406\n",
      "Epoch 00026: val_loss improved from 1.03996 to 0.93806, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.6207 - val_loss: 0.9381\n",
      "Epoch 27/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.5153\n",
      "Epoch 00027: val_loss improved from 0.93806 to 0.84338, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.5095 - val_loss: 0.8434\n",
      "Epoch 28/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.4387\n",
      "Epoch 00028: val_loss improved from 0.84338 to 0.75908, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4189 - val_loss: 0.7591\n",
      "Epoch 29/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.3653\n",
      "Epoch 00029: val_loss improved from 0.75908 to 0.68290, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.3415 - val_loss: 0.6829\n",
      "Epoch 30/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.2655\n",
      "Epoch 00030: val_loss improved from 0.68290 to 0.61346, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2867 - val_loss: 0.6135\n",
      "Epoch 31/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.2306\n",
      "Epoch 00031: val_loss improved from 0.61346 to 0.55258, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2239 - val_loss: 0.5526\n",
      "Epoch 32/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.1892\n",
      "Epoch 00032: val_loss improved from 0.55258 to 0.49854, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1821 - val_loss: 0.4985\n",
      "Epoch 33/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.1323\n",
      "Epoch 00033: val_loss improved from 0.49854 to 0.45004, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1209 - val_loss: 0.4500\n",
      "Epoch 34/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.0930\n",
      "Epoch 00034: val_loss improved from 0.45004 to 0.40779, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.0784 - val_loss: 0.4078\n",
      "Epoch 35/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.0300\n",
      "Epoch 00035: val_loss improved from 0.40779 to 0.37143, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.0150 - val_loss: 0.3714\n",
      "Epoch 36/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.9732\n",
      "Epoch 00036: val_loss improved from 0.37143 to 0.34039, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.9713 - val_loss: 0.3404\n",
      "Epoch 37/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.9579\n",
      "Epoch 00037: val_loss improved from 0.34039 to 0.31250, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9579 - val_loss: 0.3125\n",
      "Epoch 38/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.9135\n",
      "Epoch 00038: val_loss improved from 0.31250 to 0.28949, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9265 - val_loss: 0.2895\n",
      "Epoch 39/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.9173\n",
      "Epoch 00039: val_loss improved from 0.28949 to 0.26924, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.9107 - val_loss: 0.2692\n",
      "Epoch 40/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8716\n",
      "Epoch 00040: val_loss improved from 0.26924 to 0.25098, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.8745 - val_loss: 0.2510\n",
      "Epoch 41/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8731\n",
      "Epoch 00041: val_loss improved from 0.25098 to 0.23544, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8655 - val_loss: 0.2354\n",
      "Epoch 42/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8588\n",
      "Epoch 00042: val_loss improved from 0.23544 to 0.22270, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8483 - val_loss: 0.2227\n",
      "Epoch 43/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8222\n",
      "Epoch 00043: val_loss improved from 0.22270 to 0.21228, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8318 - val_loss: 0.2123\n",
      "Epoch 44/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8225\n",
      "Epoch 00044: val_loss improved from 0.21228 to 0.20234, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8312 - val_loss: 0.2023\n",
      "Epoch 45/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8220\n",
      "Epoch 00045: val_loss improved from 0.20234 to 0.19284, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8315 - val_loss: 0.1928\n",
      "Epoch 46/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8087\n",
      "Epoch 00046: val_loss improved from 0.19284 to 0.18545, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.8035 - val_loss: 0.1855\n",
      "Epoch 47/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7654\n",
      "Epoch 00047: val_loss improved from 0.18545 to 0.17976, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7741 - val_loss: 0.1798\n",
      "Epoch 48/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7833\n",
      "Epoch 00048: val_loss improved from 0.17976 to 0.17530, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7769 - val_loss: 0.1753\n",
      "Epoch 49/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7720\n",
      "Epoch 00049: val_loss improved from 0.17530 to 0.17091, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7706 - val_loss: 0.1709\n",
      "Epoch 50/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7465\n",
      "Epoch 00050: val_loss improved from 0.17091 to 0.16683, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7502 - val_loss: 0.1668\n",
      "Epoch 51/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7653\n",
      "Epoch 00051: val_loss improved from 0.16683 to 0.16291, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7591 - val_loss: 0.1629\n",
      "Epoch 52/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7283\n",
      "Epoch 00052: val_loss improved from 0.16291 to 0.15897, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7303 - val_loss: 0.1590\n",
      "Epoch 53/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7546\n",
      "Epoch 00053: val_loss improved from 0.15897 to 0.15721, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7565 - val_loss: 0.1572\n",
      "Epoch 54/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7236\n",
      "Epoch 00054: val_loss improved from 0.15721 to 0.15524, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7248 - val_loss: 0.1552\n",
      "Epoch 55/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7094\n",
      "Epoch 00055: val_loss improved from 0.15524 to 0.15289, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7182 - val_loss: 0.1529\n",
      "Epoch 56/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7467\n",
      "Epoch 00056: val_loss improved from 0.15289 to 0.15131, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7224 - val_loss: 0.1513\n",
      "Epoch 57/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7056\n",
      "Epoch 00057: val_loss improved from 0.15131 to 0.14781, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7132 - val_loss: 0.1478\n",
      "Epoch 58/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6979\n",
      "Epoch 00058: val_loss improved from 0.14781 to 0.14712, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7022 - val_loss: 0.1471\n",
      "Epoch 59/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7089\n",
      "Epoch 00059: val_loss improved from 0.14712 to 0.14557, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7070 - val_loss: 0.1456\n",
      "Epoch 60/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6826\n",
      "Epoch 00060: val_loss improved from 0.14557 to 0.14511, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6890 - val_loss: 0.1451\n",
      "Epoch 61/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6953\n",
      "Epoch 00061: val_loss improved from 0.14511 to 0.14298, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6936 - val_loss: 0.1430\n",
      "Epoch 62/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6784\n",
      "Epoch 00062: val_loss improved from 0.14298 to 0.14206, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6834 - val_loss: 0.1421\n",
      "Epoch 63/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6658\n",
      "Epoch 00063: val_loss improved from 0.14206 to 0.14005, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6700 - val_loss: 0.1400\n",
      "Epoch 64/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6672\n",
      "Epoch 00064: val_loss improved from 0.14005 to 0.13872, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6639 - val_loss: 0.1387\n",
      "Epoch 65/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6641\n",
      "Epoch 00065: val_loss did not improve from 0.13872\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6614 - val_loss: 0.1388\n",
      "Epoch 66/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6591\n",
      "Epoch 00066: val_loss did not improve from 0.13872\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6633 - val_loss: 0.1398\n",
      " ###5 fold : val acc1 0.511, acc3 0.945, mae 0.274###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/35 [===================>..........] - ETA: 0s - loss: 25.1802 \n",
      "Epoch 00001: val_loss improved from inf to 23.08066, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 1s 6ms/step - loss: 24.7197 - val_loss: 23.0807\n",
      "Epoch 2/100\n",
      "25/35 [====================>.........] - ETA: 0s - loss: 22.3958\n",
      "Epoch 00002: val_loss improved from 23.08066 to 20.47534, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 22.0125 - val_loss: 20.4753\n",
      "Epoch 3/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 19.8020\n",
      "Epoch 00003: val_loss improved from 20.47534 to 18.03157, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 19.4301 - val_loss: 18.0316\n",
      "Epoch 4/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 17.4274\n",
      "Epoch 00004: val_loss improved from 18.03157 to 15.72233, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 17.0725 - val_loss: 15.7223\n",
      "Epoch 5/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 15.2219\n",
      "Epoch 00005: val_loss improved from 15.72233 to 13.56088, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 14.8593 - val_loss: 13.5609\n",
      "Epoch 6/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 13.0593\n",
      "Epoch 00006: val_loss improved from 13.56088 to 11.56061, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 12.7456 - val_loss: 11.5606\n",
      "Epoch 7/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 11.1805\n",
      "Epoch 00007: val_loss improved from 11.56061 to 9.76209, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.8753 - val_loss: 9.7621\n",
      "Epoch 8/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 9.5642 \n",
      "Epoch 00008: val_loss improved from 9.76209 to 8.18618, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 9.2358 - val_loss: 8.1862\n",
      "Epoch 9/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 7.9989\n",
      "Epoch 00009: val_loss improved from 8.18618 to 6.83999, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 7.7771 - val_loss: 6.8400\n",
      "Epoch 10/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 6.7679\n",
      "Epoch 00010: val_loss improved from 6.83999 to 5.72906, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 6.5976 - val_loss: 5.7291\n",
      "Epoch 11/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 5.7386\n",
      "Epoch 00011: val_loss improved from 5.72906 to 4.82832, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.6326 - val_loss: 4.8283\n",
      "Epoch 12/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 4.9994\n",
      "Epoch 00012: val_loss improved from 4.82832 to 4.11599, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.8785 - val_loss: 4.1160\n",
      "Epoch 13/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 4.3446\n",
      "Epoch 00013: val_loss improved from 4.11599 to 3.55749, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.2716 - val_loss: 3.5575\n",
      "Epoch 14/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.9159\n",
      "Epoch 00014: val_loss improved from 3.55749 to 3.11952, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.8302 - val_loss: 3.1195\n",
      "Epoch 15/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.4647\n",
      "Epoch 00015: val_loss improved from 3.11952 to 2.77099, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.4250 - val_loss: 2.7710\n",
      "Epoch 16/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.1871\n",
      "Epoch 00016: val_loss improved from 2.77099 to 2.48776, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1384 - val_loss: 2.4878\n",
      "Epoch 17/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.9940\n",
      "Epoch 00017: val_loss improved from 2.48776 to 2.25004, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.9507 - val_loss: 2.2500\n",
      "Epoch 18/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.7774\n",
      "Epoch 00018: val_loss improved from 2.25004 to 2.04222, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.7557 - val_loss: 2.0422\n",
      "Epoch 19/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 2.5689\n",
      "Epoch 00019: val_loss improved from 2.04222 to 1.85919, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.5223 - val_loss: 1.8592\n",
      "Epoch 20/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 2.4030\n",
      "Epoch 00020: val_loss improved from 1.85919 to 1.69231, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.3807 - val_loss: 1.6923\n",
      "Epoch 21/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.2354\n",
      "Epoch 00021: val_loss improved from 1.69231 to 1.54130, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.2237 - val_loss: 1.5413\n",
      "Epoch 22/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.1088\n",
      "Epoch 00022: val_loss improved from 1.54130 to 1.40011, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0813 - val_loss: 1.4001\n",
      "Epoch 23/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.0006\n",
      "Epoch 00023: val_loss improved from 1.40011 to 1.27222, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9548 - val_loss: 1.2722\n",
      "Epoch 24/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.8515\n",
      "Epoch 00024: val_loss improved from 1.27222 to 1.15323, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.8435 - val_loss: 1.1532\n",
      "Epoch 25/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.7070\n",
      "Epoch 00025: val_loss improved from 1.15323 to 1.04340, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7190 - val_loss: 1.0434\n",
      "Epoch 26/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.6412\n",
      "Epoch 00026: val_loss improved from 1.04340 to 0.94093, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.6292 - val_loss: 0.9409\n",
      "Epoch 27/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.5028\n",
      "Epoch 00027: val_loss improved from 0.94093 to 0.84679, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.5036 - val_loss: 0.8468\n",
      "Epoch 28/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00028: val_loss improved from 0.84679 to 0.76216, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4239 - val_loss: 0.7622\n",
      "Epoch 29/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.3671\n",
      "Epoch 00029: val_loss improved from 0.76216 to 0.68524, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.3464 - val_loss: 0.6852\n",
      "Epoch 30/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.2722\n",
      "Epoch 00030: val_loss improved from 0.68524 to 0.61578, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2889 - val_loss: 0.6158\n",
      "Epoch 31/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.2367\n",
      "Epoch 00031: val_loss improved from 0.61578 to 0.55439, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.2312 - val_loss: 0.5544\n",
      "Epoch 32/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.1965\n",
      "Epoch 00032: val_loss improved from 0.55439 to 0.49966, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1838 - val_loss: 0.4997\n",
      "Epoch 33/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.1464\n",
      "Epoch 00033: val_loss improved from 0.49966 to 0.44978, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1253 - val_loss: 0.4498\n",
      "Epoch 34/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.0866\n",
      "Epoch 00034: val_loss improved from 0.44978 to 0.40696, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.0751 - val_loss: 0.4070\n",
      "Epoch 35/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.0384\n",
      "Epoch 00035: val_loss improved from 0.40696 to 0.37044, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0185 - val_loss: 0.3704\n",
      "Epoch 36/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9706\n",
      "Epoch 00036: val_loss improved from 0.37044 to 0.33905, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9693 - val_loss: 0.3390\n",
      "Epoch 37/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.9577\n",
      "Epoch 00037: val_loss improved from 0.33905 to 0.31086, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9566 - val_loss: 0.3109\n",
      "Epoch 38/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.9225\n",
      "Epoch 00038: val_loss improved from 0.31086 to 0.28744, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.9274 - val_loss: 0.2874\n",
      "Epoch 39/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.9192\n",
      "Epoch 00039: val_loss improved from 0.28744 to 0.26718, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9073 - val_loss: 0.2672\n",
      "Epoch 40/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8724\n",
      "Epoch 00040: val_loss improved from 0.26718 to 0.24892, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8799 - val_loss: 0.2489\n",
      "Epoch 41/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8746\n",
      "Epoch 00041: val_loss improved from 0.24892 to 0.23308, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8693 - val_loss: 0.2331\n",
      "Epoch 42/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.8584\n",
      "Epoch 00042: val_loss improved from 0.23308 to 0.22014, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.8467 - val_loss: 0.2201\n",
      "Epoch 43/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8289\n",
      "Epoch 00043: val_loss improved from 0.22014 to 0.20944, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.8315 - val_loss: 0.2094\n",
      "Epoch 44/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8211\n",
      "Epoch 00044: val_loss improved from 0.20944 to 0.19956, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8246 - val_loss: 0.1996\n",
      "Epoch 45/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8097\n",
      "Epoch 00045: val_loss improved from 0.19956 to 0.19029, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8249 - val_loss: 0.1903\n",
      "Epoch 46/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8030\n",
      "Epoch 00046: val_loss improved from 0.19029 to 0.18286, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7993 - val_loss: 0.1829\n",
      "Epoch 47/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7778\n",
      "Epoch 00047: val_loss improved from 0.18286 to 0.17712, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7782 - val_loss: 0.1771\n",
      "Epoch 48/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7828\n",
      "Epoch 00048: val_loss improved from 0.17712 to 0.17284, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7764 - val_loss: 0.1728\n",
      "Epoch 49/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7713\n",
      "Epoch 00049: val_loss improved from 0.17284 to 0.16831, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7678 - val_loss: 0.1683\n",
      "Epoch 50/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7465\n",
      "Epoch 00050: val_loss improved from 0.16831 to 0.16412, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7473 - val_loss: 0.1641\n",
      "Epoch 51/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7576\n",
      "Epoch 00051: val_loss improved from 0.16412 to 0.16069, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7498 - val_loss: 0.1607\n",
      "Epoch 52/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7266\n",
      "Epoch 00052: val_loss improved from 0.16069 to 0.15669, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7255 - val_loss: 0.1567\n",
      "Epoch 53/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7391\n",
      "Epoch 00053: val_loss improved from 0.15669 to 0.15515, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7441 - val_loss: 0.1552\n",
      "Epoch 54/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7126\n",
      "Epoch 00054: val_loss improved from 0.15515 to 0.15277, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7173 - val_loss: 0.1528\n",
      "Epoch 55/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7108\n",
      "Epoch 00055: val_loss improved from 0.15277 to 0.15082, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7195 - val_loss: 0.1508\n",
      "Epoch 56/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7339\n",
      "Epoch 00056: val_loss improved from 0.15082 to 0.14925, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7164 - val_loss: 0.1492\n",
      "Epoch 57/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7017\n",
      "Epoch 00057: val_loss improved from 0.14925 to 0.14589, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7088 - val_loss: 0.1459\n",
      "Epoch 58/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7066\n",
      "Epoch 00058: val_loss improved from 0.14589 to 0.14453, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7050 - val_loss: 0.1445\n",
      "Epoch 59/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7061\n",
      "Epoch 00059: val_loss improved from 0.14453 to 0.14367, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7010 - val_loss: 0.1437\n",
      "Epoch 60/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6785\n",
      "Epoch 00060: val_loss improved from 0.14367 to 0.14316, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6918 - val_loss: 0.1432\n",
      "Epoch 61/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6865\n",
      "Epoch 00061: val_loss improved from 0.14316 to 0.14125, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6865 - val_loss: 0.1412\n",
      "Epoch 62/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6667\n",
      "Epoch 00062: val_loss improved from 0.14125 to 0.14025, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6770 - val_loss: 0.1403\n",
      "Epoch 63/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6685\n",
      "Epoch 00063: val_loss improved from 0.14025 to 0.13874, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6715 - val_loss: 0.1387\n",
      "Epoch 64/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6625\n",
      "Epoch 00064: val_loss improved from 0.13874 to 0.13759, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6606 - val_loss: 0.1376\n",
      "Epoch 65/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6660\n",
      "Epoch 00065: val_loss did not improve from 0.13759\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6584 - val_loss: 0.1378\n",
      "Epoch 66/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6557\n",
      "Epoch 00066: val_loss did not improve from 0.13759\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6599 - val_loss: 0.1389\n",
      " ###6 fold : val acc1 0.534, acc3 0.952, mae 0.257###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/35 [==================>...........] - ETA: 0s - loss: 25.2164 \n",
      "Epoch 00001: val_loss improved from inf to 23.07689, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 24.7340 - val_loss: 23.0769\n",
      "Epoch 2/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 22.4399\n",
      "Epoch 00002: val_loss improved from 23.07689 to 20.47206, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 22.0094 - val_loss: 20.4721\n",
      "Epoch 3/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 19.9023\n",
      "Epoch 00003: val_loss improved from 20.47206 to 18.02898, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 19.4444 - val_loss: 18.0290\n",
      "Epoch 4/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 17.4839\n",
      "Epoch 00004: val_loss improved from 18.02898 to 15.71885, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 17.0774 - val_loss: 15.7189\n",
      "Epoch 5/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 15.2303\n",
      "Epoch 00005: val_loss improved from 15.71885 to 13.55708, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 14.8649 - val_loss: 13.5571\n",
      "Epoch 6/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 13.0784\n",
      "Epoch 00006: val_loss improved from 13.55708 to 11.55676, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 12.7360 - val_loss: 11.5568\n",
      "Epoch 7/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 11.2444\n",
      "Epoch 00007: val_loss improved from 11.55676 to 9.75756, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.8820 - val_loss: 9.7576\n",
      "Epoch 8/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 9.5664 \n",
      "Epoch 00008: val_loss improved from 9.75756 to 8.18180, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 9.2402 - val_loss: 8.1818\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 8.0284\n",
      "Epoch 00009: val_loss improved from 8.18180 to 6.83470, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 7.7774 - val_loss: 6.8347\n",
      "Epoch 10/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 6.8178\n",
      "Epoch 00010: val_loss improved from 6.83470 to 5.72377, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 6.5903 - val_loss: 5.7238\n",
      "Epoch 11/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 5.8009\n",
      "Epoch 00011: val_loss improved from 5.72377 to 4.82323, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 5.6317 - val_loss: 4.8232\n",
      "Epoch 12/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 4.9787\n",
      "Epoch 00012: val_loss improved from 4.82323 to 4.11127, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.8713 - val_loss: 4.1113\n",
      "Epoch 13/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 4.3350\n",
      "Epoch 00013: val_loss improved from 4.11127 to 3.55238, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.2648 - val_loss: 3.5524\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.9321\n",
      "Epoch 00014: val_loss improved from 3.55238 to 3.11382, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.8370 - val_loss: 3.1138\n",
      "Epoch 15/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.4872\n",
      "Epoch 00015: val_loss improved from 3.11382 to 2.76590, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.4253 - val_loss: 2.7659\n",
      "Epoch 16/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.1802\n",
      "Epoch 00016: val_loss improved from 2.76590 to 2.48206, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1344 - val_loss: 2.4821\n",
      "Epoch 17/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.9839\n",
      "Epoch 00017: val_loss improved from 2.48206 to 2.24432, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9505 - val_loss: 2.2443\n",
      "Epoch 18/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.7626\n",
      "Epoch 00018: val_loss improved from 2.24432 to 2.03653, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.7515 - val_loss: 2.0365\n",
      "Epoch 19/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.5919\n",
      "Epoch 00019: val_loss improved from 2.03653 to 1.85250, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.5210 - val_loss: 1.8525\n",
      "Epoch 20/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.3883\n",
      "Epoch 00020: val_loss improved from 1.85250 to 1.68550, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3653 - val_loss: 1.6855\n",
      "Epoch 21/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.2307\n",
      "Epoch 00021: val_loss improved from 1.68550 to 1.53408, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.2179 - val_loss: 1.5341\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.1202\n",
      "Epoch 00022: val_loss improved from 1.53408 to 1.39284, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0702 - val_loss: 1.3928\n",
      "Epoch 23/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.9997\n",
      "Epoch 00023: val_loss improved from 1.39284 to 1.26493, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9431 - val_loss: 1.2649\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.8348\n",
      "Epoch 00024: val_loss improved from 1.26493 to 1.14666, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.8235 - val_loss: 1.1467\n",
      "Epoch 25/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.7120\n",
      "Epoch 00025: val_loss improved from 1.14666 to 1.03729, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.7136 - val_loss: 1.0373\n",
      "Epoch 26/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.6375\n",
      "Epoch 00026: val_loss improved from 1.03729 to 0.93523, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.6246 - val_loss: 0.9352\n",
      "Epoch 27/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.4863\n",
      "Epoch 00027: val_loss improved from 0.93523 to 0.84225, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4968 - val_loss: 0.8422\n",
      "Epoch 28/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.4352\n",
      "Epoch 00028: val_loss improved from 0.84225 to 0.75829, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4144 - val_loss: 0.7583\n",
      "Epoch 29/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.3566\n",
      "Epoch 00029: val_loss improved from 0.75829 to 0.68182, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.3275 - val_loss: 0.6818\n",
      "Epoch 30/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.2786\n",
      "Epoch 00030: val_loss improved from 0.68182 to 0.61282, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2818 - val_loss: 0.6128\n",
      "Epoch 31/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.2302\n",
      "Epoch 00031: val_loss improved from 0.61282 to 0.55187, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2232 - val_loss: 0.5519\n",
      "Epoch 32/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.1880\n",
      "Epoch 00032: val_loss improved from 0.55187 to 0.49805, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1786 - val_loss: 0.4981\n",
      "Epoch 33/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.1347\n",
      "Epoch 00033: val_loss improved from 0.49805 to 0.44885, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1233 - val_loss: 0.4489\n",
      "Epoch 34/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0780\n",
      "Epoch 00034: val_loss improved from 0.44885 to 0.40707, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0632 - val_loss: 0.4071\n",
      "Epoch 35/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0483\n",
      "Epoch 00035: val_loss improved from 0.40707 to 0.37054, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0184 - val_loss: 0.3705\n",
      "Epoch 36/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9789\n",
      "Epoch 00036: val_loss improved from 0.37054 to 0.33973, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9769 - val_loss: 0.3397\n",
      "Epoch 37/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.9550\n",
      "Epoch 00037: val_loss improved from 0.33973 to 0.31165, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9523 - val_loss: 0.3116\n",
      "Epoch 38/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9077\n",
      "Epoch 00038: val_loss improved from 0.31165 to 0.28842, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9226 - val_loss: 0.2884\n",
      "Epoch 39/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9089\n",
      "Epoch 00039: val_loss improved from 0.28842 to 0.26859, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9029 - val_loss: 0.2686\n",
      "Epoch 40/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8790\n",
      "Epoch 00040: val_loss improved from 0.26859 to 0.25059, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8819 - val_loss: 0.2506\n",
      "Epoch 41/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8776\n",
      "Epoch 00041: val_loss improved from 0.25059 to 0.23481, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8715 - val_loss: 0.2348\n",
      "Epoch 42/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8515\n",
      "Epoch 00042: val_loss improved from 0.23481 to 0.22220, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8427 - val_loss: 0.2222\n",
      "Epoch 43/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8268\n",
      "Epoch 00043: val_loss improved from 0.22220 to 0.21127, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8353 - val_loss: 0.2113\n",
      "Epoch 44/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8195\n",
      "Epoch 00044: val_loss improved from 0.21127 to 0.20127, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.8286 - val_loss: 0.2013\n",
      "Epoch 45/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.8094\n",
      "Epoch 00045: val_loss improved from 0.20127 to 0.19195, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8258 - val_loss: 0.1920\n",
      "Epoch 46/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8029\n",
      "Epoch 00046: val_loss improved from 0.19195 to 0.18447, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8031 - val_loss: 0.1845\n",
      "Epoch 47/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7836\n",
      "Epoch 00047: val_loss improved from 0.18447 to 0.17923, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7812 - val_loss: 0.1792\n",
      "Epoch 48/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7805\n",
      "Epoch 00048: val_loss improved from 0.17923 to 0.17464, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7743 - val_loss: 0.1746\n",
      "Epoch 49/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7690\n",
      "Epoch 00049: val_loss improved from 0.17464 to 0.17040, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7682 - val_loss: 0.1704\n",
      "Epoch 50/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7474\n",
      "Epoch 00050: val_loss improved from 0.17040 to 0.16619, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7453 - val_loss: 0.1662\n",
      "Epoch 51/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7609\n",
      "Epoch 00051: val_loss improved from 0.16619 to 0.16258, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7544 - val_loss: 0.1626\n",
      "Epoch 52/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7262\n",
      "Epoch 00052: val_loss improved from 0.16258 to 0.15870, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7308 - val_loss: 0.1587\n",
      "Epoch 53/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7401\n",
      "Epoch 00053: val_loss improved from 0.15870 to 0.15722, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7516 - val_loss: 0.1572\n",
      "Epoch 54/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7204\n",
      "Epoch 00054: val_loss improved from 0.15722 to 0.15511, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7273 - val_loss: 0.1551\n",
      "Epoch 55/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7137\n",
      "Epoch 00055: val_loss improved from 0.15511 to 0.15298, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7267 - val_loss: 0.1530\n",
      "Epoch 56/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7449\n",
      "Epoch 00056: val_loss improved from 0.15298 to 0.15100, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7222 - val_loss: 0.1510\n",
      "Epoch 57/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7119\n",
      "Epoch 00057: val_loss improved from 0.15100 to 0.14775, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7142 - val_loss: 0.1478\n",
      "Epoch 58/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7106\n",
      "Epoch 00058: val_loss improved from 0.14775 to 0.14646, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7098 - val_loss: 0.1465\n",
      "Epoch 59/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7042\n",
      "Epoch 00059: val_loss improved from 0.14646 to 0.14571, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6981 - val_loss: 0.1457\n",
      "Epoch 60/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6841\n",
      "Epoch 00060: val_loss improved from 0.14571 to 0.14476, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6923 - val_loss: 0.1448\n",
      "Epoch 61/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6832\n",
      "Epoch 00061: val_loss improved from 0.14476 to 0.14271, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6849 - val_loss: 0.1427\n",
      "Epoch 62/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6627\n",
      "Epoch 00062: val_loss improved from 0.14271 to 0.14175, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6815 - val_loss: 0.1417\n",
      "Epoch 63/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6720\n",
      "Epoch 00063: val_loss improved from 0.14175 to 0.14019, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6719 - val_loss: 0.1402\n",
      "Epoch 64/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6688\n",
      "Epoch 00064: val_loss improved from 0.14019 to 0.13920, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6602 - val_loss: 0.1392\n",
      "Epoch 65/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6711\n",
      "Epoch 00065: val_loss did not improve from 0.13920\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6625 - val_loss: 0.1395\n",
      "Epoch 66/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6527\n",
      "Epoch 00066: val_loss did not improve from 0.13920\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6572 - val_loss: 0.1401\n",
      " ###7 fold : val acc1 0.518, acc3 0.934, mae 0.275###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/35 [=================>............] - ETA: 0s - loss: 25.2983 \n",
      "Epoch 00001: val_loss improved from inf to 23.10798, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 24.7608 - val_loss: 23.1080\n",
      "Epoch 2/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 22.4497\n",
      "Epoch 00002: val_loss improved from 23.10798 to 20.49944, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 22.0313 - val_loss: 20.4994\n",
      "Epoch 3/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 19.9206\n",
      "Epoch 00003: val_loss improved from 20.49944 to 18.05006, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 19.4694 - val_loss: 18.0501\n",
      "Epoch 4/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 17.5301\n",
      "Epoch 00004: val_loss improved from 18.05006 to 15.73540, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 17.0969 - val_loss: 15.7354\n",
      "Epoch 5/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 15.2675\n",
      "Epoch 00005: val_loss improved from 15.73540 to 13.56771, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 14.8838 - val_loss: 13.5677\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 13.1178\n",
      "Epoch 00006: val_loss improved from 13.56771 to 11.56143, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 12.7516 - val_loss: 11.5614\n",
      "Epoch 7/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 11.2920\n",
      "Epoch 00007: val_loss improved from 11.56143 to 9.75791, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 10.8942 - val_loss: 9.7579\n",
      "Epoch 8/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 9.6283 \n",
      "Epoch 00008: val_loss improved from 9.75791 to 8.17754, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 9.2505 - val_loss: 8.1775\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 8.0345\n",
      "Epoch 00009: val_loss improved from 8.17754 to 6.82915, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 7.7885 - val_loss: 6.8291\n",
      "Epoch 10/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 6.8019\n",
      "Epoch 00010: val_loss improved from 6.82915 to 5.71753, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 6.5934 - val_loss: 5.7175\n",
      "Epoch 11/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 5.8038\n",
      "Epoch 00011: val_loss improved from 5.71753 to 4.81775, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 5.6375 - val_loss: 4.8177\n",
      "Epoch 12/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 4.9941\n",
      "Epoch 00012: val_loss improved from 4.81775 to 4.10688, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.8743 - val_loss: 4.1069\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 4.3347\n",
      "Epoch 00013: val_loss improved from 4.10688 to 3.55142, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.2694 - val_loss: 3.5514\n",
      "Epoch 14/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.9192\n",
      "Epoch 00014: val_loss improved from 3.55142 to 3.11671, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.8358 - val_loss: 3.1167\n",
      "Epoch 15/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 3.5195\n",
      "Epoch 00015: val_loss improved from 3.11671 to 2.77244, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.4288 - val_loss: 2.7724\n",
      "Epoch 16/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 3.1931\n",
      "Epoch 00016: val_loss improved from 2.77244 to 2.49233, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1418 - val_loss: 2.4923\n",
      "Epoch 17/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.9807\n",
      "Epoch 00017: val_loss improved from 2.49233 to 2.25908, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9518 - val_loss: 2.2591\n",
      "Epoch 18/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.7899\n",
      "Epoch 00018: val_loss improved from 2.25908 to 2.05452, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.7533 - val_loss: 2.0545\n",
      "Epoch 19/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.5984\n",
      "Epoch 00019: val_loss improved from 2.05452 to 1.87255, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.5247 - val_loss: 1.8726\n",
      "Epoch 20/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.3941\n",
      "Epoch 00020: val_loss improved from 1.87255 to 1.70717, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3715 - val_loss: 1.7072\n",
      "Epoch 21/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.2352\n",
      "Epoch 00021: val_loss improved from 1.70717 to 1.55731, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.2228 - val_loss: 1.5573\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.1276\n",
      "Epoch 00022: val_loss improved from 1.55731 to 1.41660, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0767 - val_loss: 1.4166\n",
      "Epoch 23/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.0097\n",
      "Epoch 00023: val_loss improved from 1.41660 to 1.28809, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9540 - val_loss: 1.2881\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.8356\n",
      "Epoch 00024: val_loss improved from 1.28809 to 1.16936, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.8237 - val_loss: 1.1694\n",
      "Epoch 25/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.7170\n",
      "Epoch 00025: val_loss improved from 1.16936 to 1.05950, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.7152 - val_loss: 1.0595\n",
      "Epoch 26/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.6350\n",
      "Epoch 00026: val_loss improved from 1.05950 to 0.95713, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.6246 - val_loss: 0.9571\n",
      "Epoch 27/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.4931\n",
      "Epoch 00027: val_loss improved from 0.95713 to 0.86251, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4992 - val_loss: 0.8625\n",
      "Epoch 28/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.4393\n",
      "Epoch 00028: val_loss improved from 0.86251 to 0.77747, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4194 - val_loss: 0.7775\n",
      "Epoch 29/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.3677\n",
      "Epoch 00029: val_loss improved from 0.77747 to 0.69970, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.3367 - val_loss: 0.6997\n",
      "Epoch 30/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.2850\n",
      "Epoch 00030: val_loss improved from 0.69970 to 0.62916, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2811 - val_loss: 0.6292\n",
      "Epoch 31/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.2306\n",
      "Epoch 00031: val_loss improved from 0.62916 to 0.56718, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2243 - val_loss: 0.5672\n",
      "Epoch 32/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.1839\n",
      "Epoch 00032: val_loss improved from 0.56718 to 0.51223, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1788 - val_loss: 0.5122\n",
      "Epoch 33/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.1380\n",
      "Epoch 00033: val_loss improved from 0.51223 to 0.46203, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1245 - val_loss: 0.4620\n",
      "Epoch 34/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0799\n",
      "Epoch 00034: val_loss improved from 0.46203 to 0.41957, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0631 - val_loss: 0.4196\n",
      "Epoch 35/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0486\n",
      "Epoch 00035: val_loss improved from 0.41957 to 0.38216, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0173 - val_loss: 0.3822\n",
      "Epoch 36/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.9803\n",
      "Epoch 00036: val_loss improved from 0.38216 to 0.35079, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9802 - val_loss: 0.3508\n",
      "Epoch 37/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.9626\n",
      "Epoch 00037: val_loss improved from 0.35079 to 0.32211, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9551 - val_loss: 0.3221\n",
      "Epoch 38/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.9106\n",
      "Epoch 00038: val_loss improved from 0.32211 to 0.29866, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9224 - val_loss: 0.2987\n",
      "Epoch 39/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.9109\n",
      "Epoch 00039: val_loss improved from 0.29866 to 0.27878, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9023 - val_loss: 0.2788\n",
      "Epoch 40/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8787\n",
      "Epoch 00040: val_loss improved from 0.27878 to 0.26017, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8849 - val_loss: 0.2602\n",
      "Epoch 41/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8804\n",
      "Epoch 00041: val_loss improved from 0.26017 to 0.24389, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8750 - val_loss: 0.2439\n",
      "Epoch 42/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8548\n",
      "Epoch 00042: val_loss improved from 0.24389 to 0.23073, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8450 - val_loss: 0.2307\n",
      "Epoch 43/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8257\n",
      "Epoch 00043: val_loss improved from 0.23073 to 0.21941, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8342 - val_loss: 0.2194\n",
      "Epoch 44/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8297\n",
      "Epoch 00044: val_loss improved from 0.21941 to 0.20924, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8310 - val_loss: 0.2092\n",
      "Epoch 45/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8098\n",
      "Epoch 00045: val_loss improved from 0.20924 to 0.19959, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8248 - val_loss: 0.1996\n",
      "Epoch 46/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8039\n",
      "Epoch 00046: val_loss improved from 0.19959 to 0.19178, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8043 - val_loss: 0.1918\n",
      "Epoch 47/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7906\n",
      "Epoch 00047: val_loss improved from 0.19178 to 0.18626, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7846 - val_loss: 0.1863\n",
      "Epoch 48/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7732\n",
      "Epoch 00048: val_loss improved from 0.18626 to 0.18149, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7718 - val_loss: 0.1815\n",
      "Epoch 49/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7763\n",
      "Epoch 00049: val_loss improved from 0.18149 to 0.17718, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7711 - val_loss: 0.1772\n",
      "Epoch 50/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7459\n",
      "Epoch 00050: val_loss improved from 0.17718 to 0.17285, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7449 - val_loss: 0.1728\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.7535\n",
      "Epoch 00051: val_loss improved from 0.17285 to 0.16910, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7535 - val_loss: 0.1691\n",
      "Epoch 52/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7275\n",
      "Epoch 00052: val_loss improved from 0.16910 to 0.16510, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7314 - val_loss: 0.1651\n",
      "Epoch 53/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7407\n",
      "Epoch 00053: val_loss improved from 0.16510 to 0.16361, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7523 - val_loss: 0.1636\n",
      "Epoch 54/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7245\n",
      "Epoch 00054: val_loss improved from 0.16361 to 0.16136, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7317 - val_loss: 0.1614\n",
      "Epoch 55/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7186\n",
      "Epoch 00055: val_loss improved from 0.16136 to 0.15882, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7301 - val_loss: 0.1588\n",
      "Epoch 56/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7484\n",
      "Epoch 00056: val_loss improved from 0.15882 to 0.15674, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7260 - val_loss: 0.1567\n",
      "Epoch 57/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7163\n",
      "Epoch 00057: val_loss improved from 0.15674 to 0.15360, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7158 - val_loss: 0.1536\n",
      "Epoch 58/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.7130\n",
      "Epoch 00058: val_loss improved from 0.15360 to 0.15221, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7140 - val_loss: 0.1522\n",
      "Epoch 59/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7063\n",
      "Epoch 00059: val_loss improved from 0.15221 to 0.15137, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7016 - val_loss: 0.1514\n",
      "Epoch 60/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6888\n",
      "Epoch 00060: val_loss improved from 0.15137 to 0.15010, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6958 - val_loss: 0.1501\n",
      "Epoch 61/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6849\n",
      "Epoch 00061: val_loss improved from 0.15010 to 0.14820, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6862 - val_loss: 0.1482\n",
      "Epoch 62/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6666\n",
      "Epoch 00062: val_loss improved from 0.14820 to 0.14703, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6862 - val_loss: 0.1470\n",
      "Epoch 63/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6744\n",
      "Epoch 00063: val_loss improved from 0.14703 to 0.14549, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6740 - val_loss: 0.1455\n",
      "Epoch 64/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6634\n",
      "Epoch 00064: val_loss improved from 0.14549 to 0.14445, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6587 - val_loss: 0.1445\n",
      "Epoch 65/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6738\n",
      "Epoch 00065: val_loss did not improve from 0.14445\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6644 - val_loss: 0.1445\n",
      "Epoch 66/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6554\n",
      "Epoch 00066: val_loss did not improve from 0.14445\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6588 - val_loss: 0.1453\n",
      " ###8 fold : val acc1 0.526, acc3 0.945, mae 0.265###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/35 [==================>...........] - ETA: 0s - loss: 25.2457 \n",
      "Epoch 00001: val_loss improved from inf to 22.98147, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 24.7608 - val_loss: 22.9815\n",
      "Epoch 2/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 22.4497\n",
      "Epoch 00002: val_loss improved from 22.98147 to 20.39536, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 22.0313 - val_loss: 20.3954\n",
      "Epoch 3/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 19.9206\n",
      "Epoch 00003: val_loss improved from 20.39536 to 17.96573, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 19.4694 - val_loss: 17.9657\n",
      "Epoch 4/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 17.5062\n",
      "Epoch 00004: val_loss improved from 17.96573 to 15.67017, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 17.0969 - val_loss: 15.6702\n",
      "Epoch 5/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 15.2609\n",
      "Epoch 00005: val_loss improved from 15.67017 to 13.52022, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 14.8838 - val_loss: 13.5202\n",
      "Epoch 6/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 13.0996\n",
      "Epoch 00006: val_loss improved from 13.52022 to 11.52963, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 12.7516 - val_loss: 11.5296\n",
      "Epoch 7/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 11.2549\n",
      "Epoch 00007: val_loss improved from 11.52963 to 9.73768, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.8942 - val_loss: 9.7377\n",
      "Epoch 8/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 9.6129 \n",
      "Epoch 00008: val_loss improved from 9.73768 to 8.16575, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 9.2505 - val_loss: 8.1657\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 8.0345\n",
      "Epoch 00009: val_loss improved from 8.16575 to 6.82270, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 7.7885 - val_loss: 6.8227\n",
      "Epoch 10/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 6.7763\n",
      "Epoch 00010: val_loss improved from 6.82270 to 5.71391, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 6.5934 - val_loss: 5.7139\n",
      "Epoch 11/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 5.7782\n",
      "Epoch 00011: val_loss improved from 5.71391 to 4.81530, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.6375 - val_loss: 4.8153\n",
      "Epoch 12/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 4.9941\n",
      "Epoch 00012: val_loss improved from 4.81530 to 4.10452, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.8743 - val_loss: 4.1045\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 4.3347\n",
      "Epoch 00013: val_loss improved from 4.10452 to 3.54840, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.2694 - val_loss: 3.5484\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.9215\n",
      "Epoch 00014: val_loss improved from 3.54840 to 3.11277, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.8358 - val_loss: 3.1128\n",
      "Epoch 15/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 3.4801\n",
      "Epoch 00015: val_loss improved from 3.11277 to 2.76749, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.4288 - val_loss: 2.7675\n",
      "Epoch 16/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.1881\n",
      "Epoch 00016: val_loss improved from 2.76749 to 2.48661, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1418 - val_loss: 2.4866\n",
      "Epoch 17/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.9809\n",
      "Epoch 00017: val_loss improved from 2.48661 to 2.25282, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.9518 - val_loss: 2.2528\n",
      "Epoch 18/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.7718\n",
      "Epoch 00018: val_loss improved from 2.25282 to 2.04824, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.7533 - val_loss: 2.0482\n",
      "Epoch 19/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.5941\n",
      "Epoch 00019: val_loss improved from 2.04824 to 1.86657, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.5247 - val_loss: 1.8666\n",
      "Epoch 20/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.3917\n",
      "Epoch 00020: val_loss improved from 1.86657 to 1.70158, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.3715 - val_loss: 1.7016\n",
      "Epoch 21/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.2374\n",
      "Epoch 00021: val_loss improved from 1.70158 to 1.55218, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.2228 - val_loss: 1.5522\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.1276\n",
      "Epoch 00022: val_loss improved from 1.55218 to 1.41197, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0767 - val_loss: 1.4120\n",
      "Epoch 23/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.0097\n",
      "Epoch 00023: val_loss improved from 1.41197 to 1.28402, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9540 - val_loss: 1.2840\n",
      "Epoch 24/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.8392\n",
      "Epoch 00024: val_loss improved from 1.28402 to 1.16596, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.8237 - val_loss: 1.1660\n",
      "Epoch 25/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.7166\n",
      "Epoch 00025: val_loss improved from 1.16596 to 1.05687, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.7152 - val_loss: 1.0569\n",
      "Epoch 26/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.6461\n",
      "Epoch 00026: val_loss improved from 1.05687 to 0.95526, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.6246 - val_loss: 0.9553\n",
      "Epoch 27/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.4931\n",
      "Epoch 00027: val_loss improved from 0.95526 to 0.86130, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4992 - val_loss: 0.8613\n",
      "Epoch 28/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.4393\n",
      "Epoch 00028: val_loss improved from 0.86130 to 0.77698, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.4194 - val_loss: 0.7770\n",
      "Epoch 29/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.3626\n",
      "Epoch 00029: val_loss improved from 0.77698 to 0.70001, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.3367 - val_loss: 0.7000\n",
      "Epoch 30/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.2850\n",
      "Epoch 00030: val_loss improved from 0.70001 to 0.62999, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2811 - val_loss: 0.6300\n",
      "Epoch 31/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.2306\n",
      "Epoch 00031: val_loss improved from 0.62999 to 0.56858, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2243 - val_loss: 0.5686\n",
      "Epoch 32/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.1899\n",
      "Epoch 00032: val_loss improved from 0.56858 to 0.51425, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1788 - val_loss: 0.5142\n",
      "Epoch 33/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.1380\n",
      "Epoch 00033: val_loss improved from 0.51425 to 0.46444, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1245 - val_loss: 0.4644\n",
      "Epoch 34/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0799\n",
      "Epoch 00034: val_loss improved from 0.46444 to 0.42245, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0631 - val_loss: 0.4225\n",
      "Epoch 35/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.0486\n",
      "Epoch 00035: val_loss improved from 0.42245 to 0.38515, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.0173 - val_loss: 0.3852\n",
      "Epoch 36/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9811\n",
      "Epoch 00036: val_loss improved from 0.38515 to 0.35410, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9802 - val_loss: 0.3541\n",
      "Epoch 37/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9621\n",
      "Epoch 00037: val_loss improved from 0.35410 to 0.32537, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9551 - val_loss: 0.3254\n",
      "Epoch 38/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.9106\n",
      "Epoch 00038: val_loss improved from 0.32537 to 0.30213, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9224 - val_loss: 0.3021\n",
      "Epoch 39/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9102\n",
      "Epoch 00039: val_loss improved from 0.30213 to 0.28259, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9023 - val_loss: 0.2826\n",
      "Epoch 40/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8768\n",
      "Epoch 00040: val_loss improved from 0.28259 to 0.26399, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8849 - val_loss: 0.2640\n",
      "Epoch 41/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8834\n",
      "Epoch 00041: val_loss improved from 0.26399 to 0.24778, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8750 - val_loss: 0.2478\n",
      "Epoch 42/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8533\n",
      "Epoch 00042: val_loss improved from 0.24778 to 0.23469, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8450 - val_loss: 0.2347\n",
      "Epoch 43/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8239\n",
      "Epoch 00043: val_loss improved from 0.23469 to 0.22350, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8342 - val_loss: 0.2235\n",
      "Epoch 44/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8200\n",
      "Epoch 00044: val_loss improved from 0.22350 to 0.21347, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8310 - val_loss: 0.2135\n",
      "Epoch 45/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8064\n",
      "Epoch 00045: val_loss improved from 0.21347 to 0.20386, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8248 - val_loss: 0.2039\n",
      "Epoch 46/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8039\n",
      "Epoch 00046: val_loss improved from 0.20386 to 0.19616, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8043 - val_loss: 0.1962\n",
      "Epoch 47/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7906\n",
      "Epoch 00047: val_loss improved from 0.19616 to 0.19094, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7846 - val_loss: 0.1909\n",
      "Epoch 48/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7759\n",
      "Epoch 00048: val_loss improved from 0.19094 to 0.18623, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7718 - val_loss: 0.1862\n",
      "Epoch 49/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7742\n",
      "Epoch 00049: val_loss improved from 0.18623 to 0.18211, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7711 - val_loss: 0.1821\n",
      "Epoch 50/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7459\n",
      "Epoch 00050: val_loss improved from 0.18211 to 0.17772, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7449 - val_loss: 0.1777\n",
      "Epoch 51/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7563\n",
      "Epoch 00051: val_loss improved from 0.17772 to 0.17392, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7535 - val_loss: 0.1739\n",
      "Epoch 52/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7275\n",
      "Epoch 00052: val_loss improved from 0.17392 to 0.16968, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7314 - val_loss: 0.1697\n",
      "Epoch 53/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7407\n",
      "Epoch 00053: val_loss improved from 0.16968 to 0.16823, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7523 - val_loss: 0.1682\n",
      "Epoch 54/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7244\n",
      "Epoch 00054: val_loss improved from 0.16823 to 0.16602, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7317 - val_loss: 0.1660\n",
      "Epoch 55/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7181\n",
      "Epoch 00055: val_loss improved from 0.16602 to 0.16349, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7301 - val_loss: 0.1635\n",
      "Epoch 56/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7484\n",
      "Epoch 00056: val_loss improved from 0.16349 to 0.16131, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7260 - val_loss: 0.1613\n",
      "Epoch 57/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7110\n",
      "Epoch 00057: val_loss improved from 0.16131 to 0.15810, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7158 - val_loss: 0.1581\n",
      "Epoch 58/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.7140\n",
      "Epoch 00058: val_loss improved from 0.15810 to 0.15672, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7140 - val_loss: 0.1567\n",
      "Epoch 59/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.7063\n",
      "Epoch 00059: val_loss improved from 0.15672 to 0.15586, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7016 - val_loss: 0.1559\n",
      "Epoch 60/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6859\n",
      "Epoch 00060: val_loss improved from 0.15586 to 0.15458, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6958 - val_loss: 0.1546\n",
      "Epoch 61/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6831\n",
      "Epoch 00061: val_loss improved from 0.15458 to 0.15264, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6862 - val_loss: 0.1526\n",
      "Epoch 62/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6719\n",
      "Epoch 00062: val_loss improved from 0.15264 to 0.15145, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6862 - val_loss: 0.1515\n",
      "Epoch 63/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6721\n",
      "Epoch 00063: val_loss improved from 0.15145 to 0.14990, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6740 - val_loss: 0.1499\n",
      "Epoch 64/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.6607\n",
      "Epoch 00064: val_loss improved from 0.14990 to 0.14881, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6587 - val_loss: 0.1488\n",
      "Epoch 65/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.6728\n",
      "Epoch 00065: val_loss did not improve from 0.14881\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6644 - val_loss: 0.1489\n",
      "Epoch 66/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6541\n",
      "Epoch 00066: val_loss did not improve from 0.14881\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6588 - val_loss: 0.1499\n",
      " ###9 fold : val acc1 0.529, acc3 0.956, mae 0.259###\n",
      "acc10.535_acc30.952\n",
      "random search 88/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/279 [============================>.] - ETA: 0s - loss: 5.7301\n",
      "Epoch 00001: val_loss improved from inf to 0.31814, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 5.7029 - val_loss: 0.3181\n",
      "Epoch 2/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.8009\n",
      "Epoch 00002: val_loss improved from 0.31814 to 0.14721, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.8002 - val_loss: 0.1472\n",
      "Epoch 3/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.5458\n",
      "Epoch 00003: val_loss improved from 0.14721 to 0.12899, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.5440 - val_loss: 0.1290\n",
      "Epoch 4/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.4096\n",
      "Epoch 00004: val_loss improved from 0.12899 to 0.12093, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4096 - val_loss: 0.1209\n",
      "Epoch 5/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.3372\n",
      "Epoch 00005: val_loss did not improve from 0.12093\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3366 - val_loss: 0.1240\n",
      "Epoch 6/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.2806\n",
      "Epoch 00006: val_loss did not improve from 0.12093\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2790 - val_loss: 0.1233\n",
      " ###0 fold : val acc1 0.548, acc3 0.962, mae 0.247###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - ETA: 0s - loss: 5.7015\n",
      "Epoch 00001: val_loss improved from inf to 0.32046, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 5.7015 - val_loss: 0.3205\n",
      "Epoch 2/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.8020\n",
      "Epoch 00002: val_loss improved from 0.32046 to 0.14340, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7984 - val_loss: 0.1434\n",
      "Epoch 3/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.5417\n",
      "Epoch 00003: val_loss improved from 0.14340 to 0.13162, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.5415 - val_loss: 0.1316\n",
      "Epoch 4/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.4133\n",
      "Epoch 00004: val_loss improved from 0.13162 to 0.11943, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4119 - val_loss: 0.1194\n",
      "Epoch 5/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.3374\n",
      "Epoch 00005: val_loss did not improve from 0.11943\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3362 - val_loss: 0.1209\n",
      "Epoch 6/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.2815\n",
      "Epoch 00006: val_loss improved from 0.11943 to 0.11639, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2803 - val_loss: 0.1164\n",
      "Epoch 7/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.2261\n",
      "Epoch 00007: val_loss did not improve from 0.11639\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2256 - val_loss: 0.1236\n",
      "Epoch 8/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1895\n",
      "Epoch 00008: val_loss did not improve from 0.11639\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1877 - val_loss: 0.1172\n",
      " ###1 fold : val acc1 0.553, acc3 0.966, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/279 [============================>.] - ETA: 0s - loss: 5.7941\n",
      "Epoch 00001: val_loss improved from inf to 0.31787, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 5.7164 - val_loss: 0.3179\n",
      "Epoch 2/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.7976\n",
      "Epoch 00002: val_loss improved from 0.31787 to 0.14444, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7923 - val_loss: 0.1444\n",
      "Epoch 3/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.5419\n",
      "Epoch 00003: val_loss improved from 0.14444 to 0.13071, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.5411 - val_loss: 0.1307\n",
      "Epoch 4/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.4126\n",
      "Epoch 00004: val_loss improved from 0.13071 to 0.12210, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4113 - val_loss: 0.1221\n",
      "Epoch 5/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.3396\n",
      "Epoch 00005: val_loss improved from 0.12210 to 0.12065, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3387 - val_loss: 0.1206\n",
      "Epoch 6/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.2801\n",
      "Epoch 00006: val_loss improved from 0.12065 to 0.11897, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2801 - val_loss: 0.1190\n",
      "Epoch 7/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2266\n",
      "Epoch 00007: val_loss did not improve from 0.11897\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2262 - val_loss: 0.1221\n",
      "Epoch 8/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1897\n",
      "Epoch 00008: val_loss did not improve from 0.11897\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1879 - val_loss: 0.1191\n",
      " ###2 fold : val acc1 0.550, acc3 0.962, mae 0.245###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271/279 [============================>.] - ETA: 0s - loss: 5.8645\n",
      "Epoch 00001: val_loss improved from inf to 0.31562, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 5.7339 - val_loss: 0.3156\n",
      "Epoch 2/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.7893\n",
      "Epoch 00002: val_loss improved from 0.31562 to 0.14602, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.7879 - val_loss: 0.1460\n",
      "Epoch 3/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.5451\n",
      "Epoch 00003: val_loss improved from 0.14602 to 0.13114, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.5418 - val_loss: 0.1311\n",
      "Epoch 4/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.4127\n",
      "Epoch 00004: val_loss improved from 0.13114 to 0.11884, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.4125 - val_loss: 0.1188\n",
      "Epoch 5/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.3372\n",
      "Epoch 00005: val_loss did not improve from 0.11884\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3359 - val_loss: 0.1242\n",
      "Epoch 6/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.2819\n",
      "Epoch 00006: val_loss improved from 0.11884 to 0.11647, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes32_dropout0.3,dnodes16_dropout0,lr0.001/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2814 - val_loss: 0.1165\n",
      "Epoch 7/100\n",
      " 34/279 [==>...........................] - ETA: 0s - loss: 0.2544"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/140 [===========================>..] - ETA: 0s - loss: 0.4189\n",
      "Epoch 00017: val_loss did not improve from 0.09837\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4178 - val_loss: 0.1069\n",
      " ###5 fold : val acc1 0.582, acc3 0.979, mae 0.220###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 8.7546\n",
      "Epoch 00001: val_loss improved from inf to 1.25579, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 7.9712 - val_loss: 1.2558\n",
      "Epoch 2/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 1.0768\n",
      "Epoch 00002: val_loss improved from 1.25579 to 0.19434, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.0325 - val_loss: 0.1943\n",
      "Epoch 3/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.5668\n",
      "Epoch 00003: val_loss improved from 0.19434 to 0.11861, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5661 - val_loss: 0.1186\n",
      "Epoch 4/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.5203\n",
      "Epoch 00004: val_loss improved from 0.11861 to 0.11523, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5200 - val_loss: 0.1152\n",
      "Epoch 5/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.4970\n",
      "Epoch 00005: val_loss improved from 0.11523 to 0.11276, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4964 - val_loss: 0.1128\n",
      "Epoch 6/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4822\n",
      "Epoch 00006: val_loss improved from 0.11276 to 0.10626, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4822 - val_loss: 0.1063\n",
      "Epoch 7/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.4711\n",
      "Epoch 00007: val_loss did not improve from 0.10626\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4716 - val_loss: 0.1103\n",
      "Epoch 8/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.4651\n",
      "Epoch 00008: val_loss improved from 0.10626 to 0.10440, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4613 - val_loss: 0.1044\n",
      "Epoch 9/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.4547\n",
      "Epoch 00009: val_loss did not improve from 0.10440\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4543 - val_loss: 0.1099\n",
      "Epoch 10/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.4389\n",
      "Epoch 00010: val_loss improved from 0.10440 to 0.10170, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4383 - val_loss: 0.1017\n",
      "Epoch 11/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4328\n",
      "Epoch 00011: val_loss did not improve from 0.10170\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4328 - val_loss: 0.1018\n",
      "Epoch 12/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.4179\n",
      "Epoch 00012: val_loss improved from 0.10170 to 0.10144, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4180 - val_loss: 0.1014\n",
      "Epoch 13/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4223\n",
      "Epoch 00013: val_loss did not improve from 0.10144\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4223 - val_loss: 0.1063\n",
      "Epoch 14/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.4134\n",
      "Epoch 00014: val_loss did not improve from 0.10144\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4142 - val_loss: 0.1020\n",
      " ###6 fold : val acc1 0.588, acc3 0.976, mae 0.218###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/140 [============================>.] - ETA: 0s - loss: 8.1171\n",
      "Epoch 00001: val_loss improved from inf to 1.25923, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 7.9668 - val_loss: 1.2592\n",
      "Epoch 2/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 1.0482\n",
      "Epoch 00002: val_loss improved from 1.25923 to 0.19340, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.0310 - val_loss: 0.1934\n",
      "Epoch 3/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.5684\n",
      "Epoch 00003: val_loss improved from 0.19340 to 0.11898, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5630 - val_loss: 0.1190\n",
      "Epoch 4/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.5169\n",
      "Epoch 00004: val_loss improved from 0.11898 to 0.11576, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5158 - val_loss: 0.1158\n",
      "Epoch 5/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.4969\n",
      "Epoch 00005: val_loss improved from 0.11576 to 0.11451, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4952 - val_loss: 0.1145\n",
      "Epoch 6/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.4815\n",
      "Epoch 00006: val_loss improved from 0.11451 to 0.10810, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4792 - val_loss: 0.1081\n",
      "Epoch 7/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.4671\n",
      "Epoch 00007: val_loss did not improve from 0.10810\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4670 - val_loss: 0.1102\n",
      "Epoch 8/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.4629\n",
      "Epoch 00008: val_loss improved from 0.10810 to 0.10218, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4630 - val_loss: 0.1022\n",
      "Epoch 9/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4669\n",
      "Epoch 00009: val_loss did not improve from 0.10218\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4669 - val_loss: 0.1140\n",
      "Epoch 10/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.4606\n",
      "Epoch 00010: val_loss did not improve from 0.10218\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4596 - val_loss: 0.1048\n",
      " ###7 fold : val acc1 0.559, acc3 0.969, mae 0.236###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/140 [============================>.] - ETA: 0s - loss: 8.0474\n",
      "Epoch 00001: val_loss improved from inf to 1.26200, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 7.9865 - val_loss: 1.2620\n",
      "Epoch 2/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 1.0447\n",
      "Epoch 00002: val_loss improved from 1.26200 to 0.18760, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 1.0179 - val_loss: 0.1876\n",
      "Epoch 3/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.5522\n",
      "Epoch 00003: val_loss improved from 0.18760 to 0.12266, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5512 - val_loss: 0.1227\n",
      "Epoch 4/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.5093\n",
      "Epoch 00004: val_loss improved from 0.12266 to 0.11906, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5092 - val_loss: 0.1191\n",
      "Epoch 5/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.4866\n",
      "Epoch 00005: val_loss improved from 0.11906 to 0.11825, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4870 - val_loss: 0.1183\n",
      "Epoch 6/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.4742\n",
      "Epoch 00006: val_loss improved from 0.11825 to 0.11238, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4750 - val_loss: 0.1124\n",
      "Epoch 7/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.4609\n",
      "Epoch 00007: val_loss did not improve from 0.11238\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4611 - val_loss: 0.1137\n",
      "Epoch 8/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.4577\n",
      "Epoch 00008: val_loss improved from 0.11238 to 0.10631, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4574 - val_loss: 0.1063\n",
      "Epoch 9/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.4626\n",
      "Epoch 00009: val_loss did not improve from 0.10631\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4619 - val_loss: 0.1178\n",
      "Epoch 10/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.4576\n",
      "Epoch 00010: val_loss did not improve from 0.10631\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4559 - val_loss: 0.1076\n",
      " ###8 fold : val acc1 0.578, acc3 0.977, mae 0.223###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/140 [===========================>..] - ETA: 0s - loss: 8.3216\n",
      "Epoch 00001: val_loss improved from inf to 1.27100, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 7.9865 - val_loss: 1.2710\n",
      "Epoch 2/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 1.0285\n",
      "Epoch 00002: val_loss improved from 1.27100 to 0.19024, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 1.0179 - val_loss: 0.1902\n",
      "Epoch 3/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.5572\n",
      "Epoch 00003: val_loss improved from 0.19024 to 0.12606, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5512 - val_loss: 0.1261\n",
      "Epoch 4/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.5092\n",
      "Epoch 00004: val_loss improved from 0.12606 to 0.12201, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5092 - val_loss: 0.1220\n",
      "Epoch 5/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.4878\n",
      "Epoch 00005: val_loss improved from 0.12201 to 0.12080, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4870 - val_loss: 0.1208\n",
      "Epoch 6/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.4778\n",
      "Epoch 00006: val_loss improved from 0.12080 to 0.11506, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4750 - val_loss: 0.1151\n",
      "Epoch 7/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.4600\n",
      "Epoch 00007: val_loss did not improve from 0.11506\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4611 - val_loss: 0.1174\n",
      "Epoch 8/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.4581\n",
      "Epoch 00008: val_loss improved from 0.11506 to 0.10844, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes32_dropout0,dnodes32_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4574 - val_loss: 0.1084\n",
      "Epoch 9/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.4639\n",
      "Epoch 00009: val_loss did not improve from 0.10844\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.4619 - val_loss: 0.1206\n",
      "Epoch 10/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.4568\n",
      "Epoch 00010: val_loss did not improve from 0.10844\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4559 - val_loss: 0.1101\n",
      " ###9 fold : val acc1 0.573, acc3 0.982, mae 0.223###\n",
      "acc10.580_acc30.975\n",
      "random search 92/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/140 [==========================>...] - ETA: 0s - loss: 2.7972\n",
      "Epoch 00001: val_loss improved from inf to 0.16358, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes128_dropout0.1,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 2.6500 - val_loss: 0.1636\n",
      "Epoch 2/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.4627\n",
      "Epoch 00002: val_loss did not improve from 0.16358\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4534 - val_loss: 0.1725\n",
      "Epoch 3/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2914\n",
      "Epoch 00003: val_loss improved from 0.16358 to 0.12520, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes128_dropout0.1,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2887 - val_loss: 0.1252\n",
      "Epoch 4/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2105\n",
      "Epoch 00004: val_loss did not improve from 0.12520\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2104 - val_loss: 0.1606\n",
      "Epoch 5/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1859\n",
      "Epoch 00005: val_loss did not improve from 0.12520\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1859 - val_loss: 0.1358\n",
      " ###0 fold : val acc1 0.562, acc3 0.958, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/140 [==========================>...] - ETA: 0s - loss: 2.8305\n",
      "Epoch 00001: val_loss improved from inf to 0.15670, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes128_dropout0.1,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 2.6815 - val_loss: 0.1567\n",
      "Epoch 2/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4529\n",
      "Epoch 00002: val_loss did not improve from 0.15670\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4529 - val_loss: 0.1738\n",
      "Epoch 3/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2949\n",
      "Epoch 00003: val_loss improved from 0.15670 to 0.12906, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes128_dropout0.1,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2936 - val_loss: 0.1291\n",
      "Epoch 4/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2125\n",
      "Epoch 00004: val_loss did not improve from 0.12906\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2123 - val_loss: 0.1569\n",
      "Epoch 5/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1870\n",
      "Epoch 00005: val_loss improved from 0.12906 to 0.12656, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes128_dropout0.1,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1870 - val_loss: 0.1266\n",
      "Epoch 6/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1791\n",
      "Epoch 00006: val_loss did not improve from 0.12656\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1791 - val_loss: 0.1567\n",
      "Epoch 7/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1704\n",
      "Epoch 00007: val_loss did not improve from 0.12656\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1703 - val_loss: 0.1336\n",
      " ###1 fold : val acc1 0.529, acc3 0.966, mae 0.253###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/140 [============================>.] - ETA: 0s - loss: 2.7400\n",
      "Epoch 00001: val_loss improved from inf to 0.15185, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes128_dropout0.1,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 2.6897 - val_loss: 0.1519\n",
      "Epoch 2/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.4577\n",
      "Epoch 00002: val_loss did not improve from 0.15185\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4552 - val_loss: 0.1698\n",
      "Epoch 3/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.2972\n",
      "Epoch 00003: val_loss improved from 0.15185 to 0.12359, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes128_dropout0.1,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2960 - val_loss: 0.1236\n",
      "Epoch 4/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.2116\n",
      "Epoch 00004: val_loss did not improve from 0.12359\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2113 - val_loss: 0.1465\n",
      "Epoch 5/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1888\n",
      "Epoch 00005: val_loss did not improve from 0.12359\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1887 - val_loss: 0.1306\n",
      " ###2 fold : val acc1 0.552, acc3 0.958, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/140 [===========================>..] - ETA: 0s - loss: 2.7872\n",
      "Epoch 00001: val_loss improved from inf to 0.14558, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes128_dropout0.1,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 2.6884 - val_loss: 0.1456\n",
      "Epoch 2/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4562\n",
      "Epoch 00002: val_loss did not improve from 0.14558\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4562 - val_loss: 0.1566\n",
      "Epoch 3/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2977\n",
      "Epoch 00003: val_loss improved from 0.14558 to 0.12828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes128_dropout0.1,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2971 - val_loss: 0.1283\n",
      "Epoch 4/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2104\n",
      "Epoch 00004: val_loss did not improve from 0.12828\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2104 - val_loss: 0.1418\n",
      "Epoch 5/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1882\n",
      "Epoch 00005: val_loss did not improve from 0.12828\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1882 - val_loss: 0.1450\n",
      " ###3 fold : val acc1 0.556, acc3 0.954, mae 0.247###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/140 [============================>.] - ETA: 0s - loss: 2.7072\n",
      "Epoch 00001: val_loss improved from inf to 0.15669, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes128_dropout0.1,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 2.6881 - val_loss: 0.1567\n",
      "Epoch 2/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.4617\n",
      "Epoch 00002: val_loss did not improve from 0.15669\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4529 - val_loss: 0.1751\n",
      "Epoch 3/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2980\n",
      "Epoch 00003: val_loss did not improve from 0.15669\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2962 - val_loss: 0.1886\n",
      " ###4 fold : val acc1 0.485, acc3 0.935, mae 0.291###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/140 [===========================>..] - ETA: 0s - loss: 2.7790\n",
      "Epoch 00001: val_loss improved from inf to 0.15668, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes128_dropout0.1,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 2.6800 - val_loss: 0.1567\n",
      "Epoch 2/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4564\n",
      "Epoch 00002: val_loss did not improve from 0.15668\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4564 - val_loss: 0.1760\n",
      "Epoch 3/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2997\n",
      "Epoch 00003: val_loss did not improve from 0.15668\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2997 - val_loss: 0.1769\n",
      " ###5 fold : val acc1 0.467, acc3 0.934, mae 0.301###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/140 [===========================>..] - ETA: 0s - loss: 2.7896\n",
      "Epoch 00001: val_loss improved from inf to 0.15330, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes128_dropout0.1,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 2.6597 - val_loss: 0.1533\n",
      "Epoch 2/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4580\n",
      "Epoch 00002: val_loss did not improve from 0.15330\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4580 - val_loss: 0.1766\n",
      "Epoch 3/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3038\n",
      "Epoch 00003: val_loss did not improve from 0.15330\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3020 - val_loss: 0.1820\n",
      " ###6 fold : val acc1 0.496, acc3 0.937, mae 0.285###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/140 [============================>.] - ETA: 0s - loss: 2.6975\n",
      "Epoch 00001: val_loss improved from inf to 0.14701, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes128_dropout0.1,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 2.6469 - val_loss: 0.1470\n",
      "Epoch 2/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.4571\n",
      "Epoch 00002: val_loss did not improve from 0.14701\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4527 - val_loss: 0.1891\n",
      "Epoch 3/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3046\n",
      "Epoch 00003: val_loss did not improve from 0.14701\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2993 - val_loss: 0.1709\n",
      " ###7 fold : val acc1 0.499, acc3 0.932, mae 0.288###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/140 [==========================>...] - ETA: 0s - loss: 2.8370\n",
      "Epoch 00001: val_loss improved from inf to 0.15007, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes128_dropout0.1,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 2.6558 - val_loss: 0.1501\n",
      "Epoch 2/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.4547\n",
      "Epoch 00002: val_loss did not improve from 0.15007\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4536 - val_loss: 0.1957\n",
      "Epoch 3/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.3014\n",
      "Epoch 00003: val_loss did not improve from 0.15007\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2985 - val_loss: 0.1685\n",
      " ###8 fold : val acc1 0.510, acc3 0.942, mae 0.275###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/140 [==========================>...] - ETA: 0s - loss: 2.8026\n",
      "Epoch 00001: val_loss improved from inf to 0.15387, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.4,dnodes128_dropout0.1,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 2.6558 - val_loss: 0.1539\n",
      "Epoch 2/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.4539\n",
      "Epoch 00002: val_loss did not improve from 0.15387\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4536 - val_loss: 0.2004\n",
      "Epoch 3/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2993\n",
      "Epoch 00003: val_loss did not improve from 0.15387\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2985 - val_loss: 0.1733\n",
      " ###9 fold : val acc1 0.515, acc3 0.953, mae 0.268###\n",
      "acc10.517_acc30.947\n",
      "random search 93/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 18.9185 \n",
      "Epoch 00001: val_loss improved from inf to 13.25974, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 17.6107 - val_loss: 13.2597\n",
      "Epoch 2/100\n",
      "25/35 [====================>.........] - ETA: 0s - loss: 10.9077\n",
      "Epoch 00002: val_loss improved from 13.25974 to 6.96381, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0252 - val_loss: 6.9638\n",
      "Epoch 3/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 5.6589\n",
      "Epoch 00003: val_loss improved from 6.96381 to 3.61538, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.1487 - val_loss: 3.6154\n",
      "Epoch 4/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 3.0622\n",
      "Epoch 00004: val_loss improved from 3.61538 to 2.12636, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.8181 - val_loss: 2.1264\n",
      "Epoch 5/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 1.8985\n",
      "Epoch 00005: val_loss improved from 2.12636 to 1.38030, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7686 - val_loss: 1.3803\n",
      "Epoch 6/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 1.2137\n",
      "Epoch 00006: val_loss improved from 1.38030 to 0.85743, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.1329 - val_loss: 0.8574\n",
      "Epoch 7/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.7574\n",
      "Epoch 00007: val_loss improved from 0.85743 to 0.51940, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6945 - val_loss: 0.5194\n",
      "Epoch 8/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.4653\n",
      "Epoch 00008: val_loss improved from 0.51940 to 0.33319, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4308 - val_loss: 0.3332\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3039\n",
      "Epoch 00009: val_loss improved from 0.33319 to 0.23462, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2874 - val_loss: 0.2346\n",
      "Epoch 10/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2262\n",
      "Epoch 00010: val_loss improved from 0.23462 to 0.18364, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2131 - val_loss: 0.1836\n",
      "Epoch 11/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1797\n",
      "Epoch 00011: val_loss improved from 0.18364 to 0.15759, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1741 - val_loss: 0.1576\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1576\n",
      "Epoch 00012: val_loss improved from 0.15759 to 0.14340, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1537 - val_loss: 0.1434\n",
      "Epoch 13/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1430\n",
      "Epoch 00013: val_loss improved from 0.14340 to 0.13557, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1428 - val_loss: 0.1356\n",
      "Epoch 14/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1374\n",
      "Epoch 00014: val_loss improved from 0.13557 to 0.13072, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1365 - val_loss: 0.1307\n",
      "Epoch 15/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1339\n",
      "Epoch 00015: val_loss improved from 0.13072 to 0.12730, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1324 - val_loss: 0.1273\n",
      "Epoch 16/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1310\n",
      "Epoch 00016: val_loss improved from 0.12730 to 0.12472, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1293 - val_loss: 0.1247\n",
      "Epoch 17/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00017: val_loss improved from 0.12472 to 0.12254, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1272 - val_loss: 0.1225\n",
      "Epoch 18/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00018: val_loss improved from 0.12254 to 0.12073, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1252 - val_loss: 0.1207\n",
      "Epoch 19/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00019: val_loss improved from 0.12073 to 0.11916, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1234 - val_loss: 0.1192\n",
      "Epoch 20/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00020: val_loss improved from 0.11916 to 0.11799, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1218 - val_loss: 0.1180\n",
      "Epoch 21/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1211\n",
      "Epoch 00021: val_loss improved from 0.11799 to 0.11647, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1205 - val_loss: 0.1165\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1208\n",
      "Epoch 00022: val_loss improved from 0.11647 to 0.11544, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1192 - val_loss: 0.1154\n",
      "Epoch 23/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1199\n",
      "Epoch 00023: val_loss improved from 0.11544 to 0.11427, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1180 - val_loss: 0.1143\n",
      "Epoch 24/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1182\n",
      "Epoch 00024: val_loss improved from 0.11427 to 0.11336, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1169 - val_loss: 0.1134\n",
      "Epoch 25/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1171\n",
      "Epoch 00025: val_loss improved from 0.11336 to 0.11227, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1158 - val_loss: 0.1123\n",
      "Epoch 26/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1158\n",
      "Epoch 00026: val_loss improved from 0.11227 to 0.11149, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1148 - val_loss: 0.1115\n",
      "Epoch 27/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1148\n",
      "Epoch 00027: val_loss improved from 0.11149 to 0.11039, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1138 - val_loss: 0.1104\n",
      "Epoch 28/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1121\n",
      "Epoch 00028: val_loss improved from 0.11039 to 0.10958, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1129 - val_loss: 0.1096\n",
      "Epoch 29/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1127\n",
      "Epoch 00029: val_loss improved from 0.10958 to 0.10879, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1120 - val_loss: 0.1088\n",
      "Epoch 30/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1120\n",
      "Epoch 00030: val_loss improved from 0.10879 to 0.10853, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1111 - val_loss: 0.1085\n",
      "Epoch 31/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1107\n",
      "Epoch 00031: val_loss improved from 0.10853 to 0.10732, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1103 - val_loss: 0.1073\n",
      "Epoch 32/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1099\n",
      "Epoch 00032: val_loss improved from 0.10732 to 0.10658, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1095 - val_loss: 0.1066\n",
      "Epoch 33/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1077\n",
      "Epoch 00033: val_loss improved from 0.10658 to 0.10581, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1089 - val_loss: 0.1058\n",
      "Epoch 34/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1086\n",
      "Epoch 00034: val_loss improved from 0.10581 to 0.10526, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1082 - val_loss: 0.1053\n",
      "Epoch 35/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00035: val_loss improved from 0.10526 to 0.10474, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1074 - val_loss: 0.1047\n",
      "Epoch 36/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1072\n",
      "Epoch 00036: val_loss improved from 0.10474 to 0.10405, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1067 - val_loss: 0.1040\n",
      "Epoch 37/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1067\n",
      "Epoch 00037: val_loss improved from 0.10405 to 0.10340, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1061 - val_loss: 0.1034\n",
      "Epoch 38/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1046\n",
      "Epoch 00038: val_loss improved from 0.10340 to 0.10269, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1054 - val_loss: 0.1027\n",
      "Epoch 39/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1056\n",
      "Epoch 00039: val_loss improved from 0.10269 to 0.10213, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1049 - val_loss: 0.1021\n",
      "Epoch 40/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1039\n",
      "Epoch 00040: val_loss improved from 0.10213 to 0.10160, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1044 - val_loss: 0.1016\n",
      "Epoch 41/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1038\n",
      "Epoch 00041: val_loss improved from 0.10160 to 0.10115, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1038 - val_loss: 0.1011\n",
      "Epoch 42/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1031\n",
      "Epoch 00042: val_loss improved from 0.10115 to 0.10080, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1032 - val_loss: 0.1008\n",
      "Epoch 43/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1028\n",
      "Epoch 00043: val_loss improved from 0.10080 to 0.10019, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1028 - val_loss: 0.1002\n",
      "Epoch 44/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1003\n",
      "Epoch 00044: val_loss improved from 0.10019 to 0.09987, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1024 - val_loss: 0.0999\n",
      "Epoch 45/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1011\n",
      "Epoch 00045: val_loss improved from 0.09987 to 0.09951, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1019 - val_loss: 0.0995\n",
      "Epoch 46/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1016\n",
      "Epoch 00046: val_loss improved from 0.09951 to 0.09936, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1015 - val_loss: 0.0994\n",
      "Epoch 47/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1012\n",
      "Epoch 00047: val_loss improved from 0.09936 to 0.09878, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1012 - val_loss: 0.0988\n",
      "Epoch 48/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1024\n",
      "Epoch 00048: val_loss improved from 0.09878 to 0.09857, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1007 - val_loss: 0.0986\n",
      "Epoch 49/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0992\n",
      "Epoch 00049: val_loss improved from 0.09857 to 0.09810, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1005 - val_loss: 0.0981\n",
      "Epoch 50/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1002\n",
      "Epoch 00050: val_loss improved from 0.09810 to 0.09787, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1001 - val_loss: 0.0979\n",
      "Epoch 51/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00051: val_loss improved from 0.09787 to 0.09765, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0997 - val_loss: 0.0976\n",
      "Epoch 52/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1000\n",
      "Epoch 00052: val_loss improved from 0.09765 to 0.09735, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0994 - val_loss: 0.0973\n",
      "Epoch 53/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00053: val_loss improved from 0.09735 to 0.09722, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0991 - val_loss: 0.0972\n",
      "Epoch 54/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00054: val_loss improved from 0.09722 to 0.09677, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0989 - val_loss: 0.0968\n",
      "Epoch 55/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0985\n",
      "Epoch 00055: val_loss improved from 0.09677 to 0.09668, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0987 - val_loss: 0.0967\n",
      "Epoch 56/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00056: val_loss improved from 0.09668 to 0.09639, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0984 - val_loss: 0.0964\n",
      "Epoch 57/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00057: val_loss improved from 0.09639 to 0.09634, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0985 - val_loss: 0.0963\n",
      "Epoch 58/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0964\n",
      "Epoch 00058: val_loss improved from 0.09634 to 0.09615, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0983 - val_loss: 0.0962\n",
      "Epoch 59/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00059: val_loss improved from 0.09615 to 0.09593, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0979 - val_loss: 0.0959\n",
      "Epoch 60/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0960\n",
      "Epoch 00060: val_loss did not improve from 0.09593\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0978 - val_loss: 0.0960\n",
      "Epoch 61/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00061: val_loss did not improve from 0.09593\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0977 - val_loss: 0.0960\n",
      " ###0 fold : val acc1 0.607, acc3 0.980, mae 0.207###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/35 [====================>.........] - ETA: 0s - loss: 18.8145 \n",
      "Epoch 00001: val_loss improved from inf to 13.25415, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 17.6186 - val_loss: 13.2541\n",
      "Epoch 2/100\n",
      "25/35 [====================>.........] - ETA: 0s - loss: 10.9134\n",
      "Epoch 00002: val_loss improved from 13.25415 to 6.95282, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0369 - val_loss: 6.9528\n",
      "Epoch 3/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 5.6640\n",
      "Epoch 00003: val_loss improved from 6.95282 to 3.60841, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.1537 - val_loss: 3.6084\n",
      "Epoch 4/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 3.0574\n",
      "Epoch 00004: val_loss improved from 3.60841 to 2.12044, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.8169 - val_loss: 2.1204\n",
      "Epoch 5/100\n",
      "25/35 [====================>.........] - ETA: 0s - loss: 1.8707\n",
      "Epoch 00005: val_loss improved from 2.12044 to 1.37399, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7663 - val_loss: 1.3740\n",
      "Epoch 6/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 1.2037\n",
      "Epoch 00006: val_loss improved from 1.37399 to 0.85469, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.1298 - val_loss: 0.8547\n",
      "Epoch 7/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.7510\n",
      "Epoch 00007: val_loss improved from 0.85469 to 0.52013, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6941 - val_loss: 0.5201\n",
      "Epoch 8/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.4589\n",
      "Epoch 00008: val_loss improved from 0.52013 to 0.33373, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4313 - val_loss: 0.3337\n",
      "Epoch 9/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.3006\n",
      "Epoch 00009: val_loss improved from 0.33373 to 0.23476, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2873 - val_loss: 0.2348\n",
      "Epoch 10/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.2208\n",
      "Epoch 00010: val_loss improved from 0.23476 to 0.18328, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2125 - val_loss: 0.1833\n",
      "Epoch 11/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1776\n",
      "Epoch 00011: val_loss improved from 0.18328 to 0.15696, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1731 - val_loss: 0.1570\n",
      "Epoch 12/100\n",
      "25/35 [====================>.........] - ETA: 0s - loss: 0.1557\n",
      "Epoch 00012: val_loss improved from 0.15696 to 0.14305, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1527 - val_loss: 0.1430\n",
      "Epoch 13/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.1429\n",
      "Epoch 00013: val_loss improved from 0.14305 to 0.13522, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1419 - val_loss: 0.1352\n",
      "Epoch 14/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1358\n",
      "Epoch 00014: val_loss improved from 0.13522 to 0.13071, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1359 - val_loss: 0.1307\n",
      "Epoch 15/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1334\n",
      "Epoch 00015: val_loss improved from 0.13071 to 0.12727, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1318 - val_loss: 0.1273\n",
      "Epoch 16/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1297\n",
      "Epoch 00016: val_loss improved from 0.12727 to 0.12483, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1289 - val_loss: 0.1248\n",
      "Epoch 17/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1280\n",
      "Epoch 00017: val_loss improved from 0.12483 to 0.12266, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1268 - val_loss: 0.1227\n",
      "Epoch 18/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00018: val_loss improved from 0.12266 to 0.12087, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1248 - val_loss: 0.1209\n",
      "Epoch 19/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00019: val_loss improved from 0.12087 to 0.11926, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1231 - val_loss: 0.1193\n",
      "Epoch 20/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1213\n",
      "Epoch 00020: val_loss improved from 0.11926 to 0.11814, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1215 - val_loss: 0.1181\n",
      "Epoch 21/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1213\n",
      "Epoch 00021: val_loss improved from 0.11814 to 0.11656, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1201 - val_loss: 0.1166\n",
      "Epoch 22/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1198\n",
      "Epoch 00022: val_loss improved from 0.11656 to 0.11552, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1189 - val_loss: 0.1155\n",
      "Epoch 23/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1187\n",
      "Epoch 00023: val_loss improved from 0.11552 to 0.11437, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1176 - val_loss: 0.1144\n",
      "Epoch 24/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1176\n",
      "Epoch 00024: val_loss improved from 0.11437 to 0.11351, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1165 - val_loss: 0.1135\n",
      "Epoch 25/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1159\n",
      "Epoch 00025: val_loss improved from 0.11351 to 0.11226, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1153 - val_loss: 0.1123\n",
      "Epoch 26/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1155\n",
      "Epoch 00026: val_loss improved from 0.11226 to 0.11148, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1143 - val_loss: 0.1115\n",
      "Epoch 27/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1131\n",
      "Epoch 00027: val_loss improved from 0.11148 to 0.11043, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1132 - val_loss: 0.1104\n",
      "Epoch 28/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1113\n",
      "Epoch 00028: val_loss improved from 0.11043 to 0.10965, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1124 - val_loss: 0.1096\n",
      "Epoch 29/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1115\n",
      "Epoch 00029: val_loss improved from 0.10965 to 0.10878, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1114 - val_loss: 0.1088\n",
      "Epoch 30/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00030: val_loss improved from 0.10878 to 0.10848, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1106 - val_loss: 0.1085\n",
      "Epoch 31/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1096\n",
      "Epoch 00031: val_loss improved from 0.10848 to 0.10716, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1097 - val_loss: 0.1072\n",
      "Epoch 32/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1096\n",
      "Epoch 00032: val_loss improved from 0.10716 to 0.10653, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1089 - val_loss: 0.1065\n",
      "Epoch 33/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1071\n",
      "Epoch 00033: val_loss improved from 0.10653 to 0.10581, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1084 - val_loss: 0.1058\n",
      "Epoch 34/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1071\n",
      "Epoch 00034: val_loss improved from 0.10581 to 0.10513, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1076 - val_loss: 0.1051\n",
      "Epoch 35/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00035: val_loss improved from 0.10513 to 0.10479, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1068 - val_loss: 0.1048\n",
      "Epoch 36/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1069\n",
      "Epoch 00036: val_loss improved from 0.10479 to 0.10405, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1061 - val_loss: 0.1041\n",
      "Epoch 37/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1059\n",
      "Epoch 00037: val_loss improved from 0.10405 to 0.10351, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1055 - val_loss: 0.1035\n",
      "Epoch 38/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1048\n",
      "Epoch 00038: val_loss improved from 0.10351 to 0.10283, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1049 - val_loss: 0.1028\n",
      "Epoch 39/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1045\n",
      "Epoch 00039: val_loss improved from 0.10283 to 0.10212, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1044 - val_loss: 0.1021\n",
      "Epoch 40/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1040\n",
      "Epoch 00040: val_loss improved from 0.10212 to 0.10164, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1039 - val_loss: 0.1016\n",
      "Epoch 41/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1035\n",
      "Epoch 00041: val_loss improved from 0.10164 to 0.10116, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1033 - val_loss: 0.1012\n",
      "Epoch 42/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1034\n",
      "Epoch 00042: val_loss improved from 0.10116 to 0.10094, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1028 - val_loss: 0.1009\n",
      "Epoch 43/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1025\n",
      "Epoch 00043: val_loss improved from 0.10094 to 0.10031, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1024 - val_loss: 0.1003\n",
      "Epoch 44/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1011\n",
      "Epoch 00044: val_loss improved from 0.10031 to 0.10008, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1019 - val_loss: 0.1001\n",
      "Epoch 45/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1003\n",
      "Epoch 00045: val_loss improved from 0.10008 to 0.09966, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1015 - val_loss: 0.0997\n",
      "Epoch 46/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1002\n",
      "Epoch 00046: val_loss did not improve from 0.09966\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1011 - val_loss: 0.0997\n",
      "Epoch 47/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1014\n",
      "Epoch 00047: val_loss improved from 0.09966 to 0.09888, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1007 - val_loss: 0.0989\n",
      "Epoch 48/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1010\n",
      "Epoch 00048: val_loss improved from 0.09888 to 0.09882, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1003 - val_loss: 0.0988\n",
      "Epoch 49/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0989\n",
      "Epoch 00049: val_loss improved from 0.09882 to 0.09825, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1000 - val_loss: 0.0982\n",
      "Epoch 50/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1005\n",
      "Epoch 00050: val_loss improved from 0.09825 to 0.09814, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0996 - val_loss: 0.0981\n",
      "Epoch 51/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00051: val_loss improved from 0.09814 to 0.09781, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0991 - val_loss: 0.0978\n",
      "Epoch 52/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.0998\n",
      "Epoch 00052: val_loss improved from 0.09781 to 0.09747, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0989 - val_loss: 0.0975\n",
      "Epoch 53/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0981\n",
      "Epoch 00053: val_loss improved from 0.09747 to 0.09728, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0987 - val_loss: 0.0973\n",
      "Epoch 54/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0986\n",
      "Epoch 00054: val_loss improved from 0.09728 to 0.09688, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0985 - val_loss: 0.0969\n",
      "Epoch 55/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0985\n",
      "Epoch 00055: val_loss improved from 0.09688 to 0.09678, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0982 - val_loss: 0.0968\n",
      "Epoch 56/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0981\n",
      "Epoch 00056: val_loss improved from 0.09678 to 0.09649, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0980 - val_loss: 0.0965\n",
      "Epoch 57/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.0978\n",
      "Epoch 00057: val_loss did not improve from 0.09649\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0979 - val_loss: 0.0966\n",
      "Epoch 58/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00058: val_loss improved from 0.09649 to 0.09643, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0978 - val_loss: 0.0964\n",
      "Epoch 59/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00059: val_loss improved from 0.09643 to 0.09613, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0975 - val_loss: 0.0961\n",
      "Epoch 60/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0968\n",
      "Epoch 00060: val_loss did not improve from 0.09613\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0974 - val_loss: 0.0962\n",
      "Epoch 61/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00061: val_loss did not improve from 0.09613\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0973 - val_loss: 0.0961\n",
      " ###1 fold : val acc1 0.589, acc3 0.982, mae 0.215###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/35 [====================>.........] - ETA: 0s - loss: 18.7862 \n",
      "Epoch 00001: val_loss improved from inf to 13.25567, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 1s 6ms/step - loss: 17.5947 - val_loss: 13.2557\n",
      "Epoch 2/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 10.9649\n",
      "Epoch 00002: val_loss improved from 13.25567 to 6.95586, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0218 - val_loss: 6.9559\n",
      "Epoch 3/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 5.6971\n",
      "Epoch 00003: val_loss improved from 6.95586 to 3.60934, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.1524 - val_loss: 3.6093\n",
      "Epoch 4/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 3.0588\n",
      "Epoch 00004: val_loss improved from 3.60934 to 2.11945, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.8189 - val_loss: 2.1194\n",
      "Epoch 5/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 1.8868\n",
      "Epoch 00005: val_loss improved from 2.11945 to 1.37116, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7666 - val_loss: 1.3712\n",
      "Epoch 6/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 1.2057\n",
      "Epoch 00006: val_loss improved from 1.37116 to 0.85077, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.1281 - val_loss: 0.8508\n",
      "Epoch 7/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.7538\n",
      "Epoch 00007: val_loss improved from 0.85077 to 0.51540, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6914 - val_loss: 0.5154\n",
      "Epoch 8/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.4655\n",
      "Epoch 00008: val_loss improved from 0.51540 to 0.32945, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4283 - val_loss: 0.3295\n",
      "Epoch 9/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.3004\n",
      "Epoch 00009: val_loss improved from 0.32945 to 0.23231, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2851 - val_loss: 0.2323\n",
      "Epoch 10/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.2209\n",
      "Epoch 00010: val_loss improved from 0.23231 to 0.18177, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2114 - val_loss: 0.1818\n",
      "Epoch 11/100\n",
      "25/35 [====================>.........] - ETA: 0s - loss: 0.1787\n",
      "Epoch 00011: val_loss improved from 0.18177 to 0.15621, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1727 - val_loss: 0.1562\n",
      "Epoch 12/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.1557\n",
      "Epoch 00012: val_loss improved from 0.15621 to 0.14284, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1527 - val_loss: 0.1428\n",
      "Epoch 13/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.1431\n",
      "Epoch 00013: val_loss improved from 0.14284 to 0.13522, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1422 - val_loss: 0.1352\n",
      "Epoch 14/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1358\n",
      "Epoch 00014: val_loss improved from 0.13522 to 0.13066, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1362 - val_loss: 0.1307\n",
      "Epoch 15/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1341\n",
      "Epoch 00015: val_loss improved from 0.13066 to 0.12724, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1321 - val_loss: 0.1272\n",
      "Epoch 16/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1299\n",
      "Epoch 00016: val_loss improved from 0.12724 to 0.12481, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1292 - val_loss: 0.1248\n",
      "Epoch 17/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1284\n",
      "Epoch 00017: val_loss improved from 0.12481 to 0.12275, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1271 - val_loss: 0.1228\n",
      "Epoch 18/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00018: val_loss improved from 0.12275 to 0.12088, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1251 - val_loss: 0.1209\n",
      "Epoch 19/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00019: val_loss improved from 0.12088 to 0.11932, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1234 - val_loss: 0.1193\n",
      "Epoch 20/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1219\n",
      "Epoch 00020: val_loss improved from 0.11932 to 0.11804, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1219 - val_loss: 0.1180\n",
      "Epoch 21/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1206\n",
      "Epoch 00021: val_loss improved from 0.11804 to 0.11671, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1205 - val_loss: 0.1167\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1210\n",
      "Epoch 00022: val_loss improved from 0.11671 to 0.11559, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1192 - val_loss: 0.1156\n",
      "Epoch 23/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1190\n",
      "Epoch 00023: val_loss improved from 0.11559 to 0.11451, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1180 - val_loss: 0.1145\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1198\n",
      "Epoch 00024: val_loss improved from 0.11451 to 0.11342, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1168 - val_loss: 0.1134\n",
      "Epoch 25/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1165\n",
      "Epoch 00025: val_loss improved from 0.11342 to 0.11240, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1157 - val_loss: 0.1124\n",
      "Epoch 26/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1154\n",
      "Epoch 00026: val_loss improved from 0.11240 to 0.11147, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1146 - val_loss: 0.1115\n",
      "Epoch 27/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1141\n",
      "Epoch 00027: val_loss improved from 0.11147 to 0.11042, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1136 - val_loss: 0.1104\n",
      "Epoch 28/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1115\n",
      "Epoch 00028: val_loss improved from 0.11042 to 0.10964, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1127 - val_loss: 0.1096\n",
      "Epoch 29/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1122\n",
      "Epoch 00029: val_loss improved from 0.10964 to 0.10882, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1117 - val_loss: 0.1088\n",
      "Epoch 30/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1125\n",
      "Epoch 00030: val_loss improved from 0.10882 to 0.10825, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1109 - val_loss: 0.1082\n",
      "Epoch 31/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1099\n",
      "Epoch 00031: val_loss improved from 0.10825 to 0.10719, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1101 - val_loss: 0.1072\n",
      "Epoch 32/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1105\n",
      "Epoch 00032: val_loss improved from 0.10719 to 0.10654, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1093 - val_loss: 0.1065\n",
      "Epoch 33/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1071\n",
      "Epoch 00033: val_loss improved from 0.10654 to 0.10593, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1088 - val_loss: 0.1059\n",
      "Epoch 34/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1092\n",
      "Epoch 00034: val_loss improved from 0.10593 to 0.10512, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1081 - val_loss: 0.1051\n",
      "Epoch 35/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00035: val_loss improved from 0.10512 to 0.10454, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1072 - val_loss: 0.1045\n",
      "Epoch 36/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1081\n",
      "Epoch 00036: val_loss improved from 0.10454 to 0.10396, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1065 - val_loss: 0.1040\n",
      "Epoch 37/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1063\n",
      "Epoch 00037: val_loss improved from 0.10396 to 0.10341, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1059 - val_loss: 0.1034\n",
      "Epoch 38/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1049\n",
      "Epoch 00038: val_loss improved from 0.10341 to 0.10269, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1053 - val_loss: 0.1027\n",
      "Epoch 39/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1047\n",
      "Epoch 00039: val_loss improved from 0.10269 to 0.10210, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1047 - val_loss: 0.1021\n",
      "Epoch 40/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1039\n",
      "Epoch 00040: val_loss improved from 0.10210 to 0.10161, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1042 - val_loss: 0.1016\n",
      "Epoch 41/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1037\n",
      "Epoch 00041: val_loss improved from 0.10161 to 0.10110, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1037 - val_loss: 0.1011\n",
      "Epoch 42/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1040\n",
      "Epoch 00042: val_loss improved from 0.10110 to 0.10085, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1031 - val_loss: 0.1008\n",
      "Epoch 43/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1021\n",
      "Epoch 00043: val_loss improved from 0.10085 to 0.10025, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1027 - val_loss: 0.1002\n",
      "Epoch 44/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1009\n",
      "Epoch 00044: val_loss improved from 0.10025 to 0.09998, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1023 - val_loss: 0.1000\n",
      "Epoch 45/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1002\n",
      "Epoch 00045: val_loss improved from 0.09998 to 0.09950, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1018 - val_loss: 0.0995\n",
      "Epoch 46/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1008\n",
      "Epoch 00046: val_loss improved from 0.09950 to 0.09937, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1014 - val_loss: 0.0994\n",
      "Epoch 47/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1011\n",
      "Epoch 00047: val_loss improved from 0.09937 to 0.09884, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1010 - val_loss: 0.0988\n",
      "Epoch 48/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1022\n",
      "Epoch 00048: val_loss improved from 0.09884 to 0.09861, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1006 - val_loss: 0.0986\n",
      "Epoch 49/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0994\n",
      "Epoch 00049: val_loss improved from 0.09861 to 0.09816, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1006 - val_loss: 0.0982\n",
      "Epoch 50/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1011\n",
      "Epoch 00050: val_loss did not improve from 0.09816\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1000 - val_loss: 0.0983\n",
      "Epoch 51/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0995\n",
      "Epoch 00051: val_loss improved from 0.09816 to 0.09775, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0996 - val_loss: 0.0978\n",
      "Epoch 52/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00052: val_loss improved from 0.09775 to 0.09758, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0993 - val_loss: 0.0976\n",
      "Epoch 53/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00053: val_loss improved from 0.09758 to 0.09710, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0991 - val_loss: 0.0971\n",
      "Epoch 54/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0990\n",
      "Epoch 00054: val_loss improved from 0.09710 to 0.09671, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0988 - val_loss: 0.0967\n",
      "Epoch 55/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0989\n",
      "Epoch 00055: val_loss did not improve from 0.09671\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0985 - val_loss: 0.0968\n",
      "Epoch 56/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0980\n",
      "Epoch 00056: val_loss improved from 0.09671 to 0.09637, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0983 - val_loss: 0.0964\n",
      "Epoch 57/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0988\n",
      "Epoch 00057: val_loss did not improve from 0.09637\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0983 - val_loss: 0.0966\n",
      "Epoch 58/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00058: val_loss improved from 0.09637 to 0.09613, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0981 - val_loss: 0.0961\n",
      "Epoch 59/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00059: val_loss improved from 0.09613 to 0.09596, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0977 - val_loss: 0.0960\n",
      "Epoch 60/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00060: val_loss improved from 0.09596 to 0.09587, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0976 - val_loss: 0.0959\n",
      "Epoch 61/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0985\n",
      "Epoch 00061: val_loss improved from 0.09587 to 0.09579, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0975 - val_loss: 0.0958\n",
      "Epoch 62/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00062: val_loss improved from 0.09579 to 0.09566, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0973 - val_loss: 0.0957\n",
      "Epoch 63/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0981\n",
      "Epoch 00063: val_loss improved from 0.09566 to 0.09560, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0973 - val_loss: 0.0956\n",
      "Epoch 64/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0978\n",
      "Epoch 00064: val_loss improved from 0.09560 to 0.09544, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0972 - val_loss: 0.0954\n",
      "Epoch 65/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00065: val_loss did not improve from 0.09544\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0972 - val_loss: 0.0956\n",
      "Epoch 66/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0972\n",
      "Epoch 00066: val_loss improved from 0.09544 to 0.09527, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0970 - val_loss: 0.0953\n",
      "Epoch 67/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.0963\n",
      "Epoch 00067: val_loss improved from 0.09527 to 0.09526, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0970 - val_loss: 0.0953\n",
      "Epoch 68/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00068: val_loss did not improve from 0.09526\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0970 - val_loss: 0.0953\n",
      "Epoch 69/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.0988\n",
      "Epoch 00069: val_loss did not improve from 0.09526\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0968 - val_loss: 0.0953\n",
      " ###2 fold : val acc1 0.590, acc3 0.979, mae 0.216###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/35 [===================>..........] - ETA: 0s - loss: 18.9484 \n",
      "Epoch 00001: val_loss improved from inf to 13.24288, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 17.6310 - val_loss: 13.2429\n",
      "Epoch 2/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 10.9839\n",
      "Epoch 00002: val_loss improved from 13.24288 to 6.94273, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0394 - val_loss: 6.9427\n",
      "Epoch 3/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 5.6522\n",
      "Epoch 00003: val_loss improved from 6.94273 to 3.60648, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.1638 - val_loss: 3.6065\n",
      "Epoch 4/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 3.0644\n",
      "Epoch 00004: val_loss improved from 3.60648 to 2.11993, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.8283 - val_loss: 2.1199\n",
      "Epoch 5/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 1.8886\n",
      "Epoch 00005: val_loss improved from 2.11993 to 1.37162, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7746 - val_loss: 1.3716\n",
      "Epoch 6/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 1.2115\n",
      "Epoch 00006: val_loss improved from 1.37162 to 0.85054, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.1320 - val_loss: 0.8505\n",
      "Epoch 7/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.7519\n",
      "Epoch 00007: val_loss improved from 0.85054 to 0.51468, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6930 - val_loss: 0.5147\n",
      "Epoch 8/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.4606\n",
      "Epoch 00008: val_loss improved from 0.51468 to 0.32831, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4283 - val_loss: 0.3283\n",
      "Epoch 9/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.2984\n",
      "Epoch 00009: val_loss improved from 0.32831 to 0.23130, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2845 - val_loss: 0.2313\n",
      "Epoch 10/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.2217\n",
      "Epoch 00010: val_loss improved from 0.23130 to 0.18102, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2107 - val_loss: 0.1810\n",
      "Epoch 11/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1766\n",
      "Epoch 00011: val_loss improved from 0.18102 to 0.15572, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1720 - val_loss: 0.1557\n",
      "Epoch 12/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.1555\n",
      "Epoch 00012: val_loss improved from 0.15572 to 0.14296, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1522 - val_loss: 0.1430\n",
      "Epoch 13/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1427\n",
      "Epoch 00013: val_loss improved from 0.14296 to 0.13517, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1417 - val_loss: 0.1352\n",
      "Epoch 14/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.1348\n",
      "Epoch 00014: val_loss improved from 0.13517 to 0.13083, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1358 - val_loss: 0.1308\n",
      "Epoch 15/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.1341\n",
      "Epoch 00015: val_loss improved from 0.13083 to 0.12732, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1318 - val_loss: 0.1273\n",
      "Epoch 16/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00016: val_loss improved from 0.12732 to 0.12490, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1289 - val_loss: 0.1249\n",
      "Epoch 17/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1275\n",
      "Epoch 00017: val_loss improved from 0.12490 to 0.12294, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1268 - val_loss: 0.1229\n",
      "Epoch 18/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00018: val_loss improved from 0.12294 to 0.12100, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1248 - val_loss: 0.1210\n",
      "Epoch 19/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00019: val_loss improved from 0.12100 to 0.11932, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1231 - val_loss: 0.1193\n",
      "Epoch 20/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1202\n",
      "Epoch 00020: val_loss improved from 0.11932 to 0.11812, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1216 - val_loss: 0.1181\n",
      "Epoch 21/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1196\n",
      "Epoch 00021: val_loss improved from 0.11812 to 0.11669, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1202 - val_loss: 0.1167\n",
      "Epoch 22/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1197\n",
      "Epoch 00022: val_loss improved from 0.11669 to 0.11561, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1189 - val_loss: 0.1156\n",
      "Epoch 23/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1180\n",
      "Epoch 00023: val_loss improved from 0.11561 to 0.11457, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1177 - val_loss: 0.1146\n",
      "Epoch 24/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1178\n",
      "Epoch 00024: val_loss improved from 0.11457 to 0.11351, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1165 - val_loss: 0.1135\n",
      "Epoch 25/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1172\n",
      "Epoch 00025: val_loss improved from 0.11351 to 0.11234, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1154 - val_loss: 0.1123\n",
      "Epoch 26/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1156\n",
      "Epoch 00026: val_loss improved from 0.11234 to 0.11150, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1144 - val_loss: 0.1115\n",
      "Epoch 27/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1122\n",
      "Epoch 00027: val_loss improved from 0.11150 to 0.11048, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1133 - val_loss: 0.1105\n",
      "Epoch 28/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1119\n",
      "Epoch 00028: val_loss improved from 0.11048 to 0.10975, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1125 - val_loss: 0.1097\n",
      "Epoch 29/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1124\n",
      "Epoch 00029: val_loss improved from 0.10975 to 0.10879, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1115 - val_loss: 0.1088\n",
      "Epoch 30/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1116\n",
      "Epoch 00030: val_loss improved from 0.10879 to 0.10827, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1107 - val_loss: 0.1083\n",
      "Epoch 31/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1095\n",
      "Epoch 00031: val_loss improved from 0.10827 to 0.10722, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1099 - val_loss: 0.1072\n",
      "Epoch 32/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1091\n",
      "Epoch 00032: val_loss improved from 0.10722 to 0.10663, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1090 - val_loss: 0.1066\n",
      "Epoch 33/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1069\n",
      "Epoch 00033: val_loss improved from 0.10663 to 0.10597, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1085 - val_loss: 0.1060\n",
      "Epoch 34/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00034: val_loss improved from 0.10597 to 0.10512, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1078 - val_loss: 0.1051\n",
      "Epoch 35/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1065\n",
      "Epoch 00035: val_loss improved from 0.10512 to 0.10467, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1069 - val_loss: 0.1047\n",
      "Epoch 36/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1069\n",
      "Epoch 00036: val_loss improved from 0.10467 to 0.10398, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1062 - val_loss: 0.1040\n",
      "Epoch 37/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1053\n",
      "Epoch 00037: val_loss improved from 0.10398 to 0.10351, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1056 - val_loss: 0.1035\n",
      "Epoch 38/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1052\n",
      "Epoch 00038: val_loss improved from 0.10351 to 0.10279, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1050 - val_loss: 0.1028\n",
      "Epoch 39/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1048\n",
      "Epoch 00039: val_loss improved from 0.10279 to 0.10211, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1044 - val_loss: 0.1021\n",
      "Epoch 40/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1044\n",
      "Epoch 00040: val_loss improved from 0.10211 to 0.10183, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1039 - val_loss: 0.1018\n",
      "Epoch 41/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1033\n",
      "Epoch 00041: val_loss improved from 0.10183 to 0.10116, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1034 - val_loss: 0.1012\n",
      "Epoch 42/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1040\n",
      "Epoch 00042: val_loss improved from 0.10116 to 0.10092, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1028 - val_loss: 0.1009\n",
      "Epoch 43/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1015\n",
      "Epoch 00043: val_loss improved from 0.10092 to 0.10026, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1023 - val_loss: 0.1003\n",
      "Epoch 44/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1018\n",
      "Epoch 00044: val_loss improved from 0.10026 to 0.10009, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1020 - val_loss: 0.1001\n",
      "Epoch 45/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1005\n",
      "Epoch 00045: val_loss improved from 0.10009 to 0.09952, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1015 - val_loss: 0.0995\n",
      "Epoch 46/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1010\n",
      "Epoch 00046: val_loss improved from 0.09952 to 0.09932, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1011 - val_loss: 0.0993\n",
      "Epoch 47/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1009\n",
      "Epoch 00047: val_loss improved from 0.09932 to 0.09882, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1007 - val_loss: 0.0988\n",
      "Epoch 48/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1018\n",
      "Epoch 00048: val_loss did not improve from 0.09882\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1003 - val_loss: 0.0988\n",
      "Epoch 49/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0997\n",
      "Epoch 00049: val_loss improved from 0.09882 to 0.09820, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1001 - val_loss: 0.0982\n",
      "Epoch 50/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1005\n",
      "Epoch 00050: val_loss improved from 0.09820 to 0.09809, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0996 - val_loss: 0.0981\n",
      "Epoch 51/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0997\n",
      "Epoch 00051: val_loss improved from 0.09809 to 0.09783, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0992 - val_loss: 0.0978\n",
      "Epoch 52/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0995\n",
      "Epoch 00052: val_loss improved from 0.09783 to 0.09755, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0990 - val_loss: 0.0976\n",
      "Epoch 53/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00053: val_loss improved from 0.09755 to 0.09707, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0988 - val_loss: 0.0971\n",
      "Epoch 54/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00054: val_loss improved from 0.09707 to 0.09679, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0985 - val_loss: 0.0968\n",
      "Epoch 55/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0985\n",
      "Epoch 00055: val_loss improved from 0.09679 to 0.09665, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0983 - val_loss: 0.0966\n",
      "Epoch 56/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00056: val_loss improved from 0.09665 to 0.09647, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0981 - val_loss: 0.0965\n",
      "Epoch 57/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0977\n",
      "Epoch 00057: val_loss did not improve from 0.09647\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0981 - val_loss: 0.0969\n",
      "Epoch 58/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0971\n",
      "Epoch 00058: val_loss improved from 0.09647 to 0.09639, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0979 - val_loss: 0.0964\n",
      "Epoch 59/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0998\n",
      "Epoch 00059: val_loss improved from 0.09639 to 0.09610, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0975 - val_loss: 0.0961\n",
      "Epoch 60/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00060: val_loss improved from 0.09610 to 0.09590, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0974 - val_loss: 0.0959\n",
      "Epoch 61/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00061: val_loss did not improve from 0.09590\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0972 - val_loss: 0.0960\n",
      "Epoch 62/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.0986\n",
      "Epoch 00062: val_loss improved from 0.09590 to 0.09588, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0972 - val_loss: 0.0959\n",
      "Epoch 63/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00063: val_loss improved from 0.09588 to 0.09566, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0971 - val_loss: 0.0957\n",
      "Epoch 64/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0977\n",
      "Epoch 00064: val_loss improved from 0.09566 to 0.09561, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0970 - val_loss: 0.0956\n",
      "Epoch 65/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0966\n",
      "Epoch 00065: val_loss did not improve from 0.09561\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0969 - val_loss: 0.0958\n",
      "Epoch 66/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0980\n",
      "Epoch 00066: val_loss improved from 0.09561 to 0.09540, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0968 - val_loss: 0.0954\n",
      "Epoch 67/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.0966\n",
      "Epoch 00067: val_loss improved from 0.09540 to 0.09528, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0968 - val_loss: 0.0953\n",
      "Epoch 68/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0968\n",
      "Epoch 00068: val_loss did not improve from 0.09528\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0968 - val_loss: 0.0956\n",
      "Epoch 69/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00069: val_loss did not improve from 0.09528\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0966 - val_loss: 0.0955\n",
      " ###3 fold : val acc1 0.605, acc3 0.977, mae 0.209###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/35 [====================>.........] - ETA: 0s - loss: 18.7806 \n",
      "Epoch 00001: val_loss improved from inf to 13.23586, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 1s 6ms/step - loss: 17.5902 - val_loss: 13.2359\n",
      "Epoch 2/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 11.0066\n",
      "Epoch 00002: val_loss improved from 13.23586 to 6.94264, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0230 - val_loss: 6.9426\n",
      "Epoch 3/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 5.6505\n",
      "Epoch 00003: val_loss improved from 6.94264 to 3.61365, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.1580 - val_loss: 3.6137\n",
      "Epoch 4/100\n",
      "25/35 [====================>.........] - ETA: 0s - loss: 3.0382\n",
      "Epoch 00004: val_loss improved from 3.61365 to 2.11434, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.8243 - val_loss: 2.1143\n",
      "Epoch 5/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 1.8789\n",
      "Epoch 00005: val_loss improved from 2.11434 to 1.37039, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7658 - val_loss: 1.3704\n",
      "Epoch 6/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 1.2078\n",
      "Epoch 00006: val_loss improved from 1.37039 to 0.84533, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.1274 - val_loss: 0.8453\n",
      "Epoch 7/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.7452\n",
      "Epoch 00007: val_loss improved from 0.84533 to 0.51252, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6873 - val_loss: 0.5125\n",
      "Epoch 8/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.4577\n",
      "Epoch 00008: val_loss improved from 0.51252 to 0.32677, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4248 - val_loss: 0.3268\n",
      "Epoch 9/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.2975\n",
      "Epoch 00009: val_loss improved from 0.32677 to 0.23166, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2820 - val_loss: 0.2317\n",
      "Epoch 10/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.2202\n",
      "Epoch 00010: val_loss improved from 0.23166 to 0.18119, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2089 - val_loss: 0.1812\n",
      "Epoch 11/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1763\n",
      "Epoch 00011: val_loss improved from 0.18119 to 0.15610, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1712 - val_loss: 0.1561\n",
      "Epoch 12/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1535\n",
      "Epoch 00012: val_loss improved from 0.15610 to 0.14328, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1518 - val_loss: 0.1433\n",
      "Epoch 13/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.1425\n",
      "Epoch 00013: val_loss improved from 0.14328 to 0.13591, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1416 - val_loss: 0.1359\n",
      "Epoch 14/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.1349\n",
      "Epoch 00014: val_loss improved from 0.13591 to 0.13106, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1357 - val_loss: 0.1311\n",
      "Epoch 15/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1301\n",
      "Epoch 00015: val_loss improved from 0.13106 to 0.12790, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1318 - val_loss: 0.1279\n",
      "Epoch 16/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1293\n",
      "Epoch 00016: val_loss improved from 0.12790 to 0.12525, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1290 - val_loss: 0.1252\n",
      "Epoch 17/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00017: val_loss improved from 0.12525 to 0.12315, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1268 - val_loss: 0.1231\n",
      "Epoch 18/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00018: val_loss improved from 0.12315 to 0.12186, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1247 - val_loss: 0.1219\n",
      "Epoch 19/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1235\n",
      "Epoch 00019: val_loss improved from 0.12186 to 0.11986, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1231 - val_loss: 0.1199\n",
      "Epoch 20/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1225\n",
      "Epoch 00020: val_loss improved from 0.11986 to 0.11812, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1216 - val_loss: 0.1181\n",
      "Epoch 21/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1197\n",
      "Epoch 00021: val_loss improved from 0.11812 to 0.11698, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1201 - val_loss: 0.1170\n",
      "Epoch 22/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1196\n",
      "Epoch 00022: val_loss improved from 0.11698 to 0.11558, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1189 - val_loss: 0.1156\n",
      "Epoch 23/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1179\n",
      "Epoch 00023: val_loss improved from 0.11558 to 0.11475, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1178 - val_loss: 0.1147\n",
      "Epoch 24/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1169\n",
      "Epoch 00024: val_loss improved from 0.11475 to 0.11374, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1165 - val_loss: 0.1137\n",
      "Epoch 25/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1161\n",
      "Epoch 00025: val_loss improved from 0.11374 to 0.11239, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1154 - val_loss: 0.1124\n",
      "Epoch 26/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1119\n",
      "Epoch 00026: val_loss improved from 0.11239 to 0.11161, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1144 - val_loss: 0.1116\n",
      "Epoch 27/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1157\n",
      "Epoch 00027: val_loss improved from 0.11161 to 0.11087, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1134 - val_loss: 0.1109\n",
      "Epoch 28/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1110\n",
      "Epoch 00028: val_loss improved from 0.11087 to 0.10978, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1124 - val_loss: 0.1098\n",
      "Epoch 29/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1089\n",
      "Epoch 00029: val_loss improved from 0.10978 to 0.10952, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1116 - val_loss: 0.1095\n",
      "Epoch 30/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1097\n",
      "Epoch 00030: val_loss improved from 0.10952 to 0.10824, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1107 - val_loss: 0.1082\n",
      "Epoch 31/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1082\n",
      "Epoch 00031: val_loss improved from 0.10824 to 0.10745, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1099 - val_loss: 0.1074\n",
      "Epoch 32/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1092\n",
      "Epoch 00032: val_loss improved from 0.10745 to 0.10697, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1091 - val_loss: 0.1070\n",
      "Epoch 33/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00033: val_loss improved from 0.10697 to 0.10605, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1083 - val_loss: 0.1060\n",
      "Epoch 34/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1081\n",
      "Epoch 00034: val_loss improved from 0.10605 to 0.10562, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1077 - val_loss: 0.1056\n",
      "Epoch 35/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1069\n",
      "Epoch 00035: val_loss improved from 0.10562 to 0.10467, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1070 - val_loss: 0.1047\n",
      "Epoch 36/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1064\n",
      "Epoch 00036: val_loss improved from 0.10467 to 0.10410, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1063 - val_loss: 0.1041\n",
      "Epoch 37/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1076\n",
      "Epoch 00037: val_loss improved from 0.10410 to 0.10341, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1057 - val_loss: 0.1034\n",
      "Epoch 38/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1037\n",
      "Epoch 00038: val_loss improved from 0.10341 to 0.10285, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1052 - val_loss: 0.1029\n",
      "Epoch 39/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1041\n",
      "Epoch 00039: val_loss improved from 0.10285 to 0.10234, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1045 - val_loss: 0.1023\n",
      "Epoch 40/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1049\n",
      "Epoch 00040: val_loss improved from 0.10234 to 0.10194, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1038 - val_loss: 0.1019\n",
      "Epoch 41/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1040\n",
      "Epoch 00041: val_loss improved from 0.10194 to 0.10143, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1033 - val_loss: 0.1014\n",
      "Epoch 42/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1043\n",
      "Epoch 00042: val_loss improved from 0.10143 to 0.10077, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1028 - val_loss: 0.1008\n",
      "Epoch 43/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1038\n",
      "Epoch 00043: val_loss improved from 0.10077 to 0.10032, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1024 - val_loss: 0.1003\n",
      "Epoch 44/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1023\n",
      "Epoch 00044: val_loss improved from 0.10032 to 0.10000, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1019 - val_loss: 0.1000\n",
      "Epoch 45/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1015\n",
      "Epoch 00045: val_loss improved from 0.10000 to 0.09972, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1014 - val_loss: 0.0997\n",
      "Epoch 46/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1006\n",
      "Epoch 00046: val_loss improved from 0.09972 to 0.09956, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1011 - val_loss: 0.0996\n",
      "Epoch 47/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1007\n",
      "Epoch 00047: val_loss improved from 0.09956 to 0.09886, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1006 - val_loss: 0.0989\n",
      "Epoch 48/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1013\n",
      "Epoch 00048: val_loss improved from 0.09886 to 0.09876, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1002 - val_loss: 0.0988\n",
      "Epoch 49/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1015\n",
      "Epoch 00049: val_loss improved from 0.09876 to 0.09872, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0999 - val_loss: 0.0987\n",
      "Epoch 50/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00050: val_loss improved from 0.09872 to 0.09822, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0996 - val_loss: 0.0982\n",
      "Epoch 51/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1007\n",
      "Epoch 00051: val_loss improved from 0.09822 to 0.09789, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0993 - val_loss: 0.0979\n",
      "Epoch 52/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.0980\n",
      "Epoch 00052: val_loss improved from 0.09789 to 0.09734, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0989 - val_loss: 0.0973\n",
      "Epoch 53/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00053: val_loss improved from 0.09734 to 0.09691, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0986 - val_loss: 0.0969\n",
      "Epoch 54/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.0968\n",
      "Epoch 00054: val_loss did not improve from 0.09691\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0982 - val_loss: 0.0971\n",
      "Epoch 55/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00055: val_loss improved from 0.09691 to 0.09662, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0980 - val_loss: 0.0966\n",
      "Epoch 56/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00056: val_loss improved from 0.09662 to 0.09640, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0978 - val_loss: 0.0964\n",
      "Epoch 57/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00057: val_loss improved from 0.09640 to 0.09622, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0976 - val_loss: 0.0962\n",
      "Epoch 58/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0964\n",
      "Epoch 00058: val_loss improved from 0.09622 to 0.09616, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0974 - val_loss: 0.0962\n",
      "Epoch 59/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.0986\n",
      "Epoch 00059: val_loss improved from 0.09616 to 0.09614, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0974 - val_loss: 0.0961\n",
      "Epoch 60/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0962\n",
      "Epoch 00060: val_loss improved from 0.09614 to 0.09602, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0972 - val_loss: 0.0960\n",
      "Epoch 61/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0943\n",
      "Epoch 00061: val_loss improved from 0.09602 to 0.09582, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0970 - val_loss: 0.0958\n",
      "Epoch 62/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0964\n",
      "Epoch 00062: val_loss improved from 0.09582 to 0.09556, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0969 - val_loss: 0.0956\n",
      "Epoch 63/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0979\n",
      "Epoch 00063: val_loss did not improve from 0.09556\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 0.0956\n",
      "Epoch 64/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00064: val_loss improved from 0.09556 to 0.09541, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0967 - val_loss: 0.0954\n",
      "Epoch 65/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0949\n",
      "Epoch 00065: val_loss did not improve from 0.09541\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0968 - val_loss: 0.0955\n",
      "Epoch 66/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00066: val_loss did not improve from 0.09541\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0966 - val_loss: 0.0955\n",
      " ###4 fold : val acc1 0.594, acc3 0.976, mae 0.215###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/35 [===================>..........] - ETA: 0s - loss: 18.8853 \n",
      "Epoch 00001: val_loss improved from inf to 13.23675, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 17.5917 - val_loss: 13.2368\n",
      "Epoch 2/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 10.9965\n",
      "Epoch 00002: val_loss improved from 13.23675 to 6.93893, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0254 - val_loss: 6.9389\n",
      "Epoch 3/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 5.6520\n",
      "Epoch 00003: val_loss improved from 6.93893 to 3.60714, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.1506 - val_loss: 3.6071\n",
      "Epoch 4/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 3.0482\n",
      "Epoch 00004: val_loss improved from 3.60714 to 2.11365, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.8141 - val_loss: 2.1137\n",
      "Epoch 5/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 1.8719\n",
      "Epoch 00005: val_loss improved from 2.11365 to 1.37290, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7624 - val_loss: 1.3729\n",
      "Epoch 6/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 1.2095\n",
      "Epoch 00006: val_loss improved from 1.37290 to 0.84930, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.1262 - val_loss: 0.8493\n",
      "Epoch 7/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.7444\n",
      "Epoch 00007: val_loss improved from 0.84930 to 0.51639, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6889 - val_loss: 0.5164\n",
      "Epoch 8/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.4570\n",
      "Epoch 00008: val_loss improved from 0.51639 to 0.32941, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4265 - val_loss: 0.3294\n",
      "Epoch 9/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.2977\n",
      "Epoch 00009: val_loss improved from 0.32941 to 0.23300, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2829 - val_loss: 0.2330\n",
      "Epoch 10/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.2191\n",
      "Epoch 00010: val_loss improved from 0.23300 to 0.18154, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2089 - val_loss: 0.1815\n",
      "Epoch 11/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.1753\n",
      "Epoch 00011: val_loss improved from 0.18154 to 0.15597, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1704 - val_loss: 0.1560\n",
      "Epoch 12/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.1525\n",
      "Epoch 00012: val_loss improved from 0.15597 to 0.14294, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1509 - val_loss: 0.1429\n",
      "Epoch 13/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.1419\n",
      "Epoch 00013: val_loss improved from 0.14294 to 0.13559, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1407 - val_loss: 0.1356\n",
      "Epoch 14/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1349\n",
      "Epoch 00014: val_loss improved from 0.13559 to 0.13079, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1348 - val_loss: 0.1308\n",
      "Epoch 15/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1304\n",
      "Epoch 00015: val_loss improved from 0.13079 to 0.12772, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1310 - val_loss: 0.1277\n",
      "Epoch 16/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1288\n",
      "Epoch 00016: val_loss improved from 0.12772 to 0.12513, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1283 - val_loss: 0.1251\n",
      "Epoch 17/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1276\n",
      "Epoch 00017: val_loss improved from 0.12513 to 0.12303, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1261 - val_loss: 0.1230\n",
      "Epoch 18/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00018: val_loss improved from 0.12303 to 0.12155, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1242 - val_loss: 0.1215\n",
      "Epoch 19/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1225\n",
      "Epoch 00019: val_loss improved from 0.12155 to 0.11979, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1226 - val_loss: 0.1198\n",
      "Epoch 20/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1214\n",
      "Epoch 00020: val_loss improved from 0.11979 to 0.11820, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1211 - val_loss: 0.1182\n",
      "Epoch 21/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1201\n",
      "Epoch 00021: val_loss improved from 0.11820 to 0.11706, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1198 - val_loss: 0.1171\n",
      "Epoch 22/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1198\n",
      "Epoch 00022: val_loss improved from 0.11706 to 0.11564, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1185 - val_loss: 0.1156\n",
      "Epoch 23/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1172\n",
      "Epoch 00023: val_loss improved from 0.11564 to 0.11476, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1174 - val_loss: 0.1148\n",
      "Epoch 24/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1166\n",
      "Epoch 00024: val_loss improved from 0.11476 to 0.11386, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1162 - val_loss: 0.1139\n",
      "Epoch 25/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1161\n",
      "Epoch 00025: val_loss improved from 0.11386 to 0.11254, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1152 - val_loss: 0.1125\n",
      "Epoch 26/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1110\n",
      "Epoch 00026: val_loss improved from 0.11254 to 0.11167, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1142 - val_loss: 0.1117\n",
      "Epoch 27/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1148\n",
      "Epoch 00027: val_loss improved from 0.11167 to 0.11081, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1132 - val_loss: 0.1108\n",
      "Epoch 28/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1112\n",
      "Epoch 00028: val_loss improved from 0.11081 to 0.10987, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1123 - val_loss: 0.1099\n",
      "Epoch 29/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1096\n",
      "Epoch 00029: val_loss improved from 0.10987 to 0.10943, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1114 - val_loss: 0.1094\n",
      "Epoch 30/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1089\n",
      "Epoch 00030: val_loss improved from 0.10943 to 0.10842, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1106 - val_loss: 0.1084\n",
      "Epoch 31/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1077\n",
      "Epoch 00031: val_loss improved from 0.10842 to 0.10755, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1098 - val_loss: 0.1076\n",
      "Epoch 32/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1086\n",
      "Epoch 00032: val_loss improved from 0.10755 to 0.10713, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1090 - val_loss: 0.1071\n",
      "Epoch 33/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00033: val_loss improved from 0.10713 to 0.10616, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1083 - val_loss: 0.1062\n",
      "Epoch 34/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1082\n",
      "Epoch 00034: val_loss improved from 0.10616 to 0.10590, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1076 - val_loss: 0.1059\n",
      "Epoch 35/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1064\n",
      "Epoch 00035: val_loss improved from 0.10590 to 0.10480, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1071 - val_loss: 0.1048\n",
      "Epoch 36/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1067\n",
      "Epoch 00036: val_loss improved from 0.10480 to 0.10415, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1063 - val_loss: 0.1041\n",
      "Epoch 37/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1073\n",
      "Epoch 00037: val_loss improved from 0.10415 to 0.10352, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1057 - val_loss: 0.1035\n",
      "Epoch 38/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1048\n",
      "Epoch 00038: val_loss improved from 0.10352 to 0.10297, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1051 - val_loss: 0.1030\n",
      "Epoch 39/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1058\n",
      "Epoch 00039: val_loss improved from 0.10297 to 0.10268, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1046 - val_loss: 0.1027\n",
      "Epoch 40/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1026\n",
      "Epoch 00040: val_loss improved from 0.10268 to 0.10187, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1039 - val_loss: 0.1019\n",
      "Epoch 41/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1041\n",
      "Epoch 00041: val_loss improved from 0.10187 to 0.10153, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1034 - val_loss: 0.1015\n",
      "Epoch 42/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1048\n",
      "Epoch 00042: val_loss improved from 0.10153 to 0.10086, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1029 - val_loss: 0.1009\n",
      "Epoch 43/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1036\n",
      "Epoch 00043: val_loss improved from 0.10086 to 0.10056, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1026 - val_loss: 0.1006\n",
      "Epoch 44/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1028\n",
      "Epoch 00044: val_loss improved from 0.10056 to 0.10005, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1020 - val_loss: 0.1001\n",
      "Epoch 45/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1020\n",
      "Epoch 00045: val_loss improved from 0.10005 to 0.09974, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1015 - val_loss: 0.0997\n",
      "Epoch 46/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1008\n",
      "Epoch 00046: val_loss did not improve from 0.09974\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1012 - val_loss: 0.0999\n",
      "Epoch 47/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1002\n",
      "Epoch 00047: val_loss improved from 0.09974 to 0.09899, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1008 - val_loss: 0.0990\n",
      "Epoch 48/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1015\n",
      "Epoch 00048: val_loss improved from 0.09899 to 0.09886, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1004 - val_loss: 0.0989\n",
      "Epoch 49/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1014\n",
      "Epoch 00049: val_loss improved from 0.09886 to 0.09861, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1001 - val_loss: 0.0986\n",
      "Epoch 50/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.0998\n",
      "Epoch 00050: val_loss improved from 0.09861 to 0.09833, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0998 - val_loss: 0.0983\n",
      "Epoch 51/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1010\n",
      "Epoch 00051: val_loss improved from 0.09833 to 0.09792, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0995 - val_loss: 0.0979\n",
      "Epoch 52/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00052: val_loss improved from 0.09792 to 0.09749, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0991 - val_loss: 0.0975\n",
      "Epoch 53/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0986\n",
      "Epoch 00053: val_loss improved from 0.09749 to 0.09711, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0989 - val_loss: 0.0971\n",
      "Epoch 54/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.0971\n",
      "Epoch 00054: val_loss did not improve from 0.09711\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0985 - val_loss: 0.0972\n",
      "Epoch 55/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0978\n",
      "Epoch 00055: val_loss improved from 0.09711 to 0.09673, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0983 - val_loss: 0.0967\n",
      "Epoch 56/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00056: val_loss improved from 0.09673 to 0.09656, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0980 - val_loss: 0.0966\n",
      "Epoch 57/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0980\n",
      "Epoch 00057: val_loss improved from 0.09656 to 0.09631, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0979 - val_loss: 0.0963\n",
      "Epoch 58/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00058: val_loss did not improve from 0.09631\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0975 - val_loss: 0.0964\n",
      "Epoch 59/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.0989\n",
      "Epoch 00059: val_loss did not improve from 0.09631\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0977 - val_loss: 0.0965\n",
      " ###5 fold : val acc1 0.587, acc3 0.981, mae 0.216###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/35 [====================>.........] - ETA: 0s - loss: 18.7770 \n",
      "Epoch 00001: val_loss improved from inf to 13.23901, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 1s 6ms/step - loss: 17.5950 - val_loss: 13.2390\n",
      "Epoch 2/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 10.9780\n",
      "Epoch 00002: val_loss improved from 13.23901 to 6.94395, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0197 - val_loss: 6.9439\n",
      "Epoch 3/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 5.6649\n",
      "Epoch 00003: val_loss improved from 6.94395 to 3.61272, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.1545 - val_loss: 3.6127\n",
      "Epoch 4/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 3.0752\n",
      "Epoch 00004: val_loss improved from 3.61272 to 2.11821, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.8218 - val_loss: 2.1182\n",
      "Epoch 5/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 1.8973\n",
      "Epoch 00005: val_loss improved from 2.11821 to 1.37414, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7692 - val_loss: 1.3741\n",
      "Epoch 6/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 1.2265\n",
      "Epoch 00006: val_loss improved from 1.37414 to 0.84731, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.1299 - val_loss: 0.8473\n",
      "Epoch 7/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.7520\n",
      "Epoch 00007: val_loss improved from 0.84731 to 0.51304, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6885 - val_loss: 0.5130\n",
      "Epoch 8/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.4601\n",
      "Epoch 00008: val_loss improved from 0.51304 to 0.32764, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4261 - val_loss: 0.3276\n",
      "Epoch 9/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.3056\n",
      "Epoch 00009: val_loss improved from 0.32764 to 0.23229, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2836 - val_loss: 0.2323\n",
      "Epoch 10/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.2250\n",
      "Epoch 00010: val_loss improved from 0.23229 to 0.18115, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2104 - val_loss: 0.1811\n",
      "Epoch 11/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1771\n",
      "Epoch 00011: val_loss improved from 0.18115 to 0.15575, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1723 - val_loss: 0.1558\n",
      "Epoch 12/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1556\n",
      "Epoch 00012: val_loss improved from 0.15575 to 0.14275, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1529 - val_loss: 0.1427\n",
      "Epoch 13/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1428\n",
      "Epoch 00013: val_loss improved from 0.14275 to 0.13552, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1427 - val_loss: 0.1355\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1381\n",
      "Epoch 00014: val_loss improved from 0.13552 to 0.13054, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1366 - val_loss: 0.1305\n",
      "Epoch 15/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1315\n",
      "Epoch 00015: val_loss improved from 0.13054 to 0.12751, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1327 - val_loss: 0.1275\n",
      "Epoch 16/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1309\n",
      "Epoch 00016: val_loss improved from 0.12751 to 0.12484, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1298 - val_loss: 0.1248\n",
      "Epoch 17/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1279\n",
      "Epoch 00017: val_loss improved from 0.12484 to 0.12281, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1275 - val_loss: 0.1228\n",
      "Epoch 18/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1261\n",
      "Epoch 00018: val_loss improved from 0.12281 to 0.12128, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1255 - val_loss: 0.1213\n",
      "Epoch 19/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00019: val_loss improved from 0.12128 to 0.11941, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1239 - val_loss: 0.1194\n",
      "Epoch 20/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1223\n",
      "Epoch 00020: val_loss improved from 0.11941 to 0.11779, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1223 - val_loss: 0.1178\n",
      "Epoch 21/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1207\n",
      "Epoch 00021: val_loss improved from 0.11779 to 0.11665, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1209 - val_loss: 0.1166\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1214\n",
      "Epoch 00022: val_loss improved from 0.11665 to 0.11521, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1196 - val_loss: 0.1152\n",
      "Epoch 23/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1180\n",
      "Epoch 00023: val_loss improved from 0.11521 to 0.11435, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1185 - val_loss: 0.1144\n",
      "Epoch 24/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1166\n",
      "Epoch 00024: val_loss improved from 0.11435 to 0.11342, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1172 - val_loss: 0.1134\n",
      "Epoch 25/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1172\n",
      "Epoch 00025: val_loss improved from 0.11342 to 0.11216, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1162 - val_loss: 0.1122\n",
      "Epoch 26/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1118\n",
      "Epoch 00026: val_loss improved from 0.11216 to 0.11128, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1151 - val_loss: 0.1113\n",
      "Epoch 27/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1153\n",
      "Epoch 00027: val_loss improved from 0.11128 to 0.11033, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1141 - val_loss: 0.1103\n",
      "Epoch 28/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1120\n",
      "Epoch 00028: val_loss improved from 0.11033 to 0.10942, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1131 - val_loss: 0.1094\n",
      "Epoch 29/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00029: val_loss improved from 0.10942 to 0.10881, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1122 - val_loss: 0.1088\n",
      "Epoch 30/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1099\n",
      "Epoch 00030: val_loss improved from 0.10881 to 0.10784, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1114 - val_loss: 0.1078\n",
      "Epoch 31/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1077\n",
      "Epoch 00031: val_loss improved from 0.10784 to 0.10701, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1105 - val_loss: 0.1070\n",
      "Epoch 32/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1096\n",
      "Epoch 00032: val_loss improved from 0.10701 to 0.10663, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1097 - val_loss: 0.1066\n",
      "Epoch 33/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00033: val_loss improved from 0.10663 to 0.10575, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1090 - val_loss: 0.1057\n",
      "Epoch 34/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1092\n",
      "Epoch 00034: val_loss improved from 0.10575 to 0.10532, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1082 - val_loss: 0.1053\n",
      "Epoch 35/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1066\n",
      "Epoch 00035: val_loss improved from 0.10532 to 0.10426, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1077 - val_loss: 0.1043\n",
      "Epoch 36/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1071\n",
      "Epoch 00036: val_loss improved from 0.10426 to 0.10358, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1068 - val_loss: 0.1036\n",
      "Epoch 37/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1070\n",
      "Epoch 00037: val_loss improved from 0.10358 to 0.10297, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1062 - val_loss: 0.1030\n",
      "Epoch 38/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1054\n",
      "Epoch 00038: val_loss improved from 0.10297 to 0.10241, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1055 - val_loss: 0.1024\n",
      "Epoch 39/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1058\n",
      "Epoch 00039: val_loss improved from 0.10241 to 0.10207, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1050 - val_loss: 0.1021\n",
      "Epoch 40/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1032\n",
      "Epoch 00040: val_loss improved from 0.10207 to 0.10131, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1043 - val_loss: 0.1013\n",
      "Epoch 41/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1043\n",
      "Epoch 00041: val_loss improved from 0.10131 to 0.10098, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1038 - val_loss: 0.1010\n",
      "Epoch 42/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1048\n",
      "Epoch 00042: val_loss improved from 0.10098 to 0.10038, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1033 - val_loss: 0.1004\n",
      "Epoch 43/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1035\n",
      "Epoch 00043: val_loss improved from 0.10038 to 0.09996, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1030 - val_loss: 0.1000\n",
      "Epoch 44/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1025\n",
      "Epoch 00044: val_loss improved from 0.09996 to 0.09959, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1023 - val_loss: 0.0996\n",
      "Epoch 45/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1026\n",
      "Epoch 00045: val_loss improved from 0.09959 to 0.09937, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1018 - val_loss: 0.0994\n",
      "Epoch 46/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1009\n",
      "Epoch 00046: val_loss improved from 0.09937 to 0.09924, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1016 - val_loss: 0.0992\n",
      "Epoch 47/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1009\n",
      "Epoch 00047: val_loss improved from 0.09924 to 0.09853, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1011 - val_loss: 0.0985\n",
      "Epoch 48/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1020\n",
      "Epoch 00048: val_loss improved from 0.09853 to 0.09823, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1006 - val_loss: 0.0982\n",
      "Epoch 49/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1004\n",
      "Epoch 00049: val_loss did not improve from 0.09823\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1003 - val_loss: 0.0983\n",
      "Epoch 50/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0994\n",
      "Epoch 00050: val_loss improved from 0.09823 to 0.09775, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1000 - val_loss: 0.0978\n",
      "Epoch 51/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1016\n",
      "Epoch 00051: val_loss improved from 0.09775 to 0.09740, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0997 - val_loss: 0.0974\n",
      "Epoch 52/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00052: val_loss improved from 0.09740 to 0.09708, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0993 - val_loss: 0.0971\n",
      "Epoch 53/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00053: val_loss improved from 0.09708 to 0.09677, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0991 - val_loss: 0.0968\n",
      "Epoch 54/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0971\n",
      "Epoch 00054: val_loss did not improve from 0.09677\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0989 - val_loss: 0.0968\n",
      "Epoch 55/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0980\n",
      "Epoch 00055: val_loss improved from 0.09677 to 0.09640, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0987 - val_loss: 0.0964\n",
      "Epoch 56/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00056: val_loss improved from 0.09640 to 0.09625, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0984 - val_loss: 0.0963\n",
      "Epoch 57/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0981\n",
      "Epoch 00057: val_loss improved from 0.09625 to 0.09604, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0983 - val_loss: 0.0960\n",
      "Epoch 58/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00058: val_loss improved from 0.09604 to 0.09601, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0979 - val_loss: 0.0960\n",
      "Epoch 59/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00059: val_loss did not improve from 0.09601\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0982 - val_loss: 0.0965\n",
      "Epoch 60/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00060: val_loss improved from 0.09601 to 0.09577, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0979 - val_loss: 0.0958\n",
      "Epoch 61/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0953\n",
      "Epoch 00061: val_loss improved from 0.09577 to 0.09567, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0976 - val_loss: 0.0957\n",
      "Epoch 62/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0972\n",
      "Epoch 00062: val_loss did not improve from 0.09567\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0976 - val_loss: 0.0959\n",
      "Epoch 63/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00063: val_loss improved from 0.09567 to 0.09551, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0974 - val_loss: 0.0955\n",
      "Epoch 64/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0971\n",
      "Epoch 00064: val_loss did not improve from 0.09551\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0974 - val_loss: 0.0955\n",
      "Epoch 65/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.0952\n",
      "Epoch 00065: val_loss did not improve from 0.09551\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0972 - val_loss: 0.0958\n",
      " ###6 fold : val acc1 0.599, acc3 0.984, mae 0.209###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/35 [==================>...........] - ETA: 0s - loss: 19.0369 \n",
      "Epoch 00001: val_loss improved from inf to 13.22833, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 17.5973 - val_loss: 13.2283\n",
      "Epoch 2/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 11.0522\n",
      "Epoch 00002: val_loss improved from 13.22833 to 6.93867, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0206 - val_loss: 6.9387\n",
      "Epoch 3/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 5.6791\n",
      "Epoch 00003: val_loss improved from 6.93867 to 3.60270, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.1602 - val_loss: 3.6027\n",
      "Epoch 4/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 3.0787\n",
      "Epoch 00004: val_loss improved from 3.60270 to 2.10877, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.8179 - val_loss: 2.1088\n",
      "Epoch 5/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 1.8775\n",
      "Epoch 00005: val_loss improved from 2.10877 to 1.36676, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7605 - val_loss: 1.3668\n",
      "Epoch 6/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 1.2165\n",
      "Epoch 00006: val_loss improved from 1.36676 to 0.84244, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.1218 - val_loss: 0.8424\n",
      "Epoch 7/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.7457\n",
      "Epoch 00007: val_loss improved from 0.84244 to 0.51073, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6824 - val_loss: 0.5107\n",
      "Epoch 8/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.4507\n",
      "Epoch 00008: val_loss improved from 0.51073 to 0.32569, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4208 - val_loss: 0.3257\n",
      "Epoch 9/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.2975\n",
      "Epoch 00009: val_loss improved from 0.32569 to 0.23055, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2792 - val_loss: 0.2305\n",
      "Epoch 10/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.2191\n",
      "Epoch 00010: val_loss improved from 0.23055 to 0.18033, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2067 - val_loss: 0.1803\n",
      "Epoch 11/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1735\n",
      "Epoch 00011: val_loss improved from 0.18033 to 0.15542, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1691 - val_loss: 0.1554\n",
      "Epoch 12/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1528\n",
      "Epoch 00012: val_loss improved from 0.15542 to 0.14241, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1499 - val_loss: 0.1424\n",
      "Epoch 13/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1420\n",
      "Epoch 00013: val_loss improved from 0.14241 to 0.13524, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1398 - val_loss: 0.1352\n",
      "Epoch 14/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1336\n",
      "Epoch 00014: val_loss improved from 0.13524 to 0.13054, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1339 - val_loss: 0.1305\n",
      "Epoch 15/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1286\n",
      "Epoch 00015: val_loss improved from 0.13054 to 0.12739, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1301 - val_loss: 0.1274\n",
      "Epoch 16/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1282\n",
      "Epoch 00016: val_loss improved from 0.12739 to 0.12478, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1272 - val_loss: 0.1248\n",
      "Epoch 17/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00017: val_loss improved from 0.12478 to 0.12284, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1251 - val_loss: 0.1228\n",
      "Epoch 18/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1223\n",
      "Epoch 00018: val_loss improved from 0.12284 to 0.12133, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1231 - val_loss: 0.1213\n",
      "Epoch 19/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1224\n",
      "Epoch 00019: val_loss improved from 0.12133 to 0.11945, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1215 - val_loss: 0.1195\n",
      "Epoch 20/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1202\n",
      "Epoch 00020: val_loss improved from 0.11945 to 0.11795, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1200 - val_loss: 0.1180\n",
      "Epoch 21/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1183\n",
      "Epoch 00021: val_loss improved from 0.11795 to 0.11685, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1187 - val_loss: 0.1168\n",
      "Epoch 22/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1195\n",
      "Epoch 00022: val_loss improved from 0.11685 to 0.11544, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1175 - val_loss: 0.1154\n",
      "Epoch 23/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1154\n",
      "Epoch 00023: val_loss improved from 0.11544 to 0.11445, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1163 - val_loss: 0.1145\n",
      "Epoch 24/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1137\n",
      "Epoch 00024: val_loss improved from 0.11445 to 0.11359, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1151 - val_loss: 0.1136\n",
      "Epoch 25/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1154\n",
      "Epoch 00025: val_loss improved from 0.11359 to 0.11241, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1142 - val_loss: 0.1124\n",
      "Epoch 26/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1098\n",
      "Epoch 00026: val_loss improved from 0.11241 to 0.11153, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1131 - val_loss: 0.1115\n",
      "Epoch 27/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1131\n",
      "Epoch 00027: val_loss improved from 0.11153 to 0.11060, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1122 - val_loss: 0.1106\n",
      "Epoch 28/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1102\n",
      "Epoch 00028: val_loss improved from 0.11060 to 0.10976, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1112 - val_loss: 0.1098\n",
      "Epoch 29/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1110\n",
      "Epoch 00029: val_loss improved from 0.10976 to 0.10905, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1105 - val_loss: 0.1090\n",
      "Epoch 30/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1086\n",
      "Epoch 00030: val_loss improved from 0.10905 to 0.10813, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1096 - val_loss: 0.1081\n",
      "Epoch 31/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1074\n",
      "Epoch 00031: val_loss improved from 0.10813 to 0.10742, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1089 - val_loss: 0.1074\n",
      "Epoch 32/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00032: val_loss improved from 0.10742 to 0.10703, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1081 - val_loss: 0.1070\n",
      "Epoch 33/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00033: val_loss improved from 0.10703 to 0.10621, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1074 - val_loss: 0.1062\n",
      "Epoch 34/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00034: val_loss improved from 0.10621 to 0.10596, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1067 - val_loss: 0.1060\n",
      "Epoch 35/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1053\n",
      "Epoch 00035: val_loss improved from 0.10596 to 0.10486, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1063 - val_loss: 0.1049\n",
      "Epoch 36/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1056\n",
      "Epoch 00036: val_loss improved from 0.10486 to 0.10412, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1054 - val_loss: 0.1041\n",
      "Epoch 37/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1058\n",
      "Epoch 00037: val_loss improved from 0.10412 to 0.10356, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1047 - val_loss: 0.1036\n",
      "Epoch 38/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1042\n",
      "Epoch 00038: val_loss improved from 0.10356 to 0.10296, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1042 - val_loss: 0.1030\n",
      "Epoch 39/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1034\n",
      "Epoch 00039: val_loss improved from 0.10296 to 0.10269, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1037 - val_loss: 0.1027\n",
      "Epoch 40/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1017\n",
      "Epoch 00040: val_loss improved from 0.10269 to 0.10195, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1030 - val_loss: 0.1019\n",
      "Epoch 41/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1036\n",
      "Epoch 00041: val_loss improved from 0.10195 to 0.10167, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1026 - val_loss: 0.1017\n",
      "Epoch 42/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1025\n",
      "Epoch 00042: val_loss improved from 0.10167 to 0.10091, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1022 - val_loss: 0.1009\n",
      "Epoch 43/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1016\n",
      "Epoch 00043: val_loss improved from 0.10091 to 0.10050, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1018 - val_loss: 0.1005\n",
      "Epoch 44/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1017\n",
      "Epoch 00044: val_loss improved from 0.10050 to 0.10010, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1012 - val_loss: 0.1001\n",
      "Epoch 45/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1009\n",
      "Epoch 00045: val_loss improved from 0.10010 to 0.09991, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1007 - val_loss: 0.0999\n",
      "Epoch 46/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1005\n",
      "Epoch 00046: val_loss did not improve from 0.09991\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1005 - val_loss: 0.1000\n",
      "Epoch 47/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1004\n",
      "Epoch 00047: val_loss improved from 0.09991 to 0.09909, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1000 - val_loss: 0.0991\n",
      "Epoch 48/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1009\n",
      "Epoch 00048: val_loss improved from 0.09909 to 0.09878, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0996 - val_loss: 0.0988\n",
      "Epoch 49/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0986\n",
      "Epoch 00049: val_loss improved from 0.09878 to 0.09865, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0993 - val_loss: 0.0986\n",
      "Epoch 50/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0992\n",
      "Epoch 00050: val_loss improved from 0.09865 to 0.09830, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0989 - val_loss: 0.0983\n",
      "Epoch 51/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1003\n",
      "Epoch 00051: val_loss improved from 0.09830 to 0.09781, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0986 - val_loss: 0.0978\n",
      "Epoch 52/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00052: val_loss improved from 0.09781 to 0.09748, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0983 - val_loss: 0.0975\n",
      "Epoch 53/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00053: val_loss improved from 0.09748 to 0.09717, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0980 - val_loss: 0.0972\n",
      "Epoch 54/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00054: val_loss did not improve from 0.09717\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0979 - val_loss: 0.0973\n",
      "Epoch 55/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0968\n",
      "Epoch 00055: val_loss improved from 0.09717 to 0.09671, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0976 - val_loss: 0.0967\n",
      "Epoch 56/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0985\n",
      "Epoch 00056: val_loss improved from 0.09671 to 0.09662, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0973 - val_loss: 0.0966\n",
      "Epoch 57/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0971\n",
      "Epoch 00057: val_loss improved from 0.09662 to 0.09631, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0972 - val_loss: 0.0963\n",
      "Epoch 58/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00058: val_loss improved from 0.09631 to 0.09621, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0968 - val_loss: 0.0962\n",
      "Epoch 59/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0968\n",
      "Epoch 00059: val_loss did not improve from 0.09621\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 60/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0952\n",
      "Epoch 00060: val_loss improved from 0.09621 to 0.09601, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0967 - val_loss: 0.0960\n",
      "Epoch 61/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.0945\n",
      "Epoch 00061: val_loss improved from 0.09601 to 0.09580, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0964 - val_loss: 0.0958\n",
      "Epoch 62/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.0951\n",
      "Epoch 00062: val_loss did not improve from 0.09580\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 63/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.0963\n",
      "Epoch 00063: val_loss did not improve from 0.09580\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0962 - val_loss: 0.0960\n",
      " ###7 fold : val acc1 0.581, acc3 0.973, mae 0.223###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/35 [===================>..........] - ETA: 0s - loss: 18.9142 \n",
      "Epoch 00001: val_loss improved from inf to 13.21505, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 1s 6ms/step - loss: 17.6169 - val_loss: 13.2151\n",
      "Epoch 2/100\n",
      "25/35 [====================>.........] - ETA: 0s - loss: 10.8708\n",
      "Epoch 00002: val_loss improved from 13.21505 to 6.90632, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 10.0322 - val_loss: 6.9063\n",
      "Epoch 3/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 5.6870\n",
      "Epoch 00003: val_loss improved from 6.90632 to 3.57617, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.1660 - val_loss: 3.5762\n",
      "Epoch 4/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 3.0634\n",
      "Epoch 00004: val_loss improved from 3.57617 to 2.11268, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.8225 - val_loss: 2.1127\n",
      "Epoch 5/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 1.8775\n",
      "Epoch 00005: val_loss improved from 2.11268 to 1.38199, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.7622 - val_loss: 1.3820\n",
      "Epoch 6/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 1.2075\n",
      "Epoch 00006: val_loss improved from 1.38199 to 0.85640, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.1221 - val_loss: 0.8564\n",
      "Epoch 7/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.7395\n",
      "Epoch 00007: val_loss improved from 0.85640 to 0.52314, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6821 - val_loss: 0.5231\n",
      "Epoch 8/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.4471\n",
      "Epoch 00008: val_loss improved from 0.52314 to 0.33663, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4207 - val_loss: 0.3366\n",
      "Epoch 9/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.2973\n",
      "Epoch 00009: val_loss improved from 0.33663 to 0.24018, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2793 - val_loss: 0.2402\n",
      "Epoch 10/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.2173\n",
      "Epoch 00010: val_loss improved from 0.24018 to 0.18855, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2069 - val_loss: 0.1885\n",
      "Epoch 11/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.1737\n",
      "Epoch 00011: val_loss improved from 0.18855 to 0.16258, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1694 - val_loss: 0.1626\n",
      "Epoch 12/100\n",
      "24/35 [===================>..........] - ETA: 0s - loss: 0.1527\n",
      "Epoch 00012: val_loss improved from 0.16258 to 0.14896, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1502 - val_loss: 0.1490\n",
      "Epoch 13/100\n",
      "23/35 [==================>...........] - ETA: 0s - loss: 0.1425\n",
      "Epoch 00013: val_loss improved from 0.14896 to 0.14144, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1401 - val_loss: 0.1414\n",
      "Epoch 14/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1347\n",
      "Epoch 00014: val_loss improved from 0.14144 to 0.13639, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1342 - val_loss: 0.1364\n",
      "Epoch 15/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00015: val_loss improved from 0.13639 to 0.13310, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1303 - val_loss: 0.1331\n",
      "Epoch 16/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00016: val_loss improved from 0.13310 to 0.13033, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1275 - val_loss: 0.1303\n",
      "Epoch 17/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00017: val_loss improved from 0.13033 to 0.12827, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1253 - val_loss: 0.1283\n",
      "Epoch 18/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1227\n",
      "Epoch 00018: val_loss improved from 0.12827 to 0.12685, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1233 - val_loss: 0.1269\n",
      "Epoch 19/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1226\n",
      "Epoch 00019: val_loss improved from 0.12685 to 0.12470, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1217 - val_loss: 0.1247\n",
      "Epoch 20/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1206\n",
      "Epoch 00020: val_loss improved from 0.12470 to 0.12312, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1202 - val_loss: 0.1231\n",
      "Epoch 21/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.1180\n",
      "Epoch 00021: val_loss improved from 0.12312 to 0.12189, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes32_dropout0,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1188 - val_loss: 0.1219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00012: val_loss improved from 0.17021 to 0.15671, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3371 - val_loss: 0.1567\n",
      "Epoch 13/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3278\n",
      "Epoch 00013: val_loss improved from 0.15671 to 0.14657, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.3282 - val_loss: 0.1466\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2955\n",
      "Epoch 00014: val_loss improved from 0.14657 to 0.13733, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2955 - val_loss: 0.1373\n",
      "Epoch 15/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2832\n",
      "Epoch 00015: val_loss improved from 0.13733 to 0.13471, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2831 - val_loss: 0.1347\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2664\n",
      "Epoch 00016: val_loss improved from 0.13471 to 0.12535, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2664 - val_loss: 0.1253\n",
      "Epoch 17/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2531\n",
      "Epoch 00017: val_loss improved from 0.12535 to 0.12339, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2489 - val_loss: 0.1234\n",
      "Epoch 18/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2356\n",
      "Epoch 00018: val_loss improved from 0.12339 to 0.12000, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2317 - val_loss: 0.1200\n",
      "Epoch 19/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2184\n",
      "Epoch 00019: val_loss improved from 0.12000 to 0.11709, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2186 - val_loss: 0.1171\n",
      "Epoch 20/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1980\n",
      "Epoch 00020: val_loss improved from 0.11709 to 0.11507, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1976 - val_loss: 0.1151\n",
      "Epoch 21/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1972\n",
      "Epoch 00021: val_loss did not improve from 0.11507\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1919 - val_loss: 0.1176\n",
      "Epoch 22/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1820\n",
      "Epoch 00022: val_loss improved from 0.11507 to 0.11333, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1819 - val_loss: 0.1133\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1711\n",
      "Epoch 00023: val_loss improved from 0.11333 to 0.11276, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1711 - val_loss: 0.1128\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1642\n",
      "Epoch 00024: val_loss improved from 0.11276 to 0.10999, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1642 - val_loss: 0.1100\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1548\n",
      "Epoch 00025: val_loss did not improve from 0.10999\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1548 - val_loss: 0.1119\n",
      "Epoch 26/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1489\n",
      "Epoch 00026: val_loss did not improve from 0.10999\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1492 - val_loss: 0.1115\n",
      " ###1 fold : val acc1 0.564, acc3 0.975, mae 0.232###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 21.2047 \n",
      "Epoch 00001: val_loss improved from inf to 14.54912, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 19.3221 - val_loss: 14.5491\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 11.6695\n",
      "Epoch 00002: val_loss improved from 14.54912 to 4.64477, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 9.6429 - val_loss: 4.6448\n",
      "Epoch 3/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.5705\n",
      "Epoch 00003: val_loss improved from 4.64477 to 2.10257, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1689 - val_loss: 2.1026\n",
      "Epoch 4/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.1921\n",
      "Epoch 00004: val_loss improved from 2.10257 to 1.31565, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0019 - val_loss: 1.3156\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.4161\n",
      "Epoch 00005: val_loss improved from 1.31565 to 0.77060, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2922 - val_loss: 0.7706\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9056\n",
      "Epoch 00006: val_loss improved from 0.77060 to 0.47890, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8502 - val_loss: 0.4789\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6606\n",
      "Epoch 00007: val_loss improved from 0.47890 to 0.33978, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6252 - val_loss: 0.3398\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5577\n",
      "Epoch 00008: val_loss improved from 0.33978 to 0.26050, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5208 - val_loss: 0.2605\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4629\n",
      "Epoch 00009: val_loss improved from 0.26050 to 0.21771, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4475 - val_loss: 0.2177\n",
      "Epoch 10/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4045\n",
      "Epoch 00010: val_loss improved from 0.21771 to 0.19016, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3990 - val_loss: 0.1902\n",
      "Epoch 11/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3703\n",
      "Epoch 00011: val_loss improved from 0.19016 to 0.16945, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3666 - val_loss: 0.1694\n",
      "Epoch 12/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3352\n",
      "Epoch 00012: val_loss improved from 0.16945 to 0.15569, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3339 - val_loss: 0.1557\n",
      "Epoch 13/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3256\n",
      "Epoch 00013: val_loss improved from 0.15569 to 0.14603, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3263 - val_loss: 0.1460\n",
      "Epoch 14/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2960\n",
      "Epoch 00014: val_loss improved from 0.14603 to 0.13717, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2956 - val_loss: 0.1372\n",
      "Epoch 15/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2813\n",
      "Epoch 00015: val_loss improved from 0.13717 to 0.13294, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2807 - val_loss: 0.1329\n",
      "Epoch 16/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2662\n",
      "Epoch 00016: val_loss improved from 0.13294 to 0.12462, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2652 - val_loss: 0.1246\n",
      "Epoch 17/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2492\n",
      "Epoch 00017: val_loss improved from 0.12462 to 0.12359, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2493 - val_loss: 0.1236\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2296\n",
      "Epoch 00018: val_loss improved from 0.12359 to 0.11945, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2296 - val_loss: 0.1195\n",
      "Epoch 19/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2191\n",
      "Epoch 00019: val_loss improved from 0.11945 to 0.11944, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2186 - val_loss: 0.1194\n",
      "Epoch 20/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1982\n",
      "Epoch 00020: val_loss improved from 0.11944 to 0.11527, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1978 - val_loss: 0.1153\n",
      "Epoch 21/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1927\n",
      "Epoch 00021: val_loss did not improve from 0.11527\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1922 - val_loss: 0.1179\n",
      "Epoch 22/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1824\n",
      "Epoch 00022: val_loss improved from 0.11527 to 0.11399, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1817 - val_loss: 0.1140\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1706\n",
      "Epoch 00023: val_loss improved from 0.11399 to 0.11242, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1706 - val_loss: 0.1124\n",
      "Epoch 24/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1659\n",
      "Epoch 00024: val_loss improved from 0.11242 to 0.11066, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1661 - val_loss: 0.1107\n",
      "Epoch 25/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1560\n",
      "Epoch 00025: val_loss did not improve from 0.11066\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1554 - val_loss: 0.1127\n",
      "Epoch 26/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1490\n",
      "Epoch 00026: val_loss did not improve from 0.11066\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1492 - val_loss: 0.1115\n",
      " ###2 fold : val acc1 0.572, acc3 0.971, mae 0.229###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 21.2168 \n",
      "Epoch 00001: val_loss improved from inf to 14.53628, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 19.3786 - val_loss: 14.5363\n",
      "Epoch 2/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 11.5385\n",
      "Epoch 00002: val_loss improved from 14.53628 to 4.63521, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 9.6731 - val_loss: 4.6352\n",
      "Epoch 3/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 3.5820\n",
      "Epoch 00003: val_loss improved from 4.63521 to 2.10430, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1768 - val_loss: 2.1043\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.2080\n",
      "Epoch 00004: val_loss improved from 2.10430 to 1.31050, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9996 - val_loss: 1.3105\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.4122\n",
      "Epoch 00005: val_loss improved from 1.31050 to 0.76704, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2877 - val_loss: 0.7670\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9067\n",
      "Epoch 00006: val_loss improved from 0.76704 to 0.47649, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8493 - val_loss: 0.4765\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6589\n",
      "Epoch 00007: val_loss improved from 0.47649 to 0.33700, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6249 - val_loss: 0.3370\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5618\n",
      "Epoch 00008: val_loss improved from 0.33700 to 0.25854, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5225 - val_loss: 0.2585\n",
      "Epoch 9/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4469\n",
      "Epoch 00009: val_loss improved from 0.25854 to 0.21678, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4462 - val_loss: 0.2168\n",
      "Epoch 10/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.4010\n",
      "Epoch 00010: val_loss improved from 0.21678 to 0.18951, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4003 - val_loss: 0.1895\n",
      "Epoch 11/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3629\n",
      "Epoch 00011: val_loss improved from 0.18951 to 0.16856, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3615 - val_loss: 0.1686\n",
      "Epoch 12/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3366\n",
      "Epoch 00012: val_loss improved from 0.16856 to 0.15543, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3367 - val_loss: 0.1554\n",
      "Epoch 13/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3336\n",
      "Epoch 00013: val_loss improved from 0.15543 to 0.14544, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3239 - val_loss: 0.1454\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2996\n",
      "Epoch 00014: val_loss improved from 0.14544 to 0.13783, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2996 - val_loss: 0.1378\n",
      "Epoch 15/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2823\n",
      "Epoch 00015: val_loss improved from 0.13783 to 0.13226, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2807 - val_loss: 0.1323\n",
      "Epoch 16/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2659\n",
      "Epoch 00016: val_loss improved from 0.13226 to 0.12586, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2652 - val_loss: 0.1259\n",
      "Epoch 17/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2485\n",
      "Epoch 00017: val_loss improved from 0.12586 to 0.12375, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2486 - val_loss: 0.1237\n",
      "Epoch 18/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2302\n",
      "Epoch 00018: val_loss improved from 0.12375 to 0.11960, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2302 - val_loss: 0.1196\n",
      "Epoch 19/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2213\n",
      "Epoch 00019: val_loss improved from 0.11960 to 0.11936, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2219 - val_loss: 0.1194\n",
      "Epoch 20/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1994\n",
      "Epoch 00020: val_loss improved from 0.11936 to 0.11504, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1985 - val_loss: 0.1150\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1921\n",
      "Epoch 00021: val_loss did not improve from 0.11504\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1921 - val_loss: 0.1198\n",
      "Epoch 22/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1821\n",
      "Epoch 00022: val_loss improved from 0.11504 to 0.11331, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1823 - val_loss: 0.1133\n",
      "Epoch 23/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1693\n",
      "Epoch 00023: val_loss improved from 0.11331 to 0.11226, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1694 - val_loss: 0.1123\n",
      "Epoch 24/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1655\n",
      "Epoch 00024: val_loss improved from 0.11226 to 0.11073, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1659 - val_loss: 0.1107\n",
      "Epoch 25/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1547\n",
      "Epoch 00025: val_loss did not improve from 0.11073\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1544 - val_loss: 0.1115\n",
      "Epoch 26/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1498\n",
      "Epoch 00026: val_loss did not improve from 0.11073\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1492 - val_loss: 0.1122\n",
      " ###3 fold : val acc1 0.577, acc3 0.963, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 21.0655 \n",
      "Epoch 00001: val_loss improved from inf to 14.53925, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 19.3120 - val_loss: 14.5392\n",
      "Epoch 2/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 11.5119\n",
      "Epoch 00002: val_loss improved from 14.53925 to 4.62932, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 9.6592 - val_loss: 4.6293\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.5863\n",
      "Epoch 00003: val_loss improved from 4.62932 to 2.10501, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1632 - val_loss: 2.1050\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.1966\n",
      "Epoch 00004: val_loss improved from 2.10501 to 1.32442, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0126 - val_loss: 1.3244\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.4120\n",
      "Epoch 00005: val_loss improved from 1.32442 to 0.77606, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2943 - val_loss: 0.7761\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9495\n",
      "Epoch 00006: val_loss improved from 0.77606 to 0.47910, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8670 - val_loss: 0.4791\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6832\n",
      "Epoch 00007: val_loss improved from 0.47910 to 0.33784, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6309 - val_loss: 0.3378\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5392\n",
      "Epoch 00008: val_loss improved from 0.33784 to 0.26212, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5122 - val_loss: 0.2621\n",
      "Epoch 9/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4638\n",
      "Epoch 00009: val_loss improved from 0.26212 to 0.21495, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4490 - val_loss: 0.2150\n",
      "Epoch 10/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4045\n",
      "Epoch 00010: val_loss improved from 0.21495 to 0.18967, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3975 - val_loss: 0.1897\n",
      "Epoch 11/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3722\n",
      "Epoch 00011: val_loss improved from 0.18967 to 0.17169, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3720 - val_loss: 0.1717\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3463\n",
      "Epoch 00012: val_loss improved from 0.17169 to 0.15825, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3464 - val_loss: 0.1582\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3303\n",
      "Epoch 00013: val_loss improved from 0.15825 to 0.14854, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3273 - val_loss: 0.1485\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.3108\n",
      "Epoch 00014: val_loss improved from 0.14854 to 0.14160, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3094 - val_loss: 0.1416\n",
      "Epoch 15/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2887\n",
      "Epoch 00015: val_loss improved from 0.14160 to 0.13620, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2891 - val_loss: 0.1362\n",
      "Epoch 16/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2765\n",
      "Epoch 00016: val_loss improved from 0.13620 to 0.13272, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2704 - val_loss: 0.1327\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2575\n",
      "Epoch 00017: val_loss improved from 0.13272 to 0.13091, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2575 - val_loss: 0.1309\n",
      "Epoch 18/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2383\n",
      "Epoch 00018: val_loss improved from 0.13091 to 0.12443, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2377 - val_loss: 0.1244\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2197\n",
      "Epoch 00019: val_loss did not improve from 0.12443\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2197 - val_loss: 0.1261\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2137\n",
      "Epoch 00020: val_loss improved from 0.12443 to 0.12083, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2137 - val_loss: 0.1208\n",
      "Epoch 21/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1933\n",
      "Epoch 00021: val_loss improved from 0.12083 to 0.12045, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1947 - val_loss: 0.1205\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1898\n",
      "Epoch 00022: val_loss improved from 0.12045 to 0.11714, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1898 - val_loss: 0.1171\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1751\n",
      "Epoch 00023: val_loss did not improve from 0.11714\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1751 - val_loss: 0.1180\n",
      "Epoch 24/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1674\n",
      "Epoch 00024: val_loss improved from 0.11714 to 0.11440, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1671 - val_loss: 0.1144\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1614\n",
      "Epoch 00025: val_loss did not improve from 0.11440\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1614 - val_loss: 0.1191\n",
      "Epoch 26/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1513\n",
      "Epoch 00026: val_loss improved from 0.11440 to 0.11359, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1518 - val_loss: 0.1136\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1501\n",
      "Epoch 00027: val_loss did not improve from 0.11359\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1501 - val_loss: 0.1160\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1451\n",
      "Epoch 00028: val_loss did not improve from 0.11359\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1451 - val_loss: 0.1150\n",
      " ###4 fold : val acc1 0.554, acc3 0.964, mae 0.241###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 21.0557 \n",
      "Epoch 00001: val_loss improved from inf to 14.53567, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 19.3369 - val_loss: 14.5357\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 11.8209\n",
      "Epoch 00002: val_loss improved from 14.53567 to 4.62185, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 9.6792 - val_loss: 4.6218\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.5640\n",
      "Epoch 00003: val_loss improved from 4.62185 to 2.10332, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1592 - val_loss: 2.1033\n",
      "Epoch 4/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.1966\n",
      "Epoch 00004: val_loss improved from 2.10332 to 1.32374, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9947 - val_loss: 1.3237\n",
      "Epoch 5/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.4191\n",
      "Epoch 00005: val_loss improved from 1.32374 to 0.78044, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2892 - val_loss: 0.7804\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9452\n",
      "Epoch 00006: val_loss improved from 0.78044 to 0.48542, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8646 - val_loss: 0.4854\n",
      "Epoch 7/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6844\n",
      "Epoch 00007: val_loss improved from 0.48542 to 0.34226, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6314 - val_loss: 0.3423\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5425\n",
      "Epoch 00008: val_loss improved from 0.34226 to 0.26535, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5150 - val_loss: 0.2653\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4644\n",
      "Epoch 00009: val_loss improved from 0.26535 to 0.21724, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4488 - val_loss: 0.2172\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4024\n",
      "Epoch 00010: val_loss improved from 0.21724 to 0.19173, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3954 - val_loss: 0.1917\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3758\n",
      "Epoch 00011: val_loss improved from 0.19173 to 0.17264, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3758 - val_loss: 0.1726\n",
      "Epoch 12/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3492\n",
      "Epoch 00012: val_loss improved from 0.17264 to 0.15958, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3493 - val_loss: 0.1596\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3288\n",
      "Epoch 00013: val_loss improved from 0.15958 to 0.14958, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3288 - val_loss: 0.1496\n",
      "Epoch 14/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3125\n",
      "Epoch 00014: val_loss improved from 0.14958 to 0.14217, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3101 - val_loss: 0.1422\n",
      "Epoch 15/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 0.2858\n",
      "Epoch 00015: val_loss improved from 0.14217 to 0.13607, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2866 - val_loss: 0.1361\n",
      "Epoch 16/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2725\n",
      "Epoch 00016: val_loss improved from 0.13607 to 0.13289, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2726 - val_loss: 0.1329\n",
      "Epoch 17/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2569\n",
      "Epoch 00017: val_loss improved from 0.13289 to 0.13099, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2562 - val_loss: 0.1310\n",
      "Epoch 18/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2357\n",
      "Epoch 00018: val_loss improved from 0.13099 to 0.12493, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2352 - val_loss: 0.1249\n",
      "Epoch 19/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2202\n",
      "Epoch 00019: val_loss did not improve from 0.12493\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2198 - val_loss: 0.1261\n",
      "Epoch 20/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.2098\n",
      "Epoch 00020: val_loss improved from 0.12493 to 0.12212, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2110 - val_loss: 0.1221\n",
      "Epoch 21/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1924\n",
      "Epoch 00021: val_loss improved from 0.12212 to 0.12037, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1921 - val_loss: 0.1204\n",
      "Epoch 22/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1914\n",
      "Epoch 00022: val_loss improved from 0.12037 to 0.11757, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1901 - val_loss: 0.1176\n",
      "Epoch 23/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1756\n",
      "Epoch 00023: val_loss did not improve from 0.11757\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1752 - val_loss: 0.1198\n",
      "Epoch 24/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1670\n",
      "Epoch 00024: val_loss improved from 0.11757 to 0.11455, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1656 - val_loss: 0.1146\n",
      "Epoch 25/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1619\n",
      "Epoch 00025: val_loss did not improve from 0.11455\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1608 - val_loss: 0.1198\n",
      "Epoch 26/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1521\n",
      "Epoch 00026: val_loss did not improve from 0.11455\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1522 - val_loss: 0.1159\n",
      " ###5 fold : val acc1 0.544, acc3 0.968, mae 0.245###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 21.1576 \n",
      "Epoch 00001: val_loss improved from inf to 14.53881, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 19.3332 - val_loss: 14.5388\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 11.8035\n",
      "Epoch 00002: val_loss improved from 14.53881 to 4.62669, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 9.6719 - val_loss: 4.6267\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.5917\n",
      "Epoch 00003: val_loss improved from 4.62669 to 2.09764, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1723 - val_loss: 2.0976\n",
      "Epoch 4/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.2195\n",
      "Epoch 00004: val_loss improved from 2.09764 to 1.31516, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0043 - val_loss: 1.3152\n",
      "Epoch 5/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 1.4290\n",
      "Epoch 00005: val_loss improved from 1.31516 to 0.77097, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2957 - val_loss: 0.7710\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9370\n",
      "Epoch 00006: val_loss improved from 0.77097 to 0.47799, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8598 - val_loss: 0.4780\n",
      "Epoch 7/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6809\n",
      "Epoch 00007: val_loss improved from 0.47799 to 0.33853, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6327 - val_loss: 0.3385\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5446\n",
      "Epoch 00008: val_loss improved from 0.33853 to 0.26396, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5165 - val_loss: 0.2640\n",
      "Epoch 9/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4478\n",
      "Epoch 00009: val_loss improved from 0.26396 to 0.21547, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4476 - val_loss: 0.2155\n",
      "Epoch 10/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.3973\n",
      "Epoch 00010: val_loss improved from 0.21547 to 0.19032, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.3961 - val_loss: 0.1903\n",
      "Epoch 11/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.3770\n",
      "Epoch 00011: val_loss improved from 0.19032 to 0.17091, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.3775 - val_loss: 0.1709\n",
      "Epoch 12/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.3493\n",
      "Epoch 00012: val_loss improved from 0.17091 to 0.15869, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.3484 - val_loss: 0.1587\n",
      "Epoch 13/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.3299\n",
      "Epoch 00013: val_loss improved from 0.15869 to 0.14828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.3314 - val_loss: 0.1483\n",
      "Epoch 14/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.3130\n",
      "Epoch 00014: val_loss improved from 0.14828 to 0.14073, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.3117 - val_loss: 0.1407\n",
      "Epoch 15/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2862\n",
      "Epoch 00015: val_loss improved from 0.14073 to 0.13597, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2855 - val_loss: 0.1360\n",
      "Epoch 16/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2757\n",
      "Epoch 00016: val_loss improved from 0.13597 to 0.13224, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2758 - val_loss: 0.1322\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2548\n",
      "Epoch 00017: val_loss improved from 0.13224 to 0.12984, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2548 - val_loss: 0.1298\n",
      "Epoch 18/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2364\n",
      "Epoch 00018: val_loss improved from 0.12984 to 0.12534, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2361 - val_loss: 0.1253\n",
      "Epoch 19/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2242\n",
      "Epoch 00019: val_loss improved from 0.12534 to 0.12515, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2240 - val_loss: 0.1251\n",
      "Epoch 20/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2134\n",
      "Epoch 00020: val_loss improved from 0.12515 to 0.12247, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2132 - val_loss: 0.1225\n",
      "Epoch 21/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1943\n",
      "Epoch 00021: val_loss improved from 0.12247 to 0.12054, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1932 - val_loss: 0.1205\n",
      "Epoch 22/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1902\n",
      "Epoch 00022: val_loss improved from 0.12054 to 0.11806, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1893 - val_loss: 0.1181\n",
      "Epoch 23/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1764\n",
      "Epoch 00023: val_loss did not improve from 0.11806\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1757 - val_loss: 0.1201\n",
      "Epoch 24/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1678\n",
      "Epoch 00024: val_loss improved from 0.11806 to 0.11300, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1673 - val_loss: 0.1130\n",
      "Epoch 25/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1618\n",
      "Epoch 00025: val_loss did not improve from 0.11300\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1608 - val_loss: 0.1187\n",
      "Epoch 26/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.1542\n",
      "Epoch 00026: val_loss did not improve from 0.11300\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1539 - val_loss: 0.1163\n",
      " ###6 fold : val acc1 0.569, acc3 0.971, mae 0.230###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 21.0700 \n",
      "Epoch 00001: val_loss improved from inf to 14.53313, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 19.3398 - val_loss: 14.5331\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 11.7818\n",
      "Epoch 00002: val_loss improved from 14.53313 to 4.62663, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 9.6548 - val_loss: 4.6266\n",
      "Epoch 3/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 3.6265\n",
      "Epoch 00003: val_loss improved from 4.62663 to 2.10610, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1689 - val_loss: 2.1061\n",
      "Epoch 4/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.2384\n",
      "Epoch 00004: val_loss improved from 2.10610 to 1.32260, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0038 - val_loss: 1.3226\n",
      "Epoch 5/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.4255\n",
      "Epoch 00005: val_loss improved from 1.32260 to 0.77580, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2938 - val_loss: 0.7758\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9423\n",
      "Epoch 00006: val_loss improved from 0.77580 to 0.47938, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8588 - val_loss: 0.4794\n",
      "Epoch 7/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.6793\n",
      "Epoch 00007: val_loss improved from 0.47938 to 0.33889, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6284 - val_loss: 0.3389\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5360\n",
      "Epoch 00008: val_loss improved from 0.33889 to 0.26361, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5111 - val_loss: 0.2636\n",
      "Epoch 9/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4597\n",
      "Epoch 00009: val_loss improved from 0.26361 to 0.21652, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4437 - val_loss: 0.2165\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4028\n",
      "Epoch 00010: val_loss improved from 0.21652 to 0.19147, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3938 - val_loss: 0.1915\n",
      "Epoch 11/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3786\n",
      "Epoch 00011: val_loss improved from 0.19147 to 0.17120, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3749 - val_loss: 0.1712\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3441\n",
      "Epoch 00012: val_loss improved from 0.17120 to 0.15916, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3441 - val_loss: 0.1592\n",
      "Epoch 13/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3302\n",
      "Epoch 00013: val_loss improved from 0.15916 to 0.14891, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3298 - val_loss: 0.1489\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3082\n",
      "Epoch 00014: val_loss improved from 0.14891 to 0.14146, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3082 - val_loss: 0.1415\n",
      "Epoch 15/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.2837\n",
      "Epoch 00015: val_loss improved from 0.14146 to 0.13820, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2824 - val_loss: 0.1382\n",
      "Epoch 16/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2746\n",
      "Epoch 00016: val_loss improved from 0.13820 to 0.13339, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2746 - val_loss: 0.1334\n",
      "Epoch 17/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2523\n",
      "Epoch 00017: val_loss improved from 0.13339 to 0.12985, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2523 - val_loss: 0.1299\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2354\n",
      "Epoch 00018: val_loss improved from 0.12985 to 0.12616, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2354 - val_loss: 0.1262\n",
      "Epoch 19/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2318\n",
      "Epoch 00019: val_loss did not improve from 0.12616\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2245 - val_loss: 0.1263\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2115\n",
      "Epoch 00020: val_loss improved from 0.12616 to 0.12436, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2115 - val_loss: 0.1244\n",
      "Epoch 21/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1925\n",
      "Epoch 00021: val_loss improved from 0.12436 to 0.12084, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1922 - val_loss: 0.1208\n",
      "Epoch 22/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1849\n",
      "Epoch 00022: val_loss improved from 0.12084 to 0.11930, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1850 - val_loss: 0.1193\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1741\n",
      "Epoch 00023: val_loss did not improve from 0.11930\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1741 - val_loss: 0.1201\n",
      "Epoch 24/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1649\n",
      "Epoch 00024: val_loss improved from 0.11930 to 0.11486, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1647 - val_loss: 0.1149\n",
      "Epoch 25/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1611\n",
      "Epoch 00025: val_loss did not improve from 0.11486\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1589 - val_loss: 0.1195\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1522\n",
      "Epoch 00026: val_loss did not improve from 0.11486\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1522 - val_loss: 0.1171\n",
      " ###7 fold : val acc1 0.546, acc3 0.957, mae 0.249###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 21.1006 \n",
      "Epoch 00001: val_loss improved from inf to 14.59172, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 19.3689 - val_loss: 14.5917\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 11.7864\n",
      "Epoch 00002: val_loss improved from 14.59172 to 4.70106, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 9.6686 - val_loss: 4.7011\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 3.5813\n",
      "Epoch 00003: val_loss improved from 4.70106 to 2.10059, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1744 - val_loss: 2.1006\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.2170\n",
      "Epoch 00004: val_loss improved from 2.10059 to 1.31843, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0070 - val_loss: 1.3184\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 1.4339\n",
      "Epoch 00005: val_loss improved from 1.31843 to 0.76562, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2985 - val_loss: 0.7656\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9411\n",
      "Epoch 00006: val_loss improved from 0.76562 to 0.46925, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8595 - val_loss: 0.4693\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6812\n",
      "Epoch 00007: val_loss improved from 0.46925 to 0.33372, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6301 - val_loss: 0.3337\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5117\n",
      "Epoch 00008: val_loss improved from 0.33372 to 0.26284, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5117 - val_loss: 0.2628\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4442\n",
      "Epoch 00009: val_loss improved from 0.26284 to 0.21623, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4442 - val_loss: 0.2162\n",
      "Epoch 10/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3953\n",
      "Epoch 00010: val_loss improved from 0.21623 to 0.19335, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3946 - val_loss: 0.1934\n",
      "Epoch 11/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3799\n",
      "Epoch 00011: val_loss improved from 0.19335 to 0.17404, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3749 - val_loss: 0.1740\n",
      "Epoch 12/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3437\n",
      "Epoch 00012: val_loss improved from 0.17404 to 0.16313, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3440 - val_loss: 0.1631\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3307\n",
      "Epoch 00013: val_loss improved from 0.16313 to 0.15228, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3307 - val_loss: 0.1523\n",
      "Epoch 14/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3080\n",
      "Epoch 00014: val_loss improved from 0.15228 to 0.14503, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3078 - val_loss: 0.1450\n",
      "Epoch 15/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2799\n",
      "Epoch 00015: val_loss improved from 0.14503 to 0.14204, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2794 - val_loss: 0.1420\n",
      "Epoch 16/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2743\n",
      "Epoch 00016: val_loss improved from 0.14204 to 0.13828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2741 - val_loss: 0.1383\n",
      "Epoch 17/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2524\n",
      "Epoch 00017: val_loss improved from 0.13828 to 0.13398, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2524 - val_loss: 0.1340\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2358\n",
      "Epoch 00018: val_loss improved from 0.13398 to 0.12992, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2358 - val_loss: 0.1299\n",
      "Epoch 19/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2316\n",
      "Epoch 00019: val_loss improved from 0.12992 to 0.12982, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2242 - val_loss: 0.1298\n",
      "Epoch 20/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2107\n",
      "Epoch 00020: val_loss improved from 0.12982 to 0.12863, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2108 - val_loss: 0.1286\n",
      "Epoch 21/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1923\n",
      "Epoch 00021: val_loss improved from 0.12863 to 0.12544, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1919 - val_loss: 0.1254\n",
      "Epoch 22/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1846\n",
      "Epoch 00022: val_loss improved from 0.12544 to 0.12205, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1848 - val_loss: 0.1220\n",
      "Epoch 23/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1745\n",
      "Epoch 00023: val_loss did not improve from 0.12205\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1740 - val_loss: 0.1240\n",
      "Epoch 24/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1652\n",
      "Epoch 00024: val_loss improved from 0.12205 to 0.11838, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1647 - val_loss: 0.1184\n",
      "Epoch 25/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1603\n",
      "Epoch 00025: val_loss did not improve from 0.11838\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1584 - val_loss: 0.1232\n",
      "Epoch 26/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1525\n",
      "Epoch 00026: val_loss did not improve from 0.11838\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1520 - val_loss: 0.1204\n",
      " ###8 fold : val acc1 0.555, acc3 0.960, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/35 [=================>............] - ETA: 0s - loss: 20.9848 \n",
      "Epoch 00001: val_loss improved from inf to 14.47437, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 19.3689 - val_loss: 14.4744\n",
      "Epoch 2/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 11.4869\n",
      "Epoch 00002: val_loss improved from 14.47437 to 4.65292, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 9.6686 - val_loss: 4.6529\n",
      "Epoch 3/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 3.5146\n",
      "Epoch 00003: val_loss improved from 4.65292 to 2.12739, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.1744 - val_loss: 2.1274\n",
      "Epoch 4/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.2096\n",
      "Epoch 00004: val_loss improved from 2.12739 to 1.34015, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0070 - val_loss: 1.3402\n",
      "Epoch 5/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 1.4283\n",
      "Epoch 00005: val_loss improved from 1.34015 to 0.78800, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2985 - val_loss: 0.7880\n",
      "Epoch 6/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.9324\n",
      "Epoch 00006: val_loss improved from 0.78800 to 0.48858, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8595 - val_loss: 0.4886\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.6812\n",
      "Epoch 00007: val_loss improved from 0.48858 to 0.34779, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6301 - val_loss: 0.3478\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5392\n",
      "Epoch 00008: val_loss improved from 0.34779 to 0.27330, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5117 - val_loss: 0.2733\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4600\n",
      "Epoch 00009: val_loss improved from 0.27330 to 0.22491, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4442 - val_loss: 0.2249\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4033\n",
      "Epoch 00010: val_loss improved from 0.22491 to 0.20049, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3946 - val_loss: 0.2005\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3749\n",
      "Epoch 00011: val_loss improved from 0.20049 to 0.18013, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3749 - val_loss: 0.1801\n",
      "Epoch 12/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.3448\n",
      "Epoch 00012: val_loss improved from 0.18013 to 0.16868, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3440 - val_loss: 0.1687\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3307\n",
      "Epoch 00013: val_loss improved from 0.16868 to 0.15716, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3307 - val_loss: 0.1572\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3078\n",
      "Epoch 00014: val_loss improved from 0.15716 to 0.14963, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3078 - val_loss: 0.1496\n",
      "Epoch 15/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2799\n",
      "Epoch 00015: val_loss improved from 0.14963 to 0.14651, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2794 - val_loss: 0.1465\n",
      "Epoch 16/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.2742\n",
      "Epoch 00016: val_loss improved from 0.14651 to 0.14232, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2741 - val_loss: 0.1423\n",
      "Epoch 17/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2568\n",
      "Epoch 00017: val_loss improved from 0.14232 to 0.13794, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2524 - val_loss: 0.1379\n",
      "Epoch 18/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.2393\n",
      "Epoch 00018: val_loss improved from 0.13794 to 0.13367, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2358 - val_loss: 0.1337\n",
      "Epoch 19/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2315\n",
      "Epoch 00019: val_loss improved from 0.13367 to 0.13366, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2242 - val_loss: 0.1337\n",
      "Epoch 20/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.2152\n",
      "Epoch 00020: val_loss improved from 0.13366 to 0.13228, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2108 - val_loss: 0.1323\n",
      "Epoch 21/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.1897\n",
      "Epoch 00021: val_loss improved from 0.13228 to 0.12898, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1919 - val_loss: 0.1290\n",
      "Epoch 22/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.1881\n",
      "Epoch 00022: val_loss improved from 0.12898 to 0.12579, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1848 - val_loss: 0.1258\n",
      "Epoch 23/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1745\n",
      "Epoch 00023: val_loss did not improve from 0.12579\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1740 - val_loss: 0.1275\n",
      "Epoch 24/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1649\n",
      "Epoch 00024: val_loss improved from 0.12579 to 0.12207, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes16_dropout0.1,dnodes64_dropout0,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1647 - val_loss: 0.1221\n",
      "Epoch 25/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1591\n",
      "Epoch 00025: val_loss did not improve from 0.12207\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1584 - val_loss: 0.1267\n",
      "Epoch 26/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1523\n",
      "Epoch 00026: val_loss did not improve from 0.12207\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1520 - val_loss: 0.1240\n",
      " ###9 fold : val acc1 0.565, acc3 0.970, mae 0.234###\n",
      "acc10.561_acc30.967\n",
      "random search 97/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/140 [============================>.] - ETA: 0s - loss: 6.2853\n",
      "Epoch 00001: val_loss improved from inf to 0.95289, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.2765 - val_loss: 0.9529\n",
      "Epoch 2/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.7008\n",
      "Epoch 00002: val_loss improved from 0.95289 to 0.14626, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6804 - val_loss: 0.1463\n",
      "Epoch 3/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.4076\n",
      "Epoch 00003: val_loss improved from 0.14626 to 0.11775, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4078 - val_loss: 0.1178\n",
      "Epoch 4/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.3676\n",
      "Epoch 00004: val_loss improved from 0.11775 to 0.11589, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3687 - val_loss: 0.1159\n",
      "Epoch 5/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3491\n",
      "Epoch 00005: val_loss improved from 0.11589 to 0.10938, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3482 - val_loss: 0.1094\n",
      "Epoch 6/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3255\n",
      "Epoch 00006: val_loss improved from 0.10938 to 0.10510, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3255 - val_loss: 0.1051\n",
      "Epoch 7/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.3137\n",
      "Epoch 00007: val_loss did not improve from 0.10510\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3133 - val_loss: 0.1059\n",
      "Epoch 8/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.3001\n",
      "Epoch 00008: val_loss improved from 0.10510 to 0.10481, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3000 - val_loss: 0.1048\n",
      "Epoch 9/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.2895\n",
      "Epoch 00009: val_loss improved from 0.10481 to 0.10179, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2888 - val_loss: 0.1018\n",
      "Epoch 10/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2764\n",
      "Epoch 00010: val_loss improved from 0.10179 to 0.10004, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2769 - val_loss: 0.1000\n",
      "Epoch 11/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.2700\n",
      "Epoch 00011: val_loss improved from 0.10004 to 0.09975, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2685 - val_loss: 0.0997\n",
      "Epoch 12/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.2609\n",
      "Epoch 00012: val_loss improved from 0.09975 to 0.09824, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2605 - val_loss: 0.0982\n",
      "Epoch 13/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.2573\n",
      "Epoch 00013: val_loss did not improve from 0.09824\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2580 - val_loss: 0.0995\n",
      "Epoch 14/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.2583\n",
      "Epoch 00014: val_loss did not improve from 0.09824\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2587 - val_loss: 0.1000\n",
      " ###0 fold : val acc1 0.600, acc3 0.979, mae 0.211###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/140 [===========================>..] - ETA: 0s - loss: 6.5959\n",
      "Epoch 00001: val_loss improved from inf to 0.94176, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.2837 - val_loss: 0.9418\n",
      "Epoch 2/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.6955\n",
      "Epoch 00002: val_loss improved from 0.94176 to 0.14497, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6741 - val_loss: 0.1450\n",
      "Epoch 3/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.4059\n",
      "Epoch 00003: val_loss improved from 0.14497 to 0.11756, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4062 - val_loss: 0.1176\n",
      "Epoch 4/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3727\n",
      "Epoch 00004: val_loss improved from 0.11756 to 0.11310, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3727 - val_loss: 0.1131\n",
      "Epoch 5/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.3445\n",
      "Epoch 00005: val_loss improved from 0.11310 to 0.10816, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3438 - val_loss: 0.1082\n",
      "Epoch 6/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.3270\n",
      "Epoch 00006: val_loss improved from 0.10816 to 0.10592, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3265 - val_loss: 0.1059\n",
      "Epoch 7/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3155\n",
      "Epoch 00007: val_loss improved from 0.10592 to 0.10554, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3137 - val_loss: 0.1055\n",
      "Epoch 8/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.3005\n",
      "Epoch 00008: val_loss improved from 0.10554 to 0.10451, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3000 - val_loss: 0.1045\n",
      "Epoch 9/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2874\n",
      "Epoch 00009: val_loss improved from 0.10451 to 0.10132, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2874 - val_loss: 0.1013\n",
      "Epoch 10/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2794\n",
      "Epoch 00010: val_loss improved from 0.10132 to 0.09945, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2787 - val_loss: 0.0994\n",
      "Epoch 11/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2681\n",
      "Epoch 00011: val_loss did not improve from 0.09945\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2671 - val_loss: 0.0998\n",
      "Epoch 12/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.2598\n",
      "Epoch 00012: val_loss improved from 0.09945 to 0.09895, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2595 - val_loss: 0.0989\n",
      "Epoch 13/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.2581\n",
      "Epoch 00013: val_loss improved from 0.09895 to 0.09876, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2587 - val_loss: 0.0988\n",
      "Epoch 14/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.2599\n",
      "Epoch 00014: val_loss improved from 0.09876 to 0.09851, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2606 - val_loss: 0.0985\n",
      "Epoch 15/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2513\n",
      "Epoch 00015: val_loss improved from 0.09851 to 0.09820, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2506 - val_loss: 0.0982\n",
      "Epoch 16/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.2480\n",
      "Epoch 00016: val_loss did not improve from 0.09820\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2489 - val_loss: 0.0986\n",
      "Epoch 17/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.2432\n",
      "Epoch 00017: val_loss did not improve from 0.09820\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2433 - val_loss: 0.0985\n",
      " ###1 fold : val acc1 0.598, acc3 0.980, mae 0.211###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/140 [=========================>....] - ETA: 0s - loss: 6.8421\n",
      "Epoch 00001: val_loss improved from inf to 0.92977, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.2771 - val_loss: 0.9298\n",
      "Epoch 2/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.6710\n",
      "Epoch 00002: val_loss improved from 0.92977 to 0.14306, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.6679 - val_loss: 0.1431\n",
      "Epoch 3/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.4053\n",
      "Epoch 00003: val_loss improved from 0.14306 to 0.11723, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.4053 - val_loss: 0.1172\n",
      "Epoch 4/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.3723\n",
      "Epoch 00004: val_loss improved from 0.11723 to 0.11222, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3717 - val_loss: 0.1122\n",
      "Epoch 5/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3428\n",
      "Epoch 00005: val_loss improved from 0.11222 to 0.10749, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3422 - val_loss: 0.1075\n",
      "Epoch 6/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.3267\n",
      "Epoch 00006: val_loss improved from 0.10749 to 0.10698, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3265 - val_loss: 0.1070\n",
      "Epoch 7/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.3133\n",
      "Epoch 00007: val_loss improved from 0.10698 to 0.10510, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3131 - val_loss: 0.1051\n",
      "Epoch 8/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3035\n",
      "Epoch 00008: val_loss improved from 0.10510 to 0.10382, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3034 - val_loss: 0.1038\n",
      "Epoch 9/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2876\n",
      "Epoch 00009: val_loss improved from 0.10382 to 0.10196, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2873 - val_loss: 0.1020\n",
      "Epoch 10/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2841\n",
      "Epoch 00010: val_loss improved from 0.10196 to 0.09981, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2826 - val_loss: 0.0998\n",
      "Epoch 11/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2688\n",
      "Epoch 00011: val_loss did not improve from 0.09981\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2683 - val_loss: 0.1001\n",
      "Epoch 12/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2616\n",
      "Epoch 00012: val_loss improved from 0.09981 to 0.09923, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2621 - val_loss: 0.0992\n",
      "Epoch 13/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2614\n",
      "Epoch 00013: val_loss did not improve from 0.09923\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2615 - val_loss: 0.0993\n",
      "Epoch 14/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2618\n",
      "Epoch 00014: val_loss improved from 0.09923 to 0.09902, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2625 - val_loss: 0.0990\n",
      "Epoch 15/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2516\n",
      "Epoch 00015: val_loss improved from 0.09902 to 0.09883, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2513 - val_loss: 0.0988\n",
      "Epoch 16/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.2493\n",
      "Epoch 00016: val_loss improved from 0.09883 to 0.09744, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2501 - val_loss: 0.0974\n",
      "Epoch 17/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2464\n",
      "Epoch 00017: val_loss did not improve from 0.09744\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2458 - val_loss: 0.0975\n",
      "Epoch 18/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2440\n",
      "Epoch 00018: val_loss improved from 0.09744 to 0.09713, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2437 - val_loss: 0.0971\n",
      "Epoch 19/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.2368\n",
      "Epoch 00019: val_loss improved from 0.09713 to 0.09658, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2371 - val_loss: 0.0966\n",
      "Epoch 20/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2339\n",
      "Epoch 00020: val_loss did not improve from 0.09658\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2342 - val_loss: 0.0988\n",
      "Epoch 21/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2300\n",
      "Epoch 00021: val_loss did not improve from 0.09658\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2311 - val_loss: 0.0996\n",
      " ###2 fold : val acc1 0.606, acc3 0.979, mae 0.208###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/140 [===========================>..] - ETA: 0s - loss: 6.5161\n",
      "Epoch 00001: val_loss improved from inf to 0.92809, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.2766 - val_loss: 0.9281\n",
      "Epoch 2/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.6855\n",
      "Epoch 00002: val_loss improved from 0.92809 to 0.14387, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.6699 - val_loss: 0.1439\n",
      "Epoch 3/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.4052\n",
      "Epoch 00003: val_loss improved from 0.14387 to 0.11748, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4047 - val_loss: 0.1175\n",
      "Epoch 4/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.3711\n",
      "Epoch 00004: val_loss improved from 0.11748 to 0.11195, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3696 - val_loss: 0.1120\n",
      "Epoch 5/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.3411\n",
      "Epoch 00005: val_loss improved from 0.11195 to 0.10709, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3411 - val_loss: 0.1071\n",
      "Epoch 6/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3270\n",
      "Epoch 00006: val_loss did not improve from 0.10709\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3274 - val_loss: 0.1073\n",
      "Epoch 7/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.3138\n",
      "Epoch 00007: val_loss improved from 0.10709 to 0.10491, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3134 - val_loss: 0.1049\n",
      "Epoch 8/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.3035\n",
      "Epoch 00008: val_loss did not improve from 0.10491\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3031 - val_loss: 0.1066\n",
      "Epoch 9/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.2888\n",
      "Epoch 00009: val_loss improved from 0.10491 to 0.10330, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2879 - val_loss: 0.1033\n",
      "Epoch 10/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2830\n",
      "Epoch 00010: val_loss improved from 0.10330 to 0.09975, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2820 - val_loss: 0.0998\n",
      "Epoch 11/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.2721\n",
      "Epoch 00011: val_loss improved from 0.09975 to 0.09945, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2713 - val_loss: 0.0994\n",
      "Epoch 12/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.2650\n",
      "Epoch 00012: val_loss did not improve from 0.09945\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2648 - val_loss: 0.0996\n",
      "Epoch 13/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2600\n",
      "Epoch 00013: val_loss improved from 0.09945 to 0.09920, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2607 - val_loss: 0.0992\n",
      "Epoch 14/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2639\n",
      "Epoch 00014: val_loss did not improve from 0.09920\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2641 - val_loss: 0.0992\n",
      "Epoch 15/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2518\n",
      "Epoch 00015: val_loss improved from 0.09920 to 0.09828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2514 - val_loss: 0.0983\n",
      "Epoch 16/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.2483\n",
      "Epoch 00016: val_loss improved from 0.09828 to 0.09794, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2496 - val_loss: 0.0979\n",
      "Epoch 17/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.2459\n",
      "Epoch 00017: val_loss improved from 0.09794 to 0.09702, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2457 - val_loss: 0.0970\n",
      "Epoch 18/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2444\n",
      "Epoch 00018: val_loss improved from 0.09702 to 0.09681, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2437 - val_loss: 0.0968\n",
      "Epoch 19/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2360\n",
      "Epoch 00019: val_loss did not improve from 0.09681\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2366 - val_loss: 0.0968\n",
      "Epoch 20/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2338\n",
      "Epoch 00020: val_loss did not improve from 0.09681\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2338 - val_loss: 0.0978\n",
      " ###3 fold : val acc1 0.618, acc3 0.975, mae 0.204###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/140 [==========================>...] - ETA: 0s - loss: 6.8004\n",
      "Epoch 00001: val_loss improved from inf to 0.93752, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.2836 - val_loss: 0.9375\n",
      "Epoch 2/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.6840\n",
      "Epoch 00002: val_loss improved from 0.93752 to 0.14236, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.6779 - val_loss: 0.1424\n",
      "Epoch 3/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.4072\n",
      "Epoch 00003: val_loss improved from 0.14236 to 0.11599, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4055 - val_loss: 0.1160\n",
      "Epoch 4/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.3665\n",
      "Epoch 00004: val_loss improved from 0.11599 to 0.11351, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3650 - val_loss: 0.1135\n",
      "Epoch 5/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.3291\n",
      "Epoch 00005: val_loss improved from 0.11351 to 0.11039, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3290 - val_loss: 0.1104\n",
      "Epoch 6/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.3213\n",
      "Epoch 00006: val_loss improved from 0.11039 to 0.10517, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3207 - val_loss: 0.1052\n",
      "Epoch 7/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3073\n",
      "Epoch 00007: val_loss did not improve from 0.10517\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3060 - val_loss: 0.1059\n",
      "Epoch 8/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2937\n",
      "Epoch 00008: val_loss improved from 0.10517 to 0.10385, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2939 - val_loss: 0.1038\n",
      "Epoch 9/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2819\n",
      "Epoch 00009: val_loss improved from 0.10385 to 0.10283, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2819 - val_loss: 0.1028\n",
      "Epoch 10/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.2703\n",
      "Epoch 00010: val_loss improved from 0.10283 to 0.10149, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2709 - val_loss: 0.1015\n",
      "Epoch 11/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.2641\n",
      "Epoch 00011: val_loss improved from 0.10149 to 0.10008, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2648 - val_loss: 0.1001\n",
      "Epoch 12/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.2562\n",
      "Epoch 00012: val_loss did not improve from 0.10008\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2578 - val_loss: 0.1010\n",
      "Epoch 13/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.2565\n",
      "Epoch 00013: val_loss improved from 0.10008 to 0.09970, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2570 - val_loss: 0.0997\n",
      "Epoch 14/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2456\n",
      "Epoch 00014: val_loss improved from 0.09970 to 0.09829, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2467 - val_loss: 0.0983\n",
      "Epoch 15/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2426\n",
      "Epoch 00015: val_loss did not improve from 0.09829\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2421 - val_loss: 0.0988\n",
      "Epoch 16/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2412\n",
      "Epoch 00016: val_loss improved from 0.09829 to 0.09781, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2403 - val_loss: 0.0978\n",
      "Epoch 17/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.2384\n",
      "Epoch 00017: val_loss did not improve from 0.09781\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2383 - val_loss: 0.1017\n",
      "Epoch 18/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.2372\n",
      "Epoch 00018: val_loss did not improve from 0.09781\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2374 - val_loss: 0.1002\n",
      " ###4 fold : val acc1 0.591, acc3 0.976, mae 0.216###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/140 [==========================>...] - ETA: 0s - loss: 6.6684\n",
      "Epoch 00001: val_loss improved from inf to 0.93833, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.2769 - val_loss: 0.9383\n",
      "Epoch 2/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.6827\n",
      "Epoch 00002: val_loss improved from 0.93833 to 0.14375, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6784 - val_loss: 0.1437\n",
      "Epoch 3/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.4032\n",
      "Epoch 00003: val_loss improved from 0.14375 to 0.11665, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4032 - val_loss: 0.1166\n",
      "Epoch 4/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3657\n",
      "Epoch 00004: val_loss improved from 0.11665 to 0.11373, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3657 - val_loss: 0.1137\n",
      "Epoch 5/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3283\n",
      "Epoch 00005: val_loss improved from 0.11373 to 0.11083, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3294 - val_loss: 0.1108\n",
      "Epoch 6/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.3216\n",
      "Epoch 00006: val_loss improved from 0.11083 to 0.10613, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3218 - val_loss: 0.1061\n",
      "Epoch 7/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3065\n",
      "Epoch 00007: val_loss improved from 0.10613 to 0.10466, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3067 - val_loss: 0.1047\n",
      "Epoch 8/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.2949\n",
      "Epoch 00008: val_loss improved from 0.10466 to 0.10367, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2952 - val_loss: 0.1037\n",
      "Epoch 9/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2831\n",
      "Epoch 00009: val_loss improved from 0.10367 to 0.10301, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2828 - val_loss: 0.1030\n",
      "Epoch 10/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2716\n",
      "Epoch 00010: val_loss improved from 0.10301 to 0.10221, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2715 - val_loss: 0.1022\n",
      "Epoch 11/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.2651\n",
      "Epoch 00011: val_loss improved from 0.10221 to 0.09979, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2650 - val_loss: 0.0998\n",
      "Epoch 12/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.2580\n",
      "Epoch 00012: val_loss did not improve from 0.09979\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2583 - val_loss: 0.1020\n",
      "Epoch 13/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2585\n",
      "Epoch 00013: val_loss improved from 0.09979 to 0.09918, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2588 - val_loss: 0.0992\n",
      "Epoch 14/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2461\n",
      "Epoch 00014: val_loss improved from 0.09918 to 0.09880, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2464 - val_loss: 0.0988\n",
      "Epoch 15/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2425\n",
      "Epoch 00015: val_loss did not improve from 0.09880\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2425 - val_loss: 0.1002\n",
      "Epoch 16/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.2408\n",
      "Epoch 00016: val_loss did not improve from 0.09880\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2408 - val_loss: 0.1007\n",
      " ###5 fold : val acc1 0.592, acc3 0.978, mae 0.216###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/140 [==========================>...] - ETA: 0s - loss: 6.6288\n",
      "Epoch 00001: val_loss improved from inf to 0.94105, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.2788 - val_loss: 0.9411\n",
      "Epoch 2/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.6960\n",
      "Epoch 00002: val_loss improved from 0.94105 to 0.14532, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.6795 - val_loss: 0.1453\n",
      "Epoch 3/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.4061\n",
      "Epoch 00003: val_loss improved from 0.14532 to 0.11619, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.4050 - val_loss: 0.1162\n",
      "Epoch 4/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.3662\n",
      "Epoch 00004: val_loss improved from 0.11619 to 0.11344, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3660 - val_loss: 0.1134\n",
      "Epoch 5/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.3325\n",
      "Epoch 00005: val_loss improved from 0.11344 to 0.10992, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3326 - val_loss: 0.1099\n",
      "Epoch 6/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.3231\n",
      "Epoch 00006: val_loss improved from 0.10992 to 0.10742, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3235 - val_loss: 0.1074\n",
      "Epoch 7/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3066\n",
      "Epoch 00007: val_loss improved from 0.10742 to 0.10356, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3066 - val_loss: 0.1036\n",
      "Epoch 8/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.3009\n",
      "Epoch 00008: val_loss improved from 0.10356 to 0.10236, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3009 - val_loss: 0.1024\n",
      "Epoch 9/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.2842\n",
      "Epoch 00009: val_loss did not improve from 0.10236\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2842 - val_loss: 0.1025\n",
      "Epoch 10/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2724\n",
      "Epoch 00010: val_loss improved from 0.10236 to 0.10104, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2724 - val_loss: 0.1010\n",
      "Epoch 11/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2665\n",
      "Epoch 00011: val_loss improved from 0.10104 to 0.09987, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2678 - val_loss: 0.0999\n",
      "Epoch 12/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2612\n",
      "Epoch 00012: val_loss did not improve from 0.09987\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2613 - val_loss: 0.1033\n",
      "Epoch 13/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.2584\n",
      "Epoch 00013: val_loss improved from 0.09987 to 0.09821, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2580 - val_loss: 0.0982\n",
      "Epoch 14/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2484\n",
      "Epoch 00014: val_loss did not improve from 0.09821\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2487 - val_loss: 0.1001\n",
      "Epoch 15/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.2467\n",
      "Epoch 00015: val_loss did not improve from 0.09821\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2464 - val_loss: 0.1008\n",
      " ###6 fold : val acc1 0.613, acc3 0.982, mae 0.202###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/140 [===========================>..] - ETA: 0s - loss: 6.4374\n",
      "Epoch 00001: val_loss improved from inf to 0.94025, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.2812 - val_loss: 0.9403\n",
      "Epoch 2/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.6787\n",
      "Epoch 00002: val_loss improved from 0.94025 to 0.14297, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.6760 - val_loss: 0.1430\n",
      "Epoch 3/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.4093\n",
      "Epoch 00003: val_loss improved from 0.14297 to 0.11593, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4062 - val_loss: 0.1159\n",
      "Epoch 4/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3649\n",
      "Epoch 00004: val_loss improved from 0.11593 to 0.11238, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3642 - val_loss: 0.1124\n",
      "Epoch 5/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.3319\n",
      "Epoch 00005: val_loss improved from 0.11238 to 0.10909, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3312 - val_loss: 0.1091\n",
      "Epoch 6/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.3202\n",
      "Epoch 00006: val_loss improved from 0.10909 to 0.10783, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3193 - val_loss: 0.1078\n",
      "Epoch 7/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.3055\n",
      "Epoch 00007: val_loss improved from 0.10783 to 0.10308, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3058 - val_loss: 0.1031\n",
      "Epoch 8/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.2982\n",
      "Epoch 00008: val_loss improved from 0.10308 to 0.10214, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2980 - val_loss: 0.1021\n",
      "Epoch 9/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2837\n",
      "Epoch 00009: val_loss did not improve from 0.10214\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2837 - val_loss: 0.1023\n",
      "Epoch 10/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2697\n",
      "Epoch 00010: val_loss did not improve from 0.10214\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2696 - val_loss: 0.1035\n",
      " ###7 fold : val acc1 0.586, acc3 0.968, mae 0.223###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/140 [==========================>...] - ETA: 0s - loss: 6.7236\n",
      "Epoch 00001: val_loss improved from inf to 0.95501, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.2970 - val_loss: 0.9550\n",
      "Epoch 2/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.7059\n",
      "Epoch 00002: val_loss improved from 0.95501 to 0.15025, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.6802 - val_loss: 0.1502\n",
      "Epoch 3/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.4072\n",
      "Epoch 00003: val_loss improved from 0.15025 to 0.12024, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4049 - val_loss: 0.1202\n",
      "Epoch 4/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.3644\n",
      "Epoch 00004: val_loss improved from 0.12024 to 0.11701, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3634 - val_loss: 0.1170\n",
      "Epoch 5/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.3308\n",
      "Epoch 00005: val_loss improved from 0.11701 to 0.11329, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3314 - val_loss: 0.1133\n",
      "Epoch 6/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.3202\n",
      "Epoch 00006: val_loss improved from 0.11329 to 0.11110, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.3195 - val_loss: 0.1111\n",
      "Epoch 7/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.3059\n",
      "Epoch 00007: val_loss improved from 0.11110 to 0.10649, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3062 - val_loss: 0.1065\n",
      "Epoch 8/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.2987\n",
      "Epoch 00008: val_loss improved from 0.10649 to 0.10550, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2979 - val_loss: 0.1055\n",
      "Epoch 9/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2849\n",
      "Epoch 00009: val_loss did not improve from 0.10550\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2843 - val_loss: 0.1058\n",
      "Epoch 10/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2688\n",
      "Epoch 00010: val_loss did not improve from 0.10550\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2691 - val_loss: 0.1059\n",
      " ###8 fold : val acc1 0.578, acc3 0.975, mae 0.224###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/140 [==========================>...] - ETA: 0s - loss: 6.8098\n",
      "Epoch 00001: val_loss improved from inf to 0.96241, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 6.2970 - val_loss: 0.9624\n",
      "Epoch 2/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.7006\n",
      "Epoch 00002: val_loss improved from 0.96241 to 0.15330, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.6802 - val_loss: 0.1533\n",
      "Epoch 3/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.4061\n",
      "Epoch 00003: val_loss improved from 0.15330 to 0.12416, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4049 - val_loss: 0.1242\n",
      "Epoch 4/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.3645\n",
      "Epoch 00004: val_loss improved from 0.12416 to 0.11948, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3634 - val_loss: 0.1195\n",
      "Epoch 5/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.3308\n",
      "Epoch 00005: val_loss improved from 0.11948 to 0.11605, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3314 - val_loss: 0.1161\n",
      "Epoch 6/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.3198\n",
      "Epoch 00006: val_loss improved from 0.11605 to 0.11265, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.3195 - val_loss: 0.1127\n",
      "Epoch 7/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.3059\n",
      "Epoch 00007: val_loss improved from 0.11265 to 0.10920, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3062 - val_loss: 0.1092\n",
      "Epoch 8/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.2986\n",
      "Epoch 00008: val_loss improved from 0.10920 to 0.10832, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes64_dropout0.2,lr0.002/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2979 - val_loss: 0.1083\n",
      "Epoch 9/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.2846\n",
      "Epoch 00009: val_loss did not improve from 0.10832\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2843 - val_loss: 0.1084\n",
      "Epoch 10/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.2686\n",
      "Epoch 00010: val_loss did not improve from 0.10832\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2691 - val_loss: 0.1095\n",
      " ###9 fold : val acc1 0.594, acc3 0.978, mae 0.214###\n",
      "acc10.598_acc30.977\n",
      "random search 98/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/279 [============================>.] - ETA: 0s - loss: 0.7438\n",
      "Epoch 00001: val_loss improved from inf to 0.13174, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes256_dropout0.3,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.7352 - val_loss: 0.1317\n",
      "Epoch 2/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.2365\n",
      "Epoch 00002: val_loss improved from 0.13174 to 0.12159, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes256_dropout0.3,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2368 - val_loss: 0.1216\n",
      "Epoch 3/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.2228\n",
      "Epoch 00003: val_loss improved from 0.12159 to 0.10655, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes256_dropout0.3,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2223 - val_loss: 0.1065\n",
      "Epoch 4/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.2150\n",
      "Epoch 00004: val_loss improved from 0.10655 to 0.09967, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes256_dropout0.3,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2155 - val_loss: 0.0997\n",
      "Epoch 5/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2139\n",
      "Epoch 00005: val_loss did not improve from 0.09967\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2137 - val_loss: 0.1145\n",
      "Epoch 6/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00006: val_loss improved from 0.09967 to 0.09438, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes256_dropout0.3,lr0.002/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2064 - val_loss: 0.0944\n",
      "Epoch 7/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1982\n",
      "Epoch 00007: val_loss did not improve from 0.09438\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1980 - val_loss: 0.1035\n",
      "Epoch 8/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.2005\n",
      "Epoch 00008: val_loss did not improve from 0.09438\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2000 - val_loss: 0.1628\n",
      " ###0 fold : val acc1 0.604, acc3 0.983, mae 0.207###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271/279 [============================>.] - ETA: 0s - loss: 0.7589\n",
      "Epoch 00001: val_loss improved from inf to 0.13525, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes256_dropout0.3,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.7448 - val_loss: 0.1352\n",
      "Epoch 2/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.2337\n",
      "Epoch 00002: val_loss improved from 0.13525 to 0.12090, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes256_dropout0.3,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2341 - val_loss: 0.1209\n",
      "Epoch 3/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2227\n",
      "Epoch 00003: val_loss improved from 0.12090 to 0.09932, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes256_dropout0.3,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2228 - val_loss: 0.0993\n",
      "Epoch 4/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.2136\n",
      "Epoch 00004: val_loss improved from 0.09932 to 0.09723, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes256_dropout0.3,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2138 - val_loss: 0.0972\n",
      "Epoch 5/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.2132\n",
      "Epoch 00005: val_loss did not improve from 0.09723\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2133 - val_loss: 0.1218\n",
      "Epoch 6/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.2066\n",
      "Epoch 00006: val_loss improved from 0.09723 to 0.09593, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes256_dropout0.3,lr0.002/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2063 - val_loss: 0.0959\n",
      "Epoch 7/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.1990\n",
      "Epoch 00007: val_loss did not improve from 0.09593\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1989 - val_loss: 0.1018\n",
      "Epoch 8/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00008: val_loss did not improve from 0.09593\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1987 - val_loss: 0.1285\n",
      " ###1 fold : val acc1 0.585, acc3 0.982, mae 0.217###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - ETA: 0s - loss: 0.7407\n",
      "Epoch 00001: val_loss improved from inf to 0.12214, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes256_dropout0.3,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.7407 - val_loss: 0.1221\n",
      "Epoch 2/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.2335\n",
      "Epoch 00002: val_loss did not improve from 0.12214\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2338 - val_loss: 0.1282\n",
      "Epoch 3/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.2228\n",
      "Epoch 00003: val_loss improved from 0.12214 to 0.09791, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes256_dropout0.3,lr0.002/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2224 - val_loss: 0.0979\n",
      "Epoch 4/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.2147\n",
      "Epoch 00004: val_loss did not improve from 0.09791\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2148 - val_loss: 0.1014\n",
      "Epoch 5/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.2127\n",
      "Epoch 00005: val_loss did not improve from 0.09791\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2126 - val_loss: 0.1588\n",
      " ###2 fold : val acc1 0.583, acc3 0.977, mae 0.221###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/279 [============================>.] - ETA: 0s - loss: 0.7356\n",
      "Epoch 00001: val_loss improved from inf to 0.11958, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes256_dropout0.3,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 0.7235 - val_loss: 0.1196\n",
      "Epoch 2/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.2313\n",
      "Epoch 00002: val_loss did not improve from 0.11958\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2313 - val_loss: 0.1225\n",
      "Epoch 3/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2209\n",
      "Epoch 00003: val_loss improved from 0.11958 to 0.10820, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes256_dropout0.3,lr0.002/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2211 - val_loss: 0.1082\n",
      "Epoch 4/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.2162\n",
      "Epoch 00004: val_loss did not improve from 0.10820\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2157 - val_loss: 0.1110\n",
      "Epoch 5/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.2155\n",
      "Epoch 00005: val_loss did not improve from 0.10820\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2154 - val_loss: 0.1523\n",
      " ###3 fold : val acc1 0.551, acc3 0.975, mae 0.237###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/279 [============================>.] - ETA: 0s - loss: 0.7476\n",
      "Epoch 00001: val_loss improved from inf to 0.10451, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0.2,dnodes256_dropout0.3,lr0.002/weights_4.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 0.7433 - val_loss: 0.1045\n",
      "Epoch 2/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.2312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 0s 3ms/step - loss: 1.2400 - val_loss: 0.4375\n",
      "Epoch 3/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2372\n",
      "Epoch 00003: val_loss improved from 0.43753 to 0.13588, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2362 - val_loss: 0.1359\n",
      "Epoch 4/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00004: val_loss improved from 0.13588 to 0.11673, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1245 - val_loss: 0.1167\n",
      "Epoch 5/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1160\n",
      "Epoch 00005: val_loss improved from 0.11673 to 0.11178, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1160 - val_loss: 0.1118\n",
      "Epoch 6/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1124\n",
      "Epoch 00006: val_loss improved from 0.11178 to 0.10855, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1123 - val_loss: 0.1086\n",
      "Epoch 7/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1093\n",
      "Epoch 00007: val_loss improved from 0.10855 to 0.10609, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1093 - val_loss: 0.1061\n",
      "Epoch 8/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1071\n",
      "Epoch 00008: val_loss improved from 0.10609 to 0.10448, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1069 - val_loss: 0.1045\n",
      "Epoch 9/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1053\n",
      "Epoch 00009: val_loss improved from 0.10448 to 0.10262, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1052 - val_loss: 0.1026\n",
      "Epoch 10/100\n",
      "117/140 [========================>.....] - ETA: 0s - loss: 0.1039\n",
      "Epoch 00010: val_loss improved from 0.10262 to 0.10102, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1038 - val_loss: 0.1010\n",
      "Epoch 11/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1024\n",
      "Epoch 00011: val_loss improved from 0.10102 to 0.09996, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1024 - val_loss: 0.1000\n",
      "Epoch 12/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1012\n",
      "Epoch 00012: val_loss improved from 0.09996 to 0.09922, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1012 - val_loss: 0.0992\n",
      "Epoch 13/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 00013: val_loss improved from 0.09922 to 0.09862, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1004 - val_loss: 0.0986\n",
      "Epoch 14/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.0994\n",
      "Epoch 00014: val_loss improved from 0.09862 to 0.09807, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0997 - val_loss: 0.0981\n",
      "Epoch 15/100\n",
      "117/140 [========================>.....] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00015: val_loss improved from 0.09807 to 0.09734, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0991 - val_loss: 0.0973\n",
      "Epoch 16/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.0988\n",
      "Epoch 00016: val_loss improved from 0.09734 to 0.09669, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0988 - val_loss: 0.0967\n",
      "Epoch 17/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 00017: val_loss improved from 0.09669 to 0.09659, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0981 - val_loss: 0.0966\n",
      "Epoch 18/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.0979\n",
      "Epoch 00018: val_loss improved from 0.09659 to 0.09627, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0977 - val_loss: 0.0963\n",
      "Epoch 19/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00019: val_loss did not improve from 0.09627\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0975 - val_loss: 0.0970\n",
      "Epoch 20/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.0972\n",
      "Epoch 00020: val_loss improved from 0.09627 to 0.09558, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0971 - val_loss: 0.0956\n",
      "Epoch 21/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00021: val_loss did not improve from 0.09558\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 0.0965\n",
      "Epoch 22/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.0971\n",
      "Epoch 00022: val_loss improved from 0.09558 to 0.09554, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0967 - val_loss: 0.0955\n",
      "Epoch 23/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.0960\n",
      "Epoch 00023: val_loss improved from 0.09554 to 0.09534, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0962 - val_loss: 0.0953\n",
      "Epoch 24/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.0958\n",
      "Epoch 00024: val_loss improved from 0.09534 to 0.09423, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0959 - val_loss: 0.0942\n",
      "Epoch 25/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.0965\n",
      "Epoch 00025: val_loss did not improve from 0.09423\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 0.0949\n",
      "Epoch 26/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0962\n",
      "Epoch 00026: val_loss improved from 0.09423 to 0.09419, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0962 - val_loss: 0.0942\n",
      "Epoch 27/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.0957\n",
      "Epoch 00027: val_loss did not improve from 0.09419\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0957 - val_loss: 0.0948\n",
      "Epoch 28/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.0955\n",
      "Epoch 00028: val_loss did not improve from 0.09419\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0955 - val_loss: 0.0950\n",
      " ###0 fold : val acc1 0.599, acc3 0.982, mae 0.210###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/140 [===========================>..] - ETA: 0s - loss: 10.0019\n",
      "Epoch 00001: val_loss improved from inf to 2.55358, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 9.6726 - val_loss: 2.5536\n",
      "Epoch 2/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 1.2745\n",
      "Epoch 00002: val_loss improved from 2.55358 to 0.43660, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.2449 - val_loss: 0.4366\n",
      "Epoch 3/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2416\n",
      "Epoch 00003: val_loss improved from 0.43660 to 0.13720, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2379 - val_loss: 0.1372\n",
      "Epoch 4/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00004: val_loss improved from 0.13720 to 0.11654, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1243 - val_loss: 0.1165\n",
      "Epoch 5/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1148\n",
      "Epoch 00005: val_loss improved from 0.11654 to 0.11161, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1151 - val_loss: 0.1116\n",
      "Epoch 6/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1121\n",
      "Epoch 00006: val_loss improved from 0.11161 to 0.10876, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1116 - val_loss: 0.1088\n",
      "Epoch 7/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.1100\n",
      "Epoch 00007: val_loss improved from 0.10876 to 0.10597, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1087 - val_loss: 0.1060\n",
      "Epoch 8/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1064\n",
      "Epoch 00008: val_loss improved from 0.10597 to 0.10451, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1064 - val_loss: 0.1045\n",
      "Epoch 9/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1050\n",
      "Epoch 00009: val_loss improved from 0.10451 to 0.10239, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1046 - val_loss: 0.1024\n",
      "Epoch 10/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1032\n",
      "Epoch 00010: val_loss improved from 0.10239 to 0.10148, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1032 - val_loss: 0.1015\n",
      "Epoch 11/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1019\n",
      "Epoch 00011: val_loss improved from 0.10148 to 0.09990, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1020 - val_loss: 0.0999\n",
      "Epoch 12/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1011\n",
      "Epoch 00012: val_loss improved from 0.09990 to 0.09982, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1010 - val_loss: 0.0998\n",
      "Epoch 13/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.0994\n",
      "Epoch 00013: val_loss improved from 0.09982 to 0.09851, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1000 - val_loss: 0.0985\n",
      "Epoch 14/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00014: val_loss improved from 0.09851 to 0.09835, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0994 - val_loss: 0.0983\n",
      "Epoch 15/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00015: val_loss improved from 0.09835 to 0.09750, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0989 - val_loss: 0.0975\n",
      "Epoch 16/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00016: val_loss improved from 0.09750 to 0.09664, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0984 - val_loss: 0.0966\n",
      "Epoch 17/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00017: val_loss did not improve from 0.09664\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0978 - val_loss: 0.0967\n",
      "Epoch 18/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00018: val_loss did not improve from 0.09664\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0974 - val_loss: 0.0976\n",
      " ###1 fold : val acc1 0.591, acc3 0.983, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/140 [========================>.....] - ETA: 0s - loss: 10.7915\n",
      "Epoch 00001: val_loss improved from inf to 2.54306, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 3ms/step - loss: 9.6559 - val_loss: 2.5431\n",
      "Epoch 2/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 1.3398\n",
      "Epoch 00002: val_loss improved from 2.54306 to 0.43476, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.2375 - val_loss: 0.4348\n",
      "Epoch 3/100\n",
      "118/140 [========================>.....] - ETA: 0s - loss: 0.2541\n",
      "Epoch 00003: val_loss improved from 0.43476 to 0.13653, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2379 - val_loss: 0.1365\n",
      "Epoch 4/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00004: val_loss improved from 0.13653 to 0.11619, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 0.1162\n",
      "Epoch 5/100\n",
      "120/140 [========================>.....] - ETA: 0s - loss: 0.1152\n",
      "Epoch 00005: val_loss improved from 0.11619 to 0.11137, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1150 - val_loss: 0.1114\n",
      "Epoch 6/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1117\n",
      "Epoch 00006: val_loss improved from 0.11137 to 0.10845, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1116 - val_loss: 0.1085\n",
      "Epoch 7/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1090\n",
      "Epoch 00007: val_loss improved from 0.10845 to 0.10599, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1090 - val_loss: 0.1060\n",
      "Epoch 8/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1068\n",
      "Epoch 00008: val_loss improved from 0.10599 to 0.10428, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1068 - val_loss: 0.1043\n",
      "Epoch 9/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1051\n",
      "Epoch 00009: val_loss improved from 0.10428 to 0.10258, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1050 - val_loss: 0.1026\n",
      "Epoch 10/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1036\n",
      "Epoch 00010: val_loss improved from 0.10258 to 0.10191, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1036 - val_loss: 0.1019\n",
      "Epoch 11/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 00011: val_loss improved from 0.10191 to 0.10047, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1026 - val_loss: 0.1005\n",
      "Epoch 12/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1015\n",
      "Epoch 00012: val_loss improved from 0.10047 to 0.09969, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1015 - val_loss: 0.0997\n",
      "Epoch 13/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1004\n",
      "Epoch 00013: val_loss improved from 0.09969 to 0.09839, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1005 - val_loss: 0.0984\n",
      "Epoch 14/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.0997\n",
      "Epoch 00014: val_loss improved from 0.09839 to 0.09836, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1000 - val_loss: 0.0984\n",
      "Epoch 15/100\n",
      "117/140 [========================>.....] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00015: val_loss improved from 0.09836 to 0.09721, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0993 - val_loss: 0.0972\n",
      "Epoch 16/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00016: val_loss improved from 0.09721 to 0.09650, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0986 - val_loss: 0.0965\n",
      "Epoch 17/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00017: val_loss improved from 0.09650 to 0.09608, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0982 - val_loss: 0.0961\n",
      "Epoch 18/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.0979\n",
      "Epoch 00018: val_loss did not improve from 0.09608\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0977 - val_loss: 0.0971\n",
      "Epoch 19/100\n",
      "118/140 [========================>.....] - ETA: 0s - loss: 0.0974\n",
      "Epoch 00019: val_loss did not improve from 0.09608\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0977 - val_loss: 0.0971\n",
      " ###2 fold : val acc1 0.595, acc3 0.979, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/140 [============================>.] - ETA: 0s - loss: 9.7841 \n",
      "Epoch 00001: val_loss improved from inf to 2.53443, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 9.6639 - val_loss: 2.5344\n",
      "Epoch 2/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 1.3320\n",
      "Epoch 00002: val_loss improved from 2.53443 to 0.43376, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.2417 - val_loss: 0.4338\n",
      "Epoch 3/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2358\n",
      "Epoch 00003: val_loss improved from 0.43376 to 0.13542, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2352 - val_loss: 0.1354\n",
      "Epoch 4/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1234\n",
      "Epoch 00004: val_loss improved from 0.13542 to 0.11640, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1234 - val_loss: 0.1164\n",
      "Epoch 5/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1148\n",
      "Epoch 00005: val_loss improved from 0.11640 to 0.11147, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1148 - val_loss: 0.1115\n",
      "Epoch 6/100\n",
      "118/140 [========================>.....] - ETA: 0s - loss: 0.1118\n",
      "Epoch 00006: val_loss improved from 0.11147 to 0.10903, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1113 - val_loss: 0.1090\n",
      "Epoch 7/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1087\n",
      "Epoch 00007: val_loss improved from 0.10903 to 0.10619, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1087 - val_loss: 0.1062\n",
      "Epoch 8/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 00008: val_loss improved from 0.10619 to 0.10419, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 0.1042\n",
      "Epoch 9/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1048\n",
      "Epoch 00009: val_loss improved from 0.10419 to 0.10270, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1047 - val_loss: 0.1027\n",
      "Epoch 10/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.1036\n",
      "Epoch 00010: val_loss improved from 0.10270 to 0.10177, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1034 - val_loss: 0.1018\n",
      "Epoch 11/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1022\n",
      "Epoch 00011: val_loss improved from 0.10177 to 0.10045, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1022 - val_loss: 0.1004\n",
      "Epoch 12/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1012\n",
      "Epoch 00012: val_loss improved from 0.10045 to 0.09937, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 0.0994\n",
      "Epoch 13/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1004\n",
      "Epoch 00013: val_loss improved from 0.09937 to 0.09846, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1004 - val_loss: 0.0985\n",
      "Epoch 14/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.0997\n",
      "Epoch 00014: val_loss improved from 0.09846 to 0.09808, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0998 - val_loss: 0.0981\n",
      "Epoch 15/100\n",
      "118/140 [========================>.....] - ETA: 0s - loss: 0.0997\n",
      "Epoch 00015: val_loss improved from 0.09808 to 0.09743, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0990 - val_loss: 0.0974\n",
      "Epoch 16/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00016: val_loss improved from 0.09743 to 0.09644, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0984 - val_loss: 0.0964\n",
      "Epoch 17/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00017: val_loss improved from 0.09644 to 0.09623, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0977 - val_loss: 0.0962\n",
      "Epoch 18/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00018: val_loss did not improve from 0.09623\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0972 - val_loss: 0.0967\n",
      "Epoch 19/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00019: val_loss did not improve from 0.09623\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0973 - val_loss: 0.0973\n",
      " ###3 fold : val acc1 0.609, acc3 0.976, mae 0.208###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/140 [============================>.] - ETA: 0s - loss: 9.6433 \n",
      "Epoch 00001: val_loss improved from inf to 2.51897, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 3ms/step - loss: 9.6285 - val_loss: 2.5190\n",
      "Epoch 2/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 1.2293\n",
      "Epoch 00002: val_loss improved from 2.51897 to 0.41641, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.2277 - val_loss: 0.4164\n",
      "Epoch 3/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2281\n",
      "Epoch 00003: val_loss improved from 0.41641 to 0.13552, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2270 - val_loss: 0.1355\n",
      "Epoch 4/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00004: val_loss improved from 0.13552 to 0.11682, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 0.1168\n",
      "Epoch 5/100\n",
      "120/140 [========================>.....] - ETA: 0s - loss: 0.1155\n",
      "Epoch 00005: val_loss improved from 0.11682 to 0.11225, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1155 - val_loss: 0.1123\n",
      "Epoch 6/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.1124\n",
      "Epoch 00006: val_loss improved from 0.11225 to 0.10894, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1118 - val_loss: 0.1089\n",
      "Epoch 7/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1092\n",
      "Epoch 00007: val_loss improved from 0.10894 to 0.10702, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1089 - val_loss: 0.1070\n",
      "Epoch 8/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1067\n",
      "Epoch 00008: val_loss improved from 0.10702 to 0.10446, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1068 - val_loss: 0.1045\n",
      "Epoch 9/100\n",
      "120/140 [========================>.....] - ETA: 0s - loss: 0.1046\n",
      "Epoch 00009: val_loss improved from 0.10446 to 0.10346, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1049 - val_loss: 0.1035\n",
      "Epoch 10/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1034\n",
      "Epoch 00010: val_loss improved from 0.10346 to 0.10247, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1035 - val_loss: 0.1025\n",
      "Epoch 11/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1029\n",
      "Epoch 00011: val_loss improved from 0.10247 to 0.10033, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1023 - val_loss: 0.1003\n",
      "Epoch 12/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1014\n",
      "Epoch 00012: val_loss improved from 0.10033 to 0.09925, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1012 - val_loss: 0.0992\n",
      "Epoch 13/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00013: val_loss improved from 0.09925 to 0.09880, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1003 - val_loss: 0.0988\n",
      "Epoch 14/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00014: val_loss improved from 0.09880 to 0.09776, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0994 - val_loss: 0.0978\n",
      "Epoch 15/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.0988\n",
      "Epoch 00015: val_loss improved from 0.09776 to 0.09729, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0987 - val_loss: 0.0973\n",
      "Epoch 16/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00016: val_loss improved from 0.09729 to 0.09663, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0983 - val_loss: 0.0966\n",
      "Epoch 17/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00017: val_loss did not improve from 0.09663\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0976 - val_loss: 0.0968\n",
      "Epoch 18/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.0974\n",
      "Epoch 00018: val_loss did not improve from 0.09663\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0973 - val_loss: 0.0967\n",
      " ###4 fold : val acc1 0.586, acc3 0.979, mae 0.217###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/140 [==========================>...] - ETA: 0s - loss: 10.3030\n",
      "Epoch 00001: val_loss improved from inf to 2.50272, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 9.6380 - val_loss: 2.5027\n",
      "Epoch 2/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 1.2953\n",
      "Epoch 00002: val_loss improved from 2.50272 to 0.42617, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.2331 - val_loss: 0.4262\n",
      "Epoch 3/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2353\n",
      "Epoch 00003: val_loss improved from 0.42617 to 0.13456, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2290 - val_loss: 0.1346\n",
      "Epoch 4/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00004: val_loss improved from 0.13456 to 0.11655, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1228 - val_loss: 0.1165\n",
      "Epoch 5/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.1147\n",
      "Epoch 00005: val_loss improved from 0.11655 to 0.11254, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1151 - val_loss: 0.1125\n",
      "Epoch 6/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1118\n",
      "Epoch 00006: val_loss improved from 0.11254 to 0.10913, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1118 - val_loss: 0.1091\n",
      "Epoch 7/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1090\n",
      "Epoch 00007: val_loss improved from 0.10913 to 0.10699, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1089 - val_loss: 0.1070\n",
      "Epoch 8/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.1070\n",
      "Epoch 00008: val_loss improved from 0.10699 to 0.10471, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1068 - val_loss: 0.1047\n",
      "Epoch 9/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.1051\n",
      "Epoch 00009: val_loss improved from 0.10471 to 0.10307, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1051 - val_loss: 0.1031\n",
      "Epoch 10/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1037\n",
      "Epoch 00010: val_loss improved from 0.10307 to 0.10203, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1036 - val_loss: 0.1020\n",
      "Epoch 11/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1026\n",
      "Epoch 00011: val_loss improved from 0.10203 to 0.10038, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1026 - val_loss: 0.1004\n",
      "Epoch 12/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1014\n",
      "Epoch 00012: val_loss improved from 0.10038 to 0.09942, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 0.0994\n",
      "Epoch 13/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1008\n",
      "Epoch 00013: val_loss improved from 0.09942 to 0.09888, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1006 - val_loss: 0.0989\n",
      "Epoch 14/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.0998\n",
      "Epoch 00014: val_loss improved from 0.09888 to 0.09779, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0998 - val_loss: 0.0978\n",
      "Epoch 15/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.0990\n",
      "Epoch 00015: val_loss improved from 0.09779 to 0.09705, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0991 - val_loss: 0.0970\n",
      "Epoch 16/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00016: val_loss improved from 0.09705 to 0.09648, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0984 - val_loss: 0.0965\n",
      "Epoch 17/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 00017: val_loss improved from 0.09648 to 0.09634, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0978 - val_loss: 0.0963\n",
      "Epoch 18/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00018: val_loss improved from 0.09634 to 0.09621, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0976 - val_loss: 0.0962\n",
      "Epoch 19/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00019: val_loss improved from 0.09621 to 0.09539, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0973 - val_loss: 0.0954\n",
      "Epoch 20/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00020: val_loss did not improve from 0.09539\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0968 - val_loss: 0.0959\n",
      "Epoch 21/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00021: val_loss improved from 0.09539 to 0.09470, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0967 - val_loss: 0.0947\n",
      "Epoch 22/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00022: val_loss did not improve from 0.09470\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0966 - val_loss: 0.0948\n",
      "Epoch 23/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.0960\n",
      "Epoch 00023: val_loss did not improve from 0.09470\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0960 - val_loss: 0.0953\n",
      " ###5 fold : val acc1 0.600, acc3 0.981, mae 0.210###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/140 [==========================>...] - ETA: 0s - loss: 10.2521\n",
      "Epoch 00001: val_loss improved from inf to 2.50521, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 3ms/step - loss: 9.6473 - val_loss: 2.5052\n",
      "Epoch 2/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 1.2489\n",
      "Epoch 00002: val_loss improved from 2.50521 to 0.42650, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.2298 - val_loss: 0.4265\n",
      "Epoch 3/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2328\n",
      "Epoch 00003: val_loss improved from 0.42650 to 0.13407, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2315 - val_loss: 0.1341\n",
      "Epoch 4/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00004: val_loss improved from 0.13407 to 0.11608, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1240 - val_loss: 0.1161\n",
      "Epoch 5/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1159\n",
      "Epoch 00005: val_loss improved from 0.11608 to 0.11199, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1159 - val_loss: 0.1120\n",
      "Epoch 6/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1125\n",
      "Epoch 00006: val_loss improved from 0.11199 to 0.10852, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1125 - val_loss: 0.1085\n",
      "Epoch 7/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.1107\n",
      "Epoch 00007: val_loss improved from 0.10852 to 0.10636, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1095 - val_loss: 0.1064\n",
      "Epoch 8/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.1075\n",
      "Epoch 00008: val_loss improved from 0.10636 to 0.10425, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1074 - val_loss: 0.1043\n",
      "Epoch 9/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.1052\n",
      "Epoch 00009: val_loss improved from 0.10425 to 0.10260, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1056 - val_loss: 0.1026\n",
      "Epoch 10/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1040\n",
      "Epoch 00010: val_loss improved from 0.10260 to 0.10122, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1041 - val_loss: 0.1012\n",
      "Epoch 11/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1031\n",
      "Epoch 00011: val_loss improved from 0.10122 to 0.09994, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 0.0999\n",
      "Epoch 12/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1018\n",
      "Epoch 00012: val_loss improved from 0.09994 to 0.09934, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1015 - val_loss: 0.0993\n",
      "Epoch 13/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.1002\n",
      "Epoch 00013: val_loss improved from 0.09934 to 0.09836, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1008 - val_loss: 0.0984\n",
      "Epoch 14/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00014: val_loss improved from 0.09836 to 0.09770, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0999 - val_loss: 0.0977\n",
      "Epoch 15/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.0995\n",
      "Epoch 00015: val_loss improved from 0.09770 to 0.09688, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0996 - val_loss: 0.0969\n",
      "Epoch 16/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.0988\n",
      "Epoch 00016: val_loss improved from 0.09688 to 0.09630, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0988 - val_loss: 0.0963\n",
      "Epoch 17/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.0981\n",
      "Epoch 00017: val_loss improved from 0.09630 to 0.09610, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0982 - val_loss: 0.0961\n",
      "Epoch 18/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00018: val_loss did not improve from 0.09610\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0979 - val_loss: 0.0962\n",
      "Epoch 19/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.0977\n",
      "Epoch 00019: val_loss improved from 0.09610 to 0.09573, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0976 - val_loss: 0.0957\n",
      "Epoch 20/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.0968\n",
      "Epoch 00020: val_loss improved from 0.09573 to 0.09548, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 0.0955\n",
      "Epoch 21/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.0974\n",
      "Epoch 00021: val_loss improved from 0.09548 to 0.09450, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0969 - val_loss: 0.0945\n",
      "Epoch 22/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.0977\n",
      "Epoch 00022: val_loss improved from 0.09450 to 0.09450, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0968 - val_loss: 0.0945\n",
      "Epoch 23/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00023: val_loss improved from 0.09450 to 0.09445, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 0.0945\n",
      "Epoch 24/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.0958\n",
      "Epoch 00024: val_loss improved from 0.09445 to 0.09422, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0961 - val_loss: 0.0942\n",
      "Epoch 25/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00025: val_loss did not improve from 0.09422\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0960 - val_loss: 0.0957\n",
      "Epoch 26/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.0960\n",
      "Epoch 00026: val_loss did not improve from 0.09422\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 0.0955\n",
      " ###6 fold : val acc1 0.605, acc3 0.983, mae 0.206###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - ETA: 0s - loss: 9.6206 \n",
      "Epoch 00001: val_loss improved from inf to 2.50088, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 9.6206 - val_loss: 2.5009\n",
      "Epoch 2/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 1.2520\n",
      "Epoch 00002: val_loss improved from 2.50088 to 0.42148, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.2203 - val_loss: 0.4215\n",
      "Epoch 3/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.2373\n",
      "Epoch 00003: val_loss improved from 0.42148 to 0.13432, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2272 - val_loss: 0.1343\n",
      "Epoch 4/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1227\n",
      "Epoch 00004: val_loss improved from 0.13432 to 0.11642, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1219 - val_loss: 0.1164\n",
      "Epoch 5/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1140\n",
      "Epoch 00005: val_loss improved from 0.11642 to 0.11212, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1140 - val_loss: 0.1121\n",
      "Epoch 6/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00006: val_loss improved from 0.11212 to 0.10908, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1106 - val_loss: 0.1091\n",
      "Epoch 7/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1082\n",
      "Epoch 00007: val_loss improved from 0.10908 to 0.10648, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1079 - val_loss: 0.1065\n",
      "Epoch 8/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1059\n",
      "Epoch 00008: val_loss improved from 0.10648 to 0.10480, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1058 - val_loss: 0.1048\n",
      "Epoch 9/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1041\n",
      "Epoch 00009: val_loss improved from 0.10480 to 0.10308, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1041 - val_loss: 0.1031\n",
      "Epoch 10/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.1030\n",
      "Epoch 00010: val_loss improved from 0.10308 to 0.10142, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 0.1014\n",
      "Epoch 11/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.1020\n",
      "Epoch 00011: val_loss improved from 0.10142 to 0.10067, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1015 - val_loss: 0.1007\n",
      "Epoch 12/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1005\n",
      "Epoch 00012: val_loss improved from 0.10067 to 0.09963, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1005 - val_loss: 0.0996\n",
      "Epoch 13/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00013: val_loss improved from 0.09963 to 0.09899, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0996 - val_loss: 0.0990\n",
      "Epoch 14/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00014: val_loss improved from 0.09899 to 0.09799, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0988 - val_loss: 0.0980\n",
      "Epoch 15/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.0986\n",
      "Epoch 00015: val_loss improved from 0.09799 to 0.09752, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0985 - val_loss: 0.0975\n",
      "Epoch 16/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00016: val_loss improved from 0.09752 to 0.09673, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0977 - val_loss: 0.0967\n",
      "Epoch 17/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.0968\n",
      "Epoch 00017: val_loss improved from 0.09673 to 0.09663, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0971 - val_loss: 0.0966\n",
      "Epoch 18/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00018: val_loss improved from 0.09663 to 0.09636, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0969 - val_loss: 0.0964\n",
      "Epoch 19/100\n",
      "120/140 [========================>.....] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00019: val_loss did not improve from 0.09636\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0965 - val_loss: 0.0964\n",
      "Epoch 20/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.0959\n",
      "Epoch 00020: val_loss improved from 0.09636 to 0.09582, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0961 - val_loss: 0.0958\n",
      "Epoch 21/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00021: val_loss improved from 0.09582 to 0.09535, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0962 - val_loss: 0.0954\n",
      "Epoch 22/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.0958\n",
      "Epoch 00022: val_loss improved from 0.09535 to 0.09492, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0958 - val_loss: 0.0949\n",
      "Epoch 23/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.0954\n",
      "Epoch 00023: val_loss did not improve from 0.09492\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0954 - val_loss: 0.0951\n",
      "Epoch 24/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.0948\n",
      "Epoch 00024: val_loss improved from 0.09492 to 0.09443, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0953 - val_loss: 0.0944\n",
      "Epoch 25/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.0960\n",
      "Epoch 00025: val_loss did not improve from 0.09443\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0949 - val_loss: 0.0957\n",
      "Epoch 26/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.0953\n",
      "Epoch 00026: val_loss did not improve from 0.09443\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0953 - val_loss: 0.0955\n",
      " ###7 fold : val acc1 0.588, acc3 0.975, mae 0.219###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/140 [=========================>....] - ETA: 0s - loss: 10.5472\n",
      "Epoch 00001: val_loss improved from inf to 2.50604, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 9.6586 - val_loss: 2.5060\n",
      "Epoch 2/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 1.2218\n",
      "Epoch 00002: val_loss improved from 2.50604 to 0.43235, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.2218 - val_loss: 0.4323\n",
      "Epoch 3/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2306\n",
      "Epoch 00003: val_loss improved from 0.43235 to 0.14192, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2293 - val_loss: 0.1419\n",
      "Epoch 4/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1232\n",
      "Epoch 00004: val_loss improved from 0.14192 to 0.12116, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1232 - val_loss: 0.1212\n",
      "Epoch 5/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1147\n",
      "Epoch 00005: val_loss improved from 0.12116 to 0.11637, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1147 - val_loss: 0.1164\n",
      "Epoch 6/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1110\n",
      "Epoch 00006: val_loss improved from 0.11637 to 0.11317, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1111 - val_loss: 0.1132\n",
      "Epoch 7/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1088\n",
      "Epoch 00007: val_loss improved from 0.11317 to 0.11020, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1083 - val_loss: 0.1102\n",
      "Epoch 8/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1064\n",
      "Epoch 00008: val_loss improved from 0.11020 to 0.10844, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1061 - val_loss: 0.1084\n",
      "Epoch 9/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1041\n",
      "Epoch 00009: val_loss improved from 0.10844 to 0.10650, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1043 - val_loss: 0.1065\n",
      "Epoch 10/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1033\n",
      "Epoch 00010: val_loss improved from 0.10650 to 0.10493, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1029 - val_loss: 0.1049\n",
      "Epoch 11/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1017\n",
      "Epoch 00011: val_loss improved from 0.10493 to 0.10400, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1015 - val_loss: 0.1040\n",
      "Epoch 12/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1008\n",
      "Epoch 00012: val_loss improved from 0.10400 to 0.10306, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1004 - val_loss: 0.1031\n",
      "Epoch 13/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.0999\n",
      "Epoch 00013: val_loss improved from 0.10306 to 0.10192, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.0996 - val_loss: 0.1019\n",
      "Epoch 14/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.0989\n",
      "Epoch 00014: val_loss improved from 0.10192 to 0.10103, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.0987 - val_loss: 0.1010\n",
      "Epoch 15/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.0986\n",
      "Epoch 00015: val_loss improved from 0.10103 to 0.10048, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0984 - val_loss: 0.1005\n",
      "Epoch 16/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00016: val_loss improved from 0.10048 to 0.09967, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0976 - val_loss: 0.0997\n",
      "Epoch 17/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00017: val_loss improved from 0.09967 to 0.09961, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0971 - val_loss: 0.0996\n",
      "Epoch 18/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.0971\n",
      "Epoch 00018: val_loss improved from 0.09961 to 0.09922, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0968 - val_loss: 0.0992\n",
      "Epoch 19/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.0964\n",
      "Epoch 00019: val_loss did not improve from 0.09922\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0963 - val_loss: 0.0995\n",
      "Epoch 20/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.0959\n",
      "Epoch 00020: val_loss improved from 0.09922 to 0.09862, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0959 - val_loss: 0.0986\n",
      "Epoch 21/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00021: val_loss improved from 0.09862 to 0.09795, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0960 - val_loss: 0.0980\n",
      "Epoch 22/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.0958\n",
      "Epoch 00022: val_loss improved from 0.09795 to 0.09784, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.0957 - val_loss: 0.0978\n",
      "Epoch 23/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.0956\n",
      "Epoch 00023: val_loss did not improve from 0.09784\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0955 - val_loss: 0.0978\n",
      "Epoch 24/100\n",
      "122/140 [=========================>....] - ETA: 0s - loss: 0.0946\n",
      "Epoch 00024: val_loss improved from 0.09784 to 0.09702, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0952 - val_loss: 0.0970\n",
      "Epoch 25/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.0954\n",
      "Epoch 00025: val_loss did not improve from 0.09702\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0950 - val_loss: 0.0979\n",
      "Epoch 26/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.0950\n",
      "Epoch 00026: val_loss did not improve from 0.09702\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0952 - val_loss: 0.0979\n",
      " ###8 fold : val acc1 0.597, acc3 0.982, mae 0.211###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/140 [==========================>...] - ETA: 0s - loss: 10.2018\n",
      "Epoch 00001: val_loss improved from inf to 2.50355, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 9.6586 - val_loss: 2.5035\n",
      "Epoch 2/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 1.2531\n",
      "Epoch 00002: val_loss improved from 2.50355 to 0.43327, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 1.2218 - val_loss: 0.4333\n",
      "Epoch 3/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2306\n",
      "Epoch 00003: val_loss improved from 0.43327 to 0.14411, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2293 - val_loss: 0.1441\n",
      "Epoch 4/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00004: val_loss improved from 0.14411 to 0.12401, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1232 - val_loss: 0.1240\n",
      "Epoch 5/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1145\n",
      "Epoch 00005: val_loss improved from 0.12401 to 0.11926, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1147 - val_loss: 0.1193\n",
      "Epoch 6/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1108\n",
      "Epoch 00006: val_loss improved from 0.11926 to 0.11593, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1111 - val_loss: 0.1159\n",
      "Epoch 7/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00007: val_loss improved from 0.11593 to 0.11340, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 0.1134\n",
      "Epoch 8/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1064\n",
      "Epoch 00008: val_loss improved from 0.11340 to 0.11139, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1061 - val_loss: 0.1114\n",
      "Epoch 9/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1040\n",
      "Epoch 00009: val_loss improved from 0.11139 to 0.10956, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1043 - val_loss: 0.1096\n",
      "Epoch 10/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1032\n",
      "Epoch 00010: val_loss improved from 0.10956 to 0.10792, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1029 - val_loss: 0.1079\n",
      "Epoch 11/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1014\n",
      "Epoch 00011: val_loss improved from 0.10792 to 0.10674, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1015 - val_loss: 0.1067\n",
      "Epoch 12/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1006\n",
      "Epoch 00012: val_loss improved from 0.10674 to 0.10624, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1004 - val_loss: 0.1062\n",
      "Epoch 13/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.0998\n",
      "Epoch 00013: val_loss improved from 0.10624 to 0.10465, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0996 - val_loss: 0.1046\n",
      "Epoch 14/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.0989\n",
      "Epoch 00014: val_loss improved from 0.10465 to 0.10378, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.0987 - val_loss: 0.1038\n",
      "Epoch 15/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00015: val_loss improved from 0.10378 to 0.10350, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.0984 - val_loss: 0.1035\n",
      "Epoch 16/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00016: val_loss improved from 0.10350 to 0.10239, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0976 - val_loss: 0.1024\n",
      "Epoch 17/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.0971\n",
      "Epoch 00017: val_loss improved from 0.10239 to 0.10182, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0971 - val_loss: 0.1018\n",
      "Epoch 18/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.0971\n",
      "Epoch 00018: val_loss improved from 0.10182 to 0.10164, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0968 - val_loss: 0.1016\n",
      "Epoch 19/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.0962\n",
      "Epoch 00019: val_loss improved from 0.10164 to 0.10153, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0963 - val_loss: 0.1015\n",
      "Epoch 20/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.0962\n",
      "Epoch 00020: val_loss improved from 0.10153 to 0.10101, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.0959 - val_loss: 0.1010\n",
      "Epoch 21/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.0961\n",
      "Epoch 00021: val_loss improved from 0.10101 to 0.10078, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0960 - val_loss: 0.1008\n",
      "Epoch 22/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.0959\n",
      "Epoch 00022: val_loss improved from 0.10078 to 0.10053, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0957 - val_loss: 0.1005\n",
      "Epoch 23/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.0956\n",
      "Epoch 00023: val_loss improved from 0.10053 to 0.10044, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.0955 - val_loss: 0.1004\n",
      "Epoch 24/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.0949\n",
      "Epoch 00024: val_loss improved from 0.10044 to 0.09933, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes128_dropout0,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.0952 - val_loss: 0.0993\n",
      "Epoch 25/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.0953\n",
      "Epoch 00025: val_loss did not improve from 0.09933\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0950 - val_loss: 0.1009\n",
      "Epoch 26/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.0953\n",
      "Epoch 00026: val_loss did not improve from 0.09933\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0952 - val_loss: 0.1002\n",
      " ###9 fold : val acc1 0.601, acc3 0.983, mae 0.208###\n",
      "acc10.597_acc30.980\n",
      "random search 102/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 15.4132 \n",
      "Epoch 00001: val_loss improved from inf to 3.43184, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 10.9462 - val_loss: 3.4318\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.5276\n",
      "Epoch 00002: val_loss improved from 3.43184 to 0.67188, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0243 - val_loss: 0.6719\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9076\n",
      "Epoch 00003: val_loss improved from 0.67188 to 0.23343, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8125 - val_loss: 0.2334\n",
      "Epoch 4/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5917\n",
      "Epoch 00004: val_loss improved from 0.23343 to 0.14673, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5637 - val_loss: 0.1467\n",
      "Epoch 5/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4972\n",
      "Epoch 00005: val_loss improved from 0.14673 to 0.13103, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4940 - val_loss: 0.1310\n",
      "Epoch 6/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4753\n",
      "Epoch 00006: val_loss improved from 0.13103 to 0.12498, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4801 - val_loss: 0.1250\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4669\n",
      "Epoch 00007: val_loss improved from 0.12498 to 0.12138, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4604 - val_loss: 0.1214\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4661\n",
      "Epoch 00008: val_loss improved from 0.12138 to 0.11849, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4568 - val_loss: 0.1185\n",
      "Epoch 9/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4599\n",
      "Epoch 00009: val_loss improved from 0.11849 to 0.11173, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4526 - val_loss: 0.1117\n",
      "Epoch 10/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4527\n",
      "Epoch 00010: val_loss did not improve from 0.11173\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4425 - val_loss: 0.1148\n",
      "Epoch 11/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4441\n",
      "Epoch 00011: val_loss improved from 0.11173 to 0.11014, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4395 - val_loss: 0.1101\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4406\n",
      "Epoch 00012: val_loss improved from 0.11014 to 0.11000, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4411 - val_loss: 0.1100\n",
      "Epoch 13/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4343\n",
      "Epoch 00013: val_loss improved from 0.11000 to 0.10492, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4358 - val_loss: 0.1049\n",
      "Epoch 14/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4361\n",
      "Epoch 00014: val_loss did not improve from 0.10492\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4261 - val_loss: 0.1092\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4335\n",
      "Epoch 00015: val_loss improved from 0.10492 to 0.10302, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4335 - val_loss: 0.1030\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4369\n",
      "Epoch 00016: val_loss did not improve from 0.10302\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4369 - val_loss: 0.1035\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4236\n",
      "Epoch 00017: val_loss improved from 0.10302 to 0.10212, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_0.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4236 - val_loss: 0.1021\n",
      "Epoch 18/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4202\n",
      "Epoch 00018: val_loss did not improve from 0.10212\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4219 - val_loss: 0.1060\n",
      "Epoch 19/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4108\n",
      "Epoch 00019: val_loss did not improve from 0.10212\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4120 - val_loss: 0.1059\n",
      " ###0 fold : val acc1 0.604, acc3 0.976, mae 0.210###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 15.0129 \n",
      "Epoch 00001: val_loss improved from inf to 3.43812, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 1s 7ms/step - loss: 10.9472 - val_loss: 3.4381\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.4590\n",
      "Epoch 00002: val_loss improved from 3.43812 to 0.67317, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0201 - val_loss: 0.6732\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.9085\n",
      "Epoch 00003: val_loss improved from 0.67317 to 0.23832, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8132 - val_loss: 0.2383\n",
      "Epoch 4/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.5957\n",
      "Epoch 00004: val_loss improved from 0.23832 to 0.14974, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5672 - val_loss: 0.1497\n",
      "Epoch 5/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5021\n",
      "Epoch 00005: val_loss improved from 0.14974 to 0.13219, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4966 - val_loss: 0.1322\n",
      "Epoch 6/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4819\n",
      "Epoch 00006: val_loss improved from 0.13219 to 0.12717, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4810 - val_loss: 0.1272\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4671\n",
      "Epoch 00007: val_loss improved from 0.12717 to 0.12247, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4595 - val_loss: 0.1225\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4620\n",
      "Epoch 00008: val_loss improved from 0.12247 to 0.11835, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4553 - val_loss: 0.1184\n",
      "Epoch 9/100\n",
      "22/35 [=================>............] - ETA: 0s - loss: 0.4572\n",
      "Epoch 00009: val_loss improved from 0.11835 to 0.11262, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4531 - val_loss: 0.1126\n",
      "Epoch 10/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.4547\n",
      "Epoch 00010: val_loss did not improve from 0.11262\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4439 - val_loss: 0.1159\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4399\n",
      "Epoch 00011: val_loss improved from 0.11262 to 0.11007, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4399 - val_loss: 0.1101\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4377\n",
      "Epoch 00012: val_loss improved from 0.11007 to 0.10947, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4402 - val_loss: 0.1095\n",
      "Epoch 13/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4296\n",
      "Epoch 00013: val_loss improved from 0.10947 to 0.10548, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4324 - val_loss: 0.1055\n",
      "Epoch 14/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4361\n",
      "Epoch 00014: val_loss did not improve from 0.10548\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4257 - val_loss: 0.1090\n",
      "Epoch 15/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4319\n",
      "Epoch 00015: val_loss improved from 0.10548 to 0.10398, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4316 - val_loss: 0.1040\n",
      "Epoch 16/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4364\n",
      "Epoch 00016: val_loss did not improve from 0.10398\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4368 - val_loss: 0.1047\n",
      "Epoch 17/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.4235\n",
      "Epoch 00017: val_loss improved from 0.10398 to 0.10239, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_1.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4248 - val_loss: 0.1024\n",
      "Epoch 18/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4202\n",
      "Epoch 00018: val_loss did not improve from 0.10239\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4212 - val_loss: 0.1053\n",
      "Epoch 19/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.4130\n",
      "Epoch 00019: val_loss did not improve from 0.10239\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4127 - val_loss: 0.1068\n",
      " ###1 fold : val acc1 0.583, acc3 0.977, mae 0.221###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 15.4008 \n",
      "Epoch 00001: val_loss improved from inf to 3.41274, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 10.9329 - val_loss: 3.4127\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.4696\n",
      "Epoch 00002: val_loss improved from 3.41274 to 0.66122, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0231 - val_loss: 0.6612\n",
      "Epoch 3/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.8979\n",
      "Epoch 00003: val_loss improved from 0.66122 to 0.23495, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8054 - val_loss: 0.2349\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5915\n",
      "Epoch 00004: val_loss improved from 0.23495 to 0.14713, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5619 - val_loss: 0.1471\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5032\n",
      "Epoch 00005: val_loss improved from 0.14713 to 0.13111, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4947 - val_loss: 0.1311\n",
      "Epoch 6/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4719\n",
      "Epoch 00006: val_loss improved from 0.13111 to 0.12755, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4812 - val_loss: 0.1275\n",
      "Epoch 7/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4669\n",
      "Epoch 00007: val_loss improved from 0.12755 to 0.12182, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4600 - val_loss: 0.1218\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4690\n",
      "Epoch 00008: val_loss improved from 0.12182 to 0.11804, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4562 - val_loss: 0.1180\n",
      "Epoch 9/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4608\n",
      "Epoch 00009: val_loss improved from 0.11804 to 0.11287, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4516 - val_loss: 0.1129\n",
      "Epoch 10/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4457\n",
      "Epoch 00010: val_loss did not improve from 0.11287\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4458 - val_loss: 0.1171\n",
      "Epoch 11/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4408\n",
      "Epoch 00011: val_loss improved from 0.11287 to 0.10975, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4393 - val_loss: 0.1098\n",
      "Epoch 12/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4430\n",
      "Epoch 00012: val_loss improved from 0.10975 to 0.10845, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4437 - val_loss: 0.1084\n",
      "Epoch 13/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4314\n",
      "Epoch 00013: val_loss improved from 0.10845 to 0.10648, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4319 - val_loss: 0.1065\n",
      "Epoch 14/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4253\n",
      "Epoch 00014: val_loss did not improve from 0.10648\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4264 - val_loss: 0.1097\n",
      "Epoch 15/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4346\n",
      "Epoch 00015: val_loss improved from 0.10648 to 0.10401, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4348 - val_loss: 0.1040\n",
      "Epoch 16/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4342\n",
      "Epoch 00016: val_loss did not improve from 0.10401\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4349 - val_loss: 0.1051\n",
      "Epoch 17/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4225\n",
      "Epoch 00017: val_loss improved from 0.10401 to 0.10232, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_2.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4232 - val_loss: 0.1023\n",
      "Epoch 18/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4218\n",
      "Epoch 00018: val_loss did not improve from 0.10232\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4230 - val_loss: 0.1058\n",
      "Epoch 19/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4171\n",
      "Epoch 00019: val_loss did not improve from 0.10232\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4169 - val_loss: 0.1056\n",
      " ###2 fold : val acc1 0.588, acc3 0.977, mae 0.218###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 15.0038 \n",
      "Epoch 00001: val_loss improved from inf to 3.39846, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 10.9607 - val_loss: 3.3985\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.5141\n",
      "Epoch 00002: val_loss improved from 3.39846 to 0.65873, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 2.0311 - val_loss: 0.6587\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8987\n",
      "Epoch 00003: val_loss improved from 0.65873 to 0.23497, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8090 - val_loss: 0.2350\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5643\n",
      "Epoch 00004: val_loss improved from 0.23497 to 0.14615, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5643 - val_loss: 0.1462\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4897\n",
      "Epoch 00005: val_loss improved from 0.14615 to 0.12937, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4897 - val_loss: 0.1294\n",
      "Epoch 6/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4775\n",
      "Epoch 00006: val_loss improved from 0.12937 to 0.12918, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4834 - val_loss: 0.1292\n",
      "Epoch 7/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4634\n",
      "Epoch 00007: val_loss improved from 0.12918 to 0.12131, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4576 - val_loss: 0.1213\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4745\n",
      "Epoch 00008: val_loss improved from 0.12131 to 0.11748, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4606 - val_loss: 0.1175\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4666\n",
      "Epoch 00009: val_loss improved from 0.11748 to 0.11217, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4529 - val_loss: 0.1122\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4609\n",
      "Epoch 00010: val_loss did not improve from 0.11217\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4434 - val_loss: 0.1162\n",
      "Epoch 11/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 0.4491\n",
      "Epoch 00011: val_loss improved from 0.11217 to 0.10955, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4414 - val_loss: 0.1095\n",
      "Epoch 12/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4399\n",
      "Epoch 00012: val_loss improved from 0.10955 to 0.10908, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4435 - val_loss: 0.1091\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4326\n",
      "Epoch 00013: val_loss improved from 0.10908 to 0.10558, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4326 - val_loss: 0.1056\n",
      "Epoch 14/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.4216\n",
      "Epoch 00014: val_loss did not improve from 0.10558\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4215 - val_loss: 0.1103\n",
      "Epoch 15/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.4369\n",
      "Epoch 00015: val_loss improved from 0.10558 to 0.10372, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4347 - val_loss: 0.1037\n",
      "Epoch 16/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.4323\n",
      "Epoch 00016: val_loss did not improve from 0.10372\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4335 - val_loss: 0.1041\n",
      "Epoch 17/100\n",
      "30/35 [========================>.....] - ETA: 0s - loss: 0.4239\n",
      "Epoch 00017: val_loss improved from 0.10372 to 0.10212, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_3.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4250 - val_loss: 0.1021\n",
      "Epoch 18/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.4179\n",
      "Epoch 00018: val_loss did not improve from 0.10212\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4204 - val_loss: 0.1051\n",
      "Epoch 19/100\n",
      "30/35 [========================>.....] - ETA: 0s - loss: 0.4182\n",
      "Epoch 00019: val_loss did not improve from 0.10212\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4185 - val_loss: 0.1063\n",
      " ###3 fold : val acc1 0.601, acc3 0.967, mae 0.217###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 14.9424 \n",
      "Epoch 00001: val_loss improved from inf to 3.29646, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 10.9152 - val_loss: 3.2965\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.3818\n",
      "Epoch 00002: val_loss improved from 3.29646 to 0.64688, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.9495 - val_loss: 0.6469\n",
      "Epoch 3/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8726\n",
      "Epoch 00003: val_loss improved from 0.64688 to 0.21506, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7840 - val_loss: 0.2151\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5907\n",
      "Epoch 00004: val_loss improved from 0.21506 to 0.14254, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5643 - val_loss: 0.1425\n",
      "Epoch 5/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5004\n",
      "Epoch 00005: val_loss improved from 0.14254 to 0.12883, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5058 - val_loss: 0.1288\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4779\n",
      "Epoch 00006: val_loss improved from 0.12883 to 0.12265, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4707 - val_loss: 0.1226\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4768\n",
      "Epoch 00007: val_loss improved from 0.12265 to 0.11973, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4657 - val_loss: 0.1197\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4554\n",
      "Epoch 00008: val_loss did not improve from 0.11973\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4497 - val_loss: 0.1203\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4600\n",
      "Epoch 00009: val_loss improved from 0.11973 to 0.11481, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4518 - val_loss: 0.1148\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4294\n",
      "Epoch 00010: val_loss did not improve from 0.11481\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4294 - val_loss: 0.1156\n",
      "Epoch 11/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4338\n",
      "Epoch 00011: val_loss improved from 0.11481 to 0.10669, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4339 - val_loss: 0.1067\n",
      "Epoch 12/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4304\n",
      "Epoch 00012: val_loss did not improve from 0.10669\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4306 - val_loss: 0.1116\n",
      "Epoch 13/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4395\n",
      "Epoch 00013: val_loss improved from 0.10669 to 0.10645, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4362 - val_loss: 0.1064\n",
      "Epoch 14/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4270\n",
      "Epoch 00014: val_loss did not improve from 0.10645\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4303 - val_loss: 0.1077\n",
      "Epoch 15/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4167\n",
      "Epoch 00015: val_loss improved from 0.10645 to 0.10284, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_4.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4237 - val_loss: 0.1028\n",
      "Epoch 16/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4303\n",
      "Epoch 00016: val_loss did not improve from 0.10284\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4279 - val_loss: 0.1065\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4274\n",
      "Epoch 00017: val_loss did not improve from 0.10284\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4274 - val_loss: 0.1133\n",
      " ###4 fold : val acc1 0.581, acc3 0.976, mae 0.221###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/35 [=================>............] - ETA: 0s - loss: 14.9602 \n",
      "Epoch 00001: val_loss improved from inf to 3.33773, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 10.9185 - val_loss: 3.3377\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.4470\n",
      "Epoch 00002: val_loss improved from 3.33773 to 0.65638, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9614 - val_loss: 0.6564\n",
      "Epoch 3/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.8843\n",
      "Epoch 00003: val_loss improved from 0.65638 to 0.21792, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7933 - val_loss: 0.2179\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5931\n",
      "Epoch 00004: val_loss improved from 0.21792 to 0.14420, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5660 - val_loss: 0.1442\n",
      "Epoch 5/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5011\n",
      "Epoch 00005: val_loss improved from 0.14420 to 0.12846, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5040 - val_loss: 0.1285\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4714\n",
      "Epoch 00006: val_loss improved from 0.12846 to 0.12424, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4714 - val_loss: 0.1242\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4676\n",
      "Epoch 00007: val_loss improved from 0.12424 to 0.12073, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4676 - val_loss: 0.1207\n",
      "Epoch 8/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4524\n",
      "Epoch 00008: val_loss improved from 0.12073 to 0.12054, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4525 - val_loss: 0.1205\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4541\n",
      "Epoch 00009: val_loss improved from 0.12054 to 0.11445, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4541 - val_loss: 0.1144\n",
      "Epoch 10/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4310\n",
      "Epoch 00010: val_loss did not improve from 0.11445\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4310 - val_loss: 0.1160\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4367\n",
      "Epoch 00011: val_loss improved from 0.11445 to 0.10789, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4367 - val_loss: 0.1079\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4291\n",
      "Epoch 00012: val_loss did not improve from 0.10789\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4291 - val_loss: 0.1138\n",
      "Epoch 13/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4427\n",
      "Epoch 00013: val_loss improved from 0.10789 to 0.10567, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4397 - val_loss: 0.1057\n",
      "Epoch 14/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4296\n",
      "Epoch 00014: val_loss did not improve from 0.10567\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4284 - val_loss: 0.1084\n",
      "Epoch 15/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4231\n",
      "Epoch 00015: val_loss improved from 0.10567 to 0.10379, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_5.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4240 - val_loss: 0.1038\n",
      "Epoch 16/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4262\n",
      "Epoch 00016: val_loss did not improve from 0.10379\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4286 - val_loss: 0.1072\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4285\n",
      "Epoch 00017: val_loss did not improve from 0.10379\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4285 - val_loss: 0.1128\n",
      " ###5 fold : val acc1 0.578, acc3 0.977, mae 0.223###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 15.3660 \n",
      "Epoch 00001: val_loss improved from inf to 3.34474, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 10.9265 - val_loss: 3.3447\n",
      "Epoch 2/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 2.4960\n",
      "Epoch 00002: val_loss improved from 3.34474 to 0.65886, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9673 - val_loss: 0.6589\n",
      "Epoch 3/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.8067\n",
      "Epoch 00003: val_loss improved from 0.65886 to 0.21777, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8009 - val_loss: 0.2178\n",
      "Epoch 4/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.5671\n",
      "Epoch 00004: val_loss improved from 0.21777 to 0.14394, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5663 - val_loss: 0.1439\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5004\n",
      "Epoch 00005: val_loss improved from 0.14394 to 0.12925, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5004 - val_loss: 0.1292\n",
      "Epoch 6/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4730\n",
      "Epoch 00006: val_loss improved from 0.12925 to 0.12364, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4736 - val_loss: 0.1236\n",
      "Epoch 7/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4784\n",
      "Epoch 00007: val_loss improved from 0.12364 to 0.12020, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4676 - val_loss: 0.1202\n",
      "Epoch 8/100\n",
      "18/35 [==============>...............] - ETA: 0s - loss: 0.4649\n",
      "Epoch 00008: val_loss improved from 0.12020 to 0.12007, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4558 - val_loss: 0.1201\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4650\n",
      "Epoch 00009: val_loss improved from 0.12007 to 0.11444, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4540 - val_loss: 0.1144\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4383\n",
      "Epoch 00010: val_loss improved from 0.11444 to 0.11397, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4357 - val_loss: 0.1140\n",
      "Epoch 11/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4391\n",
      "Epoch 00011: val_loss improved from 0.11397 to 0.10829, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4406 - val_loss: 0.1083\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4298\n",
      "Epoch 00012: val_loss did not improve from 0.10829\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4298 - val_loss: 0.1156\n",
      "Epoch 13/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4414\n",
      "Epoch 00013: val_loss improved from 0.10829 to 0.10624, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4400 - val_loss: 0.1062\n",
      "Epoch 14/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.4288\n",
      "Epoch 00014: val_loss did not improve from 0.10624\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4286 - val_loss: 0.1100\n",
      "Epoch 15/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4247\n",
      "Epoch 00015: val_loss improved from 0.10624 to 0.10356, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_6.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4254 - val_loss: 0.1036\n",
      "Epoch 16/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4279\n",
      "Epoch 00016: val_loss did not improve from 0.10356\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4281 - val_loss: 0.1060\n",
      "Epoch 17/100\n",
      "31/35 [=========================>....] - ETA: 0s - loss: 0.4234\n",
      "Epoch 00017: val_loss did not improve from 0.10356\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4231 - val_loss: 0.1104\n",
      " ###6 fold : val acc1 0.586, acc3 0.980, mae 0.217###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 15.3539 \n",
      "Epoch 00001: val_loss improved from inf to 3.33142, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 10.9210 - val_loss: 3.3314\n",
      "Epoch 2/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 2.4174\n",
      "Epoch 00002: val_loss improved from 3.33142 to 0.65792, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9762 - val_loss: 0.6579\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8904\n",
      "Epoch 00003: val_loss improved from 0.65792 to 0.21831, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7954 - val_loss: 0.2183\n",
      "Epoch 4/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.5850\n",
      "Epoch 00004: val_loss improved from 0.21831 to 0.14387, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5612 - val_loss: 0.1439\n",
      "Epoch 5/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.5069\n",
      "Epoch 00005: val_loss improved from 0.14387 to 0.12824, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4969 - val_loss: 0.1282\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4788\n",
      "Epoch 00006: val_loss improved from 0.12824 to 0.12448, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4720 - val_loss: 0.1245\n",
      "Epoch 7/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.4774\n",
      "Epoch 00007: val_loss improved from 0.12448 to 0.12121, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4668 - val_loss: 0.1212\n",
      "Epoch 8/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.4541\n",
      "Epoch 00008: val_loss improved from 0.12121 to 0.12037, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4516 - val_loss: 0.1204\n",
      "Epoch 9/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4612\n",
      "Epoch 00009: val_loss improved from 0.12037 to 0.11375, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4510 - val_loss: 0.1137\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4344\n",
      "Epoch 00010: val_loss improved from 0.11375 to 0.11365, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4317 - val_loss: 0.1136\n",
      "Epoch 11/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4405\n",
      "Epoch 00011: val_loss improved from 0.11365 to 0.10862, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4405 - val_loss: 0.1086\n",
      "Epoch 12/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.4324\n",
      "Epoch 00012: val_loss did not improve from 0.10862\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4296 - val_loss: 0.1156\n",
      "Epoch 13/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4411\n",
      "Epoch 00013: val_loss improved from 0.10862 to 0.10642, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4367 - val_loss: 0.1064\n",
      "Epoch 14/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4289\n",
      "Epoch 00014: val_loss did not improve from 0.10642\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4284 - val_loss: 0.1101\n",
      "Epoch 15/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.4209\n",
      "Epoch 00015: val_loss improved from 0.10642 to 0.10408, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_7.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4215 - val_loss: 0.1041\n",
      "Epoch 16/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4252\n",
      "Epoch 00016: val_loss did not improve from 0.10408\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4253 - val_loss: 0.1055\n",
      "Epoch 17/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4218\n",
      "Epoch 00017: val_loss did not improve from 0.10408\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4204 - val_loss: 0.1115\n",
      " ###7 fold : val acc1 0.569, acc3 0.968, mae 0.232###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/35 [===============>..............] - ETA: 0s - loss: 15.7809 \n",
      "Epoch 00001: val_loss improved from inf to 3.28488, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 10.9394 - val_loss: 3.2849\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.4522\n",
      "Epoch 00002: val_loss improved from 3.28488 to 0.65951, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9711 - val_loss: 0.6595\n",
      "Epoch 3/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.9044\n",
      "Epoch 00003: val_loss improved from 0.65951 to 0.22028, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7976 - val_loss: 0.2203\n",
      "Epoch 4/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.5860\n",
      "Epoch 00004: val_loss improved from 0.22028 to 0.14863, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5624 - val_loss: 0.1486\n",
      "Epoch 5/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5065\n",
      "Epoch 00005: val_loss improved from 0.14863 to 0.13329, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4994 - val_loss: 0.1333\n",
      "Epoch 6/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4800\n",
      "Epoch 00006: val_loss improved from 0.13329 to 0.12903, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4733 - val_loss: 0.1290\n",
      "Epoch 7/100\n",
      "21/35 [=================>............] - ETA: 0s - loss: 0.4777\n",
      "Epoch 00007: val_loss improved from 0.12903 to 0.12595, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4678 - val_loss: 0.1260\n",
      "Epoch 8/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4567\n",
      "Epoch 00008: val_loss improved from 0.12595 to 0.12463, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4520 - val_loss: 0.1246\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4625\n",
      "Epoch 00009: val_loss improved from 0.12463 to 0.11759, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4528 - val_loss: 0.1176\n",
      "Epoch 10/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4326\n",
      "Epoch 00010: val_loss improved from 0.11759 to 0.11709, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4325 - val_loss: 0.1171\n",
      "Epoch 11/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.4421\n",
      "Epoch 00011: val_loss improved from 0.11709 to 0.11268, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4410 - val_loss: 0.1127\n",
      "Epoch 12/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4302\n",
      "Epoch 00012: val_loss did not improve from 0.11268\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4308 - val_loss: 0.1197\n",
      "Epoch 13/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.4379\n",
      "Epoch 00013: val_loss improved from 0.11268 to 0.10977, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4360 - val_loss: 0.1098\n",
      "Epoch 14/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4302\n",
      "Epoch 00014: val_loss did not improve from 0.10977\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4289 - val_loss: 0.1136\n",
      "Epoch 15/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4208\n",
      "Epoch 00015: val_loss improved from 0.10977 to 0.10751, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_8.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4222 - val_loss: 0.1075\n",
      "Epoch 16/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4258\n",
      "Epoch 00016: val_loss did not improve from 0.10751\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4258 - val_loss: 0.1094\n",
      "Epoch 17/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4225\n",
      "Epoch 00017: val_loss did not improve from 0.10751\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4211 - val_loss: 0.1156\n",
      " ###8 fold : val acc1 0.579, acc3 0.974, mae 0.224###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 15.3816 \n",
      "Epoch 00001: val_loss improved from inf to 3.31858, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 10.9394 - val_loss: 3.3186\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 2.4522\n",
      "Epoch 00002: val_loss improved from 3.31858 to 0.67020, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.9711 - val_loss: 0.6702\n",
      "Epoch 3/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.8947\n",
      "Epoch 00003: val_loss improved from 0.67020 to 0.22565, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7976 - val_loss: 0.2256\n",
      "Epoch 4/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5844\n",
      "Epoch 00004: val_loss improved from 0.22565 to 0.15197, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5624 - val_loss: 0.1520\n",
      "Epoch 5/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.5065\n",
      "Epoch 00005: val_loss improved from 0.15197 to 0.13536, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4994 - val_loss: 0.1354\n",
      "Epoch 6/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4778\n",
      "Epoch 00006: val_loss improved from 0.13536 to 0.13132, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4733 - val_loss: 0.1313\n",
      "Epoch 7/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4819\n",
      "Epoch 00007: val_loss improved from 0.13132 to 0.12900, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4678 - val_loss: 0.1290\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4583\n",
      "Epoch 00008: val_loss improved from 0.12900 to 0.12760, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4520 - val_loss: 0.1276\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4528\n",
      "Epoch 00009: val_loss improved from 0.12760 to 0.11898, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4528 - val_loss: 0.1190\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4356\n",
      "Epoch 00010: val_loss did not improve from 0.11898\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4325 - val_loss: 0.1202\n",
      "Epoch 11/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 0.4433\n",
      "Epoch 00011: val_loss improved from 0.11898 to 0.11503, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4410 - val_loss: 0.1150\n",
      "Epoch 12/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 0.4358\n",
      "Epoch 00012: val_loss did not improve from 0.11503\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.4308 - val_loss: 0.1222\n",
      "Epoch 13/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4379\n",
      "Epoch 00013: val_loss improved from 0.11503 to 0.11237, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4360 - val_loss: 0.1124\n",
      "Epoch 14/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.4302\n",
      "Epoch 00014: val_loss did not improve from 0.11237\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4289 - val_loss: 0.1161\n",
      "Epoch 15/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4208\n",
      "Epoch 00015: val_loss improved from 0.11237 to 0.10932, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch512,dnodes64_dropout0,dnodes64_dropout0.3,lr0.002/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4222 - val_loss: 0.1093\n",
      "Epoch 16/100\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.4255\n",
      "Epoch 00016: val_loss did not improve from 0.10932\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4258 - val_loss: 0.1118\n",
      "Epoch 17/100\n",
      "32/35 [==========================>...] - ETA: 0s - loss: 0.4211\n",
      "Epoch 00017: val_loss did not improve from 0.10932\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4211 - val_loss: 0.1182\n",
      " ###9 fold : val acc1 0.580, acc3 0.976, mae 0.222###\n",
      "acc10.585_acc30.975\n",
      "random search 103/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264/279 [===========================>..] - ETA: 0s - loss: 3.6663\n",
      "Epoch 00001: val_loss improved from inf to 0.14027, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_0.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.4879 - val_loss: 0.1403\n",
      "Epoch 2/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2427\n",
      "Epoch 00002: val_loss improved from 0.14027 to 0.11373, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2428 - val_loss: 0.1137\n",
      "Epoch 3/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.2166\n",
      "Epoch 00003: val_loss improved from 0.11373 to 0.10492, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2166 - val_loss: 0.1049\n",
      "Epoch 4/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.2071\n",
      "Epoch 00004: val_loss improved from 0.10492 to 0.10407, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_0.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2077 - val_loss: 0.1041\n",
      "Epoch 5/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.2016\n",
      "Epoch 00005: val_loss did not improve from 0.10407\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2018 - val_loss: 0.1068\n",
      "Epoch 6/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.1977\n",
      "Epoch 00006: val_loss did not improve from 0.10407\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1981 - val_loss: 0.1152\n",
      " ###0 fold : val acc1 0.579, acc3 0.973, mae 0.225###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - ETA: 0s - loss: 3.4981\n",
      "Epoch 00001: val_loss improved from inf to 0.13965, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.4981 - val_loss: 0.1397\n",
      "Epoch 2/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2417\n",
      "Epoch 00002: val_loss improved from 0.13965 to 0.11292, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2418 - val_loss: 0.1129\n",
      "Epoch 3/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.2155\n",
      "Epoch 00003: val_loss improved from 0.11292 to 0.10610, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2152 - val_loss: 0.1061\n",
      "Epoch 4/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.2077\n",
      "Epoch 00004: val_loss improved from 0.10610 to 0.10473, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2079 - val_loss: 0.1047\n",
      "Epoch 5/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2014\n",
      "Epoch 00005: val_loss improved from 0.10473 to 0.10415, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2014 - val_loss: 0.1041\n",
      "Epoch 6/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1974\n",
      "Epoch 00006: val_loss did not improve from 0.10415\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1976 - val_loss: 0.1130\n",
      "Epoch 7/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1957\n",
      "Epoch 00007: val_loss improved from 0.10415 to 0.09800, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1957 - val_loss: 0.0980\n",
      "Epoch 8/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1975\n",
      "Epoch 00008: val_loss did not improve from 0.09800\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1968 - val_loss: 0.1002\n",
      "Epoch 9/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1950\n",
      "Epoch 00009: val_loss improved from 0.09800 to 0.09739, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1947 - val_loss: 0.0974\n",
      "Epoch 10/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1950\n",
      "Epoch 00010: val_loss improved from 0.09739 to 0.09576, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1954 - val_loss: 0.0958\n",
      "Epoch 11/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1935\n",
      "Epoch 00011: val_loss improved from 0.09576 to 0.09547, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_1.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1933 - val_loss: 0.0955\n",
      "Epoch 12/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1916\n",
      "Epoch 00012: val_loss did not improve from 0.09547\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1916 - val_loss: 0.1001\n",
      "Epoch 13/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1930\n",
      "Epoch 00013: val_loss did not improve from 0.09547\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1932 - val_loss: 0.0999\n",
      " ###1 fold : val acc1 0.590, acc3 0.983, mae 0.214###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/279 [============================>.] - ETA: 0s - loss: 3.5195\n",
      "Epoch 00001: val_loss improved from inf to 0.13974, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 3.5009 - val_loss: 0.1397\n",
      "Epoch 2/100\n",
      "270/279 [============================>.] - ETA: 0s - loss: 0.2421\n",
      "Epoch 00002: val_loss improved from 0.13974 to 0.11294, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2413 - val_loss: 0.1129\n",
      "Epoch 3/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.2165\n",
      "Epoch 00003: val_loss improved from 0.11294 to 0.10582, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2165 - val_loss: 0.1058\n",
      "Epoch 4/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2066\n",
      "Epoch 00004: val_loss improved from 0.10582 to 0.10292, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2068 - val_loss: 0.1029\n",
      "Epoch 5/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1999\n",
      "Epoch 00005: val_loss improved from 0.10292 to 0.10166, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2000 - val_loss: 0.1017\n",
      "Epoch 6/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1983\n",
      "Epoch 00006: val_loss did not improve from 0.10166\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1987 - val_loss: 0.1101\n",
      "Epoch 7/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1969\n",
      "Epoch 00007: val_loss improved from 0.10166 to 0.09701, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_2.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1965 - val_loss: 0.0970\n",
      "Epoch 8/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1968\n",
      "Epoch 00008: val_loss did not improve from 0.09701\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1964 - val_loss: 0.0994\n",
      "Epoch 9/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.1949\n",
      "Epoch 00009: val_loss did not improve from 0.09701\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1946 - val_loss: 0.1021\n",
      " ###2 fold : val acc1 0.600, acc3 0.977, mae 0.212###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/279 [============================>.] - ETA: 0s - loss: 3.5204\n",
      "Epoch 00001: val_loss improved from inf to 0.13281, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_3.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.4903 - val_loss: 0.1328\n",
      "Epoch 2/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2417\n",
      "Epoch 00002: val_loss improved from 0.13281 to 0.11049, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2414 - val_loss: 0.1105\n",
      "Epoch 3/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2149\n",
      "Epoch 00003: val_loss improved from 0.11049 to 0.10543, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2145 - val_loss: 0.1054\n",
      "Epoch 4/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.2042\n",
      "Epoch 00004: val_loss improved from 0.10543 to 0.09950, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2044 - val_loss: 0.0995\n",
      "Epoch 5/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2002\n",
      "Epoch 00005: val_loss improved from 0.09950 to 0.09888, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2003 - val_loss: 0.0989\n",
      "Epoch 6/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1988\n",
      "Epoch 00006: val_loss did not improve from 0.09888\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1993 - val_loss: 0.1039\n",
      "Epoch 7/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1948\n",
      "Epoch 00007: val_loss improved from 0.09888 to 0.09566, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_3.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1944 - val_loss: 0.0957\n",
      "Epoch 8/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1986\n",
      "Epoch 00008: val_loss did not improve from 0.09566\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1979 - val_loss: 0.0974\n",
      "Epoch 9/100\n",
      "266/279 [===========================>..] - ETA: 0s - loss: 0.1960\n",
      "Epoch 00009: val_loss did not improve from 0.09566\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1950 - val_loss: 0.1062\n",
      " ###3 fold : val acc1 0.617, acc3 0.974, mae 0.205###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264/279 [===========================>..] - ETA: 0s - loss: 3.7171\n",
      "Epoch 00001: val_loss improved from inf to 0.14357, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 3.5356 - val_loss: 0.1436\n",
      "Epoch 2/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2444\n",
      "Epoch 00002: val_loss improved from 0.14357 to 0.11703, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2441 - val_loss: 0.1170\n",
      "Epoch 3/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.2233\n",
      "Epoch 00003: val_loss improved from 0.11703 to 0.10533, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2229 - val_loss: 0.1053\n",
      "Epoch 4/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.2049\n",
      "Epoch 00004: val_loss improved from 0.10533 to 0.10466, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2049 - val_loss: 0.1047\n",
      "Epoch 5/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2053\n",
      "Epoch 00005: val_loss improved from 0.10466 to 0.09950, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_4.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2057 - val_loss: 0.0995\n",
      "Epoch 6/100\n",
      "274/279 [============================>.] - ETA: 0s - loss: 0.2022\n",
      "Epoch 00006: val_loss did not improve from 0.09950\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2020 - val_loss: 0.1030\n",
      "Epoch 7/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2037\n",
      "Epoch 00007: val_loss did not improve from 0.09950\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2037 - val_loss: 0.1030\n",
      " ###4 fold : val acc1 0.578, acc3 0.977, mae 0.222###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/279 [============================>.] - ETA: 0s - loss: 3.5044\n",
      "Epoch 00001: val_loss improved from inf to 0.13784, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 3.4973 - val_loss: 0.1378\n",
      "Epoch 2/100\n",
      "265/279 [===========================>..] - ETA: 0s - loss: 0.2395\n",
      "Epoch 00002: val_loss improved from 0.13784 to 0.11613, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2378 - val_loss: 0.1161\n",
      "Epoch 3/100\n",
      "264/279 [===========================>..] - ETA: 0s - loss: 0.2203\n",
      "Epoch 00003: val_loss improved from 0.11613 to 0.10533, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2199 - val_loss: 0.1053\n",
      "Epoch 4/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2020\n",
      "Epoch 00004: val_loss did not improve from 0.10533\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2020 - val_loss: 0.1059\n",
      "Epoch 5/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.2011\n",
      "Epoch 00005: val_loss improved from 0.10533 to 0.09936, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_5.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2011 - val_loss: 0.0994\n",
      "Epoch 6/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1985\n",
      "Epoch 00006: val_loss did not improve from 0.09936\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1988 - val_loss: 0.1048\n",
      "Epoch 7/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.2011\n",
      "Epoch 00007: val_loss did not improve from 0.09936\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2014 - val_loss: 0.1072\n",
      " ###5 fold : val acc1 0.586, acc3 0.980, mae 0.217###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271/279 [============================>.] - ETA: 0s - loss: 3.5970\n",
      "Epoch 00001: val_loss improved from inf to 0.13703, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_6.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.5059 - val_loss: 0.1370\n",
      "Epoch 2/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.2392\n",
      "Epoch 00002: val_loss improved from 0.13703 to 0.11501, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2392 - val_loss: 0.1150\n",
      "Epoch 3/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2202\n",
      "Epoch 00003: val_loss improved from 0.11501 to 0.10396, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2198 - val_loss: 0.1040\n",
      "Epoch 4/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.2042\n",
      "Epoch 00004: val_loss did not improve from 0.10396\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2043 - val_loss: 0.1074\n",
      "Epoch 5/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.2014\n",
      "Epoch 00005: val_loss improved from 0.10396 to 0.10064, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_6.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2019 - val_loss: 0.1006\n",
      "Epoch 6/100\n",
      "271/279 [============================>.] - ETA: 0s - loss: 0.1992\n",
      "Epoch 00006: val_loss did not improve from 0.10064\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1992 - val_loss: 0.1051\n",
      "Epoch 7/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2021\n",
      "Epoch 00007: val_loss did not improve from 0.10064\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2020 - val_loss: 0.1065\n",
      " ###6 fold : val acc1 0.591, acc3 0.984, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267/279 [===========================>..] - ETA: 0s - loss: 3.6315\n",
      "Epoch 00001: val_loss improved from inf to 0.13835, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.4910 - val_loss: 0.1383\n",
      "Epoch 2/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2377\n",
      "Epoch 00002: val_loss improved from 0.13835 to 0.11514, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2366 - val_loss: 0.1151\n",
      "Epoch 3/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.2181\n",
      "Epoch 00003: val_loss improved from 0.11514 to 0.10504, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2178 - val_loss: 0.1050\n",
      "Epoch 4/100\n",
      "269/279 [===========================>..] - ETA: 0s - loss: 0.2024\n",
      "Epoch 00004: val_loss did not improve from 0.10504\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2018 - val_loss: 0.1094\n",
      "Epoch 5/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.2004\n",
      "Epoch 00005: val_loss improved from 0.10504 to 0.10031, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_7.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2003 - val_loss: 0.1003\n",
      "Epoch 6/100\n",
      "268/279 [===========================>..] - ETA: 0s - loss: 0.1978\n",
      "Epoch 00006: val_loss did not improve from 0.10031\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1979 - val_loss: 0.1023\n",
      "Epoch 7/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.1994\n",
      "Epoch 00007: val_loss did not improve from 0.10031\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1995 - val_loss: 0.1086\n",
      " ###7 fold : val acc1 0.569, acc3 0.972, mae 0.229###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/279 [===========================>..] - ETA: 0s - loss: 3.6589\n",
      "Epoch 00001: val_loss improved from inf to 0.14311, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.5049 - val_loss: 0.1431\n",
      "Epoch 2/100\n",
      "276/279 [============================>.] - ETA: 0s - loss: 0.2375\n",
      "Epoch 00002: val_loss improved from 0.14311 to 0.11988, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2371 - val_loss: 0.1199\n",
      "Epoch 3/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2183\n",
      "Epoch 00003: val_loss improved from 0.11988 to 0.10936, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2179 - val_loss: 0.1094\n",
      "Epoch 4/100\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.2020\n",
      "Epoch 00004: val_loss did not improve from 0.10936\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2020 - val_loss: 0.1123\n",
      "Epoch 5/100\n",
      "275/279 [============================>.] - ETA: 0s - loss: 0.1997\n",
      "Epoch 00005: val_loss improved from 0.10936 to 0.10354, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_8.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1999 - val_loss: 0.1035\n",
      "Epoch 6/100\n",
      "279/279 [==============================] - ETA: 0s - loss: 0.1975\n",
      "Epoch 00006: val_loss did not improve from 0.10354\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1975 - val_loss: 0.1056\n",
      "Epoch 7/100\n",
      "277/279 [============================>.] - ETA: 0s - loss: 0.1991\n",
      "Epoch 00007: val_loss did not improve from 0.10354\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1993 - val_loss: 0.1102\n",
      " ###8 fold : val acc1 0.572, acc3 0.981, mae 0.223###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 3.5837\n",
      "Epoch 00001: val_loss improved from inf to 0.14502, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 2s 4ms/step - loss: 3.5049 - val_loss: 0.1450\n",
      "Epoch 2/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.2389\n",
      "Epoch 00002: val_loss improved from 0.14502 to 0.12304, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2371 - val_loss: 0.1230\n",
      "Epoch 3/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.2183\n",
      "Epoch 00003: val_loss improved from 0.12304 to 0.11048, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2179 - val_loss: 0.1105\n",
      "Epoch 4/100\n",
      "272/279 [============================>.] - ETA: 0s - loss: 0.2023\n",
      "Epoch 00004: val_loss did not improve from 0.11048\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2020 - val_loss: 0.1139\n",
      "Epoch 5/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1993\n",
      "Epoch 00005: val_loss improved from 0.11048 to 0.10483, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch64,dnodes512_dropout0,dnodes64_dropout0.1,lr0.0005/weights_9.hdf5\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1999 - val_loss: 0.1048\n",
      "Epoch 6/100\n",
      "267/279 [===========================>..] - ETA: 0s - loss: 0.1975\n",
      "Epoch 00006: val_loss did not improve from 0.10483\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1975 - val_loss: 0.1080\n",
      "Epoch 7/100\n",
      "273/279 [============================>.] - ETA: 0s - loss: 0.1994\n",
      "Epoch 00007: val_loss did not improve from 0.10483\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.1993 - val_loss: 0.1125\n",
      " ###9 fold : val acc1 0.578, acc3 0.983, mae 0.219###\n",
      "acc10.586_acc30.978\n",
      "random search 104/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/140 [============================>.] - ETA: 0s - loss: 5.7074\n",
      "Epoch 00001: val_loss improved from inf to 0.50674, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 5.6970 - val_loss: 0.5067\n",
      "Epoch 2/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.2581\n",
      "Epoch 00002: val_loss improved from 0.50674 to 0.12230, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2558 - val_loss: 0.1223\n",
      "Epoch 3/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1679\n",
      "Epoch 00003: val_loss improved from 0.12230 to 0.11402, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1672 - val_loss: 0.1140\n",
      "Epoch 4/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1596\n",
      "Epoch 00004: val_loss improved from 0.11402 to 0.10845, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1598 - val_loss: 0.1084\n",
      "Epoch 5/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1528\n",
      "Epoch 00005: val_loss improved from 0.10845 to 0.10401, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1523 - val_loss: 0.1040\n",
      "Epoch 6/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1472\n",
      "Epoch 00006: val_loss improved from 0.10401 to 0.10283, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1469 - val_loss: 0.1028\n",
      "Epoch 7/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1432\n",
      "Epoch 00007: val_loss improved from 0.10283 to 0.10072, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1427 - val_loss: 0.1007\n",
      "Epoch 8/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1418\n",
      "Epoch 00008: val_loss improved from 0.10072 to 0.09811, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1413 - val_loss: 0.0981\n",
      "Epoch 9/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1368\n",
      "Epoch 00009: val_loss improved from 0.09811 to 0.09753, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1366 - val_loss: 0.0975\n",
      "Epoch 10/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1370\n",
      "Epoch 00010: val_loss improved from 0.09753 to 0.09651, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1367 - val_loss: 0.0965\n",
      "Epoch 11/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1341\n",
      "Epoch 00011: val_loss improved from 0.09651 to 0.09580, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1340 - val_loss: 0.0958\n",
      "Epoch 12/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.1342\n",
      "Epoch 00012: val_loss improved from 0.09580 to 0.09509, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1341 - val_loss: 0.0951\n",
      "Epoch 13/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1322\n",
      "Epoch 00013: val_loss did not improve from 0.09509\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.0965\n",
      "Epoch 14/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1297\n",
      "Epoch 00014: val_loss did not improve from 0.09509\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1297 - val_loss: 0.0955\n",
      " ###0 fold : val acc1 0.606, acc3 0.981, mae 0.207###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/140 [===========================>..] - ETA: 0s - loss: 5.9465\n",
      "Epoch 00001: val_loss improved from inf to 0.50026, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 5.7053 - val_loss: 0.5003\n",
      "Epoch 2/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.2614\n",
      "Epoch 00002: val_loss improved from 0.50026 to 0.12261, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2527 - val_loss: 0.1226\n",
      "Epoch 3/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1670\n",
      "Epoch 00003: val_loss improved from 0.12261 to 0.11419, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1670 - val_loss: 0.1142\n",
      "Epoch 4/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1593\n",
      "Epoch 00004: val_loss improved from 0.11419 to 0.10826, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1593 - val_loss: 0.1083\n",
      "Epoch 5/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1506\n",
      "Epoch 00005: val_loss improved from 0.10826 to 0.10392, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1511 - val_loss: 0.1039\n",
      "Epoch 6/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1467\n",
      "Epoch 00006: val_loss did not improve from 0.10392\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1467 - val_loss: 0.1048\n",
      "Epoch 7/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1428\n",
      "Epoch 00007: val_loss improved from 0.10392 to 0.10036, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1430 - val_loss: 0.1004\n",
      "Epoch 8/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1412\n",
      "Epoch 00008: val_loss improved from 0.10036 to 0.09785, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1412 - val_loss: 0.0979\n",
      "Epoch 9/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1375\n",
      "Epoch 00009: val_loss improved from 0.09785 to 0.09709, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1373 - val_loss: 0.0971\n",
      "Epoch 10/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1367\n",
      "Epoch 00010: val_loss improved from 0.09709 to 0.09623, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1362 - val_loss: 0.0962\n",
      "Epoch 11/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.1340\n",
      "Epoch 00011: val_loss improved from 0.09623 to 0.09535, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1340 - val_loss: 0.0954\n",
      "Epoch 12/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1330\n",
      "Epoch 00012: val_loss improved from 0.09535 to 0.09516, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1331 - val_loss: 0.0952\n",
      "Epoch 13/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1299\n",
      "Epoch 00013: val_loss did not improve from 0.09516\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1305 - val_loss: 0.0966\n",
      "Epoch 14/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1288\n",
      "Epoch 00014: val_loss did not improve from 0.09516\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1291 - val_loss: 0.0952\n",
      " ###1 fold : val acc1 0.584, acc3 0.984, mae 0.216###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/140 [===========================>..] - ETA: 0s - loss: 6.0163\n",
      "Epoch 00001: val_loss improved from inf to 0.48828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 5.6969 - val_loss: 0.4883\n",
      "Epoch 2/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2564\n",
      "Epoch 00002: val_loss improved from 0.48828 to 0.12246, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2508 - val_loss: 0.1225\n",
      "Epoch 3/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1679\n",
      "Epoch 00003: val_loss improved from 0.12246 to 0.11398, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1675 - val_loss: 0.1140\n",
      "Epoch 4/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1593\n",
      "Epoch 00004: val_loss improved from 0.11398 to 0.10808, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1594 - val_loss: 0.1081\n",
      "Epoch 5/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1513\n",
      "Epoch 00005: val_loss improved from 0.10808 to 0.10438, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1513 - val_loss: 0.1044\n",
      "Epoch 6/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1478\n",
      "Epoch 00006: val_loss did not improve from 0.10438\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1478 - val_loss: 0.1066\n",
      "Epoch 7/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1439\n",
      "Epoch 00007: val_loss improved from 0.10438 to 0.10012, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1436 - val_loss: 0.1001\n",
      "Epoch 8/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1416\n",
      "Epoch 00008: val_loss improved from 0.10012 to 0.09760, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1412 - val_loss: 0.0976\n",
      "Epoch 9/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1378\n",
      "Epoch 00009: val_loss did not improve from 0.09760\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1375 - val_loss: 0.0976\n",
      "Epoch 10/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1371\n",
      "Epoch 00010: val_loss improved from 0.09760 to 0.09684, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1366 - val_loss: 0.0968\n",
      "Epoch 11/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1347\n",
      "Epoch 00011: val_loss improved from 0.09684 to 0.09548, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1344 - val_loss: 0.0955\n",
      "Epoch 12/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1332\n",
      "Epoch 00012: val_loss improved from 0.09548 to 0.09517, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1331 - val_loss: 0.0952\n",
      "Epoch 13/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1305\n",
      "Epoch 00013: val_loss did not improve from 0.09517\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1306 - val_loss: 0.0954\n",
      "Epoch 14/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1286\n",
      "Epoch 00014: val_loss improved from 0.09517 to 0.09512, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1292 - val_loss: 0.0951\n",
      "Epoch 15/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1307\n",
      "Epoch 00015: val_loss improved from 0.09512 to 0.09434, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1306 - val_loss: 0.0943\n",
      "Epoch 16/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00016: val_loss did not improve from 0.09434\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1283 - val_loss: 0.0945\n",
      "Epoch 17/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00017: val_loss did not improve from 0.09434\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1287 - val_loss: 0.0969\n",
      " ###2 fold : val acc1 0.606, acc3 0.979, mae 0.208###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/140 [==========================>...] - ETA: 0s - loss: 6.0733\n",
      "Epoch 00001: val_loss improved from inf to 0.48859, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 5.7104 - val_loss: 0.4886\n",
      "Epoch 2/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2584\n",
      "Epoch 00002: val_loss improved from 0.48859 to 0.12285, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.2517 - val_loss: 0.1228\n",
      "Epoch 3/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1677\n",
      "Epoch 00003: val_loss improved from 0.12285 to 0.11497, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1672 - val_loss: 0.1150\n",
      "Epoch 4/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.1597\n",
      "Epoch 00004: val_loss improved from 0.11497 to 0.10875, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1591 - val_loss: 0.1087\n",
      "Epoch 5/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1504\n",
      "Epoch 00005: val_loss improved from 0.10875 to 0.10503, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1505 - val_loss: 0.1050\n",
      "Epoch 6/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1482\n",
      "Epoch 00006: val_loss did not improve from 0.10503\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1478 - val_loss: 0.1064\n",
      "Epoch 7/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1443\n",
      "Epoch 00007: val_loss improved from 0.10503 to 0.10002, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1442 - val_loss: 0.1000\n",
      "Epoch 8/100\n",
      "129/140 [==========================>...] - ETA: 0s - loss: 0.1415\n",
      "Epoch 00008: val_loss improved from 0.10002 to 0.09758, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1409 - val_loss: 0.0976\n",
      "Epoch 9/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1377\n",
      "Epoch 00009: val_loss did not improve from 0.09758\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1374 - val_loss: 0.0988\n",
      "Epoch 10/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1356\n",
      "Epoch 00010: val_loss improved from 0.09758 to 0.09706, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1358 - val_loss: 0.0971\n",
      "Epoch 11/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1343\n",
      "Epoch 00011: val_loss improved from 0.09706 to 0.09536, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1343 - val_loss: 0.0954\n",
      "Epoch 12/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1334\n",
      "Epoch 00012: val_loss improved from 0.09536 to 0.09515, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1334 - val_loss: 0.0952\n",
      "Epoch 13/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1297\n",
      "Epoch 00013: val_loss did not improve from 0.09515\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1299 - val_loss: 0.0953\n",
      "Epoch 14/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1301\n",
      "Epoch 00014: val_loss improved from 0.09515 to 0.09482, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1304 - val_loss: 0.0948\n",
      "Epoch 15/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1308\n",
      "Epoch 00015: val_loss improved from 0.09482 to 0.09460, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1306 - val_loss: 0.0946\n",
      "Epoch 16/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00016: val_loss did not improve from 0.09460\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1284 - val_loss: 0.0946\n",
      "Epoch 17/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1278\n",
      "Epoch 00017: val_loss did not improve from 0.09460\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1280 - val_loss: 0.0961\n",
      " ###3 fold : val acc1 0.613, acc3 0.974, mae 0.207###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/140 [==========================>...] - ETA: 0s - loss: 6.0606\n",
      "Epoch 00001: val_loss improved from inf to 0.47031, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 5.6966 - val_loss: 0.4703\n",
      "Epoch 2/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.2557\n",
      "Epoch 00002: val_loss improved from 0.47031 to 0.12382, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2466 - val_loss: 0.1238\n",
      "Epoch 3/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.1676\n",
      "Epoch 00003: val_loss improved from 0.12382 to 0.11500, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1667 - val_loss: 0.1150\n",
      "Epoch 4/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1585\n",
      "Epoch 00004: val_loss improved from 0.11500 to 0.10948, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1581 - val_loss: 0.1095\n",
      "Epoch 5/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1519\n",
      "Epoch 00005: val_loss improved from 0.10948 to 0.10549, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1516 - val_loss: 0.1055\n",
      "Epoch 6/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1487\n",
      "Epoch 00006: val_loss improved from 0.10549 to 0.10197, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1486 - val_loss: 0.1020\n",
      "Epoch 7/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.1453\n",
      "Epoch 00007: val_loss improved from 0.10197 to 0.10023, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1455 - val_loss: 0.1002\n",
      "Epoch 8/100\n",
      "121/140 [========================>.....] - ETA: 0s - loss: 0.1405\n",
      "Epoch 00008: val_loss improved from 0.10023 to 0.09937, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1407 - val_loss: 0.0994\n",
      "Epoch 9/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1367\n",
      "Epoch 00009: val_loss improved from 0.09937 to 0.09741, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1366 - val_loss: 0.0974\n",
      "Epoch 10/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1366\n",
      "Epoch 00010: val_loss did not improve from 0.09741\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1360 - val_loss: 0.0979\n",
      "Epoch 11/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1362\n",
      "Epoch 00011: val_loss improved from 0.09741 to 0.09655, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1363 - val_loss: 0.0965\n",
      "Epoch 12/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1341\n",
      "Epoch 00012: val_loss did not improve from 0.09655\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1338 - val_loss: 0.0978\n",
      "Epoch 13/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1312\n",
      "Epoch 00013: val_loss improved from 0.09655 to 0.09505, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1310 - val_loss: 0.0951\n",
      "Epoch 14/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1299\n",
      "Epoch 00014: val_loss improved from 0.09505 to 0.09427, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1300 - val_loss: 0.0943\n",
      "Epoch 15/100\n",
      "133/140 [===========================>..] - ETA: 0s - loss: 0.1290\n",
      "Epoch 00015: val_loss did not improve from 0.09427\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1288 - val_loss: 0.0955\n",
      "Epoch 16/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1288\n",
      "Epoch 00016: val_loss improved from 0.09427 to 0.09381, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1285 - val_loss: 0.0938\n",
      "Epoch 17/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1264\n",
      "Epoch 00017: val_loss did not improve from 0.09381\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1265 - val_loss: 0.0957\n",
      "Epoch 18/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00018: val_loss did not improve from 0.09381\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1260 - val_loss: 0.0957\n",
      " ###4 fold : val acc1 0.598, acc3 0.979, mae 0.211###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/140 [==========================>...] - ETA: 0s - loss: 6.0637\n",
      "Epoch 00001: val_loss improved from inf to 0.47054, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 5.6989 - val_loss: 0.4705\n",
      "Epoch 2/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.2475\n",
      "Epoch 00002: val_loss improved from 0.47054 to 0.12409, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2467 - val_loss: 0.1241\n",
      "Epoch 3/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1677\n",
      "Epoch 00003: val_loss improved from 0.12409 to 0.11516, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1677 - val_loss: 0.1152\n",
      "Epoch 4/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1582\n",
      "Epoch 00004: val_loss improved from 0.11516 to 0.10928, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1583 - val_loss: 0.1093\n",
      "Epoch 5/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1518\n",
      "Epoch 00005: val_loss improved from 0.10928 to 0.10573, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1517 - val_loss: 0.1057\n",
      "Epoch 6/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1495\n",
      "Epoch 00006: val_loss improved from 0.10573 to 0.10243, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1495 - val_loss: 0.1024\n",
      "Epoch 7/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1461\n",
      "Epoch 00007: val_loss improved from 0.10243 to 0.09977, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1462 - val_loss: 0.0998\n",
      "Epoch 8/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1420\n",
      "Epoch 00008: val_loss improved from 0.09977 to 0.09827, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1420 - val_loss: 0.0983\n",
      "Epoch 9/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1361\n",
      "Epoch 00009: val_loss improved from 0.09827 to 0.09731, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1363 - val_loss: 0.0973\n",
      "Epoch 10/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1360\n",
      "Epoch 00010: val_loss did not improve from 0.09731\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1357 - val_loss: 0.0985\n",
      "Epoch 11/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1364\n",
      "Epoch 00011: val_loss improved from 0.09731 to 0.09639, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1364 - val_loss: 0.0964\n",
      "Epoch 12/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1335\n",
      "Epoch 00012: val_loss did not improve from 0.09639\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1334 - val_loss: 0.0990\n",
      "Epoch 13/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1312\n",
      "Epoch 00013: val_loss improved from 0.09639 to 0.09497, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1312 - val_loss: 0.0950\n",
      "Epoch 14/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1303\n",
      "Epoch 00014: val_loss improved from 0.09497 to 0.09454, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1303 - val_loss: 0.0945\n",
      "Epoch 15/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1290\n",
      "Epoch 00015: val_loss did not improve from 0.09454\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1290 - val_loss: 0.0960\n",
      "Epoch 16/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1284\n",
      "Epoch 00016: val_loss did not improve from 0.09454\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1285 - val_loss: 0.0947\n",
      " ###5 fold : val acc1 0.592, acc3 0.982, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/140 [==========================>...] - ETA: 0s - loss: 6.0706\n",
      "Epoch 00001: val_loss improved from inf to 0.46861, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 5.7055 - val_loss: 0.4686\n",
      "Epoch 2/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2489\n",
      "Epoch 00002: val_loss improved from 0.46861 to 0.12295, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2489 - val_loss: 0.1230\n",
      "Epoch 3/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1695\n",
      "Epoch 00003: val_loss improved from 0.12295 to 0.11501, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1695 - val_loss: 0.1150\n",
      "Epoch 4/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1606\n",
      "Epoch 00004: val_loss improved from 0.11501 to 0.10919, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1607 - val_loss: 0.1092\n",
      "Epoch 5/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.1522\n",
      "Epoch 00005: val_loss improved from 0.10919 to 0.10547, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1520 - val_loss: 0.1055\n",
      "Epoch 6/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1514\n",
      "Epoch 00006: val_loss improved from 0.10547 to 0.10180, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1514 - val_loss: 0.1018\n",
      "Epoch 7/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1466\n",
      "Epoch 00007: val_loss improved from 0.10180 to 0.09952, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1465 - val_loss: 0.0995\n",
      "Epoch 8/100\n",
      "123/140 [=========================>....] - ETA: 0s - loss: 0.1424\n",
      "Epoch 00008: val_loss improved from 0.09952 to 0.09800, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1424 - val_loss: 0.0980\n",
      "Epoch 9/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1359\n",
      "Epoch 00009: val_loss improved from 0.09800 to 0.09744, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1360 - val_loss: 0.0974\n",
      "Epoch 10/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1369\n",
      "Epoch 00010: val_loss did not improve from 0.09744\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1366 - val_loss: 0.0976\n",
      "Epoch 11/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1358\n",
      "Epoch 00011: val_loss improved from 0.09744 to 0.09598, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1363 - val_loss: 0.0960\n",
      "Epoch 12/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1333\n",
      "Epoch 00012: val_loss did not improve from 0.09598\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1333 - val_loss: 0.0975\n",
      "Epoch 13/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1320\n",
      "Epoch 00013: val_loss improved from 0.09598 to 0.09513, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.0951\n",
      "Epoch 14/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1310\n",
      "Epoch 00014: val_loss did not improve from 0.09513\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1309 - val_loss: 0.0957\n",
      "Epoch 15/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1302\n",
      "Epoch 00015: val_loss did not improve from 0.09513\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1306 - val_loss: 0.0957\n",
      " ###6 fold : val acc1 0.604, acc3 0.983, mae 0.206###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/140 [============================>.] - ETA: 0s - loss: 5.7423\n",
      "Epoch 00001: val_loss improved from inf to 0.47736, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 5.6941 - val_loss: 0.4774\n",
      "Epoch 2/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.2513\n",
      "Epoch 00002: val_loss improved from 0.47736 to 0.12388, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2474 - val_loss: 0.1239\n",
      "Epoch 3/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1689\n",
      "Epoch 00003: val_loss improved from 0.12388 to 0.11489, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1684 - val_loss: 0.1149\n",
      "Epoch 4/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.1591\n",
      "Epoch 00004: val_loss improved from 0.11489 to 0.10982, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1589 - val_loss: 0.1098\n",
      "Epoch 5/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1505\n",
      "Epoch 00005: val_loss improved from 0.10982 to 0.10529, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1502 - val_loss: 0.1053\n",
      "Epoch 6/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1497\n",
      "Epoch 00006: val_loss improved from 0.10529 to 0.10196, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1496 - val_loss: 0.1020\n",
      "Epoch 7/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1441\n",
      "Epoch 00007: val_loss improved from 0.10196 to 0.09959, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1438 - val_loss: 0.0996\n",
      "Epoch 8/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1409\n",
      "Epoch 00008: val_loss improved from 0.09959 to 0.09828, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1410 - val_loss: 0.0983\n",
      "Epoch 9/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1352\n",
      "Epoch 00009: val_loss improved from 0.09828 to 0.09743, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1350 - val_loss: 0.0974\n",
      "Epoch 10/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.1352\n",
      "Epoch 00010: val_loss improved from 0.09743 to 0.09644, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1347 - val_loss: 0.0964\n",
      "Epoch 11/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1330\n",
      "Epoch 00011: val_loss did not improve from 0.09644\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1333 - val_loss: 0.0971\n",
      "Epoch 12/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.1327\n",
      "Epoch 00012: val_loss did not improve from 0.09644\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1324 - val_loss: 0.0970\n",
      " ###7 fold : val acc1 0.586, acc3 0.972, mae 0.221###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/140 [===========================>..] - ETA: 0s - loss: 5.8755\n",
      "Epoch 00001: val_loss improved from inf to 0.48928, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 5.7125 - val_loss: 0.4893\n",
      "Epoch 2/100\n",
      "131/140 [===========================>..] - ETA: 0s - loss: 0.2536\n",
      "Epoch 00002: val_loss improved from 0.48928 to 0.12833, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2475 - val_loss: 0.1283\n",
      "Epoch 3/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1683\n",
      "Epoch 00003: val_loss improved from 0.12833 to 0.11885, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1680 - val_loss: 0.1189\n",
      "Epoch 4/100\n",
      "138/140 [============================>.] - ETA: 0s - loss: 0.1587\n",
      "Epoch 00004: val_loss improved from 0.11885 to 0.11349, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1590 - val_loss: 0.1135\n",
      "Epoch 5/100\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1498\n",
      "Epoch 00005: val_loss improved from 0.11349 to 0.10875, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1498 - val_loss: 0.1088\n",
      "Epoch 6/100\n",
      "135/140 [===========================>..] - ETA: 0s - loss: 0.1488\n",
      "Epoch 00006: val_loss improved from 0.10875 to 0.10547, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1491 - val_loss: 0.1055\n",
      "Epoch 7/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1435\n",
      "Epoch 00007: val_loss improved from 0.10547 to 0.10305, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1436 - val_loss: 0.1030\n",
      "Epoch 8/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1408\n",
      "Epoch 00008: val_loss improved from 0.10305 to 0.10163, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1409 - val_loss: 0.1016\n",
      "Epoch 9/100\n",
      "139/140 [============================>.] - ETA: 0s - loss: 0.1353\n",
      "Epoch 00009: val_loss improved from 0.10163 to 0.10112, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1353 - val_loss: 0.1011\n",
      "Epoch 10/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1348\n",
      "Epoch 00010: val_loss improved from 0.10112 to 0.09961, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1347 - val_loss: 0.0996\n",
      "Epoch 11/100\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00011: val_loss did not improve from 0.09961\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1329 - val_loss: 0.1003\n",
      "Epoch 12/100\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00012: val_loss did not improve from 0.09961\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1000\n",
      " ###8 fold : val acc1 0.577, acc3 0.981, mae 0.221###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/140 [============================>.] - ETA: 0s - loss: 5.7982\n",
      "Epoch 00001: val_loss improved from inf to 0.49441, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 5.7125 - val_loss: 0.4944\n",
      "Epoch 2/100\n",
      "128/140 [==========================>...] - ETA: 0s - loss: 0.2549\n",
      "Epoch 00002: val_loss improved from 0.49441 to 0.13122, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2475 - val_loss: 0.1312\n",
      "Epoch 3/100\n",
      "132/140 [===========================>..] - ETA: 0s - loss: 0.1683\n",
      "Epoch 00003: val_loss improved from 0.13122 to 0.12186, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1680 - val_loss: 0.1219\n",
      "Epoch 4/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1590\n",
      "Epoch 00004: val_loss improved from 0.12186 to 0.11680, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1590 - val_loss: 0.1168\n",
      "Epoch 5/100\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 0.1501\n",
      "Epoch 00005: val_loss improved from 0.11680 to 0.11119, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1498 - val_loss: 0.1112\n",
      "Epoch 6/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1495\n",
      "Epoch 00006: val_loss improved from 0.11119 to 0.10813, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1491 - val_loss: 0.1081\n",
      "Epoch 7/100\n",
      "130/140 [==========================>...] - ETA: 0s - loss: 0.1443\n",
      "Epoch 00007: val_loss improved from 0.10813 to 0.10575, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.1436 - val_loss: 0.1057\n",
      "Epoch 8/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1408\n",
      "Epoch 00008: val_loss improved from 0.10575 to 0.10455, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1409 - val_loss: 0.1046\n",
      "Epoch 9/100\n",
      "127/140 [==========================>...] - ETA: 0s - loss: 0.1357\n",
      "Epoch 00009: val_loss improved from 0.10455 to 0.10360, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1353 - val_loss: 0.1036\n",
      "Epoch 10/100\n",
      "125/140 [=========================>....] - ETA: 0s - loss: 0.1349\n",
      "Epoch 00010: val_loss improved from 0.10360 to 0.10193, saving model to result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1347 - val_loss: 0.1019\n",
      "Epoch 11/100\n",
      "124/140 [=========================>....] - ETA: 0s - loss: 0.1323\n",
      "Epoch 00011: val_loss did not improve from 0.10193\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1329 - val_loss: 0.1025\n",
      "Epoch 12/100\n",
      "126/140 [==========================>...] - ETA: 0s - loss: 0.1330\n",
      "Epoch 00012: val_loss did not improve from 0.10193\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1323 - val_loss: 0.1025\n",
      " ###9 fold : val acc1 0.596, acc3 0.983, mae 0.210###\n",
      "acc10.596_acc30.980\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 39] Directory not empty: 'result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001' -> 'result/outliers+w+h/size/DNN_size_4inputs/acc1-0.596_acc3-0.980_batch128,dnodes512_dropout0.2,lr0.001'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_477451/2999729197.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0modir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/model.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0modir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrootdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mf'/acc1-{np.mean(acc1s):.3f}_acc3-{np.mean(acc3s):.3f}_{odir_f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 39] Directory not empty: 'result/outliers+w+h/size/DNN_size_4inputs/batch128,dnodes512_dropout0.2,lr0.001' -> 'result/outliers+w+h/size/DNN_size_4inputs/acc1-0.596_acc3-0.980_batch128,dnodes512_dropout0.2,lr0.001'"
     ]
    }
   ],
   "source": [
    "from keras import metrics\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "# random search for hyperparameter\n",
    "ntrial = ntest\n",
    "train_errs, val_errs = [] ,[]\n",
    "test_acc, test_roc, test_prc = [], [], []\n",
    "#test_rmse, test_mae, test_auc = [], [], []\n",
    "random_settings = []\n",
    "\n",
    "\n",
    "for itrial in range(ntrial):\n",
    "    # grid search\n",
    "    # test_setting = test_settings[itrial]\n",
    "\n",
    "    # random search\n",
    "    print('random search {}/{}'.format(itrial, ntrial))\n",
    "    \n",
    "    # total conv layers of the model\n",
    "    nlayer = random.choice([1,2]) \n",
    "    # test settings\n",
    "    dnodes[0], dropouts[0], dnodes[1], dropouts[1], batch_size, learning_rate = random.choice(test_settings)\n",
    "    \n",
    "\n",
    "    # 이번 옵션에 대한 결과 디렉토리\n",
    "    odir_f = f'batch{batch_size},'\n",
    "    for i in range(nlayer):\n",
    "        odir_f += f'dnodes{dnodes[i]}_dropout{dropouts[i]},'\n",
    "    odir_f += f'lr{learning_rate}'\n",
    "    random_settings.append(odir_f)\n",
    "    \n",
    "    odir = rootdir + '/' + odir_f\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "\n",
    "    # build a model\n",
    "    inp = Input(shape=(x_train.shape[1],))\n",
    "    out = inp\n",
    "\n",
    "    \n",
    "    for i in range(nlayer):      \n",
    "        out = Dense(dnodes[i], activation='relu')(out)\n",
    "        out = Dropout(dropouts[i])(out)\n",
    "    \n",
    "    out = Dense(1)(out)\n",
    "\n",
    "\n",
    "    model = Model(inputs=[inp], outputs=[out])\n",
    "    model.save_weights(f'{odir}/initial_weights.hdf5')\n",
    "        \n",
    "\n",
    "    # 4-fold cv\n",
    "    kfold = KFold(nfold)\n",
    "    acc1s, acc3s, maes = [], [], []\n",
    "\n",
    "    switch = 0\n",
    "    for fold, (train_mask, test_mask) in enumerate(kfold.split(y_train)):\n",
    "        X_train = x_train_imputed[train_mask]\n",
    "        X_test = x_train_imputed[test_mask] \n",
    "        \n",
    "        Y_train = y_train[train_mask] \n",
    "        Y_test = y_train[test_mask]\n",
    "\n",
    "\n",
    "        # model 학습\n",
    "        try:\n",
    "            weightcache = f\"{odir}/weights_{fold}.hdf5\"\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=learning_rate), metrics=[])\n",
    "            hist = model.fit(X_train, Y_train, validation_split=0.2, epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=2, verbose=0, mode='auto')])\n",
    "\n",
    "            model.load_weights(weightcache)\n",
    "            y_pred = model.predict(X_test).flatten() \n",
    "            y_pred = np.round(y_pred * 2) / 2\n",
    "            \n",
    "            acc1 = np.mean(y_pred==Y_test)\n",
    "            acc3 = np.mean((y_pred >= Y_test-0.5) & (y_pred <= Y_test+0.5))\n",
    "            mae = mean_absolute_error(Y_test, y_pred)           \n",
    "            \n",
    "            acc1s.append(acc1)\n",
    "            acc3s.append(acc3)\n",
    "            maes.append(mae)\n",
    "\n",
    "            print(f' ###{fold} fold : val acc1 {acc1:.3f}, acc3 {acc3:.3f}, mae {mae:.3f}###')\n",
    "            tf.keras.backend.clear_session()\n",
    "            model.load_weights(f'{odir}/initial_weights.hdf5')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            switch = 1\n",
    "            shutil.rmtree(odir)\n",
    "            itrial -= 1\n",
    "            break\n",
    "\n",
    "    if switch:\n",
    "        switch = 0\n",
    "        continue\n",
    "    \n",
    "\n",
    "    print(f'acc1{np.mean(acc1s):.3f}_acc3{np.mean(acc3s):.3f}')\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    os.rename(odir, rootdir+f'/acc1-{np.mean(acc1s):.3f}_acc3-{np.mean(acc3s):.3f}_{odir_f}')\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93ad53-f26e-413c-b1dd-d8dbb7015d9d",
   "metadata": {},
   "source": [
    "## Retrain with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6fd17ade-68e6-4aec-894a-0c5db6829083",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T14:09:39.226494Z",
     "iopub.status.busy": "2023-03-06T14:09:39.226009Z",
     "iopub.status.idle": "2023-03-06T14:12:06.403388Z",
     "shell.execute_reply": "2023-03-06T14:12:06.402686Z",
     "shell.execute_reply.started": "2023-03-06T14:09:39.226444Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 2.3320\n",
      "Epoch 00001: val_loss improved from inf to 0.11197, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 2.3074 - val_loss: 0.1120\n",
      "Epoch 2/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00002: val_loss improved from 0.11197 to 0.10176, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1083 - val_loss: 0.1018\n",
      "Epoch 3/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.1029\n",
      "Epoch 00003: val_loss improved from 0.10176 to 0.10058, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1028 - val_loss: 0.1006\n",
      "Epoch 4/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.1001\n",
      "Epoch 00004: val_loss improved from 0.10058 to 0.09718, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0999 - val_loss: 0.0972\n",
      "Epoch 5/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 0.0988\n",
      "Epoch 00005: val_loss did not improve from 0.09718\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0989 - val_loss: 0.0993\n",
      "Epoch 6/100\n",
      "540/558 [============================>.] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00006: val_loss improved from 0.09718 to 0.09422, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_0.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0986 - val_loss: 0.0942\n",
      "Epoch 7/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00007: val_loss did not improve from 0.09422\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0984 - val_loss: 0.0944\n",
      "Epoch 8/100\n",
      "540/558 [============================>.] - ETA: 0s - loss: 0.0981\n",
      "Epoch 00008: val_loss did not improve from 0.09422\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0977 - val_loss: 0.0971\n",
      " ###0 fold : val acc1 0.609, acc3 0.979, mae 0.206###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543/558 [============================>.] - ETA: 0s - loss: 2.3722\n",
      "Epoch 00001: val_loss improved from inf to 0.11263, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 2.3147 - val_loss: 0.1126\n",
      "Epoch 2/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.1075\n",
      "Epoch 00002: val_loss improved from 0.11263 to 0.10190, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1079 - val_loss: 0.1019\n",
      "Epoch 3/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.1024\n",
      "Epoch 00003: val_loss did not improve from 0.10190\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1027 - val_loss: 0.1022\n",
      "Epoch 4/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.1000\n",
      "Epoch 00004: val_loss improved from 0.10190 to 0.09603, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0999 - val_loss: 0.0960\n",
      "Epoch 5/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00005: val_loss did not improve from 0.09603\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0989 - val_loss: 0.1038\n",
      "Epoch 6/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00006: val_loss improved from 0.09603 to 0.09511, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_1.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0983 - val_loss: 0.0951\n",
      "Epoch 7/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00007: val_loss did not improve from 0.09511\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0982 - val_loss: 0.0955\n",
      "Epoch 8/100\n",
      "543/558 [============================>.] - ETA: 0s - loss: 0.0981\n",
      "Epoch 00008: val_loss did not improve from 0.09511\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0978 - val_loss: 0.0959\n",
      " ###1 fold : val acc1 0.606, acc3 0.982, mae 0.206###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557/558 [============================>.] - ETA: 0s - loss: 2.3109\n",
      "Epoch 00001: val_loss improved from inf to 0.11217, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 2.3103 - val_loss: 0.1122\n",
      "Epoch 2/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00002: val_loss improved from 0.11217 to 0.10167, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1080 - val_loss: 0.1017\n",
      "Epoch 3/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.1025\n",
      "Epoch 00003: val_loss improved from 0.10167 to 0.10133, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1029 - val_loss: 0.1013\n",
      "Epoch 4/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 0.1003\n",
      "Epoch 00004: val_loss improved from 0.10133 to 0.09661, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1000 - val_loss: 0.0966\n",
      "Epoch 5/100\n",
      "541/558 [============================>.] - ETA: 0s - loss: 0.0994\n",
      "Epoch 00005: val_loss did not improve from 0.09661\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0996 - val_loss: 0.1027\n",
      "Epoch 6/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 00006: val_loss improved from 0.09661 to 0.09557, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_2.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0986 - val_loss: 0.0956\n",
      "Epoch 7/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.0986\n",
      "Epoch 00007: val_loss did not improve from 0.09557\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0985 - val_loss: 0.0959\n",
      "Epoch 8/100\n",
      "554/558 [============================>.] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00008: val_loss did not improve from 0.09557\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0982 - val_loss: 0.1002\n",
      " ###2 fold : val acc1 0.598, acc3 0.977, mae 0.213###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541/558 [============================>.] - ETA: 0s - loss: 2.3781\n",
      "Epoch 00001: val_loss improved from inf to 0.11311, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 2.3125 - val_loss: 0.1131\n",
      "Epoch 2/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00002: val_loss improved from 0.11311 to 0.10195, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1079 - val_loss: 0.1020\n",
      "Epoch 3/100\n",
      "546/558 [============================>.] - ETA: 0s - loss: 0.1021\n",
      "Epoch 00003: val_loss did not improve from 0.10195\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1026 - val_loss: 0.1031\n",
      "Epoch 4/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.1003\n",
      "Epoch 00004: val_loss improved from 0.10195 to 0.09772, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1001 - val_loss: 0.0977\n",
      "Epoch 5/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00005: val_loss did not improve from 0.09772\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0991 - val_loss: 0.0989\n",
      "Epoch 6/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 00006: val_loss improved from 0.09772 to 0.09558, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_3.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0983 - val_loss: 0.0956\n",
      "Epoch 7/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00007: val_loss did not improve from 0.09558\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0985 - val_loss: 0.0966\n",
      "Epoch 8/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00008: val_loss did not improve from 0.09558\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0983 - val_loss: 0.0989\n",
      " ###3 fold : val acc1 0.629, acc3 0.976, mae 0.198###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549/558 [============================>.] - ETA: 0s - loss: 2.3302\n",
      "Epoch 00001: val_loss improved from inf to 0.11064, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 2.2978 - val_loss: 0.1106\n",
      "Epoch 2/100\n",
      "555/558 [============================>.] - ETA: 0s - loss: 0.1071\n",
      "Epoch 00002: val_loss improved from 0.11064 to 0.10165, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1071 - val_loss: 0.1017\n",
      "Epoch 3/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.1015\n",
      "Epoch 00003: val_loss improved from 0.10165 to 0.09773, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1016 - val_loss: 0.0977\n",
      "Epoch 4/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00004: val_loss improved from 0.09773 to 0.09730, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0995 - val_loss: 0.0973\n",
      "Epoch 5/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.0984\n",
      "Epoch 00005: val_loss did not improve from 0.09730\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0985 - val_loss: 0.0998\n",
      "Epoch 6/100\n",
      "549/558 [============================>.] - ETA: 0s - loss: 0.0978\n",
      "Epoch 00006: val_loss improved from 0.09730 to 0.09491, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0977 - val_loss: 0.0949\n",
      "Epoch 7/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.0980\n",
      "Epoch 00007: val_loss improved from 0.09491 to 0.09415, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_4.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0980 - val_loss: 0.0942\n",
      "Epoch 8/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00008: val_loss did not improve from 0.09415\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0977 - val_loss: 0.0954\n",
      "Epoch 9/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.0975\n",
      "Epoch 00009: val_loss did not improve from 0.09415\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0974 - val_loss: 0.0945\n",
      " ###4 fold : val acc1 0.588, acc3 0.981, mae 0.215###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556/558 [============================>.] - ETA: 0s - loss: 2.2790\n",
      "Epoch 00001: val_loss improved from inf to 0.11059, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 2.2744 - val_loss: 0.1106\n",
      "Epoch 2/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.1070\n",
      "Epoch 00002: val_loss improved from 0.11059 to 0.10248, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1068 - val_loss: 0.1025\n",
      "Epoch 3/100\n",
      "545/558 [============================>.] - ETA: 0s - loss: 0.1018\n",
      "Epoch 00003: val_loss improved from 0.10248 to 0.09821, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1018 - val_loss: 0.0982\n",
      "Epoch 4/100\n",
      "550/558 [============================>.] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00004: val_loss improved from 0.09821 to 0.09584, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0994 - val_loss: 0.0958\n",
      "Epoch 5/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.0983\n",
      "Epoch 00005: val_loss did not improve from 0.09584\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0984 - val_loss: 0.1005\n",
      "Epoch 6/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00006: val_loss improved from 0.09584 to 0.09502, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0982 - val_loss: 0.0950\n",
      "Epoch 7/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.0980\n",
      "Epoch 00007: val_loss improved from 0.09502 to 0.09490, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_5.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0980 - val_loss: 0.0949\n",
      "Epoch 8/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.0980\n",
      "Epoch 00008: val_loss did not improve from 0.09490\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0980 - val_loss: 0.0953\n",
      "Epoch 9/100\n",
      "542/558 [============================>.] - ETA: 0s - loss: 0.0978\n",
      "Epoch 00009: val_loss did not improve from 0.09490\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0978 - val_loss: 0.0977\n",
      " ###5 fold : val acc1 0.587, acc3 0.984, mae 0.215###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539/558 [===========================>..] - ETA: 0s - loss: 2.3570\n",
      "Epoch 00001: val_loss improved from inf to 0.10981, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 2.2836 - val_loss: 0.1098\n",
      "Epoch 2/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00002: val_loss improved from 0.10981 to 0.10298, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1079 - val_loss: 0.1030\n",
      "Epoch 3/100\n",
      "543/558 [============================>.] - ETA: 0s - loss: 0.1029\n",
      "Epoch 00003: val_loss improved from 0.10298 to 0.09890, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1026 - val_loss: 0.0989\n",
      "Epoch 4/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.0998\n",
      "Epoch 00004: val_loss improved from 0.09890 to 0.09713, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1000 - val_loss: 0.0971\n",
      "Epoch 5/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.0989\n",
      "Epoch 00005: val_loss did not improve from 0.09713\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0990 - val_loss: 0.1046\n",
      "Epoch 6/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.0985\n",
      "Epoch 00006: val_loss improved from 0.09713 to 0.09679, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0985 - val_loss: 0.0968\n",
      "Epoch 7/100\n",
      "540/558 [============================>.] - ETA: 0s - loss: 0.0985\n",
      "Epoch 00007: val_loss improved from 0.09679 to 0.09527, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_6.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0985 - val_loss: 0.0953\n",
      "Epoch 8/100\n",
      "543/558 [============================>.] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00008: val_loss did not improve from 0.09527\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0982 - val_loss: 0.0954\n",
      "Epoch 9/100\n",
      "540/558 [============================>.] - ETA: 0s - loss: 0.0979\n",
      "Epoch 00009: val_loss did not improve from 0.09527\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0981 - val_loss: 0.0960\n",
      " ###6 fold : val acc1 0.596, acc3 0.983, mae 0.211###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553/558 [============================>.] - ETA: 0s - loss: 2.2845\n",
      "Epoch 00001: val_loss improved from inf to 0.11004, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 2.2683 - val_loss: 0.1100\n",
      "Epoch 2/100\n",
      "553/558 [============================>.] - ETA: 0s - loss: 0.1067\n",
      "Epoch 00002: val_loss improved from 0.11004 to 0.10280, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1066 - val_loss: 0.1028\n",
      "Epoch 3/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.1015\n",
      "Epoch 00003: val_loss improved from 0.10280 to 0.09933, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1014 - val_loss: 0.0993\n",
      "Epoch 4/100\n",
      "539/558 [===========================>..] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00004: val_loss improved from 0.09933 to 0.09636, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_7.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0988 - val_loss: 0.0964\n",
      "Epoch 5/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00005: val_loss did not improve from 0.09636\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0976 - val_loss: 0.1039\n",
      "Epoch 6/100\n",
      "556/558 [============================>.] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00006: val_loss did not improve from 0.09636\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0974 - val_loss: 0.0979\n",
      " ###7 fold : val acc1 0.592, acc3 0.973, mae 0.218###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556/558 [============================>.] - ETA: 0s - loss: 2.2873\n",
      "Epoch 00001: val_loss improved from inf to 0.11473, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 2.2828 - val_loss: 0.1147\n",
      "Epoch 2/100\n",
      "547/558 [============================>.] - ETA: 0s - loss: 0.1070\n",
      "Epoch 00002: val_loss improved from 0.11473 to 0.10696, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1067 - val_loss: 0.1070\n",
      "Epoch 3/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.1014\n",
      "Epoch 00003: val_loss improved from 0.10696 to 0.10444, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1014 - val_loss: 0.1044\n",
      "Epoch 4/100\n",
      "541/558 [============================>.] - ETA: 0s - loss: 0.0986\n",
      "Epoch 00004: val_loss improved from 0.10444 to 0.09949, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_8.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0988 - val_loss: 0.0995\n",
      "Epoch 5/100\n",
      "551/558 [============================>.] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00005: val_loss did not improve from 0.09949\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0977 - val_loss: 0.1059\n",
      "Epoch 6/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00006: val_loss did not improve from 0.09949\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0974 - val_loss: 0.1000\n",
      " ###8 fold : val acc1 0.585, acc3 0.982, mae 0.216###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543/558 [============================>.] - ETA: 0s - loss: 2.3397\n",
      "Epoch 00001: val_loss improved from inf to 0.11678, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 2.2828 - val_loss: 0.1168\n",
      "Epoch 2/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.1067\n",
      "Epoch 00002: val_loss improved from 0.11678 to 0.10955, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1067 - val_loss: 0.1096\n",
      "Epoch 3/100\n",
      "539/558 [===========================>..] - ETA: 0s - loss: 0.1015\n",
      "Epoch 00003: val_loss improved from 0.10955 to 0.10500, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.1014 - val_loss: 0.1050\n",
      "Epoch 4/100\n",
      "541/558 [============================>.] - ETA: 0s - loss: 0.0986\n",
      "Epoch 00004: val_loss improved from 0.10500 to 0.10173, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0988 - val_loss: 0.1017\n",
      "Epoch 5/100\n",
      "541/558 [============================>.] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00005: val_loss did not improve from 0.10173\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0977 - val_loss: 0.1059\n",
      "Epoch 6/100\n",
      "540/558 [============================>.] - ETA: 0s - loss: 0.0971\n",
      "Epoch 00006: val_loss improved from 0.10173 to 0.10163, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0974 - val_loss: 0.1016\n",
      "Epoch 7/100\n",
      "544/558 [============================>.] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00007: val_loss did not improve from 0.10163\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0972 - val_loss: 0.1017\n",
      "Epoch 8/100\n",
      "548/558 [============================>.] - ETA: 0s - loss: 0.0980\n",
      "Epoch 00008: val_loss improved from 0.10163 to 0.09845, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 1s 3ms/step - loss: 0.0978 - val_loss: 0.0985\n",
      "Epoch 9/100\n",
      "552/558 [============================>.] - ETA: 0s - loss: 0.0966\n",
      "Epoch 00009: val_loss did not improve from 0.09845\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0968 - val_loss: 0.0993\n",
      "Epoch 10/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00010: val_loss improved from 0.09845 to 0.09808, saving model to result/outliers+w+h/size/batch32,dnodes64_dropout0,lr0.002/weights_9.hdf5\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0970 - val_loss: 0.0981\n",
      "Epoch 11/100\n",
      "557/558 [============================>.] - ETA: 0s - loss: 0.0971\n",
      "Epoch 00011: val_loss did not improve from 0.09808\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0971 - val_loss: 0.1039\n",
      "Epoch 12/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00012: val_loss did not improve from 0.09808\n",
      "558/558 [==============================] - 2s 3ms/step - loss: 0.0970 - val_loss: 0.0990\n",
      " ###9 fold : val acc1 0.587, acc3 0.984, mae 0.215###\n",
      "acc10.598_acc30.980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1571"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rootdir = f\"result/outliers+w+h/size\"\n",
    "nfold = 10\n",
    "dnodes, dropouts = [], []\n",
    "for i in range(2):\n",
    "    dnodes.append(0)\n",
    "    dropouts.append(0)\n",
    "\n",
    "# total conv layers of the model\n",
    "nlayer = 1\n",
    "# test settings\n",
    "dnodes[0], dropouts[0], dnodes[1], dropouts[1], batch_size, learning_rate = 64, 0, 0, 0, 32, 0.002\n",
    "\n",
    "\n",
    "# 이번 옵션에 대한 결과 디렉토리\n",
    "odir_f = f'batch{batch_size},'\n",
    "for i in range(nlayer):\n",
    "    odir_f += f'dnodes{dnodes[i]}_dropout{dropouts[i]},'\n",
    "odir_f += f'lr{learning_rate}'\n",
    "\n",
    "\n",
    "odir = rootdir + '/' + odir_f\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "\n",
    "\n",
    "# build a model\n",
    "inp = Input(shape=(x_train.shape[1],))\n",
    "out = inp\n",
    "\n",
    "\n",
    "for i in range(nlayer):      \n",
    "    out = Dense(dnodes[i], activation='relu')(out)\n",
    "    out = Dropout(dropouts[i])(out)\n",
    "\n",
    "out = Dense(1)(out)\n",
    "\n",
    "\n",
    "model = Model(inputs=[inp], outputs=[out])\n",
    "model.save_weights(f'{odir}/initial_weights.hdf5')\n",
    "\n",
    "\n",
    "# 4-fold cv\n",
    "kfold = KFold(nfold)\n",
    "acc1s, acc3s, maes, mses = [], [], [], []\n",
    "\n",
    "switch = 0\n",
    "for fold, (train_mask, test_mask) in enumerate(kfold.split(y_train)):\n",
    "    X_train = x_train_imputed[train_mask]\n",
    "    X_test = x_train_imputed[test_mask] \n",
    "\n",
    "    Y_train = y_train[train_mask]\n",
    "    Y_test = y_train[test_mask]\n",
    "\n",
    "\n",
    "    # model 학습\n",
    "    try:\n",
    "        weightcache = f\"{odir}/weights_{fold}.hdf5\"\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=learning_rate), metrics=[])\n",
    "        hist = model.fit(X_train, Y_train, validation_split=0.2, epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                            EarlyStopping(monitor='val_loss', patience=2, verbose=0, mode='auto')])\n",
    "\n",
    "        model.load_weights(weightcache)\n",
    "        y_pred = model.predict(X_test).flatten() \n",
    "        y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "        acc1 = np.mean(y_pred==Y_test)\n",
    "        acc3 = np.mean((y_pred >= Y_test-0.5) & (y_pred <= Y_test+0.5))\n",
    "        mae = mean_absolute_error(Y_test, y_pred)           \n",
    "\n",
    "        acc1s.append(acc1)\n",
    "        acc3s.append(acc3)\n",
    "        maes.append(mae)\n",
    "\n",
    "        print(f' ###{fold} fold : val acc1 {acc1:.3f}, acc3 {acc3:.3f}, mae {mae:.3f}###')\n",
    "        tf.keras.backend.clear_session()\n",
    "        model.load_weights(f'{odir}/initial_weights.hdf5')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "min_idx = np.array(acc1s).argmin()\n",
    "\n",
    "print(f'acc1{np.mean(acc1s):.3f}_acc3{np.mean(acc3s):.3f}')\n",
    "open(odir+\"/model.json\", \"wt\").write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1a462186-176f-46f5-8d7f-18af02d31372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T14:49:25.635161Z",
     "iopub.status.busy": "2023-03-06T14:49:25.634590Z",
     "iopub.status.idle": "2023-03-06T14:49:25.641637Z",
     "shell.execute_reply": "2023-03-06T14:49:25.640481Z",
     "shell.execute_reply.started": "2023-03-06T14:49:25.635107Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc10.603_acc30.979\n"
     ]
    }
   ],
   "source": [
    "min_idx = np.array(acc1s).argmin()\n",
    "model.load_weights(f'{odir}/weights_{min_idx}.hdf5')\n",
    "\n",
    "y_pred = model.predict(x_test_imputed).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "acc1 = np.mean(y_pred==y_test)\n",
    "acc3 = np.mean((y_pred >= y_test-0.5) & (y_pred <= y_test+0.5))\n",
    "\n",
    "# performance\n",
    "print(f'acc1{acc1:.3f}_acc3{acc3:.3f}')\n",
    "\n",
    "\n",
    "os.rename(odir, rootdir+f'/DNN-best_acc1-{acc1:.3f}_acc3-{acc3:.3f}_{odir_f}_idx{min_idx}')\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5a2a9-94ff-452a-8d63-9513685c3012",
   "metadata": {},
   "source": [
    "### test validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7476b8d9-df17-4ce9-878c-d2b9b4007014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:40:19.025963Z",
     "iopub.status.busy": "2023-04-12T14:40:19.025564Z",
     "iopub.status.idle": "2023-04-12T14:40:47.073657Z",
     "shell.execute_reply": "2023-04-12T14:40:47.072925Z",
     "shell.execute_reply.started": "2023-04-12T14:40:19.025921Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.603+-0.012 -> 0.591 ~ 0.615\n",
      "acc(+-0.5mm): 0.979+-0.004 -> 0.976 ~ 0.983 \n"
     ]
    }
   ],
   "source": [
    "# dnn model\n",
    "# DNN model\n",
    "load_path = 'result/outliers+w+h/size/DNN-best_acc1-0.603_acc3-0.979_batch32,dnodes64_dropout0,lr0.002_idx8'\n",
    "with open(load_path+\"/model.json\", 'r') as f_json:\n",
    "    loaded_model_json = f_json.read()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights(load_path+'/weights_8.hdf5')\n",
    "\n",
    "y1_dnns, y2_dnns = [], []\n",
    "for i in range(200):\n",
    "    mask = np.array([random.randint(0,1)==1 for j in range(len(x_test_imputed))])\n",
    "    X_test = x_test_imputed[mask]\n",
    "    Y_test = y_test[mask]\n",
    "    \n",
    "    y_dnn = model.predict(X_test).flatten()\n",
    "    y_dnn = np.round(y_dnn * 2) / 2\n",
    "    y1_dnns.append(np.mean(y_dnn == Y_test))\n",
    "    y2_dnns.append(np.mean((y_dnn >= Y_test-0.5) & (y_dnn <= Y_test+0.5)))\n",
    "    \n",
    "print(f'acc: {np.mean(y1_dnns):.3f}+-{1.96*np.std(y1_dnns):.3f} -> {np.mean(y1_dnns)-1.96*np.std(y1_dnns):.3f} ~ {np.mean(y1_dnns)+1.96*np.std(y1_dnns):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(y2_dnns):.3f}+-{1.96*np.std(y2_dnns):.3f} -> {np.mean(y2_dnns)-1.96*np.std(y2_dnns):.3f} ~ {np.mean(y2_dnns)+1.96*np.std(y2_dnns):.3f} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77db5ad-110d-4f1e-83b7-662252466a4f",
   "metadata": {},
   "source": [
    "# Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd9c1757-4d22-4c13-91f9-7eb0f7ad6b3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T02:29:46.322831Z",
     "iopub.status.busy": "2023-03-06T02:29:46.322267Z",
     "iopub.status.idle": "2023-03-06T02:29:46.332020Z",
     "shell.execute_reply": "2023-03-06T02:29:46.330939Z",
     "shell.execute_reply.started": "2023-03-06T02:29:46.322776Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (24764, 4), x_test: (6190, 4)\n"
     ]
    }
   ],
   "source": [
    "# input: age, weight, height, cuff\n",
    "x_train = np.concatenate((x_train[:,0:1], x_train[:,2:5]),axis=-1)\n",
    "x_test = np.concatenate((x_test[:,0:1], x_test[:,2:5]),axis=-1)\n",
    "\n",
    "print(f'x_train: {x_train.shape}, x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dccd58a8-6071-48d2-b639-4b8b95f8d0c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T02:33:39.735777Z",
     "iopub.status.busy": "2023-03-06T02:33:39.735150Z",
     "iopub.status.idle": "2023-03-06T02:33:39.778961Z",
     "shell.execute_reply": "2023-03-06T02:33:39.777950Z",
     "shell.execute_reply.started": "2023-03-06T02:33:39.735696Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신생아의 size 성능\n",
      "acc: 0.610\n",
      "acc(+-0.5mm): 0.982\n",
      "\n",
      "영아의 size 성능\n",
      "acc: 0.666\n",
      "acc(+-0.5mm): 0.984\n",
      "\n",
      "나머지 연령(>1살)의 size 성능\n",
      "acc: 0.608\n",
      "acc(+-0.5mm): 0.981\n",
      "\n",
      "\n",
      "cuffed ETT의 size 성능\n",
      "acc: 0.714\n",
      "acc(+-0.5mm): 0.973\n",
      "uncuffed ETT의 size 성능\n",
      "acc: 0.585\n",
      "acc(+-0.5mm): 0.985\n"
     ]
    }
   ],
   "source": [
    "# xgbr 모델 예측값\n",
    "xgbr = xgb.XGBRegressor()\n",
    "xgbr.load_model('result/outliers+w+h/size/acc1-0.622_acc3-0.981_XGBR_4inputs_10fold/model.model')\n",
    "\n",
    "# 신생아\n",
    "age_mask = x_test[:,0]<1/12\n",
    "y_pred = xgbr.predict(x_test[age_mask]).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "acc1 = np.mean(y_pred==y_test[age_mask])\n",
    "acc3 = np.mean((y_pred >= y_test[age_mask]-0.5) & (y_pred <= y_test[age_mask]+0.5))\n",
    "print(f'신생아의 size 성능')\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}\\n')\n",
    "\n",
    "\n",
    "# 영아\n",
    "age_mask = (x_test[:,0]>1/12) & (x_test[:,0]<1)\n",
    "y_pred = xgbr.predict(x_test[age_mask]).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "acc1 = np.mean(y_pred==y_test[age_mask])\n",
    "acc3 = np.mean((y_pred >= y_test[age_mask]-0.5) & (y_pred <= y_test[age_mask]+0.5))\n",
    "print(f'영아의 size 성능')\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}\\n')\n",
    "\n",
    "\n",
    "# 나머지\n",
    "age_mask = (x_test[:,0]>1)\n",
    "y_pred = xgbr.predict(x_test[age_mask]).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "acc1 = np.mean(y_pred==y_test[age_mask])\n",
    "acc3 = np.mean((y_pred >= y_test[age_mask]-0.5) & (y_pred <= y_test[age_mask]+0.5))\n",
    "print(f'나머지 연령(>1살)의 size 성능')\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}\\n\\n')\n",
    "\n",
    "\n",
    "# cuffed\n",
    "age_mask = (x_test[:,3]==1)\n",
    "y_pred = xgbr.predict(x_test[age_mask]).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "acc1 = np.mean(y_pred==y_test[age_mask])\n",
    "acc3 = np.mean((y_pred >= y_test[age_mask]-0.5) & (y_pred <= y_test[age_mask]+0.5))\n",
    "print(f'cuffed ETT의 size 성능')\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')\n",
    "\n",
    "\n",
    "# uncuffed\n",
    "age_mask = (x_test[:,3]==0)\n",
    "y_pred = xgbr.predict(x_test[age_mask]).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "acc1 = np.mean(y_pred==y_test[age_mask])\n",
    "acc3 = np.mean((y_pred >= y_test[age_mask]-0.5) & (y_pred <= y_test[age_mask]+0.5))\n",
    "print(f'uncuffed ETT의 size 성능')\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f1b613-ee02-414f-971a-1b0a16bdc88b",
   "metadata": {},
   "source": [
    "# Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49cbce38-7957-4eed-a740-b8ead9909e23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T03:31:30.121513Z",
     "iopub.status.busy": "2023-05-10T03:31:30.121018Z",
     "iopub.status.idle": "2023-05-10T03:31:30.883914Z",
     "shell.execute_reply": "2023-05-10T03:31:30.883313Z",
     "shell.execute_reply.started": "2023-05-10T03:31:30.121454Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to unpickle estimator DecisionTreeRegressor from version 1.0.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "Trying to unpickle estimator RandomForestRegressor from version 1.0.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "Trying to unpickle estimator GridSearchCV from version 1.0.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "Trying to unpickle estimator LinearRegression from version 1.0.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from scipy import stats\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# traditional formula\n",
    "y1_trad = y_test_old == y_test\n",
    "y2_trad = (y_test_old >= y_test-0.5) & (y_test_old <= y_test+0.5)\n",
    "cuff_mask = x_test[:,3]==1\n",
    "y1_cuff, y2_cuff = y1_trad[cuff_mask], y2_trad[cuff_mask]\n",
    "y1_uncuff, y2_uncuff = y1_trad[~cuff_mask], y2_trad[~cuff_mask]\n",
    "\n",
    "# xgbr model\n",
    "xgbr = xgb.XGBRegressor()\n",
    "xgbr.load_model('result/outliers+w+h/size/acc1-0.622_acc3-0.981_XGBR_4inputs_10fold/model.model')\n",
    "y_xgbr = xgbr.predict(x_test)\n",
    "y_xgbr = np.round(y_xgbr * 2) / 2\n",
    "y1_xgbr = y_xgbr == y_test\n",
    "y2_xgbr = (y_xgbr >= y_test-0.5) & (y_xgbr <= y_test+0.5)\n",
    "\n",
    "# RF model\n",
    "imp = IterativeImputer().fit(x_train)\n",
    "x_train_imputed = imp.transform(x_train)\n",
    "x_test_imputed = imp.transform(x_test)\n",
    "\n",
    "rfr = pickle.load(open(f'result/outliers+w+h/size/acc1-0.606_acc3-0.980_RF_4inputs_10fold/gridSearch','rb'))\n",
    "y_rfr = rfr.predict(x_test_imputed).flatten()\n",
    "y_rfr = np.round(y_rfr * 2) / 2\n",
    "y1_rfr = y_rfr == y_test\n",
    "y2_rfr = (y_rfr >= y_test-0.5) & (y_rfr <= y_test+0.5)\n",
    "\n",
    "# LR model\n",
    "lr = pickle.load(open(f'result/outliers+w+h/size/acc1-0.588_acc3-0.974_LR_4inputs/model','rb'))\n",
    "y_lr = lr.predict(x_test_imputed)\n",
    "y_lr = np.round(y_lr * 2) / 2\n",
    "y1_lr = y_lr == y_test\n",
    "y2_lr = (y_lr >= y_test-0.5) & (y_lr <= y_test+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "790b6021-95c6-4564-bb06-a20156346d39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T16:39:22.829240Z",
     "iopub.status.busy": "2023-03-09T16:39:22.828943Z",
     "iopub.status.idle": "2023-03-09T16:39:26.564981Z",
     "shell.execute_reply": "2023-03-09T16:39:26.564406Z",
     "shell.execute_reply.started": "2023-03-09T16:39:22.829208Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 01:39:22.984737: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-10 01:39:25.336998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30691 MB memory:  -> device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "2023-03-10 01:39:25.337756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 142 MB memory:  -> device: 1, name: Tesla V100-DGXS-32GB, pci bus id: 0000:08:00.0, compute capability: 7.0\n",
      "2023-03-10 01:39:25.338439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 190 MB memory:  -> device: 2, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0e:00.0, compute capability: 7.0\n",
      "2023-03-10 01:39:25.339076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30973 MB memory:  -> device: 3, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0f:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(pd.DataFrame(x_train))\n",
    "x_test = sc.transform(pd.DataFrame(x_test))\n",
    "\n",
    "imp = IterativeImputer().fit(x_train)\n",
    "x_train_imputed = imp.transform(x_train)\n",
    "x_test_imputed = imp.transform(x_test)\n",
    "\n",
    "\n",
    "# DNN model\n",
    "load_path = 'result/outliers+w+h/size/DNN-best_acc1-0.603_acc3-0.979_batch32,dnodes64_dropout0,lr0.002_idx8'\n",
    "with open(load_path+\"/model.json\", 'r') as f_json:\n",
    "    loaded_model_json = f_json.read()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights(load_path+'/weights_8.hdf5')\n",
    "\n",
    "y_pred = model.predict(x_test_imputed).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "y1_dnn = y_pred == y_test\n",
    "y2_dnn = (y_pred >= y_test-0.5) & (y_pred <= y_test+0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee732416-f6b9-48a2-8489-a6734cf7f696",
   "metadata": {},
   "source": [
    "## comparison of xgbr with other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed4583a8-8bc3-4f77-949e-7a4df4864351",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T03:31:38.073279Z",
     "iopub.status.busy": "2023-05-10T03:31:38.072875Z",
     "iopub.status.idle": "2023-05-10T03:31:38.102076Z",
     "shell.execute_reply": "2023-05-10T03:31:38.101338Z",
     "shell.execute_reply.started": "2023-05-10T03:31:38.073227Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Comparison of xgbr vs Penlington for size accuracy1###\n",
      "contingency table:\n",
      "   gbrt  trad\n",
      "1  2589  2215\n",
      "0  1838  2212\n",
      "Chi square: 63.653853476012785\n",
      "P-value: 1.483185739851742e-15\n",
      "\n",
      "###Comparison of xgbr vs Penlington for size accuracy3###\n",
      "contingency table:\n",
      "   gbrt  trad\n",
      "1  4360  4129\n",
      "0    67   298\n",
      "Chi square: 152.48041994716775\n",
      "P-value: 4.97539524267751e-35\n"
     ]
    }
   ],
   "source": [
    "# XGBR vs Penlington (uncuffed; age/4+4.5)\n",
    "#y_test_old1 = y_test_old + 0.5\n",
    "y1_trad = y_test_old1 == y_test\n",
    "y2_trad = (y_test_old1 >= y_test-0.5) & (y_test_old1 <= y_test+0.5)\n",
    "cuff_mask = x_test[:,3]==0\n",
    "\n",
    "obs1 = pd.DataFrame({'gbrt': [np.sum(y1_xgbr[cuff_mask]),np.sum(~y1_xgbr[cuff_mask])], 'trad': [np.sum(y1_trad[cuff_mask]),np.sum(~y1_trad[cuff_mask])]})\n",
    "obs1.index = ['1', '0']\n",
    "\n",
    "chiresult = chi2_contingency(obs1, correction=False)\n",
    "print('###Comparison of xgbr vs Penlington for size accuracy1###')\n",
    "print(f'contingency table:\\n{obs1}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))\n",
    "\n",
    "obs2 = pd.DataFrame({'gbrt': [np.sum(y2_xgbr[cuff_mask]),np.sum(~y2_xgbr[cuff_mask])], 'trad': [np.sum(y2_trad[cuff_mask]),np.sum(~y2_trad[cuff_mask])]})\n",
    "obs2.index = ['1', '0']\n",
    "\n",
    "\n",
    "chiresult = chi2_contingency(obs2, correction=False)\n",
    "print('\\n###Comparison of xgbr vs Penlington for size accuracy3###')\n",
    "print(f'contingency table:\\n{obs2}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7358644-1951-4ff7-ae93-eb686fecbcf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T16:42:11.529763Z",
     "iopub.status.busy": "2023-03-09T16:42:11.529162Z",
     "iopub.status.idle": "2023-03-09T16:42:11.552203Z",
     "shell.execute_reply": "2023-03-09T16:42:11.551340Z",
     "shell.execute_reply.started": "2023-03-09T16:42:11.529708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Comparison of xgbr vs lr for size accuracy1###\n",
      "contingency table:\n",
      "   gbrt  trad\n",
      "1  1259   518\n",
      "0   504  1245\n",
      "Chi square: 622.9332127402652\n",
      "P-value: 1.7210618341515443e-137\n",
      "\n",
      "###Comparison of xgbr vs lr for size accuracy3###\n",
      "contingency table:\n",
      "   gbrt  trad\n",
      "1  1715  1557\n",
      "0    48   206\n",
      "Chi square: 105.91304891900738\n",
      "P-value: 7.704262159873917e-25\n"
     ]
    }
   ],
   "source": [
    "# XGBR vs traditional\n",
    "cuff_mask = x_test[:,3]>0\n",
    "\n",
    "obs1 = pd.DataFrame({'gbrt': [np.sum(y1_xgbr[cuff_mask]),np.sum(~y1_xgbr[cuff_mask])], 'trad': [np.sum(y1_trad[cuff_mask]),np.sum(~y1_trad[cuff_mask])]})\n",
    "obs1.index = ['1', '0']\n",
    "\n",
    "chiresult = chi2_contingency(obs1, correction=False)\n",
    "print('###Comparison of xgbr vs lr for size accuracy1###')\n",
    "print(f'contingency table:\\n{obs1}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))\n",
    "\n",
    "obs2 = pd.DataFrame({'gbrt': [np.sum(y2_xgbr[cuff_mask]),np.sum(~y2_xgbr[cuff_mask])], 'trad': [np.sum(y2_trad[cuff_mask]),np.sum(~y2_trad[cuff_mask])]})\n",
    "obs2.index = ['1', '0']\n",
    "\n",
    "\n",
    "chiresult = chi2_contingency(obs2, correction=False)\n",
    "print('\\n###Comparison of xgbr vs lr for size accuracy3###')\n",
    "print(f'contingency table:\\n{obs2}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "473d6d22-17f3-40c5-a126-87be34dec634",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T16:33:11.834533Z",
     "iopub.status.busy": "2023-03-09T16:33:11.834235Z",
     "iopub.status.idle": "2023-03-09T16:33:11.849824Z",
     "shell.execute_reply": "2023-03-09T16:33:11.849299Z",
     "shell.execute_reply.started": "2023-03-09T16:33:11.834501Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Comparison of xgbr vs lr for size accuracy1###\n",
      "contingency table:\n",
      "   gbrt    lr\n",
      "1  3848  3638\n",
      "0  2342  2552\n",
      "Chi square: 14.902030445934713\n",
      "P-value: 0.00011324103606880015\n",
      "\n",
      "###Comparison of xgbr vs lr for size accuracy3###\n",
      "contingency table:\n",
      "   gbrt    lr\n",
      "1  6075  6030\n",
      "0   115   160\n",
      "Chi square: 7.530922608989523\n",
      "P-value: 0.006064884065087531\n"
     ]
    }
   ],
   "source": [
    "# XGBR vs LR\n",
    "obs1 = pd.DataFrame({'gbrt': [np.sum(y1_xgbr),np.sum(~y1_xgbr)], 'lr': [np.sum(y1_lr),np.sum(~y1_lr)]})\n",
    "obs1.index = ['1', '0']\n",
    "\n",
    "chiresult = chi2_contingency(obs1, correction=False)\n",
    "print('###Comparison of xgbr vs lr for size accuracy1###')\n",
    "print(f'contingency table:\\n{obs1}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))\n",
    "\n",
    "obs2 = pd.DataFrame({'gbrt': [np.sum(y2_xgbr),np.sum(~y2_xgbr)], 'lr': [np.sum(y2_lr),np.sum(~y2_lr)]})\n",
    "obs2.index = ['1', '0']\n",
    "\n",
    "\n",
    "chiresult = chi2_contingency(obs2, correction=False)\n",
    "print('\\n###Comparison of xgbr vs lr for size accuracy3###')\n",
    "print(f'contingency table:\\n{obs2}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a434d0e7-d117-412d-9ff5-46b41024e18d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T16:39:45.162540Z",
     "iopub.status.busy": "2023-03-09T16:39:45.161981Z",
     "iopub.status.idle": "2023-03-09T16:39:45.184909Z",
     "shell.execute_reply": "2023-03-09T16:39:45.184086Z",
     "shell.execute_reply.started": "2023-03-09T16:39:45.162483Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Comparison of xgbr vs dnn for size accuracy1###\n",
      "contingency table:\n",
      "   gbrt   dnn\n",
      "1  3848  3735\n",
      "0  2342  2455\n",
      "Chi square: 4.345770196662597\n",
      "P-value: 0.0371009826863702\n",
      "\n",
      "###Comparison of xgbr vs rfr for size accuracy3###\n",
      "contingency table:\n",
      "   gbrt   dnn\n",
      "1  6075  6063\n",
      "0   115   127\n",
      "Chi square: 0.6069048912710442\n",
      "P-value: 0.4359555589229944\n"
     ]
    }
   ],
   "source": [
    "# XGBR vs LR\n",
    "obs1 = pd.DataFrame({'gbrt': [np.sum(y1_xgbr),np.sum(~y1_xgbr)], 'dnn': [np.sum(y1_dnn),np.sum(~y1_dnn)]})\n",
    "obs1.index = ['1', '0']\n",
    "\n",
    "chiresult = chi2_contingency(obs1, correction=False)\n",
    "print('###Comparison of xgbr vs dnn for size accuracy1###')\n",
    "print(f'contingency table:\\n{obs1}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))\n",
    "\n",
    "obs2 = pd.DataFrame({'gbrt': [np.sum(y2_xgbr),np.sum(~y2_xgbr)], 'dnn': [np.sum(y2_dnn),np.sum(~y2_dnn)]})\n",
    "obs2.index = ['1', '0']\n",
    "\n",
    "\n",
    "chiresult = chi2_contingency(obs2, correction=False)\n",
    "print('\\n###Comparison of xgbr vs rfr for size accuracy3###')\n",
    "print(f'contingency table:\\n{obs2}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149925a7-ced9-466a-9998-bd6c1d4c9e30",
   "metadata": {},
   "source": [
    "## comparison within three machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fa7a63f1-d66d-4306-b92c-61d0976710b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T02:14:22.569107Z",
     "iopub.status.busy": "2023-03-07T02:14:22.568510Z",
     "iopub.status.idle": "2023-03-07T02:14:22.970174Z",
     "shell.execute_reply": "2023-03-07T02:14:22.969663Z",
     "shell.execute_reply.started": "2023-03-07T02:14:22.569048Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Comparison of three machine learning models for size accuracy1###\n",
      "contingency table:\n",
      "   gbrt    rf   dnn\n",
      "1  3848  3749  3735\n",
      "0  2342  2441  2455\n",
      "Chi square: 5.154345076632554\n",
      "P-value: 0.07598855529065028\n",
      "\n",
      "###Comparison of three machine learning models for size accuracy3###\n",
      "contingency table:\n",
      "   gbrt    rf   dnn\n",
      "1  6075  6068  6063\n",
      "0   115   122   127\n",
      "Chi square: 0.6108751733820392\n",
      "P-value: 0.7368008895760321\n"
     ]
    }
   ],
   "source": [
    "obs1 = pd.DataFrame({'gbrt': [np.sum(y1_xgbr),np.sum(~y1_xgbr)], 'rf': [np.sum(y1_rfr),np.sum(~y1_rfr)], 'dnn': [np.sum(y1_dnn),np.sum(~y1_dnn)]})\n",
    "obs1.index = ['1', '0']\n",
    "\n",
    "chiresult = chi2_contingency(obs1, correction=False)\n",
    "print('###Comparison of three machine learning models for size accuracy1###')\n",
    "print(f'contingency table:\\n{obs1}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))\n",
    "\n",
    "obs2 = pd.DataFrame({'gbrt': [np.sum(y2_xgbr),np.sum(~y2_xgbr)], 'rf': [np.sum(y2_rfr),np.sum(~y2_rfr)], 'dnn': [np.sum(y2_dnn),np.sum(~y2_dnn)]})\n",
    "obs2.index = ['1', '0']\n",
    "\n",
    "\n",
    "chiresult = chi2_contingency(obs2, correction=False)\n",
    "print('\\n###Comparison of three machine learning models for size accuracy3###')\n",
    "print(f'contingency table:\\n{obs2}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7bc474ae-aa72-4383-a807-349272ff23d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T02:20:08.736147Z",
     "iopub.status.busy": "2023-03-07T02:20:08.735565Z",
     "iopub.status.idle": "2023-03-07T02:20:08.759605Z",
     "shell.execute_reply": "2023-03-07T02:20:08.758725Z",
     "shell.execute_reply.started": "2023-03-07T02:20:08.736092Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###Comparison of traditional formula and gbrt for size accuracy1###\n",
      "contingency table:\n",
      "   traditional  gbrt\n",
      "1         1725  3848\n",
      "0         4465  2342\n",
      "Chi square: 6557.51175680215\n",
      "P-value: 0.0\n",
      "\n",
      "###Comparison of traditional formula and gbrt for size accuracy3###\n",
      "contingency table:\n",
      "   traditional  gbrt\n",
      "1         5277  6075\n",
      "0          913   115\n",
      "Chi square: 6557.51175680215\n",
      "P-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# compare with age-based formula and GBRT model\n",
    "y1_trad = y_test_old == y_test\n",
    "y2_trad = (y_test_old >= y_test-0.5) & (y_test_old <= y_test+0.5)\n",
    "\n",
    "obs1 = pd.DataFrame({'traditional': [np.sum(y1_trad), np.sum(~y1_trad)], 'gbrt': [np.sum(y1_xgbr),np.sum(~y1_xgbr)]})\n",
    "obs1.index = ['1', '0']\n",
    "\n",
    "chiresult = chi2_contingency(obs1, correction=False)\n",
    "print('\\n###Comparison of traditional formula and gbrt for size accuracy1###')\n",
    "print(f'contingency table:\\n{obs1}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))\n",
    "\n",
    "\n",
    "obs2 = pd.DataFrame({'traditional': [np.sum(y2_trad), np.sum(~y2_trad)], 'gbrt': [np.sum(y2_xgbr),np.sum(~y2_xgbr)]})\n",
    "obs2.index = ['1', '0']\n",
    "\n",
    "chiresult = chi2_contingency(obs2, correction=False)\n",
    "print('\\n###Comparison of traditional formula and gbrt for size accuracy3###')\n",
    "print(f'contingency table:\\n{obs2}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8cb81503-8679-445c-a9d2-b7ded2f278ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T07:45:12.156056Z",
     "iopub.status.busy": "2023-03-06T07:45:12.155513Z",
     "iopub.status.idle": "2023-03-06T07:45:12.164117Z",
     "shell.execute_reply": "2023-03-06T07:45:12.163319Z",
     "shell.execute_reply.started": "2023-03-06T07:45:12.156001Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=-114.89969337130972, pvalue=0.0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_rel(y_test_old, y_xgbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed46a7-5f7b-4b95-9006-816151e93d9e",
   "metadata": {},
   "source": [
    "# Sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d4a7049-5c42-4219-9b8d-f176b4b089ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T05:40:10.572410Z",
     "iopub.status.busy": "2023-05-18T05:40:10.571901Z",
     "iopub.status.idle": "2023-05-18T05:40:10.581901Z",
     "shell.execute_reply": "2023-05-18T05:40:10.580721Z",
     "shell.execute_reply.started": "2023-05-18T05:40:10.572349Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (24764, 2), X_test: (6190, 2)\n"
     ]
    }
   ],
   "source": [
    "# 0: age, 1: weight, 2: height, 3: cuffed\n",
    "X_train = np.concatenate((x_train[:,0:1], x_train[:,3:4]),axis=-1)\n",
    "X_test = np.concatenate((x_test[:,0:1], x_test[:,3:4]),axis=-1)\n",
    "\n",
    "print(f'X_train: {X_train.shape}, X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6545bd1c-59f5-4c1b-a069-0522654db3ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T16:23:40.754920Z",
     "iopub.status.busy": "2023-03-08T16:23:40.754373Z",
     "iopub.status.idle": "2023-03-08T16:23:40.775707Z",
     "shell.execute_reply": "2023-03-08T16:23:40.774862Z",
     "shell.execute_reply.started": "2023-03-08T16:23:40.754868Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = x_train.copy()\n",
    "X_train[:,0] = np.array([math.floor(ele) for ele in X_train[:,0]])\n",
    "\n",
    "X_test = x_test.copy()\n",
    "X_test[:,0] = np.array([math.floor(ele) for ele in X_test[:,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa2e43ab-eaad-463e-8e47-eae22699271e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T16:23:43.315933Z",
     "iopub.status.busy": "2023-03-08T16:23:43.315557Z",
     "iopub.status.idle": "2023-03-08T16:25:04.720973Z",
     "shell.execute_reply": "2023-03-08T16:25:04.720238Z",
     "shell.execute_reply.started": "2023-03-08T16:23:43.315893Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 180 candidates, totalling 1800 fits\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'colsample_bytree': 1, 'max_depth': 5, 'n_estimators': 25, 'subsample': 1}\n",
      "0.9045592404927584\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.886\n",
      "mean_squared_errors: 0.109\n",
      "mean_absolute_errors: 0.199\n",
      "r2_score: 0.886\n",
      "acc: 0.621\n",
      "acc(+-0.5mm): 0.982\n"
     ]
    }
   ],
   "source": [
    "# age (일단위)\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.01, 0.03, 0.05, 0.07] #, #[0.01, 0.03, 0.05],\n",
    "                'max_depth': [3, 4, 5, 7],#[3,4,5],\n",
    "                'n_estimators': [25, 50, 75, 100, 300],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                'subsample': [0.5, 0.8, 1], #[0.5, 0.8, 1],\n",
    "                'colsample_bytree': [0.5, 0.8, 1], #[0.8, 1],\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=xgb.sklearn.XGBRegressor(),\n",
    "                n_jobs=-1,\n",
    "                verbose=3,\n",
    "                param_grid=param_dict, cv=nfold)\n",
    "gs.fit(X_train, y_train)\n",
    "model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = gs.predict(X_test).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test)\n",
    "acc3 = np.mean((y_pred >= y_test-0.5) & (y_pred <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cceefcb7-12c1-4ee4-8b4c-7a1a250c657b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T16:25:54.315292Z",
     "iopub.status.busy": "2023-03-08T16:25:54.314738Z",
     "iopub.status.idle": "2023-03-08T16:25:54.328776Z",
     "shell.execute_reply": "2023-03-08T16:25:54.327945Z",
     "shell.execute_reply.started": "2023-03-08T16:25:54.315239Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.907 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.904 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.902 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.906 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.905 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.905 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.904 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.902 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.902 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.902 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.902 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.901 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.899 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.897 total time=   3.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.905 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.893 total time=   5.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.902 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.902 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.906 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.901 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.900 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.905 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.899 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.896 total time=   4.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.901 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.898 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.893 total time=   4.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.891 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.895 total time=   2.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.901 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.901 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.902 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.901 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.901 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.902 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.904 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.906 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.905 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.906 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.901 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.895 total time=   4.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.906 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.900 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.896 total time=   5.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.892 total time=   2.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.894 total time=   2.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.902 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.908 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.904 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.903 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.904 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.899 total time=   3.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.904 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.902 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.900 total time=   3.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.898 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.902 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.908 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.907 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.904 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.905 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.901 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.905 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.905 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.905 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.905 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.907 total time=   3.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.901 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.903 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.900 total time=   3.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.902 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.899 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.904 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.897 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.898 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.901 total time=   1.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.898 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.903 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.903 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.904 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.905 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.901 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.901 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.906 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.901 total time=   3.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.901 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.899 total time=   4.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.905 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.898 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.898 total time=   5.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.894 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.891 total time=   2.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.904 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.901 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.906 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.904 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.900 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.903 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.903 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.908 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.900 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.908 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.908 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.906 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.905 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.902 total time=   3.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.902 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.889 total time=   5.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.904 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.902 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.899 total time=   2.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.908 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.901 total time=   4.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.901 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.897 total time=   4.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.898 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.900 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.907 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.907 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.907 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.908 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.908 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.908 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.908 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.905 total time=   1.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.906 total time=   3.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.903 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.900 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.901 total time=   4.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.900 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.900 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.907 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.907 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.907 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.903 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.905 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.898 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.899 total time=   2.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.902 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.905 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.903 total time=   2.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.904 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.901 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.898 total time=   3.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.902 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.902 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.901 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.897 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.899 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.902 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.898 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.899 total time=   4.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.901 total time=   3.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.906 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.900 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.901 total time=   3.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.905 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.907 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.900 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.894 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.898 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.899 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.906 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.897 total time=   6.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.903 total time=   3.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.905 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.909 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.904 total time=   4.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.899 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.893 total time=   5.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.895 total time=   2.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.898 total time=   2.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.903 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.905 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.908 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.905 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.900 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.905 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.903 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.904 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.900 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.903 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.903 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.903 total time=   1.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.905 total time=   3.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.899 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.890 total time=   5.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.901 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.905 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.900 total time=   2.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.904 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.899 total time=   4.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.902 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.898 total time=   4.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.901 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.900 total time=   2.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.897 total time=   6.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.898 total time=   3.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.904 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.900 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.902 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.900 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.901 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.899 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.902 total time=   1.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.898 total time=   5.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.903 total time=   1.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.892 total time=   2.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.900 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.905 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.905 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.901 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.905 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.900 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.905 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.900 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.900 total time=   2.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.908 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.898 total time=   4.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.905 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.899 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.888 total time=   5.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.904 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.902 total time=   2.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.905 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.904 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.904 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.900 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.901 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.901 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.899 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.902 total time=   4.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.906 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.886 total time=   7.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.901 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.899 total time=   3.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.904 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.901 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.899 total time=   4.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.899 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.895 total time=   5.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.898 total time=   2.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.898 total time=   2.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.903 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.902 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.905 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.903 total time=   2.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.906 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.901 total time=   3.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.906 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.905 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.904 total time=   4.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.905 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.888 total time=   5.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.909 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.909 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.908 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.908 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.908 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.908 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.905 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.906 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.902 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.898 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.901 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.904 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.902 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.899 total time=   4.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.901 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.886 total time=   7.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.908 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.904 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.908 total time=   3.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.905 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.900 total time=   4.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.906 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.903 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.898 total time=   5.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.895 total time=   2.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.895 total time=   2.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.902 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.905 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.903 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.902 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.900 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.905 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.904 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.899 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.899 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.900 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.900 total time=   3.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.901 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.892 total time=   5.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.905 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.904 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.905 total time=   2.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.905 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.900 total time=   4.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.904 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.900 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.891 total time=   4.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.896 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.897 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.903 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.903 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.903 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.903 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.902 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.903 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.903 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.906 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.899 total time=   3.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.904 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.896 total time=   4.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.901 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.896 total time=   2.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.898 total time=   5.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.898 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.901 total time=   2.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.901 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.905 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.904 total time=   2.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.905 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.904 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.897 total time=   3.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.904 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.892 total time=   4.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.905 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.899 total time=   1.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.894 total time=   5.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.904 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.906 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.906 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.905 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.905 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.901 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.899 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.897 total time=   3.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.900 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.899 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.903 total time=   4.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.897 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.900 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.894 total time=   6.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.901 total time=   3.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.900 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.904 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.906 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.906 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.904 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.905 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.903 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.900 total time=   1.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.900 total time=   5.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.900 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.895 total time=   2.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.900 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.905 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.908 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.907 total time=   2.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.908 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.908 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.903 total time=   3.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.905 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.900 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.905 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.903 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.900 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.898 total time=   1.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.908 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.908 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.908 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.908 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.908 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.908 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.904 total time=   3.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.908 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.895 total time=   4.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.904 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.902 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.898 total time=   4.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.896 total time=   1.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.893 total time=   2.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.905 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.905 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.902 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.903 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.902 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.902 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.903 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.903 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.903 total time=   3.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.906 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.898 total time=   4.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.902 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.902 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.890 total time=   5.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.890 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.898 total time=   2.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.906 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.898 total time=   2.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.902 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.899 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.901 total time=   3.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.899 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.893 total time=   4.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.903 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.892 total time=   1.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.887 total time=   5.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.900 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.902 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.901 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.902 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.908 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.907 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.904 total time=   3.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.900 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.905 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.899 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.901 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.896 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.900 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.904 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.899 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.889 total time=   6.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.903 total time=   3.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.900 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.901 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.902 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.899 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.904 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.903 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.903 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.901 total time=   5.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.902 total time=   1.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.879 total time=   6.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.901 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.908 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.900 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.907 total time=   2.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.904 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.905 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.905 total time=   2.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.900 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.904 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.900 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.904 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.898 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.899 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.896 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.897 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.905 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.902 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.905 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.906 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.905 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.903 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.900 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.899 total time=   3.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.903 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.900 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.906 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.904 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.906 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.906 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.901 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.897 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.905 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.901 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.903 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.898 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.880 total time=   7.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.905 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.904 total time=   3.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.906 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.899 total time=   4.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.902 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.901 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.907 total time=   1.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.903 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.905 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.900 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.902 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.897 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.872 total time=   6.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.903 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.900 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.902 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.902 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.902 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.902 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.901 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.900 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.908 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.902 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.899 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.900 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.898 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.899 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.902 total time=   3.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.899 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.892 total time=   5.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.906 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.906 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.901 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.901 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.904 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.901 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.898 total time=   3.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.904 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.895 total time=   5.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.901 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.893 total time=   2.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.879 total time=   6.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.901 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.904 total time=   3.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.903 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.901 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.897 total time=   4.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.903 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.899 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.905 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.900 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.901 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.894 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.878 total time=   6.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.905 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.902 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.906 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.901 total time=   2.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.901 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.897 total time=   3.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.904 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.900 total time=   4.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.899 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.900 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.888 total time=   5.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.903 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.903 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.903 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.901 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.903 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.906 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.900 total time=   3.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.904 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.898 total time=   5.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.907 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.898 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.891 total time=   6.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.908 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.908 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.908 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.908 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.908 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.901 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.903 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.900 total time=   4.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.905 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.892 total time=   6.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.901 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.891 total time=   2.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.880 total time=   5.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.900 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.905 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.901 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.900 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.900 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.908 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.908 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.905 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.905 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.896 total time=   3.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.902 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.899 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.898 total time=   4.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.901 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.897 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.895 total time=   4.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.904 total time=   3.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.901 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.900 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.901 total time=   3.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.903 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.907 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.904 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.900 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.901 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.900 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.901 total time=   2.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.894 total time=   6.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.903 total time=   3.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.905 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.903 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.899 total time=   4.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.908 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.897 total time=   6.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.902 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.893 total time=   2.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.880 total time=   5.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.901 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.901 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.902 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.901 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.900 total time=   2.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.902 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.900 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.906 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.905 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.902 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.902 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.898 total time=   3.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.904 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.885 total time=   5.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.903 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.908 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.900 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.906 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.904 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.901 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.903 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.904 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.903 total time=   3.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.904 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.904 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.903 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.899 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.902 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.901 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.879 total time=   7.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.900 total time=   3.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.904 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.908 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.908 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.906 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.904 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.905 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.901 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.902 total time=   1.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.893 total time=   5.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.901 total time=   1.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.895 total time=   2.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.887 total time=   4.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.901 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.905 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.906 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.905 total time=   2.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.906 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.900 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.902 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.900 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.900 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.903 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.899 total time=   3.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.904 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.906 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.905 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.905 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.903 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.903 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.905 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.903 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.905 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.904 total time=   3.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.901 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.902 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.903 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.901 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.900 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.902 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.899 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.902 total time=   4.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.901 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.883 total time=   7.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.899 total time=   3.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.904 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.902 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.908 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.909 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.907 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.901 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.903 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.899 total time=   2.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.900 total time=   5.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.897 total time=   1.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.901 total time=   2.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.894 total time=   4.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.899 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.905 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.905 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.899 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.904 total time=   2.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.901 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.904 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.904 total time=   2.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.901 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.905 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.903 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.904 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.903 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.902 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.900 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.900 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.902 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.901 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.901 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.901 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.900 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.901 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.899 total time=   3.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.905 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.905 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.903 total time=   3.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.900 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.903 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.899 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.900 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.900 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.899 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.903 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.895 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.903 total time=   2.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.892 total time=   6.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.899 total time=   3.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.908 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.899 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.899 total time=   4.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.901 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.904 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.898 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.899 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.904 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.899 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.902 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.896 total time=   2.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.899 total time=   2.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.888 total time=   4.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.903 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.901 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.905 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.904 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.905 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.901 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.899 total time=   2.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.899 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.896 total time=   4.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.905 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.901 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.894 total time=   5.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.904 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.905 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.906 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.901 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.898 total time=   4.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.908 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.905 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.902 total time=   4.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.899 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.896 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.904 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.905 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.905 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.905 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.903 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.904 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.901 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.906 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.906 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.904 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.902 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.908 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.905 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.906 total time=   4.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.897 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.894 total time=   5.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.894 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.897 total time=   2.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.893 total time=   4.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.908 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.901 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.902 total time=   2.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.908 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.906 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.907 total time=   2.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.908 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.907 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.899 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.900 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.905 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.901 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.898 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.894 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.898 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.903 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.900 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.905 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.901 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.900 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.901 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.901 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.902 total time=   3.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.901 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.901 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.905 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.903 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.900 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.899 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.901 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.894 total time=   4.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.897 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.875 total time=   7.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.901 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.901 total time=   3.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.906 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.907 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.903 total time=   4.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.900 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.899 total time=   1.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.898 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.899 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.902 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.897 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.898 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.901 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.904 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.884 total time=   4.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.901 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.904 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.902 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.905 total time=   2.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.906 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.901 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.899 total time=   2.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.906 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.904 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.900 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.902 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.902 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.903 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.901 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.900 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.901 total time=   4.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.899 total time=   3.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.901 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.906 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.906 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.906 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.905 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.905 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.904 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.904 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.897 total time=   4.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.896 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.900 total time=   2.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.894 total time=   6.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.903 total time=   3.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.906 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.901 total time=   4.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.905 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.901 total time=   1.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.900 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.905 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.901 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.897 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.900 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.885 total time=   7.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.904 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.905 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.905 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.901 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.902 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.904 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.903 total time=   3.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.902 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.894 total time=   4.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.900 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.895 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.891 total time=   5.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.905 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.904 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.904 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.903 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.901 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.903 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.900 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.903 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.900 total time=   4.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.904 total time=   1.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.882 total time=   7.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.900 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.903 total time=   3.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.899 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.902 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.905 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.903 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.903 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.901 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.898 total time=   1.9s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.895 total time=   5.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.897 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.884 total time=   7.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.900 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.902 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.904 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.901 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.901 total time=   2.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.901 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.908 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.902 total time=   2.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.905 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.899 total time=   4.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.900 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.897 total time=   1.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.890 total time=   5.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.903 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.901 total time=   2.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.907 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.902 total time=   4.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.905 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.900 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.900 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.902 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.902 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.903 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.894 total time=   1.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.897 total time=   2.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.888 total time=   6.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.901 total time=   3.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.901 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.906 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.898 total time=   4.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.902 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.904 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.900 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.901 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.897 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.899 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.900 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.872 total time=   7.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.905 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.905 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.904 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.904 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.903 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.900 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.900 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.902 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.904 total time=   2.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.900 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.905 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.906 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.901 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.902 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.899 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.902 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.903 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.903 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.903 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.902 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.902 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.901 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.902 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.903 total time=   3.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.903 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.900 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.900 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.906 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.906 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.904 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.897 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.904 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.896 total time=   4.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.903 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.880 total time=   7.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.905 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.902 total time=   3.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.899 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.902 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.904 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.905 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.899 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.904 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.897 total time=   5.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.899 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.881 total time=   7.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.898 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.901 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.908 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.908 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.907 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.905 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.905 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.902 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.897 total time=   3.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.904 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.898 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.895 total time=   4.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.903 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.898 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.899 total time=   5.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.904 total time=   3.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.900 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.905 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.904 total time=   3.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.898 total time=   1.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.899 total time=   4.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.898 total time=   1.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.900 total time=   2.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.901 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.901 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.901 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.902 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.901 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.905 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.905 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.901 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.904 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.904 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.906 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.903 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.903 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.906 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.905 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.903 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.900 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.897 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.897 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.897 total time=   5.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.902 total time=   1.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.881 total time=   7.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.905 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.903 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.900 total time=   2.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.905 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.900 total time=   3.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.905 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.902 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.898 total time=   3.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.904 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.901 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.892 total time=   4.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.899 total time=   3.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.903 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.904 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.899 total time=   3.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.902 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.905 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.901 total time=   4.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.901 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.902 total time=   2.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.890 total time=   6.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.899 total time=   3.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.903 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.900 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.903 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.901 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.902 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.899 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.903 total time=   1.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.894 total time=   5.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.902 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.881 total time=   7.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.904 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.902 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.902 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.900 total time=   2.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.904 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.904 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.900 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.904 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.902 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.907 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.908 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.897 total time=   3.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.898 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.884 total time=   5.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.906 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.903 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.904 total time=   2.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.901 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.903 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.901 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.902 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.903 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.899 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.898 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.895 total time=   4.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.902 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.878 total time=   7.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.902 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.903 total time=   3.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.903 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.904 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.901 total time=   4.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.905 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.903 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.901 total time=   5.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.905 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.878 total time=   7.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.900 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.908 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.905 total time=   2.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.904 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.897 total time=   3.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.901 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.900 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.900 total time=   4.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.897 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.900 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.897 total time=   4.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.899 total time=   3.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.904 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.897 total time=   4.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.905 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.895 total time=   5.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.904 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.897 total time=   2.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.889 total time=   6.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.901 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.904 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.904 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.903 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.904 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.902 total time=   4.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.904 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.893 total time=   6.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.898 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.894 total time=   2.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.885 total time=   6.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.904 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.901 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.903 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.900 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.900 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.901 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.903 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.899 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.908 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.900 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.901 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.904 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.901 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.904 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.904 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.902 total time=   3.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.903 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.884 total time=   5.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.902 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.906 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.903 total time=   2.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.903 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.906 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.903 total time=   3.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.903 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.889 total time=   5.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.905 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.898 total time=   2.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.891 total time=   6.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.908 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.903 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.902 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.900 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.901 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.902 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.904 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.897 total time=   4.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.902 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.894 total time=   6.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.904 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.897 total time=   2.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.887 total time=   7.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.905 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.905 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.905 total time=   2.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.900 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.904 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.902 total time=   2.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.901 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.901 total time=   4.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.904 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.898 total time=   1.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.896 total time=   5.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.901 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.908 total time=   2.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.900 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.896 total time=   4.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.901 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.898 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.895 total time=   4.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.897 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.899 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.902 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.903 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.903 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.903 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.903 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.904 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.902 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.904 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.904 total time=   3.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.902 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.893 total time=   4.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.904 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.908 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.891 total time=   6.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.900 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.896 total time=   2.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.886 total time=   6.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.903 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.905 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.900 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.903 total time=   2.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.906 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.899 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.902 total time=   3.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.902 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.897 total time=   4.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.902 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.894 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.893 total time=   5.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.902 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.903 total time=   2.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.901 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.902 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.898 total time=   3.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.906 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.897 total time=   5.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.899 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.896 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.884 total time=   6.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.905 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.906 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.906 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.906 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.905 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.905 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.906 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.896 total time=   4.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.904 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.887 total time=   6.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.902 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.888 total time=   2.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.878 total time=   7.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.902 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.903 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.900 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.907 total time=   2.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.901 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.905 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.900 total time=   2.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.904 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.900 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.899 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.899 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.900 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.897 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.895 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.901 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.901 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.901 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.904 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.903 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.904 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.901 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.900 total time=   3.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.904 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.908 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.906 total time=   3.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.908 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.892 total time=   5.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.904 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.892 total time=   2.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.886 total time=   6.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.905 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.906 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.902 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.901 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.905 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.903 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.902 total time=   4.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.903 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.898 total time=   6.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.906 total time=   1.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.899 total time=   2.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.888 total time=   6.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.901 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.903 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.899 total time=   2.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.908 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.903 total time=   3.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.901 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.898 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.895 total time=   3.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.902 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.896 total time=   1.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.902 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.903 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.905 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.906 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.903 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.905 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.902 total time=   3.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.904 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.900 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.898 total time=   3.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.900 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.896 total time=   5.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.900 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.894 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.885 total time=   6.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.904 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.902 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.901 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.901 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.904 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.899 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.898 total time=   4.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.900 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.897 total time=   6.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.904 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.892 total time=   2.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.883 total time=   6.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.900 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.908 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.905 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.902 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.901 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.902 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.902 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.905 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.904 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.904 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.903 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.902 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.903 total time=   3.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.906 total time=   1.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.893 total time=   5.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.905 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.905 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.906 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.906 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.905 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.906 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.901 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.905 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.893 total time=   4.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.905 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.897 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.894 total time=   4.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.900 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.896 total time=   2.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.891 total time=   6.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.901 total time=   3.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.902 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.900 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.895 total time=   4.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.899 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.905 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.900 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.895 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.902 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.900 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.899 total time=   2.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.887 total time=   5.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.906 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.903 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.901 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.899 total time=   2.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.901 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.906 total time=   3.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.904 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.905 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.907 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.907 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.908 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.906 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.906 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.903 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.904 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.893 total time=   4.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.901 total time=   3.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.901 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.899 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.902 total time=   3.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.905 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.903 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.900 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.905 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.899 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.900 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.898 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.897 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.902 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.903 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.902 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.905 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.906 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.905 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.905 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.904 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.902 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.899 total time=   3.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.901 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.905 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.902 total time=   4.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.905 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.894 total time=   6.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.904 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.897 total time=   2.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.883 total time=   7.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.903 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.902 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.906 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.901 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.905 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.905 total time=   3.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.901 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.902 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.895 total time=   4.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.901 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.902 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.893 total time=   4.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.901 total time=   3.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.905 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.898 total time=   3.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.899 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.893 total time=   5.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.902 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.896 total time=   2.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.890 total time=   6.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.906 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.904 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.901 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.903 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.901 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.899 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.900 total time=   4.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.898 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.890 total time=   6.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.899 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.888 total time=   2.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.877 total time=   7.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.907 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.902 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.901 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.900 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.908 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.899 total time=   3.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.905 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.900 total time=   3.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.902 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.904 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.900 total time=   4.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.903 total time=   3.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.905 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.904 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.903 total time=   4.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.905 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.899 total time=   5.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.903 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.896 total time=   2.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.881 total time=   6.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.903 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.905 total time=   3.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.906 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.900 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.906 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.900 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.909 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.906 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.908 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.901 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.903 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.895 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.896 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.894 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.901 total time=   2.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.891 total time=   6.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.903 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.902 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.904 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.904 total time=   2.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.906 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.901 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.900 total time=   3.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.905 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.900 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.903 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.900 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.901 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.899 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.901 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.897 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.901 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.900 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.900 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.901 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.905 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.903 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.904 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.903 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.905 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.905 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.904 total time=   3.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.906 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.901 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.899 total time=   3.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.907 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.891 total time=   5.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.898 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.889 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.888 total time=   6.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.901 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.901 total time=   3.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.908 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.895 total time=   4.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.905 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.903 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.902 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.902 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.901 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.899 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.903 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.900 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.898 total time=   2.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.889 total time=   6.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.905 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.905 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.903 total time=   2.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.900 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.902 total time=   3.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.908 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.906 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.903 total time=   3.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.899 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.904 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.895 total time=   5.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.907 total time=   3.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.906 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.907 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.908 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.908 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.908 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.907 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.903 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.905 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.906 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.907 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.908 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.903 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.906 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.898 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.886 total time=   7.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.908 total time=   1.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.906 total time=   3.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.909 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.903 total time=   4.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.908 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.905 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.901 total time=   5.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.902 total time=   2.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.901 total time=   2.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.894 total time=   6.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.903 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.901 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.899 total time=   2.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.904 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.905 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.903 total time=   3.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.906 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.901 total time=   4.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.907 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.897 total time=   5.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.901 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.905 total time=   2.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.905 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.903 total time=   4.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.905 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.903 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.900 total time=   4.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.901 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.902 total time=   2.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.905 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.905 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.905 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.905 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.905 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.905 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.905 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.902 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.903 total time=   3.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.903 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.900 total time=   4.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.901 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.902 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.891 total time=   5.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.898 total time=   2.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.898 total time=   2.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.896 total time=   6.2s\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "odir_f = f'acc1-{acc1:.3f}_acc3-{acc3:.3f}_XGBR_cuff+4inputs-age(floor)_{nfold}fold'\n",
    "odir = f'result/outliers+w+h/size/sens_analysis/{odir_f}'\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "model.save_model(f'{odir}/model.model')\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "pickle.dump(param_dict, open(f'{odir}/param_dict', 'wb'))\n",
    "f = open(f'{odir}/result.txt', 'w')\n",
    "f.write(f'classification model')\n",
    "f.write(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "f.write(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d570b03-f08b-4234-b56d-da3632c3f117",
   "metadata": {},
   "source": [
    "### test validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d558da3f-5097-4be5-9cae-e35687ad8662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T05:40:20.198036Z",
     "iopub.status.busy": "2023-05-18T05:40:20.197586Z",
     "iopub.status.idle": "2023-05-18T05:40:24.329713Z",
     "shell.execute_reply": "2023-05-18T05:40:24.328993Z",
     "shell.execute_reply.started": "2023-05-18T05:40:20.197977Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.598+-0.012 -> 0.586 ~ 0.610\n",
      "acc(+-0.5mm): 0.980+-0.003 -> 0.976 ~ 0.983 \n"
     ]
    }
   ],
   "source": [
    "# xgbr model\n",
    "xgbr = xgb.XGBRegressor()\n",
    "odir_f = f'acc1-0.597_acc3-0.980_XGBR_cuff+age_10fold'\n",
    "odir = f'result/outliers+w+h/size/sens_analysis/{odir_f}'\n",
    "xgbr.load_model(f'{odir}/model.model')\n",
    "\n",
    "y1_xgbrs, y2_xgbrs = [], []\n",
    "for i in range(200):\n",
    "    \n",
    "    mask = np.array([random.randint(0,1)==1 for j in range(len(x_test))])\n",
    "    X1_test = X_test[mask]\n",
    "    Y_test = y_test[mask]\n",
    "    \n",
    "    y_xgbr = xgbr.predict(X1_test)\n",
    "    y_xgbr = np.round(y_xgbr * 2) / 2\n",
    "    y1_xgbrs.append(np.mean(y_xgbr == Y_test))\n",
    "    y2_xgbrs.append(np.mean((y_xgbr >= Y_test-0.5) & (y_xgbr <= Y_test+0.5)))\n",
    "    \n",
    "print(f'acc: {np.mean(y1_xgbrs):.3f}+-{1.96*np.std(y1_xgbrs):.3f} -> {np.mean(y1_xgbrs)-1.96*np.std(y1_xgbrs):.3f} ~ {np.mean(y1_xgbrs)+1.96*np.std(y1_xgbrs):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(y2_xgbrs):.3f}+-{1.96*np.std(y2_xgbrs):.3f} -> {np.mean(y2_xgbrs)-1.96*np.std(y2_xgbrs):.3f} ~ {np.mean(y2_xgbrs)+1.96*np.std(y2_xgbrs):.3f} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fa623b-43fb-409a-8053-dca8a3d65faa",
   "metadata": {},
   "source": [
    "### P-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "734c84aa-619b-4b52-9a3c-38058c922339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T02:30:57.703518Z",
     "iopub.status.busy": "2023-03-29T02:30:57.702993Z",
     "iopub.status.idle": "2023-03-29T02:30:57.782162Z",
     "shell.execute_reply": "2023-03-29T02:30:57.781150Z",
     "shell.execute_reply.started": "2023-03-29T02:30:57.703466Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:782: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from scipy import stats\n",
    "\n",
    "# xgbr0 model\n",
    "xgbr = xgb.XGBRegressor()\n",
    "xgbr.load_model('result/outliers+w+h/size/sens_analysis/acc1-0.622_acc3-0.981_XGBR_cuff+age+weight+height_10fold/model.model')\n",
    "y_xgbr0 = xgbr.predict(x_test)\n",
    "y_xgbr0 = np.round(y_xgbr0 * 2) / 2\n",
    "y1_xgbr0 = y_xgbr0 == y_test\n",
    "y2_xgbr0 = (y_xgbr0 >= y_test-0.5) & (y_xgbr0 <= y_test+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ac3973-6561-42fc-88ea-9376ebcffcf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T02:31:59.055259Z",
     "iopub.status.busy": "2023-03-29T02:31:59.054767Z",
     "iopub.status.idle": "2023-03-29T02:31:59.086264Z",
     "shell.execute_reply": "2023-03-29T02:31:59.085233Z",
     "shell.execute_reply.started": "2023-03-29T02:31:59.055207Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###Comparison of traditional formula and gbrt for size accuracy1###\n",
      "contingency table:\n",
      "   gbrt(4inputs)  gbrt(caw)\n",
      "1           3848       3697\n",
      "0           2342       2493\n",
      "Chi square: 7.737823455680943\n",
      "P-value: 0.005407594189474648\n",
      "\n",
      "###Comparison of xgbr vs lr for size accuracy3###\n",
      "contingency table:\n",
      "   gbrt  trad\n",
      "1  6075  6064\n",
      "0   115   126\n",
      "Chi square: 0.5120425609443038\n",
      "P-value: 0.4742559814377173\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from scipy import stats\n",
    "\n",
    "# xgbr model\n",
    "X_test = np.concatenate((x_test[:,0:1], x_test[:,3:4]),axis=-1)\n",
    "#X_test = x_test.copy()\n",
    "#X_test[:,0] = np.array([math.floor(ele) for ele in X_test[:,0]])\n",
    "\n",
    "xgbr = xgb.XGBRegressor()\n",
    "xgbr.load_model('result/outliers+w+h/size/sens_analysis/acc1-0.597_acc3-0.980_XGBR_cuff+age_10fold/model.model')\n",
    "y_xgbr = xgbr.predict(X_test)\n",
    "y_xgbr = np.round(y_xgbr * 2) / 2\n",
    "y1_xgbr = y_xgbr == y_test\n",
    "y2_xgbr = (y_xgbr >= y_test-0.5) & (y_xgbr <= y_test+0.5)\n",
    "\n",
    "obs1 = pd.DataFrame({'gbrt(4inputs)': [np.sum(y1_xgbr0), np.sum(~y1_xgbr0)], 'gbrt(caw)': [np.sum(y1_xgbr),np.sum(~y1_xgbr)]})\n",
    "obs1.index = ['1', '0']\n",
    "\n",
    "chiresult = chi2_contingency(obs1, correction=False)\n",
    "print('\\n###Comparison of traditional formula and gbrt for size accuracy1###')\n",
    "print(f'contingency table:\\n{obs1}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))\n",
    "\n",
    "\n",
    "obs2 = pd.DataFrame({'gbrt': [np.sum(y2_xgbr0),np.sum(~y2_xgbr0)], 'trad': [np.sum(y2_xgbr),np.sum(~y2_xgbr)]})\n",
    "obs2.index = ['1', '0']\n",
    "\n",
    "\n",
    "chiresult = chi2_contingency(obs2, correction=False)\n",
    "print('\\n###Comparison of xgbr vs lr for size accuracy3###')\n",
    "print(f'contingency table:\\n{obs2}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290bdcc0-7d54-4a90-b441-d7d600341a8a",
   "metadata": {},
   "source": [
    "# Subgroup analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ad08aa2-6358-4b9d-b69c-60d2c5fb8791",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T02:40:12.958695Z",
     "iopub.status.busy": "2023-03-29T02:40:12.958091Z",
     "iopub.status.idle": "2023-03-29T02:40:12.970907Z",
     "shell.execute_reply": "2023-03-29T02:40:12.969813Z",
     "shell.execute_reply.started": "2023-03-29T02:40:12.958638Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:782: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n"
     ]
    }
   ],
   "source": [
    "# xgbr model\n",
    "xgbr = xgb.XGBRegressor()\n",
    "xgbr.load_model('result/outliers+w+h/size/acc1-0.622_acc3-0.981_XGBR_4inputs_10fold/model.model')\n",
    "y_xgbr = xgbr.predict(x_test)\n",
    "y_xgbr = np.round(y_xgbr * 2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d93d0c49-938f-4a57-aa68-1085877cf20a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T02:54:29.270184Z",
     "iopub.status.busy": "2023-03-29T02:54:29.269611Z",
     "iopub.status.idle": "2023-03-29T02:54:29.287487Z",
     "shell.execute_reply": "2023-03-29T02:54:29.286632Z",
     "shell.execute_reply.started": "2023-03-29T02:54:29.270130Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###newborns###\n",
      "acc: 0.610\n",
      "acc(+-0.5mm): 0.982\n",
      "\n",
      "###infants###\n",
      "acc: 0.666\n",
      "acc(+-0.5mm): 0.984\n",
      "\n",
      "###the others (>1yrs)###\n",
      "acc: 0.608\n",
      "acc(+-0.5mm): 0.981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc1s, acc3s = [], []\n",
    "\n",
    "# neonate\n",
    "sub_mask = x_test[:,0]<1/12\n",
    "\n",
    "y_pred = y_xgbr[sub_mask]\n",
    "Y_test = y_test[sub_mask]\n",
    "\n",
    "# accuracy\n",
    "acc11 = (y_pred==Y_test)\n",
    "acc13 = ((y_pred >= Y_test-0.5) & (y_pred <= Y_test+0.5))\n",
    "\n",
    "print('###newborns###')\n",
    "print(f'acc: {np.mean(acc11):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(acc13):.3f}\\n')\n",
    "\n",
    "\n",
    "# infant\n",
    "sub_mask = (x_test[:,0]<1)&(x_test[:,0]>1/12)\n",
    "\n",
    "y_pred = y_xgbr[sub_mask]\n",
    "Y_test = y_test[sub_mask]\n",
    "\n",
    "# accuracy\n",
    "acc21 = (y_pred==Y_test)\n",
    "acc23 = ((y_pred >= Y_test-0.5) & (y_pred <= Y_test+0.5))\n",
    "print('###infants###')\n",
    "print(f'acc: {np.mean(acc21):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(acc23):.3f}\\n')\n",
    "\n",
    "\n",
    "# the others\n",
    "sub_mask = x_test[:,0]>1\n",
    "\n",
    "y_pred = y_xgbr[sub_mask]\n",
    "Y_test = y_test[sub_mask]\n",
    "\n",
    "# accuracy\n",
    "acc31 = (y_pred==Y_test)\n",
    "acc33 = ((y_pred >= Y_test-0.5) & (y_pred <= Y_test+0.5))\n",
    "print('###the others (>1yrs)###')\n",
    "print(f'acc: {np.mean(acc31):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(acc33):.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a10427dd-b4c8-4dd3-9255-2b45a3c401fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T12:44:57.054999Z",
     "iopub.status.busy": "2023-05-10T12:44:57.054499Z",
     "iopub.status.idle": "2023-05-10T12:45:02.187837Z",
     "shell.execute_reply": "2023-05-10T12:45:02.186383Z",
     "shell.execute_reply.started": "2023-05-10T12:44:57.054940Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###newborns###\n",
      "acc: 0.609+-0.066 -> 0.544 ~ 0.675\n",
      "acc(+-0.5mm): 0.983+-0.017 -> 0.966 ~ 1.000 \n",
      "\n",
      "###infants (1/12mo<age<=1yr)###\n",
      "acc: 0.665+-0.023 -> 0.642 ~ 0.688\n",
      "acc(+-0.5mm): 0.984+-0.006 -> 0.978 ~ 0.990 \n",
      "\n",
      "###others(age>1yr)###\n",
      "acc: 0.608+-0.013 -> 0.595 ~ 0.621\n",
      "acc(+-0.5mm): 0.981+-0.004 -> 0.977 ~ 0.984 \n"
     ]
    }
   ],
   "source": [
    "acc1s, acc3s = [], []\n",
    "\n",
    "# neonate\n",
    "sub_mask = x_test[:,0]<1/12\n",
    "x1_test = x_test[sub_mask]\n",
    "y1_test = y_test[sub_mask]\n",
    "\n",
    "y1_xgbrs, y2_xgbrs = [], []\n",
    "for i in range(200):\n",
    "    \n",
    "    mask = np.array([random.randint(0,1)==1 for j in range(len(x1_test))])\n",
    "    X1_test = x1_test[mask]\n",
    "    Y_test = y1_test[mask]\n",
    "    \n",
    "    y_xgbr = xgbr.predict(X1_test)\n",
    "    y_xgbr = np.round(y_xgbr * 2) / 2\n",
    "    y1_xgbrs.append(np.mean(y_xgbr == Y_test))\n",
    "    y2_xgbrs.append(np.mean((y_xgbr >= Y_test-0.5) & (y_xgbr <= Y_test+0.5)))\n",
    "    \n",
    "print('###newborns###')\n",
    "print(f'acc: {np.mean(y1_xgbrs):.3f}+-{1.96*np.std(y1_xgbrs):.3f} -> {np.mean(y1_xgbrs)-1.96*np.std(y1_xgbrs):.3f} ~ {np.mean(y1_xgbrs)+1.96*np.std(y1_xgbrs):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(y2_xgbrs):.3f}+-{1.96*np.std(y2_xgbrs):.3f} -> {np.mean(y2_xgbrs)-1.96*np.std(y2_xgbrs):.3f} ~ {np.mean(y2_xgbrs)+1.96*np.std(y2_xgbrs):.3f} ')\n",
    "\n",
    "\n",
    "# infant\n",
    "sub_mask = (x_test[:,0]<=1)&(x_test[:,0]>1/12)\n",
    "x1_test = x_test[sub_mask]\n",
    "y1_test = y_test[sub_mask]\n",
    "\n",
    "y1_xgbrs, y2_xgbrs = [], []\n",
    "for i in range(200):\n",
    "    \n",
    "    mask = np.array([random.randint(0,1)==1 for j in range(len(x1_test))])\n",
    "    X1_test = x1_test[mask]\n",
    "    Y_test = y1_test[mask]\n",
    "    \n",
    "    y_xgbr = xgbr.predict(X1_test)\n",
    "    y_xgbr = np.round(y_xgbr * 2) / 2\n",
    "    y1_xgbrs.append(np.mean(y_xgbr == Y_test))\n",
    "    y2_xgbrs.append(np.mean((y_xgbr >= Y_test-0.5) & (y_xgbr <= Y_test+0.5)))\n",
    "    \n",
    "print('\\n###infants (1/12mo<age<=1yr)###')\n",
    "print(f'acc: {np.mean(y1_xgbrs):.3f}+-{1.96*np.std(y1_xgbrs):.3f} -> {np.mean(y1_xgbrs)-1.96*np.std(y1_xgbrs):.3f} ~ {np.mean(y1_xgbrs)+1.96*np.std(y1_xgbrs):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(y2_xgbrs):.3f}+-{1.96*np.std(y2_xgbrs):.3f} -> {np.mean(y2_xgbrs)-1.96*np.std(y2_xgbrs):.3f} ~ {np.mean(y2_xgbrs)+1.96*np.std(y2_xgbrs):.3f} ')\n",
    "\n",
    "\n",
    "# the others\n",
    "sub_mask = x_test[:,0]>1\n",
    "x1_test = x_test[sub_mask]\n",
    "y1_test = y_test[sub_mask]\n",
    "\n",
    "y1_xgbrs, y2_xgbrs = [], []\n",
    "for i in range(200):\n",
    "    \n",
    "    mask = np.array([random.randint(0,1)==1 for j in range(len(x1_test))])\n",
    "    X1_test = x1_test[mask]\n",
    "    Y_test = y1_test[mask]\n",
    "    \n",
    "    y_xgbr = xgbr.predict(X1_test)\n",
    "    y_xgbr = np.round(y_xgbr * 2) / 2\n",
    "    y1_xgbrs.append(np.mean(y_xgbr == Y_test))\n",
    "    y2_xgbrs.append(np.mean((y_xgbr >= Y_test-0.5) & (y_xgbr <= Y_test+0.5)))\n",
    "    \n",
    "print('\\n###others(age>1yr)###')\n",
    "print(f'acc: {np.mean(y1_xgbrs):.3f}+-{1.96*np.std(y1_xgbrs):.3f} -> {np.mean(y1_xgbrs)-1.96*np.std(y1_xgbrs):.3f} ~ {np.mean(y1_xgbrs)+1.96*np.std(y1_xgbrs):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(y2_xgbrs):.3f}+-{1.96*np.std(y2_xgbrs):.3f} -> {np.mean(y2_xgbrs)-1.96*np.std(y2_xgbrs):.3f} ~ {np.mean(y2_xgbrs)+1.96*np.std(y2_xgbrs):.3f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6a75082-eb50-412a-a842-fbfeecf60acb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T02:54:32.950765Z",
     "iopub.status.busy": "2023-03-29T02:54:32.950269Z",
     "iopub.status.idle": "2023-03-29T02:54:32.972648Z",
     "shell.execute_reply": "2023-03-29T02:54:32.971825Z",
     "shell.execute_reply.started": "2023-03-29T02:54:32.950714Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###Comparison of traditional formula and gbrt for size accuracy1###\n",
      "contingency table:\n",
      "   neonate  infant  children\n",
      "1      139     990      2719\n",
      "0       89     497      1756\n",
      "Chi square: 16.203323547070557\n",
      "P-value: 0.00030303514365056855\n",
      "\n",
      "###Comparison of xgbr vs lr for size accuracy3###\n",
      "contingency table:\n",
      "   neonate  infant  children\n",
      "1      224    1463      4388\n",
      "0        4      24        87\n",
      "Chi square: 0.6811027516241357\n",
      "P-value: 0.7113779779975373\n"
     ]
    }
   ],
   "source": [
    "obs1 = pd.DataFrame({'neonate': [np.sum(acc11), np.sum(~acc11)], 'infant': [np.sum(acc21),np.sum(~acc21)], 'children': [np.sum(acc31),np.sum(~acc31)]})\n",
    "obs1.index = ['1', '0']\n",
    "\n",
    "chiresult = chi2_contingency(obs1, correction=False)\n",
    "print('\\n###Comparison of traditional formula and gbrt for size accuracy1###')\n",
    "print(f'contingency table:\\n{obs1}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))\n",
    "\n",
    "\n",
    "obs2 = pd.DataFrame({'neonate': [np.sum(acc13), np.sum(~acc13)], 'infant': [np.sum(acc23),np.sum(~acc23)], 'children': [np.sum(acc33),np.sum(~acc33)]})\n",
    "obs2.index = ['1', '0']\n",
    "\n",
    "\n",
    "chiresult = chi2_contingency(obs2, correction=False)\n",
    "print('\\n###Comparison of xgbr vs lr for size accuracy3###')\n",
    "print(f'contingency table:\\n{obs2}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b39343e6-9830-4e3d-bef6-0c8b3bfac1bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T05:21:47.811205Z",
     "iopub.status.busy": "2023-03-29T05:21:47.810698Z",
     "iopub.status.idle": "2023-03-29T05:21:47.844495Z",
     "shell.execute_reply": "2023-03-29T05:21:47.843718Z",
     "shell.execute_reply.started": "2023-03-29T05:21:47.811149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Test Multiple Comparison ttest_ind \n",
       "FWER=0.05 method=bonf\n",
       "alphacSidak=0.02, alphacBonf=0.017</caption>\n",
       "<tr>\n",
       "   <th>group1</th>  <th>group2</th>   <th>stat</th>    <th>pval</th>  <th>pval_corr</th> <th>reject</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>children</td> <td>infant</td>  <td>-4.013</td>  <td>0.0001</td>  <td>0.0002</td>    <td>True</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>children</td> <td>neonate</td> <td>-0.0619</td> <td>0.9507</td>    <td>1.0</td>     <td>False</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>infant</td>  <td>neonate</td> <td>1.6641</td>  <td>0.0963</td>  <td>0.2888</td>    <td>False</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.sandbox.stats.multicomp import MultiComparison\n",
    "\n",
    "df_size = pd.concat([pd.DataFrame({'size': acc11, 'group': 'neonate'}),pd.DataFrame({'size': acc21, 'group': 'infant'}), pd.DataFrame({'size': acc31, 'group': 'children'})])\n",
    "\n",
    "\n",
    "comp = MultiComparison(df_size['size'], df_size['group'])\n",
    "tbl, a1, a2 = comp.allpairtest(stats.ttest_ind, method= \"bonf\")\n",
    "tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed1f4751-ccc2-4cc0-9d41-a85600666772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T02:57:21.544978Z",
     "iopub.status.busy": "2023-03-29T02:57:21.544433Z",
     "iopub.status.idle": "2023-03-29T02:57:21.560955Z",
     "shell.execute_reply": "2023-03-29T02:57:21.560113Z",
     "shell.execute_reply.started": "2023-03-29T02:57:21.544924Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###cuffed###\n",
      "acc: 0.714\n",
      "acc(+-0.5mm): 0.973\n",
      "\n",
      "###uncuffed###\n",
      "acc: 0.585\n",
      "acc(+-0.5mm): 0.985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc1s, acc3s = [], []\n",
    "\n",
    "# cuffed\n",
    "sub_mask = x_test[:,3]==1\n",
    "\n",
    "y_pred = y_xgbr[sub_mask]\n",
    "Y_test = y_test[sub_mask]\n",
    "\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==Y_test)\n",
    "acc3 = np.mean((y_pred >= Y_test-0.5) & (y_pred <= Y_test+0.5))\n",
    "acc41 = (y_pred==Y_test)\n",
    "acc43 = ((y_pred >= Y_test-0.5) & (y_pred <= Y_test+0.5))\n",
    "print('###cuffed###')\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}\\n')\n",
    "\n",
    "\n",
    "# uncuffed\n",
    "sub_mask = x_test[:,3]==0\n",
    "\n",
    "y_pred = y_xgbr[sub_mask]\n",
    "Y_test = y_test[sub_mask]\n",
    "\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==Y_test)\n",
    "acc3 = np.mean((y_pred >= Y_test-0.5) & (y_pred <= Y_test+0.5))\n",
    "acc51 = (y_pred==Y_test)\n",
    "acc53 = ((y_pred >= Y_test-0.5) & (y_pred <= Y_test+0.5))\n",
    "print('###uncuffed###')\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd2e70fd-5322-4a84-bdf0-a0d4bdd62b53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T12:41:06.566681Z",
     "iopub.status.busy": "2023-05-17T12:41:06.566185Z",
     "iopub.status.idle": "2023-05-17T12:41:11.212809Z",
     "shell.execute_reply": "2023-05-17T12:41:11.212090Z",
     "shell.execute_reply.started": "2023-05-17T12:41:06.566621Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###cuffed###\n",
      "acc: 0.714+-0.022 -> 0.692 ~ 0.737\n",
      "acc(+-0.5mm): 0.973+-0.008 -> 0.965 ~ 0.981 \n",
      "\n",
      "###uncuffed###\n",
      "acc: 0.585+-0.014 -> 0.571 ~ 0.599\n",
      "acc(+-0.5mm): 0.985+-0.004 -> 0.981 ~ 0.989 \n"
     ]
    }
   ],
   "source": [
    "acc1s, acc3s = [], []\n",
    "\n",
    "# cuffed\n",
    "sub_mask = x_test[:,3]==1\n",
    "x1_test = x_test[sub_mask]\n",
    "y1_test = y_test[sub_mask]\n",
    "\n",
    "y1_xgbrs, y2_xgbrs = [], []\n",
    "for i in range(200):\n",
    "    \n",
    "    mask = np.array([random.randint(0,1)==1 for j in range(len(x1_test))])\n",
    "    X1_test = x1_test[mask]\n",
    "    Y_test = y1_test[mask]\n",
    "    \n",
    "    y_xgbr = xgbr.predict(X1_test)\n",
    "    y_xgbr = np.round(y_xgbr * 2) / 2\n",
    "    y1_xgbrs.append(np.mean(y_xgbr == Y_test))\n",
    "    y2_xgbrs.append(np.mean((y_xgbr >= Y_test-0.5) & (y_xgbr <= Y_test+0.5)))\n",
    "    \n",
    "print('###cuffed###')\n",
    "print(f'acc: {np.mean(y1_xgbrs):.3f}+-{1.96*np.std(y1_xgbrs):.3f} -> {np.mean(y1_xgbrs)-1.96*np.std(y1_xgbrs):.3f} ~ {np.mean(y1_xgbrs)+1.96*np.std(y1_xgbrs):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(y2_xgbrs):.3f}+-{1.96*np.std(y2_xgbrs):.3f} -> {np.mean(y2_xgbrs)-1.96*np.std(y2_xgbrs):.3f} ~ {np.mean(y2_xgbrs)+1.96*np.std(y2_xgbrs):.3f} ')\n",
    "\n",
    "\n",
    "# uncuffed\n",
    "sub_mask = x_test[:,3]==0\n",
    "x1_test = x_test[sub_mask]\n",
    "y1_test = y_test[sub_mask]\n",
    "\n",
    "y1_xgbrs, y2_xgbrs = [], []\n",
    "for i in range(200):\n",
    "    \n",
    "    mask = np.array([random.randint(0,1)==1 for j in range(len(x1_test))])\n",
    "    X1_test = x1_test[mask]\n",
    "    Y_test = y1_test[mask]\n",
    "    \n",
    "    y_xgbr = xgbr.predict(X1_test)\n",
    "    y_xgbr = np.round(y_xgbr * 2) / 2\n",
    "    y1_xgbrs.append(np.mean(y_xgbr == Y_test))\n",
    "    y2_xgbrs.append(np.mean((y_xgbr >= Y_test-0.5) & (y_xgbr <= Y_test+0.5)))\n",
    "    \n",
    "print('\\n###uncuffed###')\n",
    "print(f'acc: {np.mean(y1_xgbrs):.3f}+-{1.96*np.std(y1_xgbrs):.3f} -> {np.mean(y1_xgbrs)-1.96*np.std(y1_xgbrs):.3f} ~ {np.mean(y1_xgbrs)+1.96*np.std(y1_xgbrs):.3f}')\n",
    "print(f'acc(+-0.5mm): {np.mean(y2_xgbrs):.3f}+-{1.96*np.std(y2_xgbrs):.3f} -> {np.mean(y2_xgbrs)-1.96*np.std(y2_xgbrs):.3f} ~ {np.mean(y2_xgbrs)+1.96*np.std(y2_xgbrs):.3f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2228e8c8-e0ad-43f8-b2eb-7217ec37d226",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T02:57:22.862061Z",
     "iopub.status.busy": "2023-03-29T02:57:22.861598Z",
     "iopub.status.idle": "2023-03-29T02:57:22.883020Z",
     "shell.execute_reply": "2023-03-29T02:57:22.882208Z",
     "shell.execute_reply.started": "2023-03-29T02:57:22.862011Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###Comparison of traditional formula and gbrt for size accuracy1###\n",
      "contingency table:\n",
      "   cuffed  uncuffed\n",
      "1    1259      2589\n",
      "0     504      1838\n",
      "Chi square: 89.62915351976146\n",
      "P-value: 2.8725977215711895e-21\n",
      "\n",
      "###Comparison of xgbr vs lr for size accuracy3###\n",
      "contingency table:\n",
      "   cuffed  uncuffed\n",
      "1    1715      4360\n",
      "0      48        67\n",
      "Chi square: 10.111105023199318\n",
      "P-value: 0.0014737857122030375\n"
     ]
    }
   ],
   "source": [
    "obs1 = pd.DataFrame({'cuffed': [np.sum(acc41), np.sum(~acc41)], 'uncuffed': [np.sum(acc51),np.sum(~acc51)]})\n",
    "obs1.index = ['1', '0']\n",
    "\n",
    "chiresult = chi2_contingency(obs1, correction=False)\n",
    "print('\\n###Comparison of traditional formula and gbrt for size accuracy1###')\n",
    "print(f'contingency table:\\n{obs1}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))\n",
    "\n",
    "\n",
    "obs2 = pd.DataFrame({'cuffed': [np.sum(acc43), np.sum(~acc43)], 'uncuffed': [np.sum(acc53),np.sum(~acc53)]})\n",
    "obs2.index = ['1', '0']\n",
    "\n",
    "\n",
    "chiresult = chi2_contingency(obs2, correction=False)\n",
    "print('\\n###Comparison of xgbr vs lr for size accuracy3###')\n",
    "print(f'contingency table:\\n{obs2}')\n",
    "print('Chi square: {}'.format(chiresult[0]))\n",
    "print('P-value: {}'.format(chiresult[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edbc6f0-5a59-4a73-ab3f-1322b4427643",
   "metadata": {},
   "source": [
    "# SHAP\n",
    "* [SHAP Value 계산 및 시각화 결과 해석](https://zephyrus1111.tistory.com/423)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d85b63c-75ad-4cd2-9452-8259a49fbc84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-04T17:26:19.853841Z",
     "iopub.status.busy": "2023-06-04T17:26:19.853393Z",
     "iopub.status.idle": "2023-06-04T17:26:20.056455Z",
     "shell.execute_reply": "2023-06-04T17:26:20.055714Z",
     "shell.execute_reply.started": "2023-06-04T17:26:19.853782Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align='center'><img src='data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABkAAAAWCAYAAAA1vze2AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAdxJREFUeNq0Vt1Rg0AQJjcpgBJiBWIFkgoMFYhPPAIVECogPuYpdJBYgXQQrMCUkA50V7+d2ZwXuXPGm9khHLu3f9+3l1nkWNvtNqfHLgpfQ1EUS3tz5nAQ0+NIsiAZSc6eDlI8M3J00B/mDuUKDk6kfOebAgW3pkdD0pFcODGW4gKKvOrAUm04MA4QDt1OEIXU9hDigfS5rC1eS5T90gltck1Xrizo257kgySZcNRzgCSxCvgiE9nckPJo2b/B2AcEkk2OwL8bD8gmOKR1GPbaCUqxEgTq0tLvgb6zfo7+DgYGkkWL2tqLDV4RSITfbHPPfJKIrWz4nJQTMPAWA7IbD6imcNaDeDfgk+4No+wZr40BL3g9eQJJCFqRQ54KiSt72lsLpE3o3MCBSxDuq4yOckU2hKXRuwBH3OyMR4g1UpyTYw6mlmBqNdUXRM1NfyF5EPI6JkcpIDBIX8jX6DR/6ckAZJ0wEAdLR8DEk6OfC1Pp8BKo6TQIwPJbvJ6toK5lmuvJoRtfK6Ym1iRYIarRo2UyYHvRN5qpakR3yoizWrouoyuXXQqI185LCw07op5ZyCRGL99h24InP0e9xdQukEKVmhzrqZuRIfwISB//cP3Wk3f8f/yR+BRgAHu00HjLcEQBAAAAAElFTkSuQmCC' /></div><script charset='utf-8'>!function(t){function e(r){if(n[r])return n[r].exports;var i=n[r]={i:r,l:!1,exports:{}};return t[r].call(i.exports,i,i.exports,e),i.l=!0,i.exports}var n={};return e.m=t,e.c=n,e.i=function(t){return t},e.d=function(t,n,r){e.o(t,n)||Object.defineProperty(t,n,{configurable:!1,enumerable:!0,get:r})},e.n=function(t){var n=t&&t.__esModule?function(){return t.default}:function(){return t};return e.d(n,\"a\",n),n},e.o=function(t,e){return Object.prototype.hasOwnProperty.call(t,e)},e.p=\"\",e(e.s=410)}([function(t,e,n){\"use strict\";function r(t,e,n,r,o,a,u,c){if(i(e),!t){var s;if(void 0===e)s=new Error(\"Minified exception occurred; use the non-minified dev environment for the full error message and additional helpful warnings.\");else{var l=[n,r,o,a,u,c],f=0;s=new Error(e.replace(/%s/g,function(){return l[f++]})),s.name=\"Invariant Violation\"}throw s.framesToPop=1,s}}var i=function(t){};t.exports=r},function(t,e,n){\"use strict\";var r=n(8),i=r;t.exports=i},function(t,e,n){\"use strict\";function r(t){for(var e=arguments.length-1,n=\"Minified React error #\"+t+\"; visit http://facebook.github.io/react/docs/error-decoder.html?invariant=\"+t,r=0;r<e;r++)n+=\"&args[]=\"+encodeURIComponent(arguments[r+1]);n+=\" for the full message or use the non-minified dev environment for full errors and additional helpful warnings.\";var i=new Error(n);throw i.name=\"Invariant Violation\",i.framesToPop=1,i}t.exports=r},function(t,e,n){\"use strict\";function r(t){if(null===t||void 0===t)throw new TypeError(\"Object.assign cannot be called with null or undefined\");return Object(t)}function i(){try{if(!Object.assign)return!1;var t=new String(\"abc\");if(t[5]=\"de\",\"5\"===Object.getOwnPropertyNames(t)[0])return!1;for(var e={},n=0;n<10;n++)e[\"_\"+String.fromCharCode(n)]=n;var r=Object.getOwnPropertyNames(e).map(function(t){return e[t]});if(\"0123456789\"!==r.join(\"\"))return!1;var i={};return\"abcdefghijklmnopqrst\".split(\"\").forEach(function(t){i[t]=t}),\"abcdefghijklmnopqrst\"===Object.keys(Object.assign({},i)).join(\"\")}catch(t){return!1}}/*\n",
       "object-assign\n",
       "(c) Sindre Sorhus\n",
       "@license MIT\n",
       "*/\n",
       "var o=Object.getOwnPropertySymbols,a=Object.prototype.hasOwnProperty,u=Object.prototype.propertyIsEnumerable;t.exports=i()?Object.assign:function(t,e){for(var n,i,c=r(t),s=1;s<arguments.length;s++){n=Object(arguments[s]);for(var l in n)a.call(n,l)&&(c[l]=n[l]);if(o){i=o(n);for(var f=0;f<i.length;f++)u.call(n,i[f])&&(c[i[f]]=n[i[f]])}}return c}},function(t,e,n){\"use strict\";function r(t,e){return 1===t.nodeType&&t.getAttribute(d)===String(e)||8===t.nodeType&&t.nodeValue===\" react-text: \"+e+\" \"||8===t.nodeType&&t.nodeValue===\" react-empty: \"+e+\" \"}function i(t){for(var e;e=t._renderedComponent;)t=e;return t}function o(t,e){var n=i(t);n._hostNode=e,e[g]=n}function a(t){var e=t._hostNode;e&&(delete e[g],t._hostNode=null)}function u(t,e){if(!(t._flags&v.hasCachedChildNodes)){var n=t._renderedChildren,a=e.firstChild;t:for(var u in n)if(n.hasOwnProperty(u)){var c=n[u],s=i(c)._domID;if(0!==s){for(;null!==a;a=a.nextSibling)if(r(a,s)){o(c,a);continue t}f(\"32\",s)}}t._flags|=v.hasCachedChildNodes}}function c(t){if(t[g])return t[g];for(var e=[];!t[g];){if(e.push(t),!t.parentNode)return null;t=t.parentNode}for(var n,r;t&&(r=t[g]);t=e.pop())n=r,e.length&&u(r,t);return n}function s(t){var e=c(t);return null!=e&&e._hostNode===t?e:null}function l(t){if(void 0===t._hostNode?f(\"33\"):void 0,t._hostNode)return t._hostNode;for(var e=[];!t._hostNode;)e.push(t),t._hostParent?void 0:f(\"34\"),t=t._hostParent;for(;e.length;t=e.pop())u(t,t._hostNode);return t._hostNode}var f=n(2),p=n(21),h=n(157),d=(n(0),p.ID_ATTRIBUTE_NAME),v=h,g=\"__reactInternalInstance$\"+Math.random().toString(36).slice(2),m={getClosestInstanceFromNode:c,getInstanceFromNode:s,getNodeFromInstance:l,precacheChildNodes:u,precacheNode:o,uncacheNode:a};t.exports=m},function(t,e,n){\"use strict\";function r(t,e,n,a){function u(e){return t(e=new Date(+e)),e}return u.floor=u,u.ceil=function(n){return t(n=new Date(n-1)),e(n,1),t(n),n},u.round=function(t){var e=u(t),n=u.ceil(t);return t-e<n-t?e:n},u.offset=function(t,n){return e(t=new Date(+t),null==n?1:Math.floor(n)),t},u.range=function(n,r,i){var o=[];if(n=u.ceil(n),i=null==i?1:Math.floor(i),!(n<r&&i>0))return o;do o.push(new Date(+n));while(e(n,i),t(n),n<r);return o},u.filter=function(n){return r(function(e){if(e>=e)for(;t(e),!n(e);)e.setTime(e-1)},function(t,r){if(t>=t)for(;--r>=0;)for(;e(t,1),!n(t););})},n&&(u.count=function(e,r){return i.setTime(+e),o.setTime(+r),t(i),t(o),Math.floor(n(i,o))},u.every=function(t){return t=Math.floor(t),isFinite(t)&&t>0?t>1?u.filter(a?function(e){return a(e)%t===0}:function(e){return u.count(0,e)%t===0}):u:null}),u}e.a=r;var i=new Date,o=new Date},function(t,e,n){\"use strict\";var r=!(\"undefined\"==typeof window||!window.document||!window.document.createElement),i={canUseDOM:r,canUseWorkers:\"undefined\"!=typeof Worker,canUseEventListeners:r&&!(!window.addEventListener&&!window.attachEvent),canUseViewport:r&&!!window.screen,isInWorker:!r};t.exports=i},function(t,e,n){\"use strict\";function r(t,e){this._groups=t,this._parents=e}function i(){return new r([[document.documentElement]],D)}var o=n(272),a=n(273),u=n(261),c=n(255),s=n(131),l=n(260),f=n(265),p=n(268),h=n(275),d=n(253),v=n(267),g=n(266),m=n(274),y=n(259),_=n(258),b=n(252),x=n(276),w=n(269),C=n(254),M=n(277),k=n(262),E=n(270),T=n(264),S=n(251),P=n(263),N=n(271),A=n(256),O=n(70),I=n(257);n.d(e,\"c\",function(){return D}),e.b=r;var D=[null];r.prototype=i.prototype={constructor:r,select:o.a,selectAll:a.a,filter:u.a,data:c.a,enter:s.a,exit:l.a,merge:f.a,order:p.a,sort:h.a,call:d.a,nodes:v.a,node:g.a,size:m.a,empty:y.a,each:_.a,attr:b.a,style:x.a,property:w.a,classed:C.a,text:M.a,html:k.a,raise:E.a,lower:T.a,append:S.a,insert:P.a,remove:N.a,datum:A.a,on:O.c,dispatch:I.a},e.a=i},function(t,e,n){\"use strict\";function r(t){return function(){return t}}var i=function(){};i.thatReturns=r,i.thatReturnsFalse=r(!1),i.thatReturnsTrue=r(!0),i.thatReturnsNull=r(null),i.thatReturnsThis=function(){return this},i.thatReturnsArgument=function(t){return t},t.exports=i},function(t,e,n){\"use strict\";var r=null;t.exports={debugTool:r}},function(t,e,n){\"use strict\";Object.defineProperty(e,\"__esModule\",{value:!0});var r=n(59);n.d(e,\"color\",function(){return r.a}),n.d(e,\"rgb\",function(){return r.b}),n.d(e,\"hsl\",function(){return r.c});var i=n(210);n.d(e,\"lab\",function(){return i.a}),n.d(e,\"hcl\",function(){return i.b});var o=n(209);n.d(e,\"cubehelix\",function(){return o.a})},function(t,e,n){\"use strict\";function r(){T.ReactReconcileTransaction&&x?void 0:l(\"123\")}function i(){this.reinitializeTransaction(),this.dirtyComponentsLength=null,this.callbackQueue=p.getPooled(),this.reconcileTransaction=T.ReactReconcileTransaction.getPooled(!0)}function o(t,e,n,i,o,a){return r(),x.batchedUpdates(t,e,n,i,o,a)}function a(t,e){return t._mountOrder-e._mountOrder}function u(t){var e=t.dirtyComponentsLength;e!==m.length?l(\"124\",e,m.length):void 0,m.sort(a),y++;for(var n=0;n<e;n++){var r=m[n],i=r._pendingCallbacks;r._pendingCallbacks=null;var o;if(d.logTopLevelRenders){var u=r;r._currentElement.type.isReactTopLevelWrapper&&(u=r._renderedComponent),o=\"React update: \"+u.getName(),console.time(o)}if(v.performUpdateIfNecessary(r,t.reconcileTransaction,y),o&&console.timeEnd(o),i)for(var c=0;c<i.length;c++)t.callbackQueue.enqueue(i[c],r.getPublicInstance())}}function c(t){return r(),x.isBatchingUpdates?(m.push(t),void(null==t._updateBatchNumber&&(t._updateBatchNumber=y+1))):void x.batchedUpdates(c,t)}function s(t,e){x.isBatchingUpdates?void 0:l(\"125\"),_.enqueue(t,e),b=!0}var l=n(2),f=n(3),p=n(155),h=n(17),d=n(160),v=n(24),g=n(53),m=(n(0),[]),y=0,_=p.getPooled(),b=!1,x=null,w={initialize:function(){this.dirtyComponentsLength=m.length},close:function(){this.dirtyComponentsLength!==m.length?(m.splice(0,this.dirtyComponentsLength),k()):m.length=0}},C={initialize:function(){this.callbackQueue.reset()},close:function(){this.callbackQueue.notifyAll()}},M=[w,C];f(i.prototype,g,{getTransactionWrappers:function(){return M},destructor:function(){this.dirtyComponentsLength=null,p.release(this.callbackQueue),this.callbackQueue=null,T.ReactReconcileTransaction.release(this.reconcileTransaction),this.reconcileTransaction=null},perform:function(t,e,n){return g.perform.call(this,this.reconcileTransaction.perform,this.reconcileTransaction,t,e,n)}}),h.addPoolingTo(i);var k=function(){for(;m.length||b;){if(m.length){var t=i.getPooled();t.perform(u,null,t),i.release(t)}if(b){b=!1;var e=_;_=p.getPooled(),e.notifyAll(),p.release(e)}}},E={injectReconcileTransaction:function(t){t?void 0:l(\"126\"),T.ReactReconcileTransaction=t},injectBatchingStrategy:function(t){t?void 0:l(\"127\"),\"function\"!=typeof t.batchedUpdates?l(\"128\"):void 0,\"boolean\"!=typeof t.isBatchingUpdates?l(\"129\"):void 0,x=t}},T={ReactReconcileTransaction:null,batchedUpdates:o,enqueueUpdate:c,flushBatchedUpdates:k,injection:E,asap:s};t.exports=T},function(t,e,n){\"use strict\";var r=n(102);n.d(e,\"c\",function(){return r.a});var i=n(18);n.d(e,\"f\",function(){return i.a});var o=n(103);n.d(e,\"d\",function(){return o.a});var a=(n(185),n(104),n(105),n(186),n(197),n(198),n(108),n(188),n(189),n(190),n(191),n(106),n(192),n(193),n(57));n.d(e,\"e\",function(){return a.a});var u=n(107);n.d(e,\"g\",function(){return u.a});var c=(n(194),n(195),n(196),n(109));n.d(e,\"a\",function(){return c.a}),n.d(e,\"b\",function(){return c.b});n(110),n(111),n(199)},function(t,e,n){\"use strict\";n.d(e,\"e\",function(){return r}),n.d(e,\"d\",function(){return i}),n.d(e,\"c\",function(){return o}),n.d(e,\"b\",function(){return a}),n.d(e,\"a\",function(){return u});var r=1e3,i=6e4,o=36e5,a=864e5,u=6048e5},function(t,e,n){\"use strict\";function r(t,e,n,r){this.dispatchConfig=t,this._targetInst=e,this.nativeEvent=n;var i=this.constructor.Interface;for(var o in i)if(i.hasOwnProperty(o)){var u=i[o];u?this[o]=u(n):\"target\"===o?this.target=r:this[o]=n[o]}var c=null!=n.defaultPrevented?n.defaultPrevented:n.returnValue===!1;return c?this.isDefaultPrevented=a.thatReturnsTrue:this.isDefaultPrevented=a.thatReturnsFalse,this.isPropagationStopped=a.thatReturnsFalse,this}var i=n(3),o=n(17),a=n(8),u=(n(1),\"function\"==typeof Proxy,[\"dispatchConfig\",\"_targetInst\",\"nativeEvent\",\"isDefaultPrevented\",\"isPropagationStopped\",\"_dispatchListeners\",\"_dispatchInstances\"]),c={type:null,target:null,currentTarget:a.thatReturnsNull,eventPhase:null,bubbles:null,cancelable:null,timeStamp:function(t){return t.timeStamp||Date.now()},defaultPrevented:null,isTrusted:null};i(r.prototype,{preventDefault:function(){this.defaultPrevented=!0;var t=this.nativeEvent;t&&(t.preventDefault?t.preventDefault():\"unknown\"!=typeof t.returnValue&&(t.returnValue=!1),this.isDefaultPrevented=a.thatReturnsTrue)},stopPropagation:function(){var t=this.nativeEvent;t&&(t.stopPropagation?t.stopPropagation():\"unknown\"!=typeof t.cancelBubble&&(t.cancelBubble=!0),this.isPropagationStopped=a.thatReturnsTrue)},persist:function(){this.isPersistent=a.thatReturnsTrue},isPersistent:a.thatReturnsFalse,destructor:function(){var t=this.constructor.Interface;for(var e in t)this[e]=null;for(var n=0;n<u.length;n++)this[u[n]]=null}}),r.Interface=c,r.augmentClass=function(t,e){var n=this,r=function(){};r.prototype=n.prototype;var a=new r;i(a,t.prototype),t.prototype=a,t.prototype.constructor=t,t.Interface=i({},n.Interface,e),t.augmentClass=n.augmentClass,o.addPoolingTo(t,o.fourArgumentPooler)},o.addPoolingTo(r,o.fourArgumentPooler),t.exports=r},function(t,e,n){\"use strict\";var r={current:null};t.exports=r},function(t,e,n){\"use strict\";n.d(e,\"a\",function(){return i}),n.d(e,\"b\",function(){return o});var r=Array.prototype,i=r.map,o=r.slice},function(t,e,n){\"use strict\";var r=n(2),i=(n(0),function(t){var e=this;if(e.instancePool.length){var n=e.instancePool.pop();return e.call(n,t),n}return new e(t)}),o=function(t,e){var n=this;if(n.instancePool.length){var r=n.instancePool.pop();return n.call(r,t,e),r}return new n(t,e)},a=function(t,e,n){var r=this;if(r.instancePool.length){var i=r.instancePool.pop();return r.call(i,t,e,n),i}return new r(t,e,n)},u=function(t,e,n,r){var i=this;if(i.instancePool.length){var o=i.instancePool.pop();return i.call(o,t,e,n,r),o}return new i(t,e,n,r)},c=function(t){var e=this;t instanceof e?void 0:r(\"25\"),t.destructor(),e.instancePool.length<e.poolSize&&e.instancePool.push(t)},s=10,l=i,f=function(t,e){var n=t;return n.instancePool=[],n.getPooled=e||l,n.poolSize||(n.poolSize=s),n.release=c,n},p={addPoolingTo:f,oneArgumentPooler:i,twoArgumentPooler:o,threeArgumentPooler:a,fourArgumentPooler:u};t.exports=p},function(t,e,n){\"use strict\";e.a=function(t,e){return t<e?-1:t>e?1:t>=e?0:NaN}},function(t,e,n){\"use strict\";e.a=function(t){return function(){return t}}},function(t,e,n){\"use strict\";function r(t){if(g){var e=t.node,n=t.children;if(n.length)for(var r=0;r<n.length;r++)m(e,n[r],null);else null!=t.html?f(e,t.html):null!=t.text&&h(e,t.text)}}function i(t,e){t.parentNode.replaceChild(e.node,t),r(e)}function o(t,e){g?t.children.push(e):t.node.appendChild(e.node)}function a(t,e){g?t.html=e:f(t.node,e)}function u(t,e){g?t.text=e:h(t.node,e)}function c(){return this.node.nodeName}function s(t){return{node:t,children:[],html:null,text:null,toString:c}}var l=n(82),f=n(55),p=n(90),h=n(171),d=1,v=11,g=\"undefined\"!=typeof document&&\"number\"==typeof document.documentMode||\"undefined\"!=typeof navigator&&\"string\"==typeof navigator.userAgent&&/\\bEdge\\/\\d/.test(navigator.userAgent),m=p(function(t,e,n){e.node.nodeType===v||e.node.nodeType===d&&\"object\"===e.node.nodeName.toLowerCase()&&(null==e.node.namespaceURI||e.node.namespaceURI===l.html)?(r(e),t.insertBefore(e.node,n)):(t.insertBefore(e.node,n),r(e))});s.insertTreeBefore=m,s.replaceChildWithTree=i,s.queueChild=o,s.queueHTML=a,s.queueText=u,t.exports=s},function(t,e,n){\"use strict\";function r(t,e){return(t&e)===e}var i=n(2),o=(n(0),{MUST_USE_PROPERTY:1,HAS_BOOLEAN_VALUE:4,HAS_NUMERIC_VALUE:8,HAS_POSITIVE_NUMERIC_VALUE:24,HAS_OVERLOADED_BOOLEAN_VALUE:32,injectDOMPropertyConfig:function(t){var e=o,n=t.Properties||{},a=t.DOMAttributeNamespaces||{},c=t.DOMAttributeNames||{},s=t.DOMPropertyNames||{},l=t.DOMMutationMethods||{};t.isCustomAttribute&&u._isCustomAttributeFunctions.push(t.isCustomAttribute);for(var f in n){u.properties.hasOwnProperty(f)?i(\"48\",f):void 0;var p=f.toLowerCase(),h=n[f],d={attributeName:p,attributeNamespace:null,propertyName:f,mutationMethod:null,mustUseProperty:r(h,e.MUST_USE_PROPERTY),hasBooleanValue:r(h,e.HAS_BOOLEAN_VALUE),hasNumericValue:r(h,e.HAS_NUMERIC_VALUE),hasPositiveNumericValue:r(h,e.HAS_POSITIVE_NUMERIC_VALUE),hasOverloadedBooleanValue:r(h,e.HAS_OVERLOADED_BOOLEAN_VALUE)};if(d.hasBooleanValue+d.hasNumericValue+d.hasOverloadedBooleanValue<=1?void 0:i(\"50\",f),c.hasOwnProperty(f)){var v=c[f];d.attributeName=v}a.hasOwnProperty(f)&&(d.attributeNamespace=a[f]),s.hasOwnProperty(f)&&(d.propertyName=s[f]),l.hasOwnProperty(f)&&(d.mutationMethod=l[f]),u.properties[f]=d}}}),a=\":A-Z_a-z\\\\u00C0-\\\\u00D6\\\\u00D8-\\\\u00F6\\\\u00F8-\\\\u02FF\\\\u0370-\\\\u037D\\\\u037F-\\\\u1FFF\\\\u200C-\\\\u200D\\\\u2070-\\\\u218F\\\\u2C00-\\\\u2FEF\\\\u3001-\\\\uD7FF\\\\uF900-\\\\uFDCF\\\\uFDF0-\\\\uFFFD\",u={ID_ATTRIBUTE_NAME:\"data-reactid\",ROOT_ATTRIBUTE_NAME:\"data-reactroot\",ATTRIBUTE_NAME_START_CHAR:a,ATTRIBUTE_NAME_CHAR:a+\"\\\\-.0-9\\\\u00B7\\\\u0300-\\\\u036F\\\\u203F-\\\\u2040\",properties:{},getPossibleStandardName:null,_isCustomAttributeFunctions:[],isCustomAttribute:function(t){for(var e=0;e<u._isCustomAttributeFunctions.length;e++){var n=u._isCustomAttributeFunctions[e];if(n(t))return!0}return!1},injection:o};t.exports=u},function(t,e,n){\"use strict\";function r(t){return\"button\"===t||\"input\"===t||\"select\"===t||\"textarea\"===t}function i(t,e,n){switch(t){case\"onClick\":case\"onClickCapture\":case\"onDoubleClick\":case\"onDoubleClickCapture\":case\"onMouseDown\":case\"onMouseDownCapture\":case\"onMouseMove\":case\"onMouseMoveCapture\":case\"onMouseUp\":case\"onMouseUpCapture\":return!(!n.disabled||!r(e));default:return!1}}var o=n(2),a=n(83),u=n(50),c=n(87),s=n(165),l=n(166),f=(n(0),{}),p=null,h=function(t,e){t&&(u.executeDispatchesInOrder(t,e),t.isPersistent()||t.constructor.release(t))},d=function(t){return h(t,!0)},v=function(t){return h(t,!1)},g=function(t){return\".\"+t._rootNodeID},m={injection:{injectEventPluginOrder:a.injectEventPluginOrder,injectEventPluginsByName:a.injectEventPluginsByName},putListener:function(t,e,n){\"function\"!=typeof n?o(\"94\",e,typeof n):void 0;var r=g(t),i=f[e]||(f[e]={});i[r]=n;var u=a.registrationNameModules[e];u&&u.didPutListener&&u.didPutListener(t,e,n)},getListener:function(t,e){var n=f[e];if(i(e,t._currentElement.type,t._currentElement.props))return null;var r=g(t);return n&&n[r]},deleteListener:function(t,e){var n=a.registrationNameModules[e];n&&n.willDeleteListener&&n.willDeleteListener(t,e);var r=f[e];if(r){var i=g(t);delete r[i]}},deleteAllListeners:function(t){var e=g(t);for(var n in f)if(f.hasOwnProperty(n)&&f[n][e]){var r=a.registrationNameModules[n];r&&r.willDeleteListener&&r.willDeleteListener(t,n),delete f[n][e]}},extractEvents:function(t,e,n,r){for(var i,o=a.plugins,u=0;u<o.length;u++){var c=o[u];if(c){var l=c.extractEvents(t,e,n,r);l&&(i=s(i,l))}}return i},enqueueEvents:function(t){t&&(p=s(p,t))},processEventQueue:function(t){var e=p;p=null,t?l(e,d):l(e,v),p?o(\"95\"):void 0,c.rethrowCaughtError()},__purge:function(){f={}},__getListenerBank:function(){return f}};t.exports=m},function(t,e,n){\"use strict\";function r(t,e,n){var r=e.dispatchConfig.phasedRegistrationNames[n];return m(t,r)}function i(t,e,n){var i=r(t,n,e);i&&(n._dispatchListeners=v(n._dispatchListeners,i),n._dispatchInstances=v(n._dispatchInstances,t))}function o(t){t&&t.dispatchConfig.phasedRegistrationNames&&d.traverseTwoPhase(t._targetInst,i,t)}function a(t){if(t&&t.dispatchConfig.phasedRegistrationNames){var e=t._targetInst,n=e?d.getParentInstance(e):null;d.traverseTwoPhase(n,i,t)}}function u(t,e,n){if(n&&n.dispatchConfig.registrationName){var r=n.dispatchConfig.registrationName,i=m(t,r);i&&(n._dispatchListeners=v(n._dispatchListeners,i),n._dispatchInstances=v(n._dispatchInstances,t))}}function c(t){t&&t.dispatchConfig.registrationName&&u(t._targetInst,null,t)}function s(t){g(t,o)}function l(t){g(t,a)}function f(t,e,n,r){d.traverseEnterLeave(n,r,u,t,e)}function p(t){g(t,c)}var h=n(22),d=n(50),v=n(165),g=n(166),m=(n(1),h.getListener),y={accumulateTwoPhaseDispatches:s,accumulateTwoPhaseDispatchesSkipTarget:l,accumulateDirectDispatches:p,accumulateEnterLeaveDispatches:f};t.exports=y},function(t,e,n){\"use strict\";function r(){i.attachRefs(this,this._currentElement)}var i=n(368),o=(n(9),n(1),{mountComponent:function(t,e,n,i,o,a){var u=t.mountComponent(e,n,i,o,a);return t._currentElement&&null!=t._currentElement.ref&&e.getReactMountReady().enqueue(r,t),u},getHostNode:function(t){return t.getHostNode()},unmountComponent:function(t,e){i.detachRefs(t,t._currentElement),t.unmountComponent(e)},receiveComponent:function(t,e,n,o){var a=t._currentElement;if(e!==a||o!==t._context){var u=i.shouldUpdateRefs(a,e);u&&i.detachRefs(t,a),t.receiveComponent(e,n,o),u&&t._currentElement&&null!=t._currentElement.ref&&n.getReactMountReady().enqueue(r,t)}},performUpdateIfNecessary:function(t,e,n){t._updateBatchNumber===n&&t.performUpdateIfNecessary(e)}});t.exports=o},function(t,e,n){\"use strict\";function r(t,e,n,r){return i.call(this,t,e,n,r)}var i=n(14),o=n(93),a={view:function(t){if(t.view)return t.view;var e=o(t);if(e.window===e)return e;var n=e.ownerDocument;return n?n.defaultView||n.parentWindow:window},detail:function(t){return t.detail||0}};i.augmentClass(r,a),t.exports=r},function(t,e,n){\"use strict\";var r=n(3),i=n(401),o=n(97),a=n(406),u=n(402),c=n(403),s=n(27),l=n(404),f=n(407),p=n(408),h=(n(1),s.createElement),d=s.createFactory,v=s.cloneElement,g=r,m={Children:{map:i.map,forEach:i.forEach,count:i.count,toArray:i.toArray,only:p},Component:o,PureComponent:a,createElement:h,cloneElement:v,isValidElement:s.isValidElement,PropTypes:l,createClass:u.createClass,createFactory:d,createMixin:function(t){return t},DOM:c,version:f,__spread:g};t.exports=m},function(t,e,n){\"use strict\";function r(t){return void 0!==t.ref}function i(t){return void 0!==t.key}var o=n(3),a=n(15),u=(n(1),n(176),Object.prototype.hasOwnProperty),c=n(174),s={key:!0,ref:!0,__self:!0,__source:!0},l=function(t,e,n,r,i,o,a){var u={$$typeof:c,type:t,key:e,ref:n,props:a,_owner:o};return u};l.createElement=function(t,e,n){var o,c={},f=null,p=null,h=null,d=null;if(null!=e){r(e)&&(p=e.ref),i(e)&&(f=\"\"+e.key),h=void 0===e.__self?null:e.__self,d=void 0===e.__source?null:e.__source;for(o in e)u.call(e,o)&&!s.hasOwnProperty(o)&&(c[o]=e[o])}var v=arguments.length-2;if(1===v)c.children=n;else if(v>1){for(var g=Array(v),m=0;m<v;m++)g[m]=arguments[m+2];c.children=g}if(t&&t.defaultProps){var y=t.defaultProps;for(o in y)void 0===c[o]&&(c[o]=y[o])}return l(t,f,p,h,d,a.current,c)},l.createFactory=function(t){var e=l.createElement.bind(null,t);return e.type=t,e},l.cloneAndReplaceKey=function(t,e){var n=l(t.type,e,t.ref,t._self,t._source,t._owner,t.props);return n},l.cloneElement=function(t,e,n){var c,f=o({},t.props),p=t.key,h=t.ref,d=t._self,v=t._source,g=t._owner;if(null!=e){r(e)&&(h=e.ref,g=a.current),i(e)&&(p=\"\"+e.key);var m;t.type&&t.type.defaultProps&&(m=t.type.defaultProps);for(c in e)u.call(e,c)&&!s.hasOwnProperty(c)&&(void 0===e[c]&&void 0!==m?f[c]=m[c]:f[c]=e[c])}var y=arguments.length-2;if(1===y)f.children=n;else if(y>1){for(var _=Array(y),b=0;b<y;b++)_[b]=arguments[b+2];f.children=_}return l(t.type,p,h,d,v,g,f)},l.isValidElement=function(t){return\"object\"==typeof t&&null!==t&&t.$$typeof===c},t.exports=l},function(t,e,n){\"use strict\";function r(t){for(var e=arguments.length-1,n=\"Minified React error #\"+t+\"; visit http://facebook.github.io/react/docs/error-decoder.html?invariant=\"+t,r=0;r<e;r++)n+=\"&args[]=\"+encodeURIComponent(arguments[r+1]);n+=\" for the full message or use the non-minified dev environment for full errors and additional helpful warnings.\";var i=new Error(n);throw i.name=\"Invariant Violation\",i.framesToPop=1,i}t.exports=r},function(t,e,n){\"use strict\";e.a=function(t){return null===t?NaN:+t}},function(t,e,n){\"use strict\";Object.defineProperty(e,\"__esModule\",{value:!0});var r=n(211);n.d(e,\"formatDefaultLocale\",function(){return r.a}),n.d(e,\"format\",function(){return r.b}),n.d(e,\"formatPrefix\",function(){return r.c});var i=n(117);n.d(e,\"formatLocale\",function(){return i.a});var o=n(115);n.d(e,\"formatSpecifier\",function(){return o.a});var a=n(215);n.d(e,\"precisionFixed\",function(){return a.a});var u=n(216);n.d(e,\"precisionPrefix\",function(){return u.a});var c=n(217);n.d(e,\"precisionRound\",function(){return c.a})},function(t,e,n){\"use strict\";var r=n(63);n.d(e,\"b\",function(){return r.a});var i=(n(118),n(62),n(119),n(121),n(43));n.d(e,\"a\",function(){return i.a});var o=(n(122),n(223));n.d(e,\"c\",function(){return o.a});var a=(n(124),n(225),n(227),n(123),n(220),n(221),n(219),n(218));n.d(e,\"d\",function(){return a.a});n(222)},function(t,e,n){\"use strict\";function r(t,e){return function(n){return t+n*e}}function i(t,e,n){return t=Math.pow(t,n),e=Math.pow(e,n)-t,n=1/n,function(r){return Math.pow(t+r*e,n)}}function o(t,e){var i=e-t;return i?r(t,i>180||i<-180?i-360*Math.round(i/360):i):n.i(c.a)(isNaN(t)?e:t)}function a(t){return 1===(t=+t)?u:function(e,r){return r-e?i(e,r,t):n.i(c.a)(isNaN(e)?r:e)}}function u(t,e){var i=e-t;return i?r(t,i):n.i(c.a)(isNaN(t)?e:t)}var c=n(120);e.b=o,e.c=a,e.a=u},function(t,e,n){\"use strict\";e.a=function(t){return t.match(/.{6}/g).map(function(t){return\"#\"+t})}},function(t,e,n){\"use strict\";function r(t){var e=t.domain;return t.ticks=function(t){var r=e();return n.i(o.a)(r[0],r[r.length-1],null==t?10:t)},t.tickFormat=function(t,r){return n.i(c.a)(e(),t,r)},t.nice=function(r){var i=e(),a=i.length-1,u=null==r?10:r,c=i[0],s=i[a],l=n.i(o.b)(c,s,u);return l&&(l=n.i(o.b)(Math.floor(c/l)*l,Math.ceil(s/l)*l,u),i[0]=Math.floor(c/l)*l,i[a]=Math.ceil(s/l)*l,e(i)),t},t}function i(){var t=n.i(u.a)(u.b,a.a);return t.copy=function(){return n.i(u.c)(t,i())},r(t)}var o=n(12),a=n(31),u=n(45),c=n(243);e.b=r,e.a=i},function(t,e,n){\"use strict\";n.d(e,\"a\",function(){return r}),n.d(e,\"b\",function(){return i}),n.d(e,\"d\",function(){return o}),n.d(e,\"c\",function(){return a});var r=1e-12,i=Math.PI,o=i/2,a=2*i},function(t,e,n){\"use strict\";e.a=function(t,e){if((r=t.length)>1)for(var n,r,i=1,o=t[e[0]],a=o.length;i<r;++i){n=o,o=t[e[i]];for(var u=0;u<a;++u)o[u][1]+=o[u][0]=isNaN(n[u][1])?n[u][0]:n[u][1]}}},function(t,e,n){\"use strict\";e.a=function(t){for(var e=t.length,n=new Array(e);--e>=0;)n[e]=e;return n}},function(t,e,n){\"use strict\";var r={};t.exports=r},function(t,e,n){(function(t,r){var i;(function(){function o(t,e){return t.set(e[0],e[1]),t}function a(t,e){return t.add(e),t}function u(t,e,n){switch(n.length){case 0:return t.call(e);case 1:return t.call(e,n[0]);case 2:return t.call(e,n[0],n[1]);case 3:return t.call(e,n[0],n[1],n[2])}return t.apply(e,n)}function c(t,e,n,r){for(var i=-1,o=null==t?0:t.length;++i<o;){var a=t[i];e(r,a,n(a),t)}return r}function s(t,e){for(var n=-1,r=null==t?0:t.length;++n<r&&e(t[n],n,t)!==!1;);return t}function l(t,e){for(var n=null==t?0:t.length;n--&&e(t[n],n,t)!==!1;);return t}function f(t,e){for(var n=-1,r=null==t?0:t.length;++n<r;)if(!e(t[n],n,t))return!1;return!0}function p(t,e){for(var n=-1,r=null==t?0:t.length,i=0,o=[];++n<r;){var a=t[n];e(a,n,t)&&(o[i++]=a)}return o}function h(t,e){var n=null==t?0:t.length;return!!n&&M(t,e,0)>-1}function d(t,e,n){for(var r=-1,i=null==t?0:t.length;++r<i;)if(n(e,t[r]))return!0;return!1}function v(t,e){for(var n=-1,r=null==t?0:t.length,i=Array(r);++n<r;)i[n]=e(t[n],n,t);return i}function g(t,e){for(var n=-1,r=e.length,i=t.length;++n<r;)t[i+n]=e[n];return t}function m(t,e,n,r){var i=-1,o=null==t?0:t.length;for(r&&o&&(n=t[++i]);++i<o;)n=e(n,t[i],i,t);return n}function y(t,e,n,r){var i=null==t?0:t.length;for(r&&i&&(n=t[--i]);i--;)n=e(n,t[i],i,t);return n}function _(t,e){for(var n=-1,r=null==t?0:t.length;++n<r;)if(e(t[n],n,t))return!0;return!1}function b(t){return t.split(\"\")}function x(t){return t.match(ze)||[]}function w(t,e,n){var r;return n(t,function(t,n,i){if(e(t,n,i))return r=n,!1}),r}function C(t,e,n,r){for(var i=t.length,o=n+(r?1:-1);r?o--:++o<i;)if(e(t[o],o,t))return o;return-1}function M(t,e,n){return e===e?Z(t,e,n):C(t,E,n)}function k(t,e,n,r){for(var i=n-1,o=t.length;++i<o;)if(r(t[i],e))return i;return-1}function E(t){return t!==t}function T(t,e){var n=null==t?0:t.length;return n?O(t,e)/n:Ut}function S(t){return function(e){return null==e?it:e[t]}}function P(t){return function(e){return null==t?it:t[e]}}function N(t,e,n,r,i){return i(t,function(t,i,o){n=r?(r=!1,t):e(n,t,i,o)}),n}function A(t,e){var n=t.length;for(t.sort(e);n--;)t[n]=t[n].value;return t}function O(t,e){for(var n,r=-1,i=t.length;++r<i;){var o=e(t[r]);o!==it&&(n=n===it?o:n+o)}return n}function I(t,e){for(var n=-1,r=Array(t);++n<t;)r[n]=e(n);return r}function D(t,e){return v(e,function(e){return[e,t[e]]})}function R(t){return function(e){return t(e)}}function L(t,e){return v(e,function(e){return t[e]})}function U(t,e){return t.has(e)}function F(t,e){for(var n=-1,r=t.length;++n<r&&M(e,t[n],0)>-1;);return n}function j(t,e){for(var n=t.length;n--&&M(e,t[n],0)>-1;);return n}function B(t,e){for(var n=t.length,r=0;n--;)t[n]===e&&++r;return r}function W(t){return\"\\\\\"+nr[t]}function V(t,e){return null==t?it:t[e]}function z(t){return Kn.test(t)}function H(t){return Gn.test(t)}function q(t){for(var e,n=[];!(e=t.next()).done;)n.push(e.value);return n}function Y(t){var e=-1,n=Array(t.size);return t.forEach(function(t,r){n[++e]=[r,t]}),n}function K(t,e){return function(n){return t(e(n))}}function G(t,e){for(var n=-1,r=t.length,i=0,o=[];++n<r;){var a=t[n];a!==e&&a!==ft||(t[n]=ft,o[i++]=n)}return o}function $(t){var e=-1,n=Array(t.size);return t.forEach(function(t){n[++e]=t}),n}function X(t){var e=-1,n=Array(t.size);return t.forEach(function(t){n[++e]=[t,t]}),n}function Z(t,e,n){for(var r=n-1,i=t.length;++r<i;)if(t[r]===e)return r;return-1}function Q(t,e,n){for(var r=n+1;r--;)if(t[r]===e)return r;return r}function J(t){return z(t)?et(t):_r(t)}function tt(t){return z(t)?nt(t):b(t)}function et(t){for(var e=qn.lastIndex=0;qn.test(t);)++e;return e}function nt(t){return t.match(qn)||[]}function rt(t){return t.match(Yn)||[]}var it,ot=\"4.17.4\",at=200,ut=\"Unsupported core-js use. Try https://npms.io/search?q=ponyfill.\",ct=\"Expected a function\",st=\"__lodash_hash_undefined__\",lt=500,ft=\"__lodash_placeholder__\",pt=1,ht=2,dt=4,vt=1,gt=2,mt=1,yt=2,_t=4,bt=8,xt=16,wt=32,Ct=64,Mt=128,kt=256,Et=512,Tt=30,St=\"...\",Pt=800,Nt=16,At=1,Ot=2,It=3,Dt=1/0,Rt=9007199254740991,Lt=1.7976931348623157e308,Ut=NaN,Ft=4294967295,jt=Ft-1,Bt=Ft>>>1,Wt=[[\"ary\",Mt],[\"bind\",mt],[\"bindKey\",yt],[\"curry\",bt],[\"curryRight\",xt],[\"flip\",Et],[\"partial\",wt],[\"partialRight\",Ct],[\"rearg\",kt]],Vt=\"[object Arguments]\",zt=\"[object Array]\",Ht=\"[object AsyncFunction]\",qt=\"[object Boolean]\",Yt=\"[object Date]\",Kt=\"[object DOMException]\",Gt=\"[object Error]\",$t=\"[object Function]\",Xt=\"[object GeneratorFunction]\",Zt=\"[object Map]\",Qt=\"[object Number]\",Jt=\"[object Null]\",te=\"[object Object]\",ee=\"[object Promise]\",ne=\"[object Proxy]\",re=\"[object RegExp]\",ie=\"[object Set]\",oe=\"[object String]\",ae=\"[object Symbol]\",ue=\"[object Undefined]\",ce=\"[object WeakMap]\",se=\"[object WeakSet]\",le=\"[object ArrayBuffer]\",fe=\"[object DataView]\",pe=\"[object Float32Array]\",he=\"[object Float64Array]\",de=\"[object Int8Array]\",ve=\"[object Int16Array]\",ge=\"[object Int32Array]\",me=\"[object Uint8Array]\",ye=\"[object Uint8ClampedArray]\",_e=\"[object Uint16Array]\",be=\"[object Uint32Array]\",xe=/\\b__p \\+= '';/g,we=/\\b(__p \\+=) '' \\+/g,Ce=/(__e\\(.*?\\)|\\b__t\\)) \\+\\n'';/g,Me=/&(?:amp|lt|gt|quot|#39);/g,ke=/[&<>\"']/g,Ee=RegExp(Me.source),Te=RegExp(ke.source),Se=/<%-([\\s\\S]+?)%>/g,Pe=/<%([\\s\\S]+?)%>/g,Ne=/<%=([\\s\\S]+?)%>/g,Ae=/\\.|\\[(?:[^[\\]]*|([\"'])(?:(?!\\1)[^\\\\]|\\\\.)*?\\1)\\]/,Oe=/^\\w*$/,Ie=/^\\./,De=/[^.[\\]]+|\\[(?:(-?\\d+(?:\\.\\d+)?)|([\"'])((?:(?!\\2)[^\\\\]|\\\\.)*?)\\2)\\]|(?=(?:\\.|\\[\\])(?:\\.|\\[\\]|$))/g,Re=/[\\\\^$.*+?()[\\]{}|]/g,Le=RegExp(Re.source),Ue=/^\\s+|\\s+$/g,Fe=/^\\s+/,je=/\\s+$/,Be=/\\{(?:\\n\\/\\* \\[wrapped with .+\\] \\*\\/)?\\n?/,We=/\\{\\n\\/\\* \\[wrapped with (.+)\\] \\*/,Ve=/,? & /,ze=/[^\\x00-\\x2f\\x3a-\\x40\\x5b-\\x60\\x7b-\\x7f]+/g,He=/\\\\(\\\\)?/g,qe=/\\$\\{([^\\\\}]*(?:\\\\.[^\\\\}]*)*)\\}/g,Ye=/\\w*$/,Ke=/^[-+]0x[0-9a-f]+$/i,Ge=/^0b[01]+$/i,$e=/^\\[object .+?Constructor\\]$/,Xe=/^0o[0-7]+$/i,Ze=/^(?:0|[1-9]\\d*)$/,Qe=/[\\xc0-\\xd6\\xd8-\\xf6\\xf8-\\xff\\u0100-\\u017f]/g,Je=/($^)/,tn=/['\\n\\r\\u2028\\u2029\\\\]/g,en=\"\\\\ud800-\\\\udfff\",nn=\"\\\\u0300-\\\\u036f\",rn=\"\\\\ufe20-\\\\ufe2f\",on=\"\\\\u20d0-\\\\u20ff\",an=nn+rn+on,un=\"\\\\u2700-\\\\u27bf\",cn=\"a-z\\\\xdf-\\\\xf6\\\\xf8-\\\\xff\",sn=\"\\\\xac\\\\xb1\\\\xd7\\\\xf7\",ln=\"\\\\x00-\\\\x2f\\\\x3a-\\\\x40\\\\x5b-\\\\x60\\\\x7b-\\\\xbf\",fn=\"\\\\u2000-\\\\u206f\",pn=\" \\\\t\\\\x0b\\\\f\\\\xa0\\\\ufeff\\\\n\\\\r\\\\u2028\\\\u2029\\\\u1680\\\\u180e\\\\u2000\\\\u2001\\\\u2002\\\\u2003\\\\u2004\\\\u2005\\\\u2006\\\\u2007\\\\u2008\\\\u2009\\\\u200a\\\\u202f\\\\u205f\\\\u3000\",hn=\"A-Z\\\\xc0-\\\\xd6\\\\xd8-\\\\xde\",dn=\"\\\\ufe0e\\\\ufe0f\",vn=sn+ln+fn+pn,gn=\"['’]\",mn=\"[\"+en+\"]\",yn=\"[\"+vn+\"]\",_n=\"[\"+an+\"]\",bn=\"\\\\d+\",xn=\"[\"+un+\"]\",wn=\"[\"+cn+\"]\",Cn=\"[^\"+en+vn+bn+un+cn+hn+\"]\",Mn=\"\\\\ud83c[\\\\udffb-\\\\udfff]\",kn=\"(?:\"+_n+\"|\"+Mn+\")\",En=\"[^\"+en+\"]\",Tn=\"(?:\\\\ud83c[\\\\udde6-\\\\uddff]){2}\",Sn=\"[\\\\ud800-\\\\udbff][\\\\udc00-\\\\udfff]\",Pn=\"[\"+hn+\"]\",Nn=\"\\\\u200d\",An=\"(?:\"+wn+\"|\"+Cn+\")\",On=\"(?:\"+Pn+\"|\"+Cn+\")\",In=\"(?:\"+gn+\"(?:d|ll|m|re|s|t|ve))?\",Dn=\"(?:\"+gn+\"(?:D|LL|M|RE|S|T|VE))?\",Rn=kn+\"?\",Ln=\"[\"+dn+\"]?\",Un=\"(?:\"+Nn+\"(?:\"+[En,Tn,Sn].join(\"|\")+\")\"+Ln+Rn+\")*\",Fn=\"\\\\d*(?:(?:1st|2nd|3rd|(?![123])\\\\dth)\\\\b)\",jn=\"\\\\d*(?:(?:1ST|2ND|3RD|(?![123])\\\\dTH)\\\\b)\",Bn=Ln+Rn+Un,Wn=\"(?:\"+[xn,Tn,Sn].join(\"|\")+\")\"+Bn,Vn=\"(?:\"+[En+_n+\"?\",_n,Tn,Sn,mn].join(\"|\")+\")\",zn=RegExp(gn,\"g\"),Hn=RegExp(_n,\"g\"),qn=RegExp(Mn+\"(?=\"+Mn+\")|\"+Vn+Bn,\"g\"),Yn=RegExp([Pn+\"?\"+wn+\"+\"+In+\"(?=\"+[yn,Pn,\"$\"].join(\"|\")+\")\",On+\"+\"+Dn+\"(?=\"+[yn,Pn+An,\"$\"].join(\"|\")+\")\",Pn+\"?\"+An+\"+\"+In,Pn+\"+\"+Dn,jn,Fn,bn,Wn].join(\"|\"),\"g\"),Kn=RegExp(\"[\"+Nn+en+an+dn+\"]\"),Gn=/[a-z][A-Z]|[A-Z]{2,}[a-z]|[0-9][a-zA-Z]|[a-zA-Z][0-9]|[^a-zA-Z0-9 ]/,$n=[\"Array\",\"Buffer\",\"DataView\",\"Date\",\"Error\",\"Float32Array\",\"Float64Array\",\"Function\",\"Int8Array\",\"Int16Array\",\"Int32Array\",\"Map\",\"Math\",\"Object\",\"Promise\",\"RegExp\",\"Set\",\"String\",\"Symbol\",\"TypeError\",\"Uint8Array\",\"Uint8ClampedArray\",\"Uint16Array\",\"Uint32Array\",\"WeakMap\",\"_\",\"clearTimeout\",\"isFinite\",\"parseInt\",\"setTimeout\"],Xn=-1,Zn={};Zn[pe]=Zn[he]=Zn[de]=Zn[ve]=Zn[ge]=Zn[me]=Zn[ye]=Zn[_e]=Zn[be]=!0,Zn[Vt]=Zn[zt]=Zn[le]=Zn[qt]=Zn[fe]=Zn[Yt]=Zn[Gt]=Zn[$t]=Zn[Zt]=Zn[Qt]=Zn[te]=Zn[re]=Zn[ie]=Zn[oe]=Zn[ce]=!1;var Qn={};Qn[Vt]=Qn[zt]=Qn[le]=Qn[fe]=Qn[qt]=Qn[Yt]=Qn[pe]=Qn[he]=Qn[de]=Qn[ve]=Qn[ge]=Qn[Zt]=Qn[Qt]=Qn[te]=Qn[re]=Qn[ie]=Qn[oe]=Qn[ae]=Qn[me]=Qn[ye]=Qn[_e]=Qn[be]=!0,Qn[Gt]=Qn[$t]=Qn[ce]=!1;var Jn={\"À\":\"A\",\"Á\":\"A\",\"Â\":\"A\",\"Ã\":\"A\",\"Ä\":\"A\",\"Å\":\"A\",\"à\":\"a\",\"á\":\"a\",\"â\":\"a\",\"ã\":\"a\",\"ä\":\"a\",\"å\":\"a\",\"Ç\":\"C\",\"ç\":\"c\",\"Ð\":\"D\",\"ð\":\"d\",\"È\":\"E\",\"É\":\"E\",\"Ê\":\"E\",\"Ë\":\"E\",\"è\":\"e\",\"é\":\"e\",\"ê\":\"e\",\"ë\":\"e\",\"Ì\":\"I\",\"Í\":\"I\",\"Î\":\"I\",\"Ï\":\"I\",\"ì\":\"i\",\"í\":\"i\",\"î\":\"i\",\"ï\":\"i\",\"Ñ\":\"N\",\"ñ\":\"n\",\"Ò\":\"O\",\"Ó\":\"O\",\"Ô\":\"O\",\"Õ\":\"O\",\"Ö\":\"O\",\"Ø\":\"O\",\"ò\":\"o\",\"ó\":\"o\",\"ô\":\"o\",\"õ\":\"o\",\"ö\":\"o\",\"ø\":\"o\",\"Ù\":\"U\",\"Ú\":\"U\",\"Û\":\"U\",\"Ü\":\"U\",\"ù\":\"u\",\"ú\":\"u\",\"û\":\"u\",\"ü\":\"u\",\"Ý\":\"Y\",\"ý\":\"y\",\"ÿ\":\"y\",\"Æ\":\"Ae\",\"æ\":\"ae\",\"Þ\":\"Th\",\"þ\":\"th\",\"ß\":\"ss\",\"Ā\":\"A\",\"Ă\":\"A\",\"Ą\":\"A\",\"ā\":\"a\",\"ă\":\"a\",\"ą\":\"a\",\"Ć\":\"C\",\"Ĉ\":\"C\",\"Ċ\":\"C\",\"Č\":\"C\",\"ć\":\"c\",\"ĉ\":\"c\",\"ċ\":\"c\",\"č\":\"c\",\"Ď\":\"D\",\"Đ\":\"D\",\"ď\":\"d\",\"đ\":\"d\",\"Ē\":\"E\",\"Ĕ\":\"E\",\"Ė\":\"E\",\"Ę\":\"E\",\"Ě\":\"E\",\"ē\":\"e\",\"ĕ\":\"e\",\"ė\":\"e\",\"ę\":\"e\",\"ě\":\"e\",\"Ĝ\":\"G\",\"Ğ\":\"G\",\"Ġ\":\"G\",\"Ģ\":\"G\",\"ĝ\":\"g\",\"ğ\":\"g\",\"ġ\":\"g\",\"ģ\":\"g\",\"Ĥ\":\"H\",\"Ħ\":\"H\",\"ĥ\":\"h\",\"ħ\":\"h\",\"Ĩ\":\"I\",\"Ī\":\"I\",\"Ĭ\":\"I\",\"Į\":\"I\",\"İ\":\"I\",\"ĩ\":\"i\",\"ī\":\"i\",\"ĭ\":\"i\",\"į\":\"i\",\"ı\":\"i\",\"Ĵ\":\"J\",\"ĵ\":\"j\",\"Ķ\":\"K\",\"ķ\":\"k\",\"ĸ\":\"k\",\"Ĺ\":\"L\",\"Ļ\":\"L\",\"Ľ\":\"L\",\"Ŀ\":\"L\",\"Ł\":\"L\",\"ĺ\":\"l\",\"ļ\":\"l\",\"ľ\":\"l\",\"ŀ\":\"l\",\"ł\":\"l\",\"Ń\":\"N\",\"Ņ\":\"N\",\"Ň\":\"N\",\"Ŋ\":\"N\",\"ń\":\"n\",\"ņ\":\"n\",\"ň\":\"n\",\"ŋ\":\"n\",\"Ō\":\"O\",\"Ŏ\":\"O\",\"Ő\":\"O\",\"ō\":\"o\",\"ŏ\":\"o\",\"ő\":\"o\",\"Ŕ\":\"R\",\"Ŗ\":\"R\",\"Ř\":\"R\",\"ŕ\":\"r\",\"ŗ\":\"r\",\"ř\":\"r\",\"Ś\":\"S\",\"Ŝ\":\"S\",\"Ş\":\"S\",\"Š\":\"S\",\"ś\":\"s\",\"ŝ\":\"s\",\"ş\":\"s\",\"š\":\"s\",\"Ţ\":\"T\",\"Ť\":\"T\",\"Ŧ\":\"T\",\"ţ\":\"t\",\"ť\":\"t\",\"ŧ\":\"t\",\"Ũ\":\"U\",\"Ū\":\"U\",\"Ŭ\":\"U\",\"Ů\":\"U\",\"Ű\":\"U\",\"Ų\":\"U\",\"ũ\":\"u\",\"ū\":\"u\",\"ŭ\":\"u\",\"ů\":\"u\",\"ű\":\"u\",\"ų\":\"u\",\"Ŵ\":\"W\",\"ŵ\":\"w\",\"Ŷ\":\"Y\",\"ŷ\":\"y\",\"Ÿ\":\"Y\",\"Ź\":\"Z\",\"Ż\":\"Z\",\"Ž\":\"Z\",\"ź\":\"z\",\"ż\":\"z\",\"ž\":\"z\",\"Ĳ\":\"IJ\",\n",
       "\"ĳ\":\"ij\",\"Œ\":\"Oe\",\"œ\":\"oe\",\"ŉ\":\"'n\",\"ſ\":\"s\"},tr={\"&\":\"&amp;\",\"<\":\"&lt;\",\">\":\"&gt;\",'\"':\"&quot;\",\"'\":\"&#39;\"},er={\"&amp;\":\"&\",\"&lt;\":\"<\",\"&gt;\":\">\",\"&quot;\":'\"',\"&#39;\":\"'\"},nr={\"\\\\\":\"\\\\\",\"'\":\"'\",\"\\n\":\"n\",\"\\r\":\"r\",\"\\u2028\":\"u2028\",\"\\u2029\":\"u2029\"},rr=parseFloat,ir=parseInt,or=\"object\"==typeof t&&t&&t.Object===Object&&t,ar=\"object\"==typeof self&&self&&self.Object===Object&&self,ur=or||ar||Function(\"return this\")(),cr=\"object\"==typeof e&&e&&!e.nodeType&&e,sr=cr&&\"object\"==typeof r&&r&&!r.nodeType&&r,lr=sr&&sr.exports===cr,fr=lr&&or.process,pr=function(){try{return fr&&fr.binding&&fr.binding(\"util\")}catch(t){}}(),hr=pr&&pr.isArrayBuffer,dr=pr&&pr.isDate,vr=pr&&pr.isMap,gr=pr&&pr.isRegExp,mr=pr&&pr.isSet,yr=pr&&pr.isTypedArray,_r=S(\"length\"),br=P(Jn),xr=P(tr),wr=P(er),Cr=function t(e){function n(t){if(sc(t)&&!xp(t)&&!(t instanceof b)){if(t instanceof i)return t;if(bl.call(t,\"__wrapped__\"))return aa(t)}return new i(t)}function r(){}function i(t,e){this.__wrapped__=t,this.__actions__=[],this.__chain__=!!e,this.__index__=0,this.__values__=it}function b(t){this.__wrapped__=t,this.__actions__=[],this.__dir__=1,this.__filtered__=!1,this.__iteratees__=[],this.__takeCount__=Ft,this.__views__=[]}function P(){var t=new b(this.__wrapped__);return t.__actions__=Bi(this.__actions__),t.__dir__=this.__dir__,t.__filtered__=this.__filtered__,t.__iteratees__=Bi(this.__iteratees__),t.__takeCount__=this.__takeCount__,t.__views__=Bi(this.__views__),t}function Z(){if(this.__filtered__){var t=new b(this);t.__dir__=-1,t.__filtered__=!0}else t=this.clone(),t.__dir__*=-1;return t}function et(){var t=this.__wrapped__.value(),e=this.__dir__,n=xp(t),r=e<0,i=n?t.length:0,o=No(0,i,this.__views__),a=o.start,u=o.end,c=u-a,s=r?u:a-1,l=this.__iteratees__,f=l.length,p=0,h=Xl(c,this.__takeCount__);if(!n||!r&&i==c&&h==c)return xi(t,this.__actions__);var d=[];t:for(;c--&&p<h;){s+=e;for(var v=-1,g=t[s];++v<f;){var m=l[v],y=m.iteratee,_=m.type,b=y(g);if(_==Ot)g=b;else if(!b){if(_==At)continue t;break t}}d[p++]=g}return d}function nt(t){var e=-1,n=null==t?0:t.length;for(this.clear();++e<n;){var r=t[e];this.set(r[0],r[1])}}function ze(){this.__data__=uf?uf(null):{},this.size=0}function en(t){var e=this.has(t)&&delete this.__data__[t];return this.size-=e?1:0,e}function nn(t){var e=this.__data__;if(uf){var n=e[t];return n===st?it:n}return bl.call(e,t)?e[t]:it}function rn(t){var e=this.__data__;return uf?e[t]!==it:bl.call(e,t)}function on(t,e){var n=this.__data__;return this.size+=this.has(t)?0:1,n[t]=uf&&e===it?st:e,this}function an(t){var e=-1,n=null==t?0:t.length;for(this.clear();++e<n;){var r=t[e];this.set(r[0],r[1])}}function un(){this.__data__=[],this.size=0}function cn(t){var e=this.__data__,n=In(e,t);if(n<0)return!1;var r=e.length-1;return n==r?e.pop():Dl.call(e,n,1),--this.size,!0}function sn(t){var e=this.__data__,n=In(e,t);return n<0?it:e[n][1]}function ln(t){return In(this.__data__,t)>-1}function fn(t,e){var n=this.__data__,r=In(n,t);return r<0?(++this.size,n.push([t,e])):n[r][1]=e,this}function pn(t){var e=-1,n=null==t?0:t.length;for(this.clear();++e<n;){var r=t[e];this.set(r[0],r[1])}}function hn(){this.size=0,this.__data__={hash:new nt,map:new(nf||an),string:new nt}}function dn(t){var e=Eo(this,t).delete(t);return this.size-=e?1:0,e}function vn(t){return Eo(this,t).get(t)}function gn(t){return Eo(this,t).has(t)}function mn(t,e){var n=Eo(this,t),r=n.size;return n.set(t,e),this.size+=n.size==r?0:1,this}function yn(t){var e=-1,n=null==t?0:t.length;for(this.__data__=new pn;++e<n;)this.add(t[e])}function _n(t){return this.__data__.set(t,st),this}function bn(t){return this.__data__.has(t)}function xn(t){var e=this.__data__=new an(t);this.size=e.size}function wn(){this.__data__=new an,this.size=0}function Cn(t){var e=this.__data__,n=e.delete(t);return this.size=e.size,n}function Mn(t){return this.__data__.get(t)}function kn(t){return this.__data__.has(t)}function En(t,e){var n=this.__data__;if(n instanceof an){var r=n.__data__;if(!nf||r.length<at-1)return r.push([t,e]),this.size=++n.size,this;n=this.__data__=new pn(r)}return n.set(t,e),this.size=n.size,this}function Tn(t,e){var n=xp(t),r=!n&&bp(t),i=!n&&!r&&Cp(t),o=!n&&!r&&!i&&Sp(t),a=n||r||i||o,u=a?I(t.length,hl):[],c=u.length;for(var s in t)!e&&!bl.call(t,s)||a&&(\"length\"==s||i&&(\"offset\"==s||\"parent\"==s)||o&&(\"buffer\"==s||\"byteLength\"==s||\"byteOffset\"==s)||Fo(s,c))||u.push(s);return u}function Sn(t){var e=t.length;return e?t[ni(0,e-1)]:it}function Pn(t,e){return na(Bi(t),jn(e,0,t.length))}function Nn(t){return na(Bi(t))}function An(t,e,n){(n===it||$u(t[e],n))&&(n!==it||e in t)||Un(t,e,n)}function On(t,e,n){var r=t[e];bl.call(t,e)&&$u(r,n)&&(n!==it||e in t)||Un(t,e,n)}function In(t,e){for(var n=t.length;n--;)if($u(t[n][0],e))return n;return-1}function Dn(t,e,n,r){return _f(t,function(t,i,o){e(r,t,n(t),o)}),r}function Rn(t,e){return t&&Wi(e,Hc(e),t)}function Ln(t,e){return t&&Wi(e,qc(e),t)}function Un(t,e,n){\"__proto__\"==e&&Fl?Fl(t,e,{configurable:!0,enumerable:!0,value:n,writable:!0}):t[e]=n}function Fn(t,e){for(var n=-1,r=e.length,i=al(r),o=null==t;++n<r;)i[n]=o?it:Wc(t,e[n]);return i}function jn(t,e,n){return t===t&&(n!==it&&(t=t<=n?t:n),e!==it&&(t=t>=e?t:e)),t}function Bn(t,e,n,r,i,o){var a,u=e&pt,c=e&ht,l=e&dt;if(n&&(a=i?n(t,r,i,o):n(t)),a!==it)return a;if(!cc(t))return t;var f=xp(t);if(f){if(a=Io(t),!u)return Bi(t,a)}else{var p=Af(t),h=p==$t||p==Xt;if(Cp(t))return Si(t,u);if(p==te||p==Vt||h&&!i){if(a=c||h?{}:Do(t),!u)return c?zi(t,Ln(a,t)):Vi(t,Rn(a,t))}else{if(!Qn[p])return i?t:{};a=Ro(t,p,Bn,u)}}o||(o=new xn);var d=o.get(t);if(d)return d;o.set(t,a);var v=l?c?wo:xo:c?qc:Hc,g=f?it:v(t);return s(g||t,function(r,i){g&&(i=r,r=t[i]),On(a,i,Bn(r,e,n,i,t,o))}),a}function Wn(t){var e=Hc(t);return function(n){return Vn(n,t,e)}}function Vn(t,e,n){var r=n.length;if(null==t)return!r;for(t=fl(t);r--;){var i=n[r],o=e[i],a=t[i];if(a===it&&!(i in t)||!o(a))return!1}return!0}function qn(t,e,n){if(\"function\"!=typeof t)throw new dl(ct);return Df(function(){t.apply(it,n)},e)}function Yn(t,e,n,r){var i=-1,o=h,a=!0,u=t.length,c=[],s=e.length;if(!u)return c;n&&(e=v(e,R(n))),r?(o=d,a=!1):e.length>=at&&(o=U,a=!1,e=new yn(e));t:for(;++i<u;){var l=t[i],f=null==n?l:n(l);if(l=r||0!==l?l:0,a&&f===f){for(var p=s;p--;)if(e[p]===f)continue t;c.push(l)}else o(e,f,r)||c.push(l)}return c}function Kn(t,e){var n=!0;return _f(t,function(t,r,i){return n=!!e(t,r,i)}),n}function Gn(t,e,n){for(var r=-1,i=t.length;++r<i;){var o=t[r],a=e(o);if(null!=a&&(u===it?a===a&&!bc(a):n(a,u)))var u=a,c=o}return c}function Jn(t,e,n,r){var i=t.length;for(n=Ec(n),n<0&&(n=-n>i?0:i+n),r=r===it||r>i?i:Ec(r),r<0&&(r+=i),r=n>r?0:Tc(r);n<r;)t[n++]=e;return t}function tr(t,e){var n=[];return _f(t,function(t,r,i){e(t,r,i)&&n.push(t)}),n}function er(t,e,n,r,i){var o=-1,a=t.length;for(n||(n=Uo),i||(i=[]);++o<a;){var u=t[o];e>0&&n(u)?e>1?er(u,e-1,n,r,i):g(i,u):r||(i[i.length]=u)}return i}function nr(t,e){return t&&xf(t,e,Hc)}function or(t,e){return t&&wf(t,e,Hc)}function ar(t,e){return p(e,function(e){return oc(t[e])})}function cr(t,e){e=Ei(e,t);for(var n=0,r=e.length;null!=t&&n<r;)t=t[ra(e[n++])];return n&&n==r?t:it}function sr(t,e,n){var r=e(t);return xp(t)?r:g(r,n(t))}function fr(t){return null==t?t===it?ue:Jt:Ul&&Ul in fl(t)?Po(t):Xo(t)}function pr(t,e){return t>e}function _r(t,e){return null!=t&&bl.call(t,e)}function Cr(t,e){return null!=t&&e in fl(t)}function kr(t,e,n){return t>=Xl(e,n)&&t<$l(e,n)}function Er(t,e,n){for(var r=n?d:h,i=t[0].length,o=t.length,a=o,u=al(o),c=1/0,s=[];a--;){var l=t[a];a&&e&&(l=v(l,R(e))),c=Xl(l.length,c),u[a]=!n&&(e||i>=120&&l.length>=120)?new yn(a&&l):it}l=t[0];var f=-1,p=u[0];t:for(;++f<i&&s.length<c;){var g=l[f],m=e?e(g):g;if(g=n||0!==g?g:0,!(p?U(p,m):r(s,m,n))){for(a=o;--a;){var y=u[a];if(!(y?U(y,m):r(t[a],m,n)))continue t}p&&p.push(m),s.push(g)}}return s}function Tr(t,e,n,r){return nr(t,function(t,i,o){e(r,n(t),i,o)}),r}function Sr(t,e,n){e=Ei(e,t),t=Qo(t,e);var r=null==t?t:t[ra(ka(e))];return null==r?it:u(r,t,n)}function Pr(t){return sc(t)&&fr(t)==Vt}function Nr(t){return sc(t)&&fr(t)==le}function Ar(t){return sc(t)&&fr(t)==Yt}function Or(t,e,n,r,i){return t===e||(null==t||null==e||!sc(t)&&!sc(e)?t!==t&&e!==e:Ir(t,e,n,r,Or,i))}function Ir(t,e,n,r,i,o){var a=xp(t),u=xp(e),c=a?zt:Af(t),s=u?zt:Af(e);c=c==Vt?te:c,s=s==Vt?te:s;var l=c==te,f=s==te,p=c==s;if(p&&Cp(t)){if(!Cp(e))return!1;a=!0,l=!1}if(p&&!l)return o||(o=new xn),a||Sp(t)?mo(t,e,n,r,i,o):yo(t,e,c,n,r,i,o);if(!(n&vt)){var h=l&&bl.call(t,\"__wrapped__\"),d=f&&bl.call(e,\"__wrapped__\");if(h||d){var v=h?t.value():t,g=d?e.value():e;return o||(o=new xn),i(v,g,n,r,o)}}return!!p&&(o||(o=new xn),_o(t,e,n,r,i,o))}function Dr(t){return sc(t)&&Af(t)==Zt}function Rr(t,e,n,r){var i=n.length,o=i,a=!r;if(null==t)return!o;for(t=fl(t);i--;){var u=n[i];if(a&&u[2]?u[1]!==t[u[0]]:!(u[0]in t))return!1}for(;++i<o;){u=n[i];var c=u[0],s=t[c],l=u[1];if(a&&u[2]){if(s===it&&!(c in t))return!1}else{var f=new xn;if(r)var p=r(s,l,c,t,e,f);if(!(p===it?Or(l,s,vt|gt,r,f):p))return!1}}return!0}function Lr(t){if(!cc(t)||zo(t))return!1;var e=oc(t)?El:$e;return e.test(ia(t))}function Ur(t){return sc(t)&&fr(t)==re}function Fr(t){return sc(t)&&Af(t)==ie}function jr(t){return sc(t)&&uc(t.length)&&!!Zn[fr(t)]}function Br(t){return\"function\"==typeof t?t:null==t?Ds:\"object\"==typeof t?xp(t)?Yr(t[0],t[1]):qr(t):Vs(t)}function Wr(t){if(!Ho(t))return Gl(t);var e=[];for(var n in fl(t))bl.call(t,n)&&\"constructor\"!=n&&e.push(n);return e}function Vr(t){if(!cc(t))return $o(t);var e=Ho(t),n=[];for(var r in t)(\"constructor\"!=r||!e&&bl.call(t,r))&&n.push(r);return n}function zr(t,e){return t<e}function Hr(t,e){var n=-1,r=Xu(t)?al(t.length):[];return _f(t,function(t,i,o){r[++n]=e(t,i,o)}),r}function qr(t){var e=To(t);return 1==e.length&&e[0][2]?Yo(e[0][0],e[0][1]):function(n){return n===t||Rr(n,t,e)}}function Yr(t,e){return Bo(t)&&qo(e)?Yo(ra(t),e):function(n){var r=Wc(n,t);return r===it&&r===e?zc(n,t):Or(e,r,vt|gt)}}function Kr(t,e,n,r,i){t!==e&&xf(e,function(o,a){if(cc(o))i||(i=new xn),Gr(t,e,a,n,Kr,r,i);else{var u=r?r(t[a],o,a+\"\",t,e,i):it;u===it&&(u=o),An(t,a,u)}},qc)}function Gr(t,e,n,r,i,o,a){var u=t[n],c=e[n],s=a.get(c);if(s)return void An(t,n,s);var l=o?o(u,c,n+\"\",t,e,a):it,f=l===it;if(f){var p=xp(c),h=!p&&Cp(c),d=!p&&!h&&Sp(c);l=c,p||h||d?xp(u)?l=u:Zu(u)?l=Bi(u):h?(f=!1,l=Si(c,!0)):d?(f=!1,l=Ri(c,!0)):l=[]:mc(c)||bp(c)?(l=u,bp(u)?l=Pc(u):(!cc(u)||r&&oc(u))&&(l=Do(c))):f=!1}f&&(a.set(c,l),i(l,c,r,o,a),a.delete(c)),An(t,n,l)}function $r(t,e){var n=t.length;if(n)return e+=e<0?n:0,Fo(e,n)?t[e]:it}function Xr(t,e,n){var r=-1;e=v(e.length?e:[Ds],R(ko()));var i=Hr(t,function(t,n,i){var o=v(e,function(e){return e(t)});return{criteria:o,index:++r,value:t}});return A(i,function(t,e){return Ui(t,e,n)})}function Zr(t,e){return Qr(t,e,function(e,n){return zc(t,n)})}function Qr(t,e,n){for(var r=-1,i=e.length,o={};++r<i;){var a=e[r],u=cr(t,a);n(u,a)&&ci(o,Ei(a,t),u)}return o}function Jr(t){return function(e){return cr(e,t)}}function ti(t,e,n,r){var i=r?k:M,o=-1,a=e.length,u=t;for(t===e&&(e=Bi(e)),n&&(u=v(t,R(n)));++o<a;)for(var c=0,s=e[o],l=n?n(s):s;(c=i(u,l,c,r))>-1;)u!==t&&Dl.call(u,c,1),Dl.call(t,c,1);return t}function ei(t,e){for(var n=t?e.length:0,r=n-1;n--;){var i=e[n];if(n==r||i!==o){var o=i;Fo(i)?Dl.call(t,i,1):yi(t,i)}}return t}function ni(t,e){return t+zl(Jl()*(e-t+1))}function ri(t,e,n,r){for(var i=-1,o=$l(Vl((e-t)/(n||1)),0),a=al(o);o--;)a[r?o:++i]=t,t+=n;return a}function ii(t,e){var n=\"\";if(!t||e<1||e>Rt)return n;do e%2&&(n+=t),e=zl(e/2),e&&(t+=t);while(e);return n}function oi(t,e){return Rf(Zo(t,e,Ds),t+\"\")}function ai(t){return Sn(rs(t))}function ui(t,e){var n=rs(t);return na(n,jn(e,0,n.length))}function ci(t,e,n,r){if(!cc(t))return t;e=Ei(e,t);for(var i=-1,o=e.length,a=o-1,u=t;null!=u&&++i<o;){var c=ra(e[i]),s=n;if(i!=a){var l=u[c];s=r?r(l,c,u):it,s===it&&(s=cc(l)?l:Fo(e[i+1])?[]:{})}On(u,c,s),u=u[c]}return t}function si(t){return na(rs(t))}function li(t,e,n){var r=-1,i=t.length;e<0&&(e=-e>i?0:i+e),n=n>i?i:n,n<0&&(n+=i),i=e>n?0:n-e>>>0,e>>>=0;for(var o=al(i);++r<i;)o[r]=t[r+e];return o}function fi(t,e){var n;return _f(t,function(t,r,i){return n=e(t,r,i),!n}),!!n}function pi(t,e,n){var r=0,i=null==t?r:t.length;if(\"number\"==typeof e&&e===e&&i<=Bt){for(;r<i;){var o=r+i>>>1,a=t[o];null!==a&&!bc(a)&&(n?a<=e:a<e)?r=o+1:i=o}return i}return hi(t,e,Ds,n)}function hi(t,e,n,r){e=n(e);for(var i=0,o=null==t?0:t.length,a=e!==e,u=null===e,c=bc(e),s=e===it;i<o;){var l=zl((i+o)/2),f=n(t[l]),p=f!==it,h=null===f,d=f===f,v=bc(f);if(a)var g=r||d;else g=s?d&&(r||p):u?d&&p&&(r||!h):c?d&&p&&!h&&(r||!v):!h&&!v&&(r?f<=e:f<e);g?i=l+1:o=l}return Xl(o,jt)}function di(t,e){for(var n=-1,r=t.length,i=0,o=[];++n<r;){var a=t[n],u=e?e(a):a;if(!n||!$u(u,c)){var c=u;o[i++]=0===a?0:a}}return o}function vi(t){return\"number\"==typeof t?t:bc(t)?Ut:+t}function gi(t){if(\"string\"==typeof t)return t;if(xp(t))return v(t,gi)+\"\";if(bc(t))return mf?mf.call(t):\"\";var e=t+\"\";return\"0\"==e&&1/t==-Dt?\"-0\":e}function mi(t,e,n){var r=-1,i=h,o=t.length,a=!0,u=[],c=u;if(n)a=!1,i=d;else if(o>=at){var s=e?null:Tf(t);if(s)return $(s);a=!1,i=U,c=new yn}else c=e?[]:u;t:for(;++r<o;){var l=t[r],f=e?e(l):l;if(l=n||0!==l?l:0,a&&f===f){for(var p=c.length;p--;)if(c[p]===f)continue t;e&&c.push(f),u.push(l)}else i(c,f,n)||(c!==u&&c.push(f),u.push(l))}return u}function yi(t,e){return e=Ei(e,t),t=Qo(t,e),null==t||delete t[ra(ka(e))]}function _i(t,e,n,r){return ci(t,e,n(cr(t,e)),r)}function bi(t,e,n,r){for(var i=t.length,o=r?i:-1;(r?o--:++o<i)&&e(t[o],o,t););return n?li(t,r?0:o,r?o+1:i):li(t,r?o+1:0,r?i:o)}function xi(t,e){var n=t;return n instanceof b&&(n=n.value()),m(e,function(t,e){return e.func.apply(e.thisArg,g([t],e.args))},n)}function wi(t,e,n){var r=t.length;if(r<2)return r?mi(t[0]):[];for(var i=-1,o=al(r);++i<r;)for(var a=t[i],u=-1;++u<r;)u!=i&&(o[i]=Yn(o[i]||a,t[u],e,n));return mi(er(o,1),e,n)}function Ci(t,e,n){for(var r=-1,i=t.length,o=e.length,a={};++r<i;){var u=r<o?e[r]:it;n(a,t[r],u)}return a}function Mi(t){return Zu(t)?t:[]}function ki(t){return\"function\"==typeof t?t:Ds}function Ei(t,e){return xp(t)?t:Bo(t,e)?[t]:Lf(Ac(t))}function Ti(t,e,n){var r=t.length;return n=n===it?r:n,!e&&n>=r?t:li(t,e,n)}function Si(t,e){if(e)return t.slice();var n=t.length,r=Nl?Nl(n):new t.constructor(n);return t.copy(r),r}function Pi(t){var e=new t.constructor(t.byteLength);return new Pl(e).set(new Pl(t)),e}function Ni(t,e){var n=e?Pi(t.buffer):t.buffer;return new t.constructor(n,t.byteOffset,t.byteLength)}function Ai(t,e,n){var r=e?n(Y(t),pt):Y(t);return m(r,o,new t.constructor)}function Oi(t){var e=new t.constructor(t.source,Ye.exec(t));return e.lastIndex=t.lastIndex,e}function Ii(t,e,n){var r=e?n($(t),pt):$(t);return m(r,a,new t.constructor)}function Di(t){return gf?fl(gf.call(t)):{}}function Ri(t,e){var n=e?Pi(t.buffer):t.buffer;return new t.constructor(n,t.byteOffset,t.length)}function Li(t,e){if(t!==e){var n=t!==it,r=null===t,i=t===t,o=bc(t),a=e!==it,u=null===e,c=e===e,s=bc(e);if(!u&&!s&&!o&&t>e||o&&a&&c&&!u&&!s||r&&a&&c||!n&&c||!i)return 1;if(!r&&!o&&!s&&t<e||s&&n&&i&&!r&&!o||u&&n&&i||!a&&i||!c)return-1}return 0}function Ui(t,e,n){for(var r=-1,i=t.criteria,o=e.criteria,a=i.length,u=n.length;++r<a;){var c=Li(i[r],o[r]);if(c){if(r>=u)return c;var s=n[r];return c*(\"desc\"==s?-1:1)}}return t.index-e.index}function Fi(t,e,n,r){for(var i=-1,o=t.length,a=n.length,u=-1,c=e.length,s=$l(o-a,0),l=al(c+s),f=!r;++u<c;)l[u]=e[u];for(;++i<a;)(f||i<o)&&(l[n[i]]=t[i]);for(;s--;)l[u++]=t[i++];return l}function ji(t,e,n,r){for(var i=-1,o=t.length,a=-1,u=n.length,c=-1,s=e.length,l=$l(o-u,0),f=al(l+s),p=!r;++i<l;)f[i]=t[i];for(var h=i;++c<s;)f[h+c]=e[c];for(;++a<u;)(p||i<o)&&(f[h+n[a]]=t[i++]);return f}function Bi(t,e){var n=-1,r=t.length;for(e||(e=al(r));++n<r;)e[n]=t[n];return e}function Wi(t,e,n,r){var i=!n;n||(n={});for(var o=-1,a=e.length;++o<a;){var u=e[o],c=r?r(n[u],t[u],u,n,t):it;c===it&&(c=t[u]),i?Un(n,u,c):On(n,u,c)}return n}function Vi(t,e){return Wi(t,Pf(t),e)}function zi(t,e){return Wi(t,Nf(t),e)}function Hi(t,e){return function(n,r){var i=xp(n)?c:Dn,o=e?e():{};return i(n,t,ko(r,2),o)}}function qi(t){return oi(function(e,n){var r=-1,i=n.length,o=i>1?n[i-1]:it,a=i>2?n[2]:it;for(o=t.length>3&&\"function\"==typeof o?(i--,o):it,a&&jo(n[0],n[1],a)&&(o=i<3?it:o,i=1),e=fl(e);++r<i;){var u=n[r];u&&t(e,u,r,o)}return e})}function Yi(t,e){return function(n,r){if(null==n)return n;if(!Xu(n))return t(n,r);for(var i=n.length,o=e?i:-1,a=fl(n);(e?o--:++o<i)&&r(a[o],o,a)!==!1;);return n}}function Ki(t){return function(e,n,r){for(var i=-1,o=fl(e),a=r(e),u=a.length;u--;){var c=a[t?u:++i];if(n(o[c],c,o)===!1)break}return e}}function Gi(t,e,n){function r(){var e=this&&this!==ur&&this instanceof r?o:t;return e.apply(i?n:this,arguments)}var i=e&mt,o=Zi(t);return r}function $i(t){return function(e){e=Ac(e);var n=z(e)?tt(e):it,r=n?n[0]:e.charAt(0),i=n?Ti(n,1).join(\"\"):e.slice(1);return r[t]()+i}}function Xi(t){return function(e){return m(Ps(ss(e).replace(zn,\"\")),t,\"\")}}function Zi(t){return function(){var e=arguments;switch(e.length){case 0:return new t;case 1:return new t(e[0]);case 2:return new t(e[0],e[1]);case 3:return new t(e[0],e[1],e[2]);case 4:return new t(e[0],e[1],e[2],e[3]);case 5:return new t(e[0],e[1],e[2],e[3],e[4]);case 6:return new t(e[0],e[1],e[2],e[3],e[4],e[5]);case 7:return new t(e[0],e[1],e[2],e[3],e[4],e[5],e[6])}var n=yf(t.prototype),r=t.apply(n,e);return cc(r)?r:n}}function Qi(t,e,n){function r(){for(var o=arguments.length,a=al(o),c=o,s=Mo(r);c--;)a[c]=arguments[c];var l=o<3&&a[0]!==s&&a[o-1]!==s?[]:G(a,s);if(o-=l.length,o<n)return so(t,e,eo,r.placeholder,it,a,l,it,it,n-o);var f=this&&this!==ur&&this instanceof r?i:t;return u(f,this,a)}var i=Zi(t);return r}function Ji(t){return function(e,n,r){var i=fl(e);if(!Xu(e)){var o=ko(n,3);e=Hc(e),n=function(t){return o(i[t],t,i)}}var a=t(e,n,r);return a>-1?i[o?e[a]:a]:it}}function to(t){return bo(function(e){var n=e.length,r=n,o=i.prototype.thru;for(t&&e.reverse();r--;){var a=e[r];if(\"function\"!=typeof a)throw new dl(ct);if(o&&!u&&\"wrapper\"==Co(a))var u=new i([],!0)}for(r=u?r:n;++r<n;){a=e[r];var c=Co(a),s=\"wrapper\"==c?Sf(a):it;u=s&&Vo(s[0])&&s[1]==(Mt|bt|wt|kt)&&!s[4].length&&1==s[9]?u[Co(s[0])].apply(u,s[3]):1==a.length&&Vo(a)?u[c]():u.thru(a)}return function(){var t=arguments,r=t[0];if(u&&1==t.length&&xp(r))return u.plant(r).value();for(var i=0,o=n?e[i].apply(this,t):r;++i<n;)o=e[i].call(this,o);return o}})}function eo(t,e,n,r,i,o,a,u,c,s){function l(){for(var m=arguments.length,y=al(m),_=m;_--;)y[_]=arguments[_];if(d)var b=Mo(l),x=B(y,b);if(r&&(y=Fi(y,r,i,d)),o&&(y=ji(y,o,a,d)),m-=x,d&&m<s){var w=G(y,b);return so(t,e,eo,l.placeholder,n,y,w,u,c,s-m)}var C=p?n:this,M=h?C[t]:t;return m=y.length,u?y=Jo(y,u):v&&m>1&&y.reverse(),f&&c<m&&(y.length=c),this&&this!==ur&&this instanceof l&&(M=g||Zi(M)),M.apply(C,y)}var f=e&Mt,p=e&mt,h=e&yt,d=e&(bt|xt),v=e&Et,g=h?it:Zi(t);return l}function no(t,e){return function(n,r){return Tr(n,t,e(r),{})}}function ro(t,e){return function(n,r){var i;if(n===it&&r===it)return e;if(n!==it&&(i=n),r!==it){if(i===it)return r;\"string\"==typeof n||\"string\"==typeof r?(n=gi(n),r=gi(r)):(n=vi(n),r=vi(r)),i=t(n,r)}return i}}function io(t){return bo(function(e){return e=v(e,R(ko())),oi(function(n){var r=this;return t(e,function(t){return u(t,r,n)})})})}function oo(t,e){e=e===it?\" \":gi(e);var n=e.length;if(n<2)return n?ii(e,t):e;var r=ii(e,Vl(t/J(e)));return z(e)?Ti(tt(r),0,t).join(\"\"):r.slice(0,t)}function ao(t,e,n,r){function i(){for(var e=-1,c=arguments.length,s=-1,l=r.length,f=al(l+c),p=this&&this!==ur&&this instanceof i?a:t;++s<l;)f[s]=r[s];for(;c--;)f[s++]=arguments[++e];return u(p,o?n:this,f)}var o=e&mt,a=Zi(t);return i}function uo(t){return function(e,n,r){return r&&\"number\"!=typeof r&&jo(e,n,r)&&(n=r=it),e=kc(e),n===it?(n=e,e=0):n=kc(n),r=r===it?e<n?1:-1:kc(r),ri(e,n,r,t)}}function co(t){return function(e,n){return\"string\"==typeof e&&\"string\"==typeof n||(e=Sc(e),n=Sc(n)),t(e,n)}}function so(t,e,n,r,i,o,a,u,c,s){var l=e&bt,f=l?a:it,p=l?it:a,h=l?o:it,d=l?it:o;e|=l?wt:Ct,e&=~(l?Ct:wt),e&_t||(e&=~(mt|yt));var v=[t,e,i,h,f,d,p,u,c,s],g=n.apply(it,v);return Vo(t)&&If(g,v),g.placeholder=r,ta(g,t,e)}function lo(t){var e=ll[t];return function(t,n){if(t=Sc(t),n=null==n?0:Xl(Ec(n),292)){var r=(Ac(t)+\"e\").split(\"e\"),i=e(r[0]+\"e\"+(+r[1]+n));return r=(Ac(i)+\"e\").split(\"e\"),+(r[0]+\"e\"+(+r[1]-n))}return e(t)}}function fo(t){return function(e){var n=Af(e);return n==Zt?Y(e):n==ie?X(e):D(e,t(e))}}function po(t,e,n,r,i,o,a,u){var c=e&yt;if(!c&&\"function\"!=typeof t)throw new dl(ct);var s=r?r.length:0;if(s||(e&=~(wt|Ct),r=i=it),a=a===it?a:$l(Ec(a),0),u=u===it?u:Ec(u),s-=i?i.length:0,e&Ct){var l=r,f=i;r=i=it}var p=c?it:Sf(t),h=[t,e,n,r,i,l,f,o,a,u];if(p&&Go(h,p),t=h[0],e=h[1],n=h[2],r=h[3],i=h[4],u=h[9]=h[9]===it?c?0:t.length:$l(h[9]-s,0),!u&&e&(bt|xt)&&(e&=~(bt|xt)),e&&e!=mt)d=e==bt||e==xt?Qi(t,e,u):e!=wt&&e!=(mt|wt)||i.length?eo.apply(it,h):ao(t,e,n,r);else var d=Gi(t,e,n);var v=p?Cf:If;return ta(v(d,h),t,e)}function ho(t,e,n,r){return t===it||$u(t,ml[n])&&!bl.call(r,n)?e:t}function vo(t,e,n,r,i,o){return cc(t)&&cc(e)&&(o.set(e,t),Kr(t,e,it,vo,o),o.delete(e)),t}function go(t){return mc(t)?it:t}function mo(t,e,n,r,i,o){var a=n&vt,u=t.length,c=e.length;if(u!=c&&!(a&&c>u))return!1;var s=o.get(t);if(s&&o.get(e))return s==e;var l=-1,f=!0,p=n&gt?new yn:it;for(o.set(t,e),o.set(e,t);++l<u;){var h=t[l],d=e[l];if(r)var v=a?r(d,h,l,e,t,o):r(h,d,l,t,e,o);if(v!==it){if(v)continue;f=!1;break}if(p){if(!_(e,function(t,e){if(!U(p,e)&&(h===t||i(h,t,n,r,o)))return p.push(e)})){f=!1;break}}else if(h!==d&&!i(h,d,n,r,o)){f=!1;break}}return o.delete(t),o.delete(e),f}function yo(t,e,n,r,i,o,a){switch(n){case fe:if(t.byteLength!=e.byteLength||t.byteOffset!=e.byteOffset)return!1;t=t.buffer,e=e.buffer;case le:return!(t.byteLength!=e.byteLength||!o(new Pl(t),new Pl(e)));case qt:case Yt:case Qt:return $u(+t,+e);case Gt:return t.name==e.name&&t.message==e.message;case re:case oe:return t==e+\"\";case Zt:var u=Y;case ie:var c=r&vt;if(u||(u=$),t.size!=e.size&&!c)return!1;var s=a.get(t);if(s)return s==e;r|=gt,a.set(t,e);var l=mo(u(t),u(e),r,i,o,a);return a.delete(t),l;case ae:if(gf)return gf.call(t)==gf.call(e)}return!1}function _o(t,e,n,r,i,o){var a=n&vt,u=xo(t),c=u.length,s=xo(e),l=s.length;if(c!=l&&!a)return!1;for(var f=c;f--;){var p=u[f];if(!(a?p in e:bl.call(e,p)))return!1}var h=o.get(t);if(h&&o.get(e))return h==e;var d=!0;o.set(t,e),o.set(e,t);for(var v=a;++f<c;){p=u[f];var g=t[p],m=e[p];if(r)var y=a?r(m,g,p,e,t,o):r(g,m,p,t,e,o);if(!(y===it?g===m||i(g,m,n,r,o):y)){d=!1;break}v||(v=\"constructor\"==p)}if(d&&!v){var _=t.constructor,b=e.constructor;_!=b&&\"constructor\"in t&&\"constructor\"in e&&!(\"function\"==typeof _&&_ instanceof _&&\"function\"==typeof b&&b instanceof b)&&(d=!1)}return o.delete(t),o.delete(e),d}function bo(t){return Rf(Zo(t,it,ma),t+\"\")}function xo(t){return sr(t,Hc,Pf)}function wo(t){return sr(t,qc,Nf)}function Co(t){for(var e=t.name+\"\",n=sf[e],r=bl.call(sf,e)?n.length:0;r--;){var i=n[r],o=i.func;if(null==o||o==t)return i.name}return e}function Mo(t){var e=bl.call(n,\"placeholder\")?n:t;return e.placeholder}function ko(){var t=n.iteratee||Rs;return t=t===Rs?Br:t,arguments.length?t(arguments[0],arguments[1]):t}function Eo(t,e){var n=t.__data__;return Wo(e)?n[\"string\"==typeof e?\"string\":\"hash\"]:n.map}function To(t){for(var e=Hc(t),n=e.length;n--;){var r=e[n],i=t[r];e[n]=[r,i,qo(i)]}return e}function So(t,e){var n=V(t,e);return Lr(n)?n:it}function Po(t){var e=bl.call(t,Ul),n=t[Ul];try{t[Ul]=it;var r=!0}catch(t){}var i=Cl.call(t);return r&&(e?t[Ul]=n:delete t[Ul]),i}function No(t,e,n){for(var r=-1,i=n.length;++r<i;){var o=n[r],a=o.size;switch(o.type){case\"drop\":t+=a;break;case\"dropRight\":e-=a;break;case\"take\":e=Xl(e,t+a);break;case\"takeRight\":t=$l(t,e-a)}}return{start:t,end:e}}function Ao(t){var e=t.match(We);return e?e[1].split(Ve):[]}function Oo(t,e,n){e=Ei(e,t);for(var r=-1,i=e.length,o=!1;++r<i;){var a=ra(e[r]);if(!(o=null!=t&&n(t,a)))break;t=t[a]}return o||++r!=i?o:(i=null==t?0:t.length,!!i&&uc(i)&&Fo(a,i)&&(xp(t)||bp(t)))}function Io(t){var e=t.length,n=t.constructor(e);return e&&\"string\"==typeof t[0]&&bl.call(t,\"index\")&&(n.index=t.index,n.input=t.input),n}function Do(t){return\"function\"!=typeof t.constructor||Ho(t)?{}:yf(Al(t))}function Ro(t,e,n,r){var i=t.constructor;switch(e){case le:return Pi(t);case qt:case Yt:return new i(+t);case fe:return Ni(t,r);case pe:case he:case de:case ve:case ge:case me:case ye:case _e:case be:return Ri(t,r);case Zt:return Ai(t,r,n);case Qt:case oe:return new i(t);case re:return Oi(t);case ie:return Ii(t,r,n);case ae:return Di(t)}}function Lo(t,e){var n=e.length;if(!n)return t;var r=n-1;return e[r]=(n>1?\"& \":\"\")+e[r],e=e.join(n>2?\", \":\" \"),t.replace(Be,\"{\\n/* [wrapped with \"+e+\"] */\\n\")}function Uo(t){return xp(t)||bp(t)||!!(Rl&&t&&t[Rl])}function Fo(t,e){return e=null==e?Rt:e,!!e&&(\"number\"==typeof t||Ze.test(t))&&t>-1&&t%1==0&&t<e}function jo(t,e,n){if(!cc(n))return!1;var r=typeof e;return!!(\"number\"==r?Xu(n)&&Fo(e,n.length):\"string\"==r&&e in n)&&$u(n[e],t)}function Bo(t,e){if(xp(t))return!1;var n=typeof t;return!(\"number\"!=n&&\"symbol\"!=n&&\"boolean\"!=n&&null!=t&&!bc(t))||(Oe.test(t)||!Ae.test(t)||null!=e&&t in fl(e))}function Wo(t){var e=typeof t;return\"string\"==e||\"number\"==e||\"symbol\"==e||\"boolean\"==e?\"__proto__\"!==t:null===t}function Vo(t){var e=Co(t),r=n[e];if(\"function\"!=typeof r||!(e in b.prototype))return!1;if(t===r)return!0;var i=Sf(r);return!!i&&t===i[0]}function zo(t){return!!wl&&wl in t}function Ho(t){var e=t&&t.constructor,n=\"function\"==typeof e&&e.prototype||ml;return t===n}function qo(t){return t===t&&!cc(t)}function Yo(t,e){return function(n){return null!=n&&(n[t]===e&&(e!==it||t in fl(n)))}}function Ko(t){var e=Ru(t,function(t){return n.size===lt&&n.clear(),t}),n=e.cache;return e}function Go(t,e){var n=t[1],r=e[1],i=n|r,o=i<(mt|yt|Mt),a=r==Mt&&n==bt||r==Mt&&n==kt&&t[7].length<=e[8]||r==(Mt|kt)&&e[7].length<=e[8]&&n==bt;if(!o&&!a)return t;r&mt&&(t[2]=e[2],i|=n&mt?0:_t);var u=e[3];if(u){var c=t[3];t[3]=c?Fi(c,u,e[4]):u,t[4]=c?G(t[3],ft):e[4]}return u=e[5],u&&(c=t[5],t[5]=c?ji(c,u,e[6]):u,t[6]=c?G(t[5],ft):e[6]),u=e[7],u&&(t[7]=u),r&Mt&&(t[8]=null==t[8]?e[8]:Xl(t[8],e[8])),null==t[9]&&(t[9]=e[9]),t[0]=e[0],t[1]=i,t}function $o(t){var e=[];if(null!=t)for(var n in fl(t))e.push(n);return e}function Xo(t){return Cl.call(t)}function Zo(t,e,n){return e=$l(e===it?t.length-1:e,0),function(){for(var r=arguments,i=-1,o=$l(r.length-e,0),a=al(o);++i<o;)a[i]=r[e+i];i=-1;for(var c=al(e+1);++i<e;)c[i]=r[i];return c[e]=n(a),u(t,this,c)}}function Qo(t,e){return e.length<2?t:cr(t,li(e,0,-1))}function Jo(t,e){for(var n=t.length,r=Xl(e.length,n),i=Bi(t);r--;){var o=e[r];t[r]=Fo(o,n)?i[o]:it}return t}function ta(t,e,n){var r=e+\"\";return Rf(t,Lo(r,oa(Ao(r),n)))}function ea(t){var e=0,n=0;return function(){var r=Zl(),i=Nt-(r-n);if(n=r,i>0){if(++e>=Pt)return arguments[0]}else e=0;return t.apply(it,arguments)}}function na(t,e){var n=-1,r=t.length,i=r-1;for(e=e===it?r:e;++n<e;){var o=ni(n,i),a=t[o];t[o]=t[n],t[n]=a}return t.length=e,t}function ra(t){if(\"string\"==typeof t||bc(t))return t;var e=t+\"\";return\"0\"==e&&1/t==-Dt?\"-0\":e}function ia(t){if(null!=t){try{return _l.call(t)}catch(t){}try{return t+\"\"}catch(t){}}return\"\"}function oa(t,e){return s(Wt,function(n){var r=\"_.\"+n[0];e&n[1]&&!h(t,r)&&t.push(r)}),t.sort()}function aa(t){if(t instanceof b)return t.clone();var e=new i(t.__wrapped__,t.__chain__);return e.__actions__=Bi(t.__actions__),e.__index__=t.__index__,e.__values__=t.__values__,e}function ua(t,e,n){e=(n?jo(t,e,n):e===it)?1:$l(Ec(e),0);var r=null==t?0:t.length;if(!r||e<1)return[];for(var i=0,o=0,a=al(Vl(r/e));i<r;)a[o++]=li(t,i,i+=e);return a}function ca(t){for(var e=-1,n=null==t?0:t.length,r=0,i=[];++e<n;){var o=t[e];o&&(i[r++]=o)}return i}function sa(){var t=arguments.length;if(!t)return[];for(var e=al(t-1),n=arguments[0],r=t;r--;)e[r-1]=arguments[r];return g(xp(n)?Bi(n):[n],er(e,1))}function la(t,e,n){var r=null==t?0:t.length;return r?(e=n||e===it?1:Ec(e),li(t,e<0?0:e,r)):[]}function fa(t,e,n){var r=null==t?0:t.length;return r?(e=n||e===it?1:Ec(e),e=r-e,li(t,0,e<0?0:e)):[]}function pa(t,e){return t&&t.length?bi(t,ko(e,3),!0,!0):[]}function ha(t,e){return t&&t.length?bi(t,ko(e,3),!0):[]}function da(t,e,n,r){var i=null==t?0:t.length;return i?(n&&\"number\"!=typeof n&&jo(t,e,n)&&(n=0,r=i),Jn(t,e,n,r)):[]}function va(t,e,n){var r=null==t?0:t.length;if(!r)return-1;var i=null==n?0:Ec(n);return i<0&&(i=$l(r+i,0)),C(t,ko(e,3),i)}function ga(t,e,n){var r=null==t?0:t.length;if(!r)return-1;var i=r-1;return n!==it&&(i=Ec(n),i=n<0?$l(r+i,0):Xl(i,r-1)),C(t,ko(e,3),i,!0)}function ma(t){var e=null==t?0:t.length;return e?er(t,1):[]}function ya(t){var e=null==t?0:t.length;return e?er(t,Dt):[]}function _a(t,e){var n=null==t?0:t.length;return n?(e=e===it?1:Ec(e),er(t,e)):[]}function ba(t){for(var e=-1,n=null==t?0:t.length,r={};++e<n;){var i=t[e];r[i[0]]=i[1]}return r}function xa(t){return t&&t.length?t[0]:it}function wa(t,e,n){var r=null==t?0:t.length;if(!r)return-1;var i=null==n?0:Ec(n);return i<0&&(i=$l(r+i,0)),M(t,e,i)}function Ca(t){var e=null==t?0:t.length;return e?li(t,0,-1):[]}function Ma(t,e){return null==t?\"\":Kl.call(t,e)}function ka(t){var e=null==t?0:t.length;return e?t[e-1]:it}function Ea(t,e,n){var r=null==t?0:t.length;if(!r)return-1;var i=r;return n!==it&&(i=Ec(n),i=i<0?$l(r+i,0):Xl(i,r-1)),e===e?Q(t,e,i):C(t,E,i,!0)}function Ta(t,e){return t&&t.length?$r(t,Ec(e)):it}function Sa(t,e){return t&&t.length&&e&&e.length?ti(t,e):t}function Pa(t,e,n){return t&&t.length&&e&&e.length?ti(t,e,ko(n,2)):t}function Na(t,e,n){return t&&t.length&&e&&e.length?ti(t,e,it,n):t}function Aa(t,e){var n=[];if(!t||!t.length)return n;var r=-1,i=[],o=t.length;for(e=ko(e,3);++r<o;){var a=t[r];e(a,r,t)&&(n.push(a),i.push(r))}return ei(t,i),n}function Oa(t){return null==t?t:tf.call(t)}function Ia(t,e,n){var r=null==t?0:t.length;return r?(n&&\"number\"!=typeof n&&jo(t,e,n)?(e=0,n=r):(e=null==e?0:Ec(e),n=n===it?r:Ec(n)),li(t,e,n)):[]}function Da(t,e){return pi(t,e)}function Ra(t,e,n){return hi(t,e,ko(n,2))}function La(t,e){var n=null==t?0:t.length;if(n){var r=pi(t,e);if(r<n&&$u(t[r],e))return r}return-1}function Ua(t,e){return pi(t,e,!0)}function Fa(t,e,n){return hi(t,e,ko(n,2),!0)}function ja(t,e){var n=null==t?0:t.length;if(n){var r=pi(t,e,!0)-1;if($u(t[r],e))return r}return-1}function Ba(t){return t&&t.length?di(t):[]}function Wa(t,e){return t&&t.length?di(t,ko(e,2)):[]}function Va(t){var e=null==t?0:t.length;return e?li(t,1,e):[]}function za(t,e,n){return t&&t.length?(e=n||e===it?1:Ec(e),li(t,0,e<0?0:e)):[]}function Ha(t,e,n){var r=null==t?0:t.length;return r?(e=n||e===it?1:Ec(e),e=r-e,li(t,e<0?0:e,r)):[]}function qa(t,e){return t&&t.length?bi(t,ko(e,3),!1,!0):[]}function Ya(t,e){return t&&t.length?bi(t,ko(e,3)):[]}function Ka(t){return t&&t.length?mi(t):[]}function Ga(t,e){return t&&t.length?mi(t,ko(e,2)):[]}function $a(t,e){return e=\"function\"==typeof e?e:it,t&&t.length?mi(t,it,e):[]}function Xa(t){if(!t||!t.length)return[];var e=0;return t=p(t,function(t){if(Zu(t))return e=$l(t.length,e),!0}),I(e,function(e){return v(t,S(e))})}function Za(t,e){if(!t||!t.length)return[];var n=Xa(t);return null==e?n:v(n,function(t){return u(e,it,t)})}function Qa(t,e){return Ci(t||[],e||[],On)}function Ja(t,e){return Ci(t||[],e||[],ci)}function tu(t){var e=n(t);return e.__chain__=!0,e}function eu(t,e){return e(t),t}function nu(t,e){return e(t)}function ru(){return tu(this)}function iu(){return new i(this.value(),this.__chain__)}function ou(){this.__values__===it&&(this.__values__=Mc(this.value()));var t=this.__index__>=this.__values__.length,e=t?it:this.__values__[this.__index__++];return{done:t,value:e}}function au(){return this}function uu(t){for(var e,n=this;n instanceof r;){var i=aa(n);i.__index__=0,i.__values__=it,e?o.__wrapped__=i:e=i;var o=i;n=n.__wrapped__}return o.__wrapped__=t,e}function cu(){var t=this.__wrapped__;if(t instanceof b){var e=t;return this.__actions__.length&&(e=new b(this)),e=e.reverse(),e.__actions__.push({func:nu,args:[Oa],thisArg:it}),new i(e,this.__chain__)}return this.thru(Oa)}function su(){return xi(this.__wrapped__,this.__actions__)}function lu(t,e,n){\n",
       "var r=xp(t)?f:Kn;return n&&jo(t,e,n)&&(e=it),r(t,ko(e,3))}function fu(t,e){var n=xp(t)?p:tr;return n(t,ko(e,3))}function pu(t,e){return er(yu(t,e),1)}function hu(t,e){return er(yu(t,e),Dt)}function du(t,e,n){return n=n===it?1:Ec(n),er(yu(t,e),n)}function vu(t,e){var n=xp(t)?s:_f;return n(t,ko(e,3))}function gu(t,e){var n=xp(t)?l:bf;return n(t,ko(e,3))}function mu(t,e,n,r){t=Xu(t)?t:rs(t),n=n&&!r?Ec(n):0;var i=t.length;return n<0&&(n=$l(i+n,0)),_c(t)?n<=i&&t.indexOf(e,n)>-1:!!i&&M(t,e,n)>-1}function yu(t,e){var n=xp(t)?v:Hr;return n(t,ko(e,3))}function _u(t,e,n,r){return null==t?[]:(xp(e)||(e=null==e?[]:[e]),n=r?it:n,xp(n)||(n=null==n?[]:[n]),Xr(t,e,n))}function bu(t,e,n){var r=xp(t)?m:N,i=arguments.length<3;return r(t,ko(e,4),n,i,_f)}function xu(t,e,n){var r=xp(t)?y:N,i=arguments.length<3;return r(t,ko(e,4),n,i,bf)}function wu(t,e){var n=xp(t)?p:tr;return n(t,Lu(ko(e,3)))}function Cu(t){var e=xp(t)?Sn:ai;return e(t)}function Mu(t,e,n){e=(n?jo(t,e,n):e===it)?1:Ec(e);var r=xp(t)?Pn:ui;return r(t,e)}function ku(t){var e=xp(t)?Nn:si;return e(t)}function Eu(t){if(null==t)return 0;if(Xu(t))return _c(t)?J(t):t.length;var e=Af(t);return e==Zt||e==ie?t.size:Wr(t).length}function Tu(t,e,n){var r=xp(t)?_:fi;return n&&jo(t,e,n)&&(e=it),r(t,ko(e,3))}function Su(t,e){if(\"function\"!=typeof e)throw new dl(ct);return t=Ec(t),function(){if(--t<1)return e.apply(this,arguments)}}function Pu(t,e,n){return e=n?it:e,e=t&&null==e?t.length:e,po(t,Mt,it,it,it,it,e)}function Nu(t,e){var n;if(\"function\"!=typeof e)throw new dl(ct);return t=Ec(t),function(){return--t>0&&(n=e.apply(this,arguments)),t<=1&&(e=it),n}}function Au(t,e,n){e=n?it:e;var r=po(t,bt,it,it,it,it,it,e);return r.placeholder=Au.placeholder,r}function Ou(t,e,n){e=n?it:e;var r=po(t,xt,it,it,it,it,it,e);return r.placeholder=Ou.placeholder,r}function Iu(t,e,n){function r(e){var n=p,r=h;return p=h=it,y=e,v=t.apply(r,n)}function i(t){return y=t,g=Df(u,e),_?r(t):v}function o(t){var n=t-m,r=t-y,i=e-n;return b?Xl(i,d-r):i}function a(t){var n=t-m,r=t-y;return m===it||n>=e||n<0||b&&r>=d}function u(){var t=sp();return a(t)?c(t):void(g=Df(u,o(t)))}function c(t){return g=it,x&&p?r(t):(p=h=it,v)}function s(){g!==it&&Ef(g),y=0,p=m=h=g=it}function l(){return g===it?v:c(sp())}function f(){var t=sp(),n=a(t);if(p=arguments,h=this,m=t,n){if(g===it)return i(m);if(b)return g=Df(u,e),r(m)}return g===it&&(g=Df(u,e)),v}var p,h,d,v,g,m,y=0,_=!1,b=!1,x=!0;if(\"function\"!=typeof t)throw new dl(ct);return e=Sc(e)||0,cc(n)&&(_=!!n.leading,b=\"maxWait\"in n,d=b?$l(Sc(n.maxWait)||0,e):d,x=\"trailing\"in n?!!n.trailing:x),f.cancel=s,f.flush=l,f}function Du(t){return po(t,Et)}function Ru(t,e){if(\"function\"!=typeof t||null!=e&&\"function\"!=typeof e)throw new dl(ct);var n=function(){var r=arguments,i=e?e.apply(this,r):r[0],o=n.cache;if(o.has(i))return o.get(i);var a=t.apply(this,r);return n.cache=o.set(i,a)||o,a};return n.cache=new(Ru.Cache||pn),n}function Lu(t){if(\"function\"!=typeof t)throw new dl(ct);return function(){var e=arguments;switch(e.length){case 0:return!t.call(this);case 1:return!t.call(this,e[0]);case 2:return!t.call(this,e[0],e[1]);case 3:return!t.call(this,e[0],e[1],e[2])}return!t.apply(this,e)}}function Uu(t){return Nu(2,t)}function Fu(t,e){if(\"function\"!=typeof t)throw new dl(ct);return e=e===it?e:Ec(e),oi(t,e)}function ju(t,e){if(\"function\"!=typeof t)throw new dl(ct);return e=null==e?0:$l(Ec(e),0),oi(function(n){var r=n[e],i=Ti(n,0,e);return r&&g(i,r),u(t,this,i)})}function Bu(t,e,n){var r=!0,i=!0;if(\"function\"!=typeof t)throw new dl(ct);return cc(n)&&(r=\"leading\"in n?!!n.leading:r,i=\"trailing\"in n?!!n.trailing:i),Iu(t,e,{leading:r,maxWait:e,trailing:i})}function Wu(t){return Pu(t,1)}function Vu(t,e){return vp(ki(e),t)}function zu(){if(!arguments.length)return[];var t=arguments[0];return xp(t)?t:[t]}function Hu(t){return Bn(t,dt)}function qu(t,e){return e=\"function\"==typeof e?e:it,Bn(t,dt,e)}function Yu(t){return Bn(t,pt|dt)}function Ku(t,e){return e=\"function\"==typeof e?e:it,Bn(t,pt|dt,e)}function Gu(t,e){return null==e||Vn(t,e,Hc(e))}function $u(t,e){return t===e||t!==t&&e!==e}function Xu(t){return null!=t&&uc(t.length)&&!oc(t)}function Zu(t){return sc(t)&&Xu(t)}function Qu(t){return t===!0||t===!1||sc(t)&&fr(t)==qt}function Ju(t){return sc(t)&&1===t.nodeType&&!mc(t)}function tc(t){if(null==t)return!0;if(Xu(t)&&(xp(t)||\"string\"==typeof t||\"function\"==typeof t.splice||Cp(t)||Sp(t)||bp(t)))return!t.length;var e=Af(t);if(e==Zt||e==ie)return!t.size;if(Ho(t))return!Wr(t).length;for(var n in t)if(bl.call(t,n))return!1;return!0}function ec(t,e){return Or(t,e)}function nc(t,e,n){n=\"function\"==typeof n?n:it;var r=n?n(t,e):it;return r===it?Or(t,e,it,n):!!r}function rc(t){if(!sc(t))return!1;var e=fr(t);return e==Gt||e==Kt||\"string\"==typeof t.message&&\"string\"==typeof t.name&&!mc(t)}function ic(t){return\"number\"==typeof t&&Yl(t)}function oc(t){if(!cc(t))return!1;var e=fr(t);return e==$t||e==Xt||e==Ht||e==ne}function ac(t){return\"number\"==typeof t&&t==Ec(t)}function uc(t){return\"number\"==typeof t&&t>-1&&t%1==0&&t<=Rt}function cc(t){var e=typeof t;return null!=t&&(\"object\"==e||\"function\"==e)}function sc(t){return null!=t&&\"object\"==typeof t}function lc(t,e){return t===e||Rr(t,e,To(e))}function fc(t,e,n){return n=\"function\"==typeof n?n:it,Rr(t,e,To(e),n)}function pc(t){return gc(t)&&t!=+t}function hc(t){if(Of(t))throw new cl(ut);return Lr(t)}function dc(t){return null===t}function vc(t){return null==t}function gc(t){return\"number\"==typeof t||sc(t)&&fr(t)==Qt}function mc(t){if(!sc(t)||fr(t)!=te)return!1;var e=Al(t);if(null===e)return!0;var n=bl.call(e,\"constructor\")&&e.constructor;return\"function\"==typeof n&&n instanceof n&&_l.call(n)==Ml}function yc(t){return ac(t)&&t>=-Rt&&t<=Rt}function _c(t){return\"string\"==typeof t||!xp(t)&&sc(t)&&fr(t)==oe}function bc(t){return\"symbol\"==typeof t||sc(t)&&fr(t)==ae}function xc(t){return t===it}function wc(t){return sc(t)&&Af(t)==ce}function Cc(t){return sc(t)&&fr(t)==se}function Mc(t){if(!t)return[];if(Xu(t))return _c(t)?tt(t):Bi(t);if(Ll&&t[Ll])return q(t[Ll]());var e=Af(t),n=e==Zt?Y:e==ie?$:rs;return n(t)}function kc(t){if(!t)return 0===t?t:0;if(t=Sc(t),t===Dt||t===-Dt){var e=t<0?-1:1;return e*Lt}return t===t?t:0}function Ec(t){var e=kc(t),n=e%1;return e===e?n?e-n:e:0}function Tc(t){return t?jn(Ec(t),0,Ft):0}function Sc(t){if(\"number\"==typeof t)return t;if(bc(t))return Ut;if(cc(t)){var e=\"function\"==typeof t.valueOf?t.valueOf():t;t=cc(e)?e+\"\":e}if(\"string\"!=typeof t)return 0===t?t:+t;t=t.replace(Ue,\"\");var n=Ge.test(t);return n||Xe.test(t)?ir(t.slice(2),n?2:8):Ke.test(t)?Ut:+t}function Pc(t){return Wi(t,qc(t))}function Nc(t){return t?jn(Ec(t),-Rt,Rt):0===t?t:0}function Ac(t){return null==t?\"\":gi(t)}function Oc(t,e){var n=yf(t);return null==e?n:Rn(n,e)}function Ic(t,e){return w(t,ko(e,3),nr)}function Dc(t,e){return w(t,ko(e,3),or)}function Rc(t,e){return null==t?t:xf(t,ko(e,3),qc)}function Lc(t,e){return null==t?t:wf(t,ko(e,3),qc)}function Uc(t,e){return t&&nr(t,ko(e,3))}function Fc(t,e){return t&&or(t,ko(e,3))}function jc(t){return null==t?[]:ar(t,Hc(t))}function Bc(t){return null==t?[]:ar(t,qc(t))}function Wc(t,e,n){var r=null==t?it:cr(t,e);return r===it?n:r}function Vc(t,e){return null!=t&&Oo(t,e,_r)}function zc(t,e){return null!=t&&Oo(t,e,Cr)}function Hc(t){return Xu(t)?Tn(t):Wr(t)}function qc(t){return Xu(t)?Tn(t,!0):Vr(t)}function Yc(t,e){var n={};return e=ko(e,3),nr(t,function(t,r,i){Un(n,e(t,r,i),t)}),n}function Kc(t,e){var n={};return e=ko(e,3),nr(t,function(t,r,i){Un(n,r,e(t,r,i))}),n}function Gc(t,e){return $c(t,Lu(ko(e)))}function $c(t,e){if(null==t)return{};var n=v(wo(t),function(t){return[t]});return e=ko(e),Qr(t,n,function(t,n){return e(t,n[0])})}function Xc(t,e,n){e=Ei(e,t);var r=-1,i=e.length;for(i||(i=1,t=it);++r<i;){var o=null==t?it:t[ra(e[r])];o===it&&(r=i,o=n),t=oc(o)?o.call(t):o}return t}function Zc(t,e,n){return null==t?t:ci(t,e,n)}function Qc(t,e,n,r){return r=\"function\"==typeof r?r:it,null==t?t:ci(t,e,n,r)}function Jc(t,e,n){var r=xp(t),i=r||Cp(t)||Sp(t);if(e=ko(e,4),null==n){var o=t&&t.constructor;n=i?r?new o:[]:cc(t)&&oc(o)?yf(Al(t)):{}}return(i?s:nr)(t,function(t,r,i){return e(n,t,r,i)}),n}function ts(t,e){return null==t||yi(t,e)}function es(t,e,n){return null==t?t:_i(t,e,ki(n))}function ns(t,e,n,r){return r=\"function\"==typeof r?r:it,null==t?t:_i(t,e,ki(n),r)}function rs(t){return null==t?[]:L(t,Hc(t))}function is(t){return null==t?[]:L(t,qc(t))}function os(t,e,n){return n===it&&(n=e,e=it),n!==it&&(n=Sc(n),n=n===n?n:0),e!==it&&(e=Sc(e),e=e===e?e:0),jn(Sc(t),e,n)}function as(t,e,n){return e=kc(e),n===it?(n=e,e=0):n=kc(n),t=Sc(t),kr(t,e,n)}function us(t,e,n){if(n&&\"boolean\"!=typeof n&&jo(t,e,n)&&(e=n=it),n===it&&(\"boolean\"==typeof e?(n=e,e=it):\"boolean\"==typeof t&&(n=t,t=it)),t===it&&e===it?(t=0,e=1):(t=kc(t),e===it?(e=t,t=0):e=kc(e)),t>e){var r=t;t=e,e=r}if(n||t%1||e%1){var i=Jl();return Xl(t+i*(e-t+rr(\"1e-\"+((i+\"\").length-1))),e)}return ni(t,e)}function cs(t){return th(Ac(t).toLowerCase())}function ss(t){return t=Ac(t),t&&t.replace(Qe,br).replace(Hn,\"\")}function ls(t,e,n){t=Ac(t),e=gi(e);var r=t.length;n=n===it?r:jn(Ec(n),0,r);var i=n;return n-=e.length,n>=0&&t.slice(n,i)==e}function fs(t){return t=Ac(t),t&&Te.test(t)?t.replace(ke,xr):t}function ps(t){return t=Ac(t),t&&Le.test(t)?t.replace(Re,\"\\\\$&\"):t}function hs(t,e,n){t=Ac(t),e=Ec(e);var r=e?J(t):0;if(!e||r>=e)return t;var i=(e-r)/2;return oo(zl(i),n)+t+oo(Vl(i),n)}function ds(t,e,n){t=Ac(t),e=Ec(e);var r=e?J(t):0;return e&&r<e?t+oo(e-r,n):t}function vs(t,e,n){t=Ac(t),e=Ec(e);var r=e?J(t):0;return e&&r<e?oo(e-r,n)+t:t}function gs(t,e,n){return n||null==e?e=0:e&&(e=+e),Ql(Ac(t).replace(Fe,\"\"),e||0)}function ms(t,e,n){return e=(n?jo(t,e,n):e===it)?1:Ec(e),ii(Ac(t),e)}function ys(){var t=arguments,e=Ac(t[0]);return t.length<3?e:e.replace(t[1],t[2])}function _s(t,e,n){return n&&\"number\"!=typeof n&&jo(t,e,n)&&(e=n=it),(n=n===it?Ft:n>>>0)?(t=Ac(t),t&&(\"string\"==typeof e||null!=e&&!Ep(e))&&(e=gi(e),!e&&z(t))?Ti(tt(t),0,n):t.split(e,n)):[]}function bs(t,e,n){return t=Ac(t),n=null==n?0:jn(Ec(n),0,t.length),e=gi(e),t.slice(n,n+e.length)==e}function xs(t,e,r){var i=n.templateSettings;r&&jo(t,e,r)&&(e=it),t=Ac(t),e=Ip({},e,i,ho);var o,a,u=Ip({},e.imports,i.imports,ho),c=Hc(u),s=L(u,c),l=0,f=e.interpolate||Je,p=\"__p += '\",h=pl((e.escape||Je).source+\"|\"+f.source+\"|\"+(f===Ne?qe:Je).source+\"|\"+(e.evaluate||Je).source+\"|$\",\"g\"),d=\"//# sourceURL=\"+(\"sourceURL\"in e?e.sourceURL:\"lodash.templateSources[\"+ ++Xn+\"]\")+\"\\n\";t.replace(h,function(e,n,r,i,u,c){return r||(r=i),p+=t.slice(l,c).replace(tn,W),n&&(o=!0,p+=\"' +\\n__e(\"+n+\") +\\n'\"),u&&(a=!0,p+=\"';\\n\"+u+\";\\n__p += '\"),r&&(p+=\"' +\\n((__t = (\"+r+\")) == null ? '' : __t) +\\n'\"),l=c+e.length,e}),p+=\"';\\n\";var v=e.variable;v||(p=\"with (obj) {\\n\"+p+\"\\n}\\n\"),p=(a?p.replace(xe,\"\"):p).replace(we,\"$1\").replace(Ce,\"$1;\"),p=\"function(\"+(v||\"obj\")+\") {\\n\"+(v?\"\":\"obj || (obj = {});\\n\")+\"var __t, __p = ''\"+(o?\", __e = _.escape\":\"\")+(a?\", __j = Array.prototype.join;\\nfunction print() { __p += __j.call(arguments, '') }\\n\":\";\\n\")+p+\"return __p\\n}\";var g=eh(function(){return sl(c,d+\"return \"+p).apply(it,s)});if(g.source=p,rc(g))throw g;return g}function ws(t){return Ac(t).toLowerCase()}function Cs(t){return Ac(t).toUpperCase()}function Ms(t,e,n){if(t=Ac(t),t&&(n||e===it))return t.replace(Ue,\"\");if(!t||!(e=gi(e)))return t;var r=tt(t),i=tt(e),o=F(r,i),a=j(r,i)+1;return Ti(r,o,a).join(\"\")}function ks(t,e,n){if(t=Ac(t),t&&(n||e===it))return t.replace(je,\"\");if(!t||!(e=gi(e)))return t;var r=tt(t),i=j(r,tt(e))+1;return Ti(r,0,i).join(\"\")}function Es(t,e,n){if(t=Ac(t),t&&(n||e===it))return t.replace(Fe,\"\");if(!t||!(e=gi(e)))return t;var r=tt(t),i=F(r,tt(e));return Ti(r,i).join(\"\")}function Ts(t,e){var n=Tt,r=St;if(cc(e)){var i=\"separator\"in e?e.separator:i;n=\"length\"in e?Ec(e.length):n,r=\"omission\"in e?gi(e.omission):r}t=Ac(t);var o=t.length;if(z(t)){var a=tt(t);o=a.length}if(n>=o)return t;var u=n-J(r);if(u<1)return r;var c=a?Ti(a,0,u).join(\"\"):t.slice(0,u);if(i===it)return c+r;if(a&&(u+=c.length-u),Ep(i)){if(t.slice(u).search(i)){var s,l=c;for(i.global||(i=pl(i.source,Ac(Ye.exec(i))+\"g\")),i.lastIndex=0;s=i.exec(l);)var f=s.index;c=c.slice(0,f===it?u:f)}}else if(t.indexOf(gi(i),u)!=u){var p=c.lastIndexOf(i);p>-1&&(c=c.slice(0,p))}return c+r}function Ss(t){return t=Ac(t),t&&Ee.test(t)?t.replace(Me,wr):t}function Ps(t,e,n){return t=Ac(t),e=n?it:e,e===it?H(t)?rt(t):x(t):t.match(e)||[]}function Ns(t){var e=null==t?0:t.length,n=ko();return t=e?v(t,function(t){if(\"function\"!=typeof t[1])throw new dl(ct);return[n(t[0]),t[1]]}):[],oi(function(n){for(var r=-1;++r<e;){var i=t[r];if(u(i[0],this,n))return u(i[1],this,n)}})}function As(t){return Wn(Bn(t,pt))}function Os(t){return function(){return t}}function Is(t,e){return null==t||t!==t?e:t}function Ds(t){return t}function Rs(t){return Br(\"function\"==typeof t?t:Bn(t,pt))}function Ls(t){return qr(Bn(t,pt))}function Us(t,e){return Yr(t,Bn(e,pt))}function Fs(t,e,n){var r=Hc(e),i=ar(e,r);null!=n||cc(e)&&(i.length||!r.length)||(n=e,e=t,t=this,i=ar(e,Hc(e)));var o=!(cc(n)&&\"chain\"in n&&!n.chain),a=oc(t);return s(i,function(n){var r=e[n];t[n]=r,a&&(t.prototype[n]=function(){var e=this.__chain__;if(o||e){var n=t(this.__wrapped__),i=n.__actions__=Bi(this.__actions__);return i.push({func:r,args:arguments,thisArg:t}),n.__chain__=e,n}return r.apply(t,g([this.value()],arguments))})}),t}function js(){return ur._===this&&(ur._=kl),this}function Bs(){}function Ws(t){return t=Ec(t),oi(function(e){return $r(e,t)})}function Vs(t){return Bo(t)?S(ra(t)):Jr(t)}function zs(t){return function(e){return null==t?it:cr(t,e)}}function Hs(){return[]}function qs(){return!1}function Ys(){return{}}function Ks(){return\"\"}function Gs(){return!0}function $s(t,e){if(t=Ec(t),t<1||t>Rt)return[];var n=Ft,r=Xl(t,Ft);e=ko(e),t-=Ft;for(var i=I(r,e);++n<t;)e(n);return i}function Xs(t){return xp(t)?v(t,ra):bc(t)?[t]:Bi(Lf(Ac(t)))}function Zs(t){var e=++xl;return Ac(t)+e}function Qs(t){return t&&t.length?Gn(t,Ds,pr):it}function Js(t,e){return t&&t.length?Gn(t,ko(e,2),pr):it}function tl(t){return T(t,Ds)}function el(t,e){return T(t,ko(e,2))}function nl(t){return t&&t.length?Gn(t,Ds,zr):it}function rl(t,e){return t&&t.length?Gn(t,ko(e,2),zr):it}function il(t){return t&&t.length?O(t,Ds):0}function ol(t,e){return t&&t.length?O(t,ko(e,2)):0}e=null==e?ur:Mr.defaults(ur.Object(),e,Mr.pick(ur,$n));var al=e.Array,ul=e.Date,cl=e.Error,sl=e.Function,ll=e.Math,fl=e.Object,pl=e.RegExp,hl=e.String,dl=e.TypeError,vl=al.prototype,gl=sl.prototype,ml=fl.prototype,yl=e[\"__core-js_shared__\"],_l=gl.toString,bl=ml.hasOwnProperty,xl=0,wl=function(){var t=/[^.]+$/.exec(yl&&yl.keys&&yl.keys.IE_PROTO||\"\");return t?\"Symbol(src)_1.\"+t:\"\"}(),Cl=ml.toString,Ml=_l.call(fl),kl=ur._,El=pl(\"^\"+_l.call(bl).replace(Re,\"\\\\$&\").replace(/hasOwnProperty|(function).*?(?=\\\\\\()| for .+?(?=\\\\\\])/g,\"$1.*?\")+\"$\"),Tl=lr?e.Buffer:it,Sl=e.Symbol,Pl=e.Uint8Array,Nl=Tl?Tl.allocUnsafe:it,Al=K(fl.getPrototypeOf,fl),Ol=fl.create,Il=ml.propertyIsEnumerable,Dl=vl.splice,Rl=Sl?Sl.isConcatSpreadable:it,Ll=Sl?Sl.iterator:it,Ul=Sl?Sl.toStringTag:it,Fl=function(){try{var t=So(fl,\"defineProperty\");return t({},\"\",{}),t}catch(t){}}(),jl=e.clearTimeout!==ur.clearTimeout&&e.clearTimeout,Bl=ul&&ul.now!==ur.Date.now&&ul.now,Wl=e.setTimeout!==ur.setTimeout&&e.setTimeout,Vl=ll.ceil,zl=ll.floor,Hl=fl.getOwnPropertySymbols,ql=Tl?Tl.isBuffer:it,Yl=e.isFinite,Kl=vl.join,Gl=K(fl.keys,fl),$l=ll.max,Xl=ll.min,Zl=ul.now,Ql=e.parseInt,Jl=ll.random,tf=vl.reverse,ef=So(e,\"DataView\"),nf=So(e,\"Map\"),rf=So(e,\"Promise\"),of=So(e,\"Set\"),af=So(e,\"WeakMap\"),uf=So(fl,\"create\"),cf=af&&new af,sf={},lf=ia(ef),ff=ia(nf),pf=ia(rf),hf=ia(of),df=ia(af),vf=Sl?Sl.prototype:it,gf=vf?vf.valueOf:it,mf=vf?vf.toString:it,yf=function(){function t(){}return function(e){if(!cc(e))return{};if(Ol)return Ol(e);t.prototype=e;var n=new t;return t.prototype=it,n}}();n.templateSettings={escape:Se,evaluate:Pe,interpolate:Ne,variable:\"\",imports:{_:n}},n.prototype=r.prototype,n.prototype.constructor=n,i.prototype=yf(r.prototype),i.prototype.constructor=i,b.prototype=yf(r.prototype),b.prototype.constructor=b,nt.prototype.clear=ze,nt.prototype.delete=en,nt.prototype.get=nn,nt.prototype.has=rn,nt.prototype.set=on,an.prototype.clear=un,an.prototype.delete=cn,an.prototype.get=sn,an.prototype.has=ln,an.prototype.set=fn,pn.prototype.clear=hn,pn.prototype.delete=dn,pn.prototype.get=vn,pn.prototype.has=gn,pn.prototype.set=mn,yn.prototype.add=yn.prototype.push=_n,yn.prototype.has=bn,xn.prototype.clear=wn,xn.prototype.delete=Cn,xn.prototype.get=Mn,xn.prototype.has=kn,xn.prototype.set=En;var _f=Yi(nr),bf=Yi(or,!0),xf=Ki(),wf=Ki(!0),Cf=cf?function(t,e){return cf.set(t,e),t}:Ds,Mf=Fl?function(t,e){return Fl(t,\"toString\",{configurable:!0,enumerable:!1,value:Os(e),writable:!0})}:Ds,kf=oi,Ef=jl||function(t){return ur.clearTimeout(t)},Tf=of&&1/$(new of([,-0]))[1]==Dt?function(t){return new of(t)}:Bs,Sf=cf?function(t){return cf.get(t)}:Bs,Pf=Hl?function(t){return null==t?[]:(t=fl(t),p(Hl(t),function(e){return Il.call(t,e)}))}:Hs,Nf=Hl?function(t){for(var e=[];t;)g(e,Pf(t)),t=Al(t);return e}:Hs,Af=fr;(ef&&Af(new ef(new ArrayBuffer(1)))!=fe||nf&&Af(new nf)!=Zt||rf&&Af(rf.resolve())!=ee||of&&Af(new of)!=ie||af&&Af(new af)!=ce)&&(Af=function(t){var e=fr(t),n=e==te?t.constructor:it,r=n?ia(n):\"\";if(r)switch(r){case lf:return fe;case ff:return Zt;case pf:return ee;case hf:return ie;case df:return ce}return e});var Of=yl?oc:qs,If=ea(Cf),Df=Wl||function(t,e){return ur.setTimeout(t,e)},Rf=ea(Mf),Lf=Ko(function(t){var e=[];return Ie.test(t)&&e.push(\"\"),t.replace(De,function(t,n,r,i){e.push(r?i.replace(He,\"$1\"):n||t)}),e}),Uf=oi(function(t,e){return Zu(t)?Yn(t,er(e,1,Zu,!0)):[]}),Ff=oi(function(t,e){var n=ka(e);return Zu(n)&&(n=it),Zu(t)?Yn(t,er(e,1,Zu,!0),ko(n,2)):[]}),jf=oi(function(t,e){var n=ka(e);return Zu(n)&&(n=it),Zu(t)?Yn(t,er(e,1,Zu,!0),it,n):[]}),Bf=oi(function(t){var e=v(t,Mi);return e.length&&e[0]===t[0]?Er(e):[]}),Wf=oi(function(t){var e=ka(t),n=v(t,Mi);return e===ka(n)?e=it:n.pop(),n.length&&n[0]===t[0]?Er(n,ko(e,2)):[]}),Vf=oi(function(t){var e=ka(t),n=v(t,Mi);return e=\"function\"==typeof e?e:it,e&&n.pop(),n.length&&n[0]===t[0]?Er(n,it,e):[]}),zf=oi(Sa),Hf=bo(function(t,e){var n=null==t?0:t.length,r=Fn(t,e);return ei(t,v(e,function(t){return Fo(t,n)?+t:t}).sort(Li)),r}),qf=oi(function(t){return mi(er(t,1,Zu,!0))}),Yf=oi(function(t){var e=ka(t);return Zu(e)&&(e=it),mi(er(t,1,Zu,!0),ko(e,2))}),Kf=oi(function(t){var e=ka(t);return e=\"function\"==typeof e?e:it,mi(er(t,1,Zu,!0),it,e)}),Gf=oi(function(t,e){return Zu(t)?Yn(t,e):[]}),$f=oi(function(t){return wi(p(t,Zu))}),Xf=oi(function(t){var e=ka(t);return Zu(e)&&(e=it),wi(p(t,Zu),ko(e,2))}),Zf=oi(function(t){var e=ka(t);return e=\"function\"==typeof e?e:it,wi(p(t,Zu),it,e)}),Qf=oi(Xa),Jf=oi(function(t){var e=t.length,n=e>1?t[e-1]:it;return n=\"function\"==typeof n?(t.pop(),n):it,Za(t,n)}),tp=bo(function(t){var e=t.length,n=e?t[0]:0,r=this.__wrapped__,o=function(e){return Fn(e,t)};return!(e>1||this.__actions__.length)&&r instanceof b&&Fo(n)?(r=r.slice(n,+n+(e?1:0)),r.__actions__.push({func:nu,args:[o],thisArg:it}),new i(r,this.__chain__).thru(function(t){return e&&!t.length&&t.push(it),t})):this.thru(o)}),ep=Hi(function(t,e,n){bl.call(t,n)?++t[n]:Un(t,n,1)}),np=Ji(va),rp=Ji(ga),ip=Hi(function(t,e,n){bl.call(t,n)?t[n].push(e):Un(t,n,[e])}),op=oi(function(t,e,n){var r=-1,i=\"function\"==typeof e,o=Xu(t)?al(t.length):[];return _f(t,function(t){o[++r]=i?u(e,t,n):Sr(t,e,n)}),o}),ap=Hi(function(t,e,n){Un(t,n,e)}),up=Hi(function(t,e,n){t[n?0:1].push(e)},function(){return[[],[]]}),cp=oi(function(t,e){if(null==t)return[];var n=e.length;return n>1&&jo(t,e[0],e[1])?e=[]:n>2&&jo(e[0],e[1],e[2])&&(e=[e[0]]),Xr(t,er(e,1),[])}),sp=Bl||function(){return ur.Date.now()},lp=oi(function(t,e,n){var r=mt;if(n.length){var i=G(n,Mo(lp));r|=wt}return po(t,r,e,n,i)}),fp=oi(function(t,e,n){var r=mt|yt;if(n.length){var i=G(n,Mo(fp));r|=wt}return po(e,r,t,n,i)}),pp=oi(function(t,e){return qn(t,1,e)}),hp=oi(function(t,e,n){return qn(t,Sc(e)||0,n)});Ru.Cache=pn;var dp=kf(function(t,e){e=1==e.length&&xp(e[0])?v(e[0],R(ko())):v(er(e,1),R(ko()));var n=e.length;return oi(function(r){for(var i=-1,o=Xl(r.length,n);++i<o;)r[i]=e[i].call(this,r[i]);return u(t,this,r)})}),vp=oi(function(t,e){var n=G(e,Mo(vp));return po(t,wt,it,e,n)}),gp=oi(function(t,e){var n=G(e,Mo(gp));return po(t,Ct,it,e,n)}),mp=bo(function(t,e){return po(t,kt,it,it,it,e)}),yp=co(pr),_p=co(function(t,e){return t>=e}),bp=Pr(function(){return arguments}())?Pr:function(t){return sc(t)&&bl.call(t,\"callee\")&&!Il.call(t,\"callee\")},xp=al.isArray,wp=hr?R(hr):Nr,Cp=ql||qs,Mp=dr?R(dr):Ar,kp=vr?R(vr):Dr,Ep=gr?R(gr):Ur,Tp=mr?R(mr):Fr,Sp=yr?R(yr):jr,Pp=co(zr),Np=co(function(t,e){return t<=e}),Ap=qi(function(t,e){if(Ho(e)||Xu(e))return void Wi(e,Hc(e),t);for(var n in e)bl.call(e,n)&&On(t,n,e[n])}),Op=qi(function(t,e){Wi(e,qc(e),t)}),Ip=qi(function(t,e,n,r){Wi(e,qc(e),t,r)}),Dp=qi(function(t,e,n,r){Wi(e,Hc(e),t,r)}),Rp=bo(Fn),Lp=oi(function(t){return t.push(it,ho),u(Ip,it,t)}),Up=oi(function(t){return t.push(it,vo),u(Vp,it,t)}),Fp=no(function(t,e,n){t[e]=n},Os(Ds)),jp=no(function(t,e,n){bl.call(t,e)?t[e].push(n):t[e]=[n]},ko),Bp=oi(Sr),Wp=qi(function(t,e,n){Kr(t,e,n)}),Vp=qi(function(t,e,n,r){Kr(t,e,n,r)}),zp=bo(function(t,e){var n={};if(null==t)return n;var r=!1;e=v(e,function(e){return e=Ei(e,t),r||(r=e.length>1),e}),Wi(t,wo(t),n),r&&(n=Bn(n,pt|ht|dt,go));for(var i=e.length;i--;)yi(n,e[i]);return n}),Hp=bo(function(t,e){return null==t?{}:Zr(t,e)}),qp=fo(Hc),Yp=fo(qc),Kp=Xi(function(t,e,n){return e=e.toLowerCase(),t+(n?cs(e):e)}),Gp=Xi(function(t,e,n){return t+(n?\"-\":\"\")+e.toLowerCase()}),$p=Xi(function(t,e,n){return t+(n?\" \":\"\")+e.toLowerCase()}),Xp=$i(\"toLowerCase\"),Zp=Xi(function(t,e,n){return t+(n?\"_\":\"\")+e.toLowerCase()}),Qp=Xi(function(t,e,n){return t+(n?\" \":\"\")+th(e)}),Jp=Xi(function(t,e,n){return t+(n?\" \":\"\")+e.toUpperCase()}),th=$i(\"toUpperCase\"),eh=oi(function(t,e){try{return u(t,it,e)}catch(t){return rc(t)?t:new cl(t)}}),nh=bo(function(t,e){return s(e,function(e){e=ra(e),Un(t,e,lp(t[e],t))}),t}),rh=to(),ih=to(!0),oh=oi(function(t,e){return function(n){return Sr(n,t,e)}}),ah=oi(function(t,e){return function(n){return Sr(t,n,e)}}),uh=io(v),ch=io(f),sh=io(_),lh=uo(),fh=uo(!0),ph=ro(function(t,e){return t+e},0),hh=lo(\"ceil\"),dh=ro(function(t,e){return t/e},1),vh=lo(\"floor\"),gh=ro(function(t,e){return t*e},1),mh=lo(\"round\"),yh=ro(function(t,e){return t-e},0);return n.after=Su,n.ary=Pu,n.assign=Ap,n.assignIn=Op,n.assignInWith=Ip,n.assignWith=Dp,n.at=Rp,n.before=Nu,n.bind=lp,n.bindAll=nh,n.bindKey=fp,n.castArray=zu,n.chain=tu,n.chunk=ua,n.compact=ca,n.concat=sa,n.cond=Ns,n.conforms=As,n.constant=Os,n.countBy=ep,n.create=Oc,n.curry=Au,n.curryRight=Ou,n.debounce=Iu,n.defaults=Lp,n.defaultsDeep=Up,n.defer=pp,n.delay=hp,n.difference=Uf,n.differenceBy=Ff,n.differenceWith=jf,n.drop=la,n.dropRight=fa,n.dropRightWhile=pa,n.dropWhile=ha,n.fill=da,n.filter=fu,n.flatMap=pu,n.flatMapDeep=hu,n.flatMapDepth=du,n.flatten=ma,n.flattenDeep=ya,n.flattenDepth=_a,n.flip=Du,n.flow=rh,n.flowRight=ih,n.fromPairs=ba,n.functions=jc,n.functionsIn=Bc,n.groupBy=ip,n.initial=Ca,n.intersection=Bf,n.intersectionBy=Wf,n.intersectionWith=Vf,n.invert=Fp,n.invertBy=jp,n.invokeMap=op,n.iteratee=Rs,n.keyBy=ap,n.keys=Hc,n.keysIn=qc,n.map=yu,n.mapKeys=Yc,n.mapValues=Kc,n.matches=Ls,n.matchesProperty=Us,n.memoize=Ru,n.merge=Wp,n.mergeWith=Vp,n.method=oh,n.methodOf=ah,n.mixin=Fs,n.negate=Lu,n.nthArg=Ws,n.omit=zp,n.omitBy=Gc,n.once=Uu,n.orderBy=_u,n.over=uh,n.overArgs=dp,n.overEvery=ch,n.overSome=sh,n.partial=vp,n.partialRight=gp,n.partition=up,n.pick=Hp,n.pickBy=$c,n.property=Vs,n.propertyOf=zs,n.pull=zf,n.pullAll=Sa,n.pullAllBy=Pa,n.pullAllWith=Na,n.pullAt=Hf,n.range=lh,n.rangeRight=fh,n.rearg=mp,n.reject=wu,n.remove=Aa,n.rest=Fu,n.reverse=Oa,n.sampleSize=Mu,n.set=Zc,n.setWith=Qc,n.shuffle=ku,n.slice=Ia,n.sortBy=cp,n.sortedUniq=Ba,n.sortedUniqBy=Wa,n.split=_s,n.spread=ju,n.tail=Va,n.take=za,n.takeRight=Ha,n.takeRightWhile=qa,n.takeWhile=Ya,n.tap=eu,n.throttle=Bu,n.thru=nu,n.toArray=Mc,n.toPairs=qp,n.toPairsIn=Yp,n.toPath=Xs,n.toPlainObject=Pc,n.transform=Jc,n.unary=Wu,n.union=qf,n.unionBy=Yf,n.unionWith=Kf,n.uniq=Ka,n.uniqBy=Ga,n.uniqWith=$a,n.unset=ts,n.unzip=Xa,n.unzipWith=Za,n.update=es,n.updateWith=ns,n.values=rs,n.valuesIn=is,n.without=Gf,n.words=Ps,n.wrap=Vu,n.xor=$f,n.xorBy=Xf,n.xorWith=Zf,n.zip=Qf,n.zipObject=Qa,n.zipObjectDeep=Ja,n.zipWith=Jf,n.entries=qp,n.entriesIn=Yp,n.extend=Op,n.extendWith=Ip,Fs(n,n),n.add=ph,n.attempt=eh,n.camelCase=Kp,n.capitalize=cs,n.ceil=hh,n.clamp=os,n.clone=Hu,n.cloneDeep=Yu,n.cloneDeepWith=Ku,n.cloneWith=qu,n.conformsTo=Gu,n.deburr=ss,n.defaultTo=Is,n.divide=dh,n.endsWith=ls,n.eq=$u,n.escape=fs,n.escapeRegExp=ps,n.every=lu,n.find=np,n.findIndex=va,n.findKey=Ic,n.findLast=rp,n.findLastIndex=ga,n.findLastKey=Dc,n.floor=vh,n.forEach=vu,n.forEachRight=gu,n.forIn=Rc,n.forInRight=Lc,n.forOwn=Uc,n.forOwnRight=Fc,n.get=Wc,n.gt=yp,n.gte=_p,n.has=Vc,n.hasIn=zc,n.head=xa,n.identity=Ds,n.includes=mu,n.indexOf=wa,n.inRange=as,n.invoke=Bp,n.isArguments=bp,n.isArray=xp,n.isArrayBuffer=wp,n.isArrayLike=Xu,n.isArrayLikeObject=Zu,n.isBoolean=Qu,n.isBuffer=Cp,n.isDate=Mp,n.isElement=Ju,n.isEmpty=tc,n.isEqual=ec,n.isEqualWith=nc,n.isError=rc,n.isFinite=ic,n.isFunction=oc,n.isInteger=ac,n.isLength=uc,n.isMap=kp,n.isMatch=lc,n.isMatchWith=fc,n.isNaN=pc,n.isNative=hc,n.isNil=vc,n.isNull=dc,n.isNumber=gc,n.isObject=cc,n.isObjectLike=sc,n.isPlainObject=mc,n.isRegExp=Ep,n.isSafeInteger=yc,n.isSet=Tp,n.isString=_c,n.isSymbol=bc,n.isTypedArray=Sp,n.isUndefined=xc,n.isWeakMap=wc,n.isWeakSet=Cc,n.join=Ma,n.kebabCase=Gp,n.last=ka,n.lastIndexOf=Ea,n.lowerCase=$p,n.lowerFirst=Xp,n.lt=Pp,n.lte=Np,n.max=Qs,n.maxBy=Js,n.mean=tl,n.meanBy=el,n.min=nl,n.minBy=rl,n.stubArray=Hs,n.stubFalse=qs,n.stubObject=Ys,n.stubString=Ks,n.stubTrue=Gs,n.multiply=gh,n.nth=Ta,n.noConflict=js,n.noop=Bs,n.now=sp,n.pad=hs,n.padEnd=ds,n.padStart=vs,n.parseInt=gs,n.random=us,n.reduce=bu,n.reduceRight=xu,n.repeat=ms,n.replace=ys,n.result=Xc,n.round=mh,n.runInContext=t,n.sample=Cu,n.size=Eu,n.snakeCase=Zp,n.some=Tu,n.sortedIndex=Da,n.sortedIndexBy=Ra,n.sortedIndexOf=La,n.sortedLastIndex=Ua,n.sortedLastIndexBy=Fa,n.sortedLastIndexOf=ja,n.startCase=Qp,n.startsWith=bs,n.subtract=yh,n.sum=il,n.sumBy=ol,n.template=xs,n.times=$s,n.toFinite=kc,n.toInteger=Ec,n.toLength=Tc,n.toLower=ws,n.toNumber=Sc,n.toSafeInteger=Nc,n.toString=Ac,n.toUpper=Cs,n.trim=Ms,n.trimEnd=ks,n.trimStart=Es,n.truncate=Ts,n.unescape=Ss,n.uniqueId=Zs,n.upperCase=Jp,n.upperFirst=th,n.each=vu,n.eachRight=gu,n.first=xa,Fs(n,function(){var t={};return nr(n,function(e,r){bl.call(n.prototype,r)||(t[r]=e)}),t}(),{chain:!1}),n.VERSION=ot,s([\"bind\",\"bindKey\",\"curry\",\"curryRight\",\"partial\",\"partialRight\"],function(t){n[t].placeholder=n}),s([\"drop\",\"take\"],function(t,e){b.prototype[t]=function(n){n=n===it?1:$l(Ec(n),0);var r=this.__filtered__&&!e?new b(this):this.clone();return r.__filtered__?r.__takeCount__=Xl(n,r.__takeCount__):r.__views__.push({size:Xl(n,Ft),type:t+(r.__dir__<0?\"Right\":\"\")}),r},b.prototype[t+\"Right\"]=function(e){return this.reverse()[t](e).reverse()}}),s([\"filter\",\"map\",\"takeWhile\"],function(t,e){var n=e+1,r=n==At||n==It;b.prototype[t]=function(t){var e=this.clone();return e.__iteratees__.push({iteratee:ko(t,3),type:n}),e.__filtered__=e.__filtered__||r,e}}),s([\"head\",\"last\"],function(t,e){var n=\"take\"+(e?\"Right\":\"\");b.prototype[t]=function(){return this[n](1).value()[0]}}),s([\"initial\",\"tail\"],function(t,e){var n=\"drop\"+(e?\"\":\"Right\");b.prototype[t]=function(){return this.__filtered__?new b(this):this[n](1)}}),b.prototype.compact=function(){return this.filter(Ds)},b.prototype.find=function(t){return this.filter(t).head()},b.prototype.findLast=function(t){return this.reverse().find(t)},b.prototype.invokeMap=oi(function(t,e){return\"function\"==typeof t?new b(this):this.map(function(n){return Sr(n,t,e)})}),b.prototype.reject=function(t){return this.filter(Lu(ko(t)))},b.prototype.slice=function(t,e){t=Ec(t);var n=this;return n.__filtered__&&(t>0||e<0)?new b(n):(t<0?n=n.takeRight(-t):t&&(n=n.drop(t)),e!==it&&(e=Ec(e),n=e<0?n.dropRight(-e):n.take(e-t)),n)},b.prototype.takeRightWhile=function(t){return this.reverse().takeWhile(t).reverse()},b.prototype.toArray=function(){return this.take(Ft)},nr(b.prototype,function(t,e){var r=/^(?:filter|find|map|reject)|While$/.test(e),o=/^(?:head|last)$/.test(e),a=n[o?\"take\"+(\"last\"==e?\"Right\":\"\"):e],u=o||/^find/.test(e);a&&(n.prototype[e]=function(){var e=this.__wrapped__,c=o?[1]:arguments,s=e instanceof b,l=c[0],f=s||xp(e),p=function(t){var e=a.apply(n,g([t],c));return o&&h?e[0]:e};f&&r&&\"function\"==typeof l&&1!=l.length&&(s=f=!1);var h=this.__chain__,d=!!this.__actions__.length,v=u&&!h,m=s&&!d;if(!u&&f){e=m?e:new b(this);var y=t.apply(e,c);return y.__actions__.push({func:nu,args:[p],thisArg:it}),new i(y,h)}return v&&m?t.apply(this,c):(y=this.thru(p),v?o?y.value()[0]:y.value():y)})}),s([\"pop\",\"push\",\"shift\",\"sort\",\"splice\",\"unshift\"],function(t){var e=vl[t],r=/^(?:push|sort|unshift)$/.test(t)?\"tap\":\"thru\",i=/^(?:pop|shift)$/.test(t);n.prototype[t]=function(){var t=arguments;if(i&&!this.__chain__){var n=this.value();return e.apply(xp(n)?n:[],t)}return this[r](function(n){return e.apply(xp(n)?n:[],t)})}}),nr(b.prototype,function(t,e){var r=n[e];if(r){var i=r.name+\"\",o=sf[i]||(sf[i]=[]);o.push({name:e,func:r})}}),sf[eo(it,yt).name]=[{name:\"wrapper\",func:it}],b.prototype.clone=P,b.prototype.reverse=Z,b.prototype.value=et,n.prototype.at=tp,n.prototype.chain=ru,n.prototype.commit=iu,n.prototype.next=ou,n.prototype.plant=uu,n.prototype.reverse=cu,n.prototype.toJSON=n.prototype.valueOf=n.prototype.value=su,n.prototype.first=n.prototype.head,Ll&&(n.prototype[Ll]=au),n},Mr=Cr();ur._=Mr,i=function(){return Mr}.call(e,n,e,r),!(i!==it&&(r.exports=i))}).call(this)}).call(e,n(99),n(100)(t))},function(t,e,n){\"use strict\";var r={remove:function(t){t._reactInternalInstance=void 0},get:function(t){return t._reactInternalInstance},has:function(t){return void 0!==t._reactInternalInstance},set:function(t,e){t._reactInternalInstance=e}};t.exports=r},function(t,e,n){\"use strict\";t.exports=n(26)},function(t,e,n){\"use strict\";var r=n(61);e.a=function(t){return t=n.i(r.a)(Math.abs(t)),t?t[1]:NaN}},function(t,e,n){\"use strict\";e.a=function(t,e){return t=+t,e-=t,function(n){return t+e*n}}},function(t,e,n){\"use strict\";var r=n(228);n.d(e,\"a\",function(){return r.a})},function(t,e,n){\"use strict\";function r(t,e){return(e-=t=+t)?function(n){return(n-t)/e}:n.i(h.a)(e)}function i(t){return function(e,n){var r=t(e=+e,n=+n);return function(t){return t<=e?0:t>=n?1:r(t)}}}function o(t){return function(e,n){var r=t(e=+e,n=+n);return function(t){return t<=0?e:t>=1?n:r(t)}}}function a(t,e,n,r){var i=t[0],o=t[1],a=e[0],u=e[1];return o<i?(i=n(o,i),a=r(u,a)):(i=n(i,o),a=r(a,u)),function(t){return a(i(t))}}function u(t,e,r,i){var o=Math.min(t.length,e.length)-1,a=new Array(o),u=new Array(o),c=-1;for(t[o]<t[0]&&(t=t.slice().reverse(),e=e.slice().reverse());++c<o;)a[c]=r(t[c],t[c+1]),u[c]=i(e[c],e[c+1]);return function(e){var r=n.i(l.c)(t,e,1,o)-1;return u[r](a[r](e))}}function c(t,e){return e.domain(t.domain()).range(t.range()).interpolate(t.interpolate()).clamp(t.clamp())}function s(t,e){function n(){return s=Math.min(g.length,m.length)>2?u:a,l=h=null,c}function c(e){return(l||(l=s(g,m,_?i(t):t,y)))(+e)}var s,l,h,g=v,m=v,y=f.b,_=!1;return c.invert=function(t){return(h||(h=s(m,g,r,_?o(e):e)))(+t)},c.domain=function(t){return arguments.length?(g=p.a.call(t,d.a),n()):g.slice()},c.range=function(t){return arguments.length?(m=p.b.call(t),n()):m.slice()},c.rangeRound=function(t){return m=p.b.call(t),y=f.c,n()},c.clamp=function(t){return arguments.length?(_=!!t,n()):_},c.interpolate=function(t){return arguments.length?(y=t,n()):y},n()}var l=n(12),f=n(31),p=n(16),h=n(65),d=n(126);e.b=r,e.c=c,e.a=s;var v=[0,1]},function(t,e,n){\"use strict\";function r(t,e,n){t._context.bezierCurveTo((2*t._x0+t._x1)/3,(2*t._y0+t._y1)/3,(t._x0+2*t._x1)/3,(t._y0+2*t._y1)/3,(t._x0+4*t._x1+e)/6,(t._y0+4*t._y1+n)/6)}function i(t){this._context=t}e.c=r,e.b=i,i.prototype={\n",
       "areaStart:function(){this._line=0},areaEnd:function(){this._line=NaN},lineStart:function(){this._x0=this._x1=this._y0=this._y1=NaN,this._point=0},lineEnd:function(){switch(this._point){case 3:r(this,this._x1,this._y1);case 2:this._context.lineTo(this._x1,this._y1)}(this._line||0!==this._line&&1===this._point)&&this._context.closePath(),this._line=1-this._line},point:function(t,e){switch(t=+t,e=+e,this._point){case 0:this._point=1,this._line?this._context.lineTo(t,e):this._context.moveTo(t,e);break;case 1:this._point=2;break;case 2:this._point=3,this._context.lineTo((5*this._x0+this._x1)/6,(5*this._y0+this._y1)/6);default:r(this,t,e)}this._x0=this._x1,this._x1=t,this._y0=this._y1,this._y1=e}},e.a=function(t){return new i(t)}},function(t,e,n){\"use strict\";function r(t,e,n){t._context.bezierCurveTo(t._x1+t._k*(t._x2-t._x0),t._y1+t._k*(t._y2-t._y0),t._x2+t._k*(t._x1-e),t._y2+t._k*(t._y1-n),t._x2,t._y2)}function i(t,e){this._context=t,this._k=(1-e)/6}e.c=r,e.b=i,i.prototype={areaStart:function(){this._line=0},areaEnd:function(){this._line=NaN},lineStart:function(){this._x0=this._x1=this._x2=this._y0=this._y1=this._y2=NaN,this._point=0},lineEnd:function(){switch(this._point){case 2:this._context.lineTo(this._x2,this._y2);break;case 3:r(this,this._x1,this._y1)}(this._line||0!==this._line&&1===this._point)&&this._context.closePath(),this._line=1-this._line},point:function(t,e){switch(t=+t,e=+e,this._point){case 0:this._point=1,this._line?this._context.lineTo(t,e):this._context.moveTo(t,e);break;case 1:this._point=2,this._x1=t,this._y1=e;break;case 2:this._point=3;default:r(this,t,e)}this._x0=this._x1,this._x1=this._x2,this._x2=t,this._y0=this._y1,this._y1=this._y2,this._y2=e}},e.a=function t(e){function n(t){return new i(t,e)}return n.tension=function(e){return t(+e)},n}(0)},function(t,e,n){\"use strict\";function r(t){this._context=t}r.prototype={areaStart:function(){this._line=0},areaEnd:function(){this._line=NaN},lineStart:function(){this._point=0},lineEnd:function(){(this._line||0!==this._line&&1===this._point)&&this._context.closePath(),this._line=1-this._line},point:function(t,e){switch(t=+t,e=+e,this._point){case 0:this._point=1,this._line?this._context.lineTo(t,e):this._context.moveTo(t,e);break;case 1:this._point=2;default:this._context.lineTo(t,e)}}},e.a=function(t){return new r(t)}},function(t,e,n){\"use strict\";e.a=function(){}},function(t,e,n){\"use strict\";function r(t){return\"topMouseUp\"===t||\"topTouchEnd\"===t||\"topTouchCancel\"===t}function i(t){return\"topMouseMove\"===t||\"topTouchMove\"===t}function o(t){return\"topMouseDown\"===t||\"topTouchStart\"===t}function a(t,e,n,r){var i=t.type||\"unknown-event\";t.currentTarget=m.getNodeFromInstance(r),e?v.invokeGuardedCallbackWithCatch(i,n,t):v.invokeGuardedCallback(i,n,t),t.currentTarget=null}function u(t,e){var n=t._dispatchListeners,r=t._dispatchInstances;if(Array.isArray(n))for(var i=0;i<n.length&&!t.isPropagationStopped();i++)a(t,e,n[i],r[i]);else n&&a(t,e,n,r);t._dispatchListeners=null,t._dispatchInstances=null}function c(t){var e=t._dispatchListeners,n=t._dispatchInstances;if(Array.isArray(e)){for(var r=0;r<e.length&&!t.isPropagationStopped();r++)if(e[r](t,n[r]))return n[r]}else if(e&&e(t,n))return n;return null}function s(t){var e=c(t);return t._dispatchInstances=null,t._dispatchListeners=null,e}function l(t){var e=t._dispatchListeners,n=t._dispatchInstances;Array.isArray(e)?d(\"103\"):void 0,t.currentTarget=e?m.getNodeFromInstance(n):null;var r=e?e(t):null;return t.currentTarget=null,t._dispatchListeners=null,t._dispatchInstances=null,r}function f(t){return!!t._dispatchListeners}var p,h,d=n(2),v=n(87),g=(n(0),n(1),{injectComponentTree:function(t){p=t},injectTreeTraversal:function(t){h=t}}),m={isEndish:r,isMoveish:i,isStartish:o,executeDirectDispatch:l,executeDispatchesInOrder:u,executeDispatchesInOrderStopAtTrue:s,hasDispatches:f,getInstanceFromNode:function(t){return p.getInstanceFromNode(t)},getNodeFromInstance:function(t){return p.getNodeFromInstance(t)},isAncestor:function(t,e){return h.isAncestor(t,e)},getLowestCommonAncestor:function(t,e){return h.getLowestCommonAncestor(t,e)},getParentInstance:function(t){return h.getParentInstance(t)},traverseTwoPhase:function(t,e,n){return h.traverseTwoPhase(t,e,n)},traverseEnterLeave:function(t,e,n,r,i){return h.traverseEnterLeave(t,e,n,r,i)},injection:g};t.exports=m},function(t,e,n){\"use strict\";function r(t){return Object.prototype.hasOwnProperty.call(t,v)||(t[v]=h++,f[t[v]]={}),f[t[v]]}var i,o=n(3),a=n(83),u=n(360),c=n(89),s=n(393),l=n(94),f={},p=!1,h=0,d={topAbort:\"abort\",topAnimationEnd:s(\"animationend\")||\"animationend\",topAnimationIteration:s(\"animationiteration\")||\"animationiteration\",topAnimationStart:s(\"animationstart\")||\"animationstart\",topBlur:\"blur\",topCanPlay:\"canplay\",topCanPlayThrough:\"canplaythrough\",topChange:\"change\",topClick:\"click\",topCompositionEnd:\"compositionend\",topCompositionStart:\"compositionstart\",topCompositionUpdate:\"compositionupdate\",topContextMenu:\"contextmenu\",topCopy:\"copy\",topCut:\"cut\",topDoubleClick:\"dblclick\",topDrag:\"drag\",topDragEnd:\"dragend\",topDragEnter:\"dragenter\",topDragExit:\"dragexit\",topDragLeave:\"dragleave\",topDragOver:\"dragover\",topDragStart:\"dragstart\",topDrop:\"drop\",topDurationChange:\"durationchange\",topEmptied:\"emptied\",topEncrypted:\"encrypted\",topEnded:\"ended\",topError:\"error\",topFocus:\"focus\",topInput:\"input\",topKeyDown:\"keydown\",topKeyPress:\"keypress\",topKeyUp:\"keyup\",topLoadedData:\"loadeddata\",topLoadedMetadata:\"loadedmetadata\",topLoadStart:\"loadstart\",topMouseDown:\"mousedown\",topMouseMove:\"mousemove\",topMouseOut:\"mouseout\",topMouseOver:\"mouseover\",topMouseUp:\"mouseup\",topPaste:\"paste\",topPause:\"pause\",topPlay:\"play\",topPlaying:\"playing\",topProgress:\"progress\",topRateChange:\"ratechange\",topScroll:\"scroll\",topSeeked:\"seeked\",topSeeking:\"seeking\",topSelectionChange:\"selectionchange\",topStalled:\"stalled\",topSuspend:\"suspend\",topTextInput:\"textInput\",topTimeUpdate:\"timeupdate\",topTouchCancel:\"touchcancel\",topTouchEnd:\"touchend\",topTouchMove:\"touchmove\",topTouchStart:\"touchstart\",topTransitionEnd:s(\"transitionend\")||\"transitionend\",topVolumeChange:\"volumechange\",topWaiting:\"waiting\",topWheel:\"wheel\"},v=\"_reactListenersID\"+String(Math.random()).slice(2),g=o({},u,{ReactEventListener:null,injection:{injectReactEventListener:function(t){t.setHandleTopLevel(g.handleTopLevel),g.ReactEventListener=t}},setEnabled:function(t){g.ReactEventListener&&g.ReactEventListener.setEnabled(t)},isEnabled:function(){return!(!g.ReactEventListener||!g.ReactEventListener.isEnabled())},listenTo:function(t,e){for(var n=e,i=r(n),o=a.registrationNameDependencies[t],u=0;u<o.length;u++){var c=o[u];i.hasOwnProperty(c)&&i[c]||(\"topWheel\"===c?l(\"wheel\")?g.ReactEventListener.trapBubbledEvent(\"topWheel\",\"wheel\",n):l(\"mousewheel\")?g.ReactEventListener.trapBubbledEvent(\"topWheel\",\"mousewheel\",n):g.ReactEventListener.trapBubbledEvent(\"topWheel\",\"DOMMouseScroll\",n):\"topScroll\"===c?l(\"scroll\",!0)?g.ReactEventListener.trapCapturedEvent(\"topScroll\",\"scroll\",n):g.ReactEventListener.trapBubbledEvent(\"topScroll\",\"scroll\",g.ReactEventListener.WINDOW_HANDLE):\"topFocus\"===c||\"topBlur\"===c?(l(\"focus\",!0)?(g.ReactEventListener.trapCapturedEvent(\"topFocus\",\"focus\",n),g.ReactEventListener.trapCapturedEvent(\"topBlur\",\"blur\",n)):l(\"focusin\")&&(g.ReactEventListener.trapBubbledEvent(\"topFocus\",\"focusin\",n),g.ReactEventListener.trapBubbledEvent(\"topBlur\",\"focusout\",n)),i.topBlur=!0,i.topFocus=!0):d.hasOwnProperty(c)&&g.ReactEventListener.trapBubbledEvent(c,d[c],n),i[c]=!0)}},trapBubbledEvent:function(t,e,n){return g.ReactEventListener.trapBubbledEvent(t,e,n)},trapCapturedEvent:function(t,e,n){return g.ReactEventListener.trapCapturedEvent(t,e,n)},supportsEventPageXY:function(){if(!document.createEvent)return!1;var t=document.createEvent(\"MouseEvent\");return null!=t&&\"pageX\"in t},ensureScrollValueMonitoring:function(){if(void 0===i&&(i=g.supportsEventPageXY()),!i&&!p){var t=c.refreshScrollValues;g.ReactEventListener.monitorScrollValue(t),p=!0}}});t.exports=g},function(t,e,n){\"use strict\";function r(t,e,n,r){return i.call(this,t,e,n,r)}var i=n(25),o=n(89),a=n(92),u={screenX:null,screenY:null,clientX:null,clientY:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,getModifierState:a,button:function(t){var e=t.button;return\"which\"in t?e:2===e?2:4===e?1:0},buttons:null,relatedTarget:function(t){return t.relatedTarget||(t.fromElement===t.srcElement?t.toElement:t.fromElement)},pageX:function(t){return\"pageX\"in t?t.pageX:t.clientX+o.currentScrollLeft},pageY:function(t){return\"pageY\"in t?t.pageY:t.clientY+o.currentScrollTop}};i.augmentClass(r,u),t.exports=r},function(t,e,n){\"use strict\";var r=n(2),i=(n(0),{}),o={reinitializeTransaction:function(){this.transactionWrappers=this.getTransactionWrappers(),this.wrapperInitData?this.wrapperInitData.length=0:this.wrapperInitData=[],this._isInTransaction=!1},_isInTransaction:!1,getTransactionWrappers:null,isInTransaction:function(){return!!this._isInTransaction},perform:function(t,e,n,i,o,a,u,c){this.isInTransaction()?r(\"27\"):void 0;var s,l;try{this._isInTransaction=!0,s=!0,this.initializeAll(0),l=t.call(e,n,i,o,a,u,c),s=!1}finally{try{if(s)try{this.closeAll(0)}catch(t){}else this.closeAll(0)}finally{this._isInTransaction=!1}}return l},initializeAll:function(t){for(var e=this.transactionWrappers,n=t;n<e.length;n++){var r=e[n];try{this.wrapperInitData[n]=i,this.wrapperInitData[n]=r.initialize?r.initialize.call(this):null}finally{if(this.wrapperInitData[n]===i)try{this.initializeAll(n+1)}catch(t){}}}},closeAll:function(t){this.isInTransaction()?void 0:r(\"28\");for(var e=this.transactionWrappers,n=t;n<e.length;n++){var o,a=e[n],u=this.wrapperInitData[n];try{o=!0,u!==i&&a.close&&a.close.call(this,u),o=!1}finally{if(o)try{this.closeAll(n+1)}catch(t){}}}this.wrapperInitData.length=0}};t.exports=o},function(t,e,n){\"use strict\";function r(t){var e=\"\"+t,n=o.exec(e);if(!n)return e;var r,i=\"\",a=0,u=0;for(a=n.index;a<e.length;a++){switch(e.charCodeAt(a)){case 34:r=\"&quot;\";break;case 38:r=\"&amp;\";break;case 39:r=\"&#x27;\";break;case 60:r=\"&lt;\";break;case 62:r=\"&gt;\";break;default:continue}u!==a&&(i+=e.substring(u,a)),u=a+1,i+=r}return u!==a?i+e.substring(u,a):i}function i(t){return\"boolean\"==typeof t||\"number\"==typeof t?\"\"+t:r(t)}var o=/[\"'&<>]/;t.exports=i},function(t,e,n){\"use strict\";var r,i=n(6),o=n(82),a=/^[ \\r\\n\\t\\f]/,u=/<(!--|link|noscript|meta|script|style)[ \\r\\n\\t\\f\\/>]/,c=n(90),s=c(function(t,e){if(t.namespaceURI!==o.svg||\"innerHTML\"in t)t.innerHTML=e;else{r=r||document.createElement(\"div\"),r.innerHTML=\"<svg>\"+e+\"</svg>\";for(var n=r.firstChild;n.firstChild;)t.appendChild(n.firstChild)}});if(i.canUseDOM){var l=document.createElement(\"div\");l.innerHTML=\" \",\"\"===l.innerHTML&&(s=function(t,e){if(t.parentNode&&t.parentNode.replaceChild(t,t),a.test(e)||\"<\"===e[0]&&u.test(e)){t.innerHTML=String.fromCharCode(65279)+e;var n=t.firstChild;1===n.data.length?t.removeChild(n):n.deleteData(0,1)}else t.innerHTML=e}),l=null}t.exports=s},function(t,e,n){\"use strict\";Object.defineProperty(e,\"__esModule\",{value:!0}),e.default={colors:{RdBu:[\"rgb(255, 13, 87)\",\"rgb(30, 136, 229)\"],GnPR:[\"rgb(24, 196, 93)\",\"rgb(124, 82, 255)\"],CyPU:[\"#0099C6\",\"#990099\"],PkYg:[\"#DD4477\",\"#66AA00\"],DrDb:[\"#B82E2E\",\"#316395\"],LpLb:[\"#994499\",\"#22AA99\"],YlDp:[\"#AAAA11\",\"#6633CC\"],OrId:[\"#E67300\",\"#3E0099\"]},gray:\"#777\"}},function(t,e,n){\"use strict\";var r=n(29);e.a=function(t,e,n){if(null==n&&(n=r.a),i=t.length){if((e=+e)<=0||i<2)return+n(t[0],0,t);if(e>=1)return+n(t[i-1],i-1,t);var i,o=(i-1)*e,a=Math.floor(o),u=+n(t[a],a,t),c=+n(t[a+1],a+1,t);return u+(c-u)*(o-a)}}},function(t,e,n){\"use strict\";function r(){}function i(t,e){var n=new r;if(t instanceof r)t.each(function(t,e){n.set(e,t)});else if(Array.isArray(t)){var i,o=-1,a=t.length;if(null==e)for(;++o<a;)n.set(o,t[o]);else for(;++o<a;)n.set(e(i=t[o],o,t),i)}else if(t)for(var u in t)n.set(u,t[u]);return n}n.d(e,\"b\",function(){return o});var o=\"$\";r.prototype=i.prototype={constructor:r,has:function(t){return o+t in this},get:function(t){return this[o+t]},set:function(t,e){return this[o+t]=e,this},remove:function(t){var e=o+t;return e in this&&delete this[e]},clear:function(){for(var t in this)t[0]===o&&delete this[t]},keys:function(){var t=[];for(var e in this)e[0]===o&&t.push(e.slice(1));return t},values:function(){var t=[];for(var e in this)e[0]===o&&t.push(this[e]);return t},entries:function(){var t=[];for(var e in this)e[0]===o&&t.push({key:e.slice(1),value:this[e]});return t},size:function(){var t=0;for(var e in this)e[0]===o&&++t;return t},empty:function(){for(var t in this)if(t[0]===o)return!1;return!0},each:function(t){for(var e in this)e[0]===o&&t(this[e],e.slice(1),this)}},e.a=i},function(t,e,n){\"use strict\";function r(){}function i(t){var e;return t=(t+\"\").trim().toLowerCase(),(e=x.exec(t))?(e=parseInt(e[1],16),new s(e>>8&15|e>>4&240,e>>4&15|240&e,(15&e)<<4|15&e,1)):(e=w.exec(t))?o(parseInt(e[1],16)):(e=C.exec(t))?new s(e[1],e[2],e[3],1):(e=M.exec(t))?new s(255*e[1]/100,255*e[2]/100,255*e[3]/100,1):(e=k.exec(t))?a(e[1],e[2],e[3],e[4]):(e=E.exec(t))?a(255*e[1]/100,255*e[2]/100,255*e[3]/100,e[4]):(e=T.exec(t))?l(e[1],e[2]/100,e[3]/100,1):(e=S.exec(t))?l(e[1],e[2]/100,e[3]/100,e[4]):P.hasOwnProperty(t)?o(P[t]):\"transparent\"===t?new s(NaN,NaN,NaN,0):null}function o(t){return new s(t>>16&255,t>>8&255,255&t,1)}function a(t,e,n,r){return r<=0&&(t=e=n=NaN),new s(t,e,n,r)}function u(t){return t instanceof r||(t=i(t)),t?(t=t.rgb(),new s(t.r,t.g,t.b,t.opacity)):new s}function c(t,e,n,r){return 1===arguments.length?u(t):new s(t,e,n,null==r?1:r)}function s(t,e,n,r){this.r=+t,this.g=+e,this.b=+n,this.opacity=+r}function l(t,e,n,r){return r<=0?t=e=n=NaN:n<=0||n>=1?t=e=NaN:e<=0&&(t=NaN),new h(t,e,n,r)}function f(t){if(t instanceof h)return new h(t.h,t.s,t.l,t.opacity);if(t instanceof r||(t=i(t)),!t)return new h;if(t instanceof h)return t;t=t.rgb();var e=t.r/255,n=t.g/255,o=t.b/255,a=Math.min(e,n,o),u=Math.max(e,n,o),c=NaN,s=u-a,l=(u+a)/2;return s?(c=e===u?(n-o)/s+6*(n<o):n===u?(o-e)/s+2:(e-n)/s+4,s/=l<.5?u+a:2-u-a,c*=60):s=l>0&&l<1?0:c,new h(c,s,l,t.opacity)}function p(t,e,n,r){return 1===arguments.length?f(t):new h(t,e,n,null==r?1:r)}function h(t,e,n,r){this.h=+t,this.s=+e,this.l=+n,this.opacity=+r}function d(t,e,n){return 255*(t<60?e+(n-e)*t/60:t<180?n:t<240?e+(n-e)*(240-t)/60:e)}var v=n(60);e.f=r,n.d(e,\"h\",function(){return g}),n.d(e,\"g\",function(){return m}),e.a=i,e.e=u,e.b=c,e.d=s,e.c=p;var g=.7,m=1/g,y=\"\\\\s*([+-]?\\\\d+)\\\\s*\",_=\"\\\\s*([+-]?\\\\d*\\\\.?\\\\d+(?:[eE][+-]?\\\\d+)?)\\\\s*\",b=\"\\\\s*([+-]?\\\\d*\\\\.?\\\\d+(?:[eE][+-]?\\\\d+)?)%\\\\s*\",x=/^#([0-9a-f]{3})$/,w=/^#([0-9a-f]{6})$/,C=new RegExp(\"^rgb\\\\(\"+[y,y,y]+\"\\\\)$\"),M=new RegExp(\"^rgb\\\\(\"+[b,b,b]+\"\\\\)$\"),k=new RegExp(\"^rgba\\\\(\"+[y,y,y,_]+\"\\\\)$\"),E=new RegExp(\"^rgba\\\\(\"+[b,b,b,_]+\"\\\\)$\"),T=new RegExp(\"^hsl\\\\(\"+[_,b,b]+\"\\\\)$\"),S=new RegExp(\"^hsla\\\\(\"+[_,b,b,_]+\"\\\\)$\"),P={aliceblue:15792383,antiquewhite:16444375,aqua:65535,aquamarine:8388564,azure:15794175,beige:16119260,bisque:16770244,black:0,blanchedalmond:16772045,blue:255,blueviolet:9055202,brown:10824234,burlywood:14596231,cadetblue:6266528,chartreuse:8388352,chocolate:13789470,coral:16744272,cornflowerblue:6591981,cornsilk:16775388,crimson:14423100,cyan:65535,darkblue:139,darkcyan:35723,darkgoldenrod:12092939,darkgray:11119017,darkgreen:25600,darkgrey:11119017,darkkhaki:12433259,darkmagenta:9109643,darkolivegreen:5597999,darkorange:16747520,darkorchid:10040012,darkred:9109504,darksalmon:15308410,darkseagreen:9419919,darkslateblue:4734347,darkslategray:3100495,darkslategrey:3100495,darkturquoise:52945,darkviolet:9699539,deeppink:16716947,deepskyblue:49151,dimgray:6908265,dimgrey:6908265,dodgerblue:2003199,firebrick:11674146,floralwhite:16775920,forestgreen:2263842,fuchsia:16711935,gainsboro:14474460,ghostwhite:16316671,gold:16766720,goldenrod:14329120,gray:8421504,green:32768,greenyellow:11403055,grey:8421504,honeydew:15794160,hotpink:16738740,indianred:13458524,indigo:4915330,ivory:16777200,khaki:15787660,lavender:15132410,lavenderblush:16773365,lawngreen:8190976,lemonchiffon:16775885,lightblue:11393254,lightcoral:15761536,lightcyan:14745599,lightgoldenrodyellow:16448210,lightgray:13882323,lightgreen:9498256,lightgrey:13882323,lightpink:16758465,lightsalmon:16752762,lightseagreen:2142890,lightskyblue:8900346,lightslategray:7833753,lightslategrey:7833753,lightsteelblue:11584734,lightyellow:16777184,lime:65280,limegreen:3329330,linen:16445670,magenta:16711935,maroon:8388608,mediumaquamarine:6737322,mediumblue:205,mediumorchid:12211667,mediumpurple:9662683,mediumseagreen:3978097,mediumslateblue:8087790,mediumspringgreen:64154,mediumturquoise:4772300,mediumvioletred:13047173,midnightblue:1644912,mintcream:16121850,mistyrose:16770273,moccasin:16770229,navajowhite:16768685,navy:128,oldlace:16643558,olive:8421376,olivedrab:7048739,orange:16753920,orangered:16729344,orchid:14315734,palegoldenrod:15657130,palegreen:10025880,paleturquoise:11529966,palevioletred:14381203,papayawhip:16773077,peachpuff:16767673,peru:13468991,pink:16761035,plum:14524637,powderblue:11591910,purple:8388736,rebeccapurple:6697881,red:16711680,rosybrown:12357519,royalblue:4286945,saddlebrown:9127187,salmon:16416882,sandybrown:16032864,seagreen:3050327,seashell:16774638,sienna:10506797,silver:12632256,skyblue:8900331,slateblue:6970061,slategray:7372944,slategrey:7372944,snow:16775930,springgreen:65407,steelblue:4620980,tan:13808780,teal:32896,thistle:14204888,tomato:16737095,turquoise:4251856,violet:15631086,wheat:16113331,white:16777215,whitesmoke:16119285,yellow:16776960,yellowgreen:10145074};n.i(v.a)(r,i,{displayable:function(){return this.rgb().displayable()},toString:function(){return this.rgb()+\"\"}}),n.i(v.a)(s,c,n.i(v.b)(r,{brighter:function(t){return t=null==t?m:Math.pow(m,t),new s(this.r*t,this.g*t,this.b*t,this.opacity)},darker:function(t){return t=null==t?g:Math.pow(g,t),new s(this.r*t,this.g*t,this.b*t,this.opacity)},rgb:function(){return this},displayable:function(){return 0<=this.r&&this.r<=255&&0<=this.g&&this.g<=255&&0<=this.b&&this.b<=255&&0<=this.opacity&&this.opacity<=1},toString:function(){var t=this.opacity;return t=isNaN(t)?1:Math.max(0,Math.min(1,t)),(1===t?\"rgb(\":\"rgba(\")+Math.max(0,Math.min(255,Math.round(this.r)||0))+\", \"+Math.max(0,Math.min(255,Math.round(this.g)||0))+\", \"+Math.max(0,Math.min(255,Math.round(this.b)||0))+(1===t?\")\":\", \"+t+\")\")}})),n.i(v.a)(h,p,n.i(v.b)(r,{brighter:function(t){return t=null==t?m:Math.pow(m,t),new h(this.h,this.s,this.l*t,this.opacity)},darker:function(t){return t=null==t?g:Math.pow(g,t),new h(this.h,this.s,this.l*t,this.opacity)},rgb:function(){var t=this.h%360+360*(this.h<0),e=isNaN(t)||isNaN(this.s)?0:this.s,n=this.l,r=n+(n<.5?n:1-n)*e,i=2*n-r;return new s(d(t>=240?t-240:t+120,i,r),d(t,i,r),d(t<120?t+240:t-120,i,r),this.opacity)},displayable:function(){return(0<=this.s&&this.s<=1||isNaN(this.s))&&0<=this.l&&this.l<=1&&0<=this.opacity&&this.opacity<=1}}))},function(t,e,n){\"use strict\";function r(t,e){var n=Object.create(t.prototype);for(var r in e)n[r]=e[r];return n}e.b=r,e.a=function(t,e,n){t.prototype=e.prototype=n,n.constructor=t}},function(t,e,n){\"use strict\";e.a=function(t,e){if((n=(t=e?t.toExponential(e-1):t.toExponential()).indexOf(\"e\"))<0)return null;var n,r=t.slice(0,n);return[r.length>1?r[0]+r.slice(2):r,+t.slice(n+1)]}},function(t,e,n){\"use strict\";function r(t,e,n,r,i){var o=t*t,a=o*t;return((1-3*t+3*o-a)*e+(4-6*o+3*a)*n+(1+3*t+3*o-3*a)*r+a*i)/6}e.b=r,e.a=function(t){var e=t.length-1;return function(n){var i=n<=0?n=0:n>=1?(n=1,e-1):Math.floor(n*e),o=t[i],a=t[i+1],u=i>0?t[i-1]:2*o-a,c=i<e-1?t[i+2]:2*a-o;return r((n-i/e)*e,u,o,a,c)}}},function(t,e,n){\"use strict\";var r=n(10),i=n(123),o=n(118),a=n(121),u=n(43),c=n(122),s=n(124),l=n(120);e.a=function(t,e){var f,p=typeof e;return null==e||\"boolean\"===p?n.i(l.a)(e):(\"number\"===p?u.a:\"string\"===p?(f=n.i(r.color)(e))?(e=f,i.a):s.a:e instanceof r.color?i.a:e instanceof Date?a.a:Array.isArray(e)?o.a:isNaN(e)?c.a:u.a)(t,e)}},function(t,e,n){\"use strict\";Object.defineProperty(e,\"__esModule\",{value:!0});var r=n(229);n.d(e,\"scaleBand\",function(){return r.a}),n.d(e,\"scalePoint\",function(){return r.b});var i=n(235);n.d(e,\"scaleIdentity\",function(){return i.a});var o=n(34);n.d(e,\"scaleLinear\",function(){return o.a});var a=n(236);n.d(e,\"scaleLog\",function(){return a.a});var u=n(127);n.d(e,\"scaleOrdinal\",function(){return u.a}),n.d(e,\"scaleImplicit\",function(){return u.b});var c=n(237);n.d(e,\"scalePow\",function(){return c.a}),n.d(e,\"scaleSqrt\",function(){return c.b});var s=n(238);n.d(e,\"scaleQuantile\",function(){return s.a});var l=n(239);n.d(e,\"scaleQuantize\",function(){return l.a});var f=n(242);n.d(e,\"scaleThreshold\",function(){return f.a});var p=n(128);n.d(e,\"scaleTime\",function(){return p.a});var h=n(244);n.d(e,\"scaleUtc\",function(){return h.a});var d=n(230);n.d(e,\"schemeCategory10\",function(){return d.a});var v=n(232);n.d(e,\"schemeCategory20b\",function(){return v.a});var g=n(233);n.d(e,\"schemeCategory20c\",function(){return g.a});var m=n(231);n.d(e,\"schemeCategory20\",function(){return m.a});var y=n(234);n.d(e,\"interpolateCubehelixDefault\",function(){return y.a});var _=n(240);n.d(e,\"interpolateRainbow\",function(){return _.a}),n.d(e,\"interpolateWarm\",function(){return _.b}),n.d(e,\"interpolateCool\",function(){return _.c});var b=n(245);n.d(e,\"interpolateViridis\",function(){return b.a}),n.d(e,\"interpolateMagma\",function(){return b.b}),n.d(e,\"interpolateInferno\",function(){return b.c}),n.d(e,\"interpolatePlasma\",function(){return b.d});var x=n(241);n.d(e,\"scaleSequential\",function(){return x.a})},function(t,e,n){\"use strict\";e.a=function(t){return function(){return t}}},function(t,e,n){\"use strict\";function r(t){return function(){var e=this.ownerDocument,n=this.namespaceURI;return n===a.b&&e.documentElement.namespaceURI===a.b?e.createElement(t):e.createElementNS(n,t)}}function i(t){return function(){return this.ownerDocument.createElementNS(t.space,t.local)}}var o=n(67),a=n(68);e.a=function(t){var e=n.i(o.a)(t);return(e.local?i:r)(e)}},function(t,e,n){\"use strict\";var r=n(68);e.a=function(t){var e=t+=\"\",n=e.indexOf(\":\");return n>=0&&\"xmlns\"!==(e=t.slice(0,n))&&(t=t.slice(n+1)),r.a.hasOwnProperty(e)?{space:r.a[e],local:t}:t}},function(t,e,n){\"use strict\";n.d(e,\"b\",function(){return r});var r=\"http://www.w3.org/1999/xhtml\";e.a={svg:\"http://www.w3.org/2000/svg\",xhtml:r,xlink:\"http://www.w3.org/1999/xlink\",xml:\"http://www.w3.org/XML/1998/namespace\",xmlns:\"http://www.w3.org/2000/xmlns/\"}},function(t,e,n){\"use strict\";e.a=function(t,e){var n=t.ownerSVGElement||t;if(n.createSVGPoint){var r=n.createSVGPoint();return r.x=e.clientX,r.y=e.clientY,r=r.matrixTransform(t.getScreenCTM().inverse()),[r.x,r.y]}var i=t.getBoundingClientRect();return[e.clientX-i.left-t.clientLeft,e.clientY-i.top-t.clientTop]}},function(t,e,n){\"use strict\";function r(t,e,n){return t=i(t,e,n),function(e){var n=e.relatedTarget;n&&(n===this||8&n.compareDocumentPosition(this))||t.call(this,e)}}function i(t,e,n){return function(r){var i=l;l=r;try{t.call(this,this.__data__,e,n)}finally{l=i}}}function o(t){return t.trim().split(/^|\\s+/).map(function(t){var e=\"\",n=t.indexOf(\".\");return n>=0&&(e=t.slice(n+1),t=t.slice(0,n)),{type:t,name:e}})}function a(t){return function(){var e=this.__on;if(e){for(var n,r=0,i=-1,o=e.length;r<o;++r)n=e[r],t.type&&n.type!==t.type||n.name!==t.name?e[++i]=n:this.removeEventListener(n.type,n.listener,n.capture);++i?e.length=i:delete this.__on}}}function u(t,e,n){var o=s.hasOwnProperty(t.type)?r:i;return function(r,i,a){var u,c=this.__on,s=o(e,i,a);if(c)for(var l=0,f=c.length;l<f;++l)if((u=c[l]).type===t.type&&u.name===t.name)return this.removeEventListener(u.type,u.listener,u.capture),this.addEventListener(u.type,u.listener=s,u.capture=n),void(u.value=e);this.addEventListener(t.type,s,n),u={type:t.type,name:t.name,value:e,listener:s,capture:n},c?c.push(u):this.__on=[u]}}function c(t,e,n,r){var i=l;t.sourceEvent=l,l=t;try{return e.apply(n,r)}finally{l=i}}n.d(e,\"a\",function(){return l}),e.b=c;var s={},l=null;if(\"undefined\"!=typeof document){var f=document.documentElement;\"onmouseenter\"in f||(s={mouseenter:\"mouseover\",mouseleave:\"mouseout\"})}e.c=function(t,e,n){var r,i,c=o(t+\"\"),s=c.length;{if(!(arguments.length<2)){for(l=e?u:a,null==n&&(n=!1),r=0;r<s;++r)this.each(l(c[r],e,n));return this}var l=this.node().__on;if(l)for(var f,p=0,h=l.length;p<h;++p)for(r=0,f=l[p];r<s;++r)if((i=c[r]).type===f.type&&i.name===f.name)return f.value}}},function(t,e,n){\"use strict\";function r(){}e.a=function(t){return null==t?r:function(){return this.querySelector(t)}}},function(t,e,n){\"use strict\";var r=n(70);e.a=function(){for(var t,e=r.a;t=e.sourceEvent;)e=t;return e}},function(t,e,n){\"use strict\";e.a=function(t){return t.ownerDocument&&t.ownerDocument.defaultView||t.document&&t||t.defaultView}},function(t,e,n){\"use strict\";function r(t,e,n){var r=t._x1,i=t._y1,a=t._x2,u=t._y2;if(t._l01_a>o.a){var c=2*t._l01_2a+3*t._l01_a*t._l12_a+t._l12_2a,s=3*t._l01_a*(t._l01_a+t._l12_a);r=(r*c-t._x0*t._l12_2a+t._x2*t._l01_2a)/s,i=(i*c-t._y0*t._l12_2a+t._y2*t._l01_2a)/s}if(t._l23_a>o.a){var l=2*t._l23_2a+3*t._l23_a*t._l12_a+t._l12_2a,f=3*t._l23_a*(t._l23_a+t._l12_a);a=(a*l+t._x1*t._l23_2a-e*t._l12_2a)/f,u=(u*l+t._y1*t._l23_2a-n*t._l12_2a)/f}t._context.bezierCurveTo(r,i,a,u,t._x2,t._y2)}function i(t,e){this._context=t,this._alpha=e}var o=n(35),a=n(47);e.b=r,i.prototype={areaStart:function(){this._line=0},areaEnd:function(){this._line=NaN},lineStart:function(){this._x0=this._x1=this._x2=this._y0=this._y1=this._y2=NaN,this._l01_a=this._l12_a=this._l23_a=this._l01_2a=this._l12_2a=this._l23_2a=this._point=0},lineEnd:function(){switch(this._point){case 2:this._context.lineTo(this._x2,this._y2);break;case 3:this.point(this._x2,this._y2)}(this._line||0!==this._line&&1===this._point)&&this._context.closePath(),this._line=1-this._line},point:function(t,e){if(t=+t,e=+e,this._point){var n=this._x2-t,i=this._y2-e;this._l23_a=Math.sqrt(this._l23_2a=Math.pow(n*n+i*i,this._alpha))}switch(this._point){case 0:this._point=1,this._line?this._context.lineTo(t,e):this._context.moveTo(t,e);break;case 1:this._point=2;break;case 2:this._point=3;default:r(this,t,e)}this._l01_a=this._l12_a,this._l12_a=this._l23_a,this._l01_2a=this._l12_2a,this._l12_2a=this._l23_2a,this._x0=this._x1,this._x1=this._x2,this._x2=t,this._y0=this._y1,this._y1=this._y2,this._y2=e}},e.a=function t(e){function n(t){return e?new i(t,e):new a.b(t,0)}return n.alpha=function(e){return t(+e)},n}(.5)},function(t,e,n){\"use strict\";var r=n(44),i=n(19),o=n(48),a=n(139);e.a=function(){function t(t){var i,o,a,p=t.length,h=!1;for(null==s&&(f=l(a=n.i(r.a)())),i=0;i<=p;++i)!(i<p&&c(o=t[i],i,t))===h&&((h=!h)?f.lineStart():f.lineEnd()),h&&f.point(+e(o,i,t),+u(o,i,t));if(a)return f=null,a+\"\"||null}var e=a.a,u=a.b,c=n.i(i.a)(!0),s=null,l=o.a,f=null;return t.x=function(r){return arguments.length?(e=\"function\"==typeof r?r:n.i(i.a)(+r),t):e},t.y=function(e){return arguments.length?(u=\"function\"==typeof e?e:n.i(i.a)(+e),t):u},t.defined=function(e){return arguments.length?(c=\"function\"==typeof e?e:n.i(i.a)(!!e),t):c},t.curve=function(e){return arguments.length?(l=e,null!=s&&(f=l(s)),t):l},t.context=function(e){return arguments.length?(null==e?s=f=null:f=l(s=e),t):s},t}},function(t,e,n){\"use strict\";function r(t){for(var e,n=0,r=-1,i=t.length;++r<i;)(e=+t[r][1])&&(n+=e);return n}var i=n(37);e.b=r,e.a=function(t){var e=t.map(r);return n.i(i.a)(t).sort(function(t,n){return e[t]-e[n]})}},function(t,e,n){\"use strict\";Object.defineProperty(e,\"__esModule\",{value:!0});var r=n(78);n.d(e,\"timeFormatDefaultLocale\",function(){return r.a}),n.d(e,\"timeFormat\",function(){return r.b}),n.d(e,\"timeParse\",function(){return r.c}),n.d(e,\"utcFormat\",function(){return r.d}),n.d(e,\"utcParse\",function(){return r.e});var i=n(149);n.d(e,\"timeFormatLocale\",function(){return i.a});var o=n(148);n.d(e,\"isoFormat\",function(){return o.a});var a=n(303);n.d(e,\"isoParse\",function(){return a.a})},function(t,e,n){\"use strict\";function r(t){return o=n.i(i.a)(t),a=o.format,u=o.parse,c=o.utcFormat,s=o.utcParse,o}var i=n(149);n.d(e,\"b\",function(){return a}),n.d(e,\"c\",function(){return u}),n.d(e,\"d\",function(){return c}),n.d(e,\"e\",function(){return s}),e.a=r;var o,a,u,c,s;r({dateTime:\"%x, %X\",date:\"%-m/%-d/%Y\",time:\"%-I:%M:%S %p\",periods:[\"AM\",\"PM\"],days:[\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"],shortDays:[\"Sun\",\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\"],months:[\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],shortMonths:[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]})},function(t,e,n){\"use strict\";var r=(n(5),n(306));n.d(e,\"t\",function(){return r.a}),n.d(e,\"n\",function(){return r.a});var i=n(309);n.d(e,\"s\",function(){return i.a}),n.d(e,\"m\",function(){return i.a});var o=n(307);n.d(e,\"r\",function(){return o.a});var a=n(305);n.d(e,\"q\",function(){return a.a});var u=n(304);n.d(e,\"a\",function(){return u.a});var c=n(316);n.d(e,\"p\",function(){return c.a}),n.d(e,\"c\",function(){return c.a}),n.d(e,\"d\",function(){return c.b});var s=n(308);n.d(e,\"o\",function(){return s.a});var l=n(317);n.d(e,\"b\",function(){return l.a});var f=n(312);n.d(e,\"l\",function(){return f.a});var p=n(311);n.d(e,\"k\",function(){return p.a});var h=n(310);n.d(e,\"e\",function(){return h.a});var d=n(314);n.d(e,\"j\",function(){return d.a}),n.d(e,\"g\",function(){return d.a}),n.d(e,\"h\",function(){return d.b});var v=n(313);n.d(e,\"i\",function(){return v.a});var g=n(315);n.d(e,\"f\",function(){return g.a})},function(t,e,n){\"use strict\";function r(t,e){return t===e?0!==t||0!==e||1/t===1/e:t!==t&&e!==e}function i(t,e){if(r(t,e))return!0;if(\"object\"!=typeof t||null===t||\"object\"!=typeof e||null===e)return!1;var n=Object.keys(t),i=Object.keys(e);if(n.length!==i.length)return!1;for(var a=0;a<n.length;a++)if(!o.call(e,n[a])||!r(t[n[a]],e[n[a]]))return!1;return!0}var o=Object.prototype.hasOwnProperty;t.exports=i},function(t,e,n){\"use strict\";function r(t,e){return Array.isArray(e)&&(e=e[1]),e?e.nextSibling:t.firstChild}function i(t,e,n){l.insertTreeBefore(t,e,n)}function o(t,e,n){Array.isArray(e)?u(t,e[0],e[1],n):v(t,e,n)}function a(t,e){if(Array.isArray(e)){var n=e[1];e=e[0],c(t,e,n),t.removeChild(n)}t.removeChild(e)}function u(t,e,n,r){for(var i=e;;){var o=i.nextSibling;if(v(t,i,r),i===n)break;i=o}}function c(t,e,n){for(;;){var r=e.nextSibling;if(r===n)break;t.removeChild(r)}}function s(t,e,n){var r=t.parentNode,i=t.nextSibling;i===e?n&&v(r,document.createTextNode(n),i):n?(d(i,n),c(r,i,e)):c(r,t,e)}var l=n(20),f=n(336),p=(n(4),n(9),n(90)),h=n(55),d=n(171),v=p(function(t,e,n){t.insertBefore(e,n)}),g=f.dangerouslyReplaceNodeWithMarkup,m={dangerouslyReplaceNodeWithMarkup:g,replaceDelimitedText:s,processUpdates:function(t,e){for(var n=0;n<e.length;n++){var u=e[n];switch(u.type){case\"INSERT_MARKUP\":i(t,u.content,r(t,u.afterNode));break;case\"MOVE_EXISTING\":o(t,u.fromNode,r(t,u.afterNode));break;case\"SET_MARKUP\":h(t,u.content);break;case\"TEXT_CONTENT\":d(t,u.content);break;case\"REMOVE_NODE\":a(t,u.fromNode)}}}};t.exports=m},function(t,e,n){\"use strict\";var r={html:\"http://www.w3.org/1999/xhtml\",mathml:\"http://www.w3.org/1998/Math/MathML\",svg:\"http://www.w3.org/2000/svg\"};t.exports=r},function(t,e,n){\"use strict\";function r(){if(u)for(var t in c){var e=c[t],n=u.indexOf(t);if(n>-1?void 0:a(\"96\",t),!s.plugins[n]){e.extractEvents?void 0:a(\"97\",t),s.plugins[n]=e;var r=e.eventTypes;for(var o in r)i(r[o],e,o)?void 0:a(\"98\",o,t)}}}function i(t,e,n){s.eventNameDispatchConfigs.hasOwnProperty(n)?a(\"99\",n):void 0,s.eventNameDispatchConfigs[n]=t;var r=t.phasedRegistrationNames;if(r){for(var i in r)if(r.hasOwnProperty(i)){var u=r[i];o(u,e,n)}return!0}return!!t.registrationName&&(o(t.registrationName,e,n),!0)}function o(t,e,n){s.registrationNameModules[t]?a(\"100\",t):void 0,s.registrationNameModules[t]=e,s.registrationNameDependencies[t]=e.eventTypes[n].dependencies}var a=n(2),u=(n(0),null),c={},s={plugins:[],eventNameDispatchConfigs:{},registrationNameModules:{},registrationNameDependencies:{},possibleRegistrationNames:null,injectEventPluginOrder:function(t){\n",
       "u?a(\"101\"):void 0,u=Array.prototype.slice.call(t),r()},injectEventPluginsByName:function(t){var e=!1;for(var n in t)if(t.hasOwnProperty(n)){var i=t[n];c.hasOwnProperty(n)&&c[n]===i||(c[n]?a(\"102\",n):void 0,c[n]=i,e=!0)}e&&r()},getPluginModuleForEvent:function(t){var e=t.dispatchConfig;if(e.registrationName)return s.registrationNameModules[e.registrationName]||null;if(void 0!==e.phasedRegistrationNames){var n=e.phasedRegistrationNames;for(var r in n)if(n.hasOwnProperty(r)){var i=s.registrationNameModules[n[r]];if(i)return i}}return null},_resetEventPlugins:function(){u=null;for(var t in c)c.hasOwnProperty(t)&&delete c[t];s.plugins.length=0;var e=s.eventNameDispatchConfigs;for(var n in e)e.hasOwnProperty(n)&&delete e[n];var r=s.registrationNameModules;for(var i in r)r.hasOwnProperty(i)&&delete r[i]}};t.exports=s},function(t,e,n){\"use strict\";function r(t){var e=/[=:]/g,n={\"=\":\"=0\",\":\":\"=2\"},r=(\"\"+t).replace(e,function(t){return n[t]});return\"$\"+r}function i(t){var e=/(=0|=2)/g,n={\"=0\":\"=\",\"=2\":\":\"},r=\".\"===t[0]&&\"$\"===t[1]?t.substring(2):t.substring(1);return(\"\"+r).replace(e,function(t){return n[t]})}var o={escape:r,unescape:i};t.exports=o},function(t,e,n){\"use strict\";function r(t){null!=t.checkedLink&&null!=t.valueLink?u(\"87\"):void 0}function i(t){r(t),null!=t.value||null!=t.onChange?u(\"88\"):void 0}function o(t){r(t),null!=t.checked||null!=t.onChange?u(\"89\"):void 0}function a(t){if(t){var e=t.getName();if(e)return\" Check the render method of `\"+e+\"`.\"}return\"\"}var u=n(2),c=n(26),s=n(366),l=(n(0),n(1),{button:!0,checkbox:!0,image:!0,hidden:!0,radio:!0,reset:!0,submit:!0}),f={value:function(t,e,n){return!t[e]||l[t.type]||t.onChange||t.readOnly||t.disabled?null:new Error(\"You provided a `value` prop to a form field without an `onChange` handler. This will render a read-only field. If the field should be mutable use `defaultValue`. Otherwise, set either `onChange` or `readOnly`.\")},checked:function(t,e,n){return!t[e]||t.onChange||t.readOnly||t.disabled?null:new Error(\"You provided a `checked` prop to a form field without an `onChange` handler. This will render a read-only field. If the field should be mutable use `defaultChecked`. Otherwise, set either `onChange` or `readOnly`.\")},onChange:c.PropTypes.func},p={},h={checkPropTypes:function(t,e,n){for(var r in f){if(f.hasOwnProperty(r))var i=f[r](e,r,t,\"prop\",null,s);if(i instanceof Error&&!(i.message in p)){p[i.message]=!0;a(n)}}},getValue:function(t){return t.valueLink?(i(t),t.valueLink.value):t.value},getChecked:function(t){return t.checkedLink?(o(t),t.checkedLink.value):t.checked},executeOnChange:function(t,e){return t.valueLink?(i(t),t.valueLink.requestChange(e.target.value)):t.checkedLink?(o(t),t.checkedLink.requestChange(e.target.checked)):t.onChange?t.onChange.call(void 0,e):void 0}};t.exports=h},function(t,e,n){\"use strict\";var r=n(2),i=(n(0),!1),o={replaceNodeWithMarkup:null,processChildrenUpdates:null,injection:{injectEnvironment:function(t){i?r(\"104\"):void 0,o.replaceNodeWithMarkup=t.replaceNodeWithMarkup,o.processChildrenUpdates=t.processChildrenUpdates,i=!0}}};t.exports=o},function(t,e,n){\"use strict\";function r(t,e,n){try{e(n)}catch(t){null===i&&(i=t)}}var i=null,o={invokeGuardedCallback:r,invokeGuardedCallbackWithCatch:r,rethrowCaughtError:function(){if(i){var t=i;throw i=null,t}}};t.exports=o},function(t,e,n){\"use strict\";function r(t){c.enqueueUpdate(t)}function i(t){var e=typeof t;if(\"object\"!==e)return e;var n=t.constructor&&t.constructor.name||e,r=Object.keys(t);return r.length>0&&r.length<20?n+\" (keys: \"+r.join(\", \")+\")\":n}function o(t,e){var n=u.get(t);if(!n){return null}return n}var a=n(2),u=(n(15),n(40)),c=(n(9),n(11)),s=(n(0),n(1),{isMounted:function(t){var e=u.get(t);return!!e&&!!e._renderedComponent},enqueueCallback:function(t,e,n){s.validateCallback(e,n);var i=o(t);return i?(i._pendingCallbacks?i._pendingCallbacks.push(e):i._pendingCallbacks=[e],void r(i)):null},enqueueCallbackInternal:function(t,e){t._pendingCallbacks?t._pendingCallbacks.push(e):t._pendingCallbacks=[e],r(t)},enqueueForceUpdate:function(t){var e=o(t,\"forceUpdate\");e&&(e._pendingForceUpdate=!0,r(e))},enqueueReplaceState:function(t,e){var n=o(t,\"replaceState\");n&&(n._pendingStateQueue=[e],n._pendingReplaceState=!0,r(n))},enqueueSetState:function(t,e){var n=o(t,\"setState\");if(n){var i=n._pendingStateQueue||(n._pendingStateQueue=[]);i.push(e),r(n)}},enqueueElementInternal:function(t,e,n){t._pendingElement=e,t._context=n,r(t)},validateCallback:function(t,e){t&&\"function\"!=typeof t?a(\"122\",e,i(t)):void 0}});t.exports=s},function(t,e,n){\"use strict\";var r={currentScrollLeft:0,currentScrollTop:0,refreshScrollValues:function(t){r.currentScrollLeft=t.x,r.currentScrollTop=t.y}};t.exports=r},function(t,e,n){\"use strict\";var r=function(t){return\"undefined\"!=typeof MSApp&&MSApp.execUnsafeLocalFunction?function(e,n,r,i){MSApp.execUnsafeLocalFunction(function(){return t(e,n,r,i)})}:t};t.exports=r},function(t,e,n){\"use strict\";function r(t){var e,n=t.keyCode;return\"charCode\"in t?(e=t.charCode,0===e&&13===n&&(e=13)):e=n,e>=32||13===e?e:0}t.exports=r},function(t,e,n){\"use strict\";function r(t){var e=this,n=e.nativeEvent;if(n.getModifierState)return n.getModifierState(t);var r=o[t];return!!r&&!!n[r]}function i(t){return r}var o={Alt:\"altKey\",Control:\"ctrlKey\",Meta:\"metaKey\",Shift:\"shiftKey\"};t.exports=i},function(t,e,n){\"use strict\";function r(t){var e=t.target||t.srcElement||window;return e.correspondingUseElement&&(e=e.correspondingUseElement),3===e.nodeType?e.parentNode:e}t.exports=r},function(t,e,n){\"use strict\";/**\n",
       " * Checks if an event is supported in the current execution environment.\n",
       " *\n",
       " * NOTE: This will not work correctly for non-generic events such as `change`,\n",
       " * `reset`, `load`, `error`, and `select`.\n",
       " *\n",
       " * Borrows from Modernizr.\n",
       " *\n",
       " * @param {string} eventNameSuffix Event name, e.g. \"click\".\n",
       " * @param {?boolean} capture Check if the capture phase is supported.\n",
       " * @return {boolean} True if the event is supported.\n",
       " * @internal\n",
       " * @license Modernizr 3.0.0pre (Custom Build) | MIT\n",
       " */\n",
       "function r(t,e){if(!o.canUseDOM||e&&!(\"addEventListener\"in document))return!1;var n=\"on\"+t,r=n in document;if(!r){var a=document.createElement(\"div\");a.setAttribute(n,\"return;\"),r=\"function\"==typeof a[n]}return!r&&i&&\"wheel\"===t&&(r=document.implementation.hasFeature(\"Events.wheel\",\"3.0\")),r}var i,o=n(6);o.canUseDOM&&(i=document.implementation&&document.implementation.hasFeature&&document.implementation.hasFeature(\"\",\"\")!==!0),t.exports=r},function(t,e,n){\"use strict\";function r(t,e){var n=null===t||t===!1,r=null===e||e===!1;if(n||r)return n===r;var i=typeof t,o=typeof e;return\"string\"===i||\"number\"===i?\"string\"===o||\"number\"===o:\"object\"===o&&t.type===e.type&&t.key===e.key}t.exports=r},function(t,e,n){\"use strict\";var r=(n(3),n(8)),i=(n(1),r);t.exports=i},function(t,e,n){\"use strict\";function r(t,e,n){this.props=t,this.context=e,this.refs=a,this.updater=n||o}var i=n(28),o=n(98),a=(n(176),n(38));n(0),n(1);r.prototype.isReactComponent={},r.prototype.setState=function(t,e){\"object\"!=typeof t&&\"function\"!=typeof t&&null!=t?i(\"85\"):void 0,this.updater.enqueueSetState(this,t),e&&this.updater.enqueueCallback(this,e,\"setState\")},r.prototype.forceUpdate=function(t){this.updater.enqueueForceUpdate(this),t&&this.updater.enqueueCallback(this,t,\"forceUpdate\")};t.exports=r},function(t,e,n){\"use strict\";function r(t,e){}var i=(n(1),{isMounted:function(t){return!1},enqueueCallback:function(t,e){},enqueueForceUpdate:function(t){r(t,\"forceUpdate\")},enqueueReplaceState:function(t,e){r(t,\"replaceState\")},enqueueSetState:function(t,e){r(t,\"setState\")}});t.exports=i},function(t,e){var n;n=function(){return this}();try{n=n||Function(\"return this\")()||(0,eval)(\"this\")}catch(t){\"object\"==typeof window&&(n=window)}t.exports=n},function(t,e){t.exports=function(t){return t.webpackPolyfill||(t.deprecate=function(){},t.paths=[],t.children||(t.children=[]),Object.defineProperty(t,\"loaded\",{enumerable:!0,get:function(){return t.l}}),Object.defineProperty(t,\"id\",{enumerable:!0,get:function(){return t.i}}),t.webpackPolyfill=1),t}},function(t,e,n){\"use strict\";n.d(e,\"b\",function(){return i}),n.d(e,\"a\",function(){return o});var r=Array.prototype,i=r.slice,o=r.map},function(t,e,n){\"use strict\";var r=n(18),i=n(103),o=n.i(i.a)(r.a),a=o.right;o.left;e.a=a},function(t,e,n){\"use strict\";function r(t){return function(e,r){return n.i(i.a)(t(e),r)}}var i=n(18);e.a=function(t){return 1===t.length&&(t=r(t)),{left:function(e,n,r,i){for(null==r&&(r=0),null==i&&(i=e.length);r<i;){var o=r+i>>>1;t(e[o],n)<0?r=o+1:i=o}return r},right:function(e,n,r,i){for(null==r&&(r=0),null==i&&(i=e.length);r<i;){var o=r+i>>>1;t(e[o],n)>0?i=o:r=o+1}return r}}}},function(t,e,n){\"use strict\";var r=n(111);e.a=function(t,e){var i=n.i(r.a)(t,e);return i?Math.sqrt(i):i}},function(t,e,n){\"use strict\";e.a=function(t,e){var n,r,i,o=-1,a=t.length;if(null==e){for(;++o<a;)if(null!=(r=t[o])&&r>=r){n=i=r;break}for(;++o<a;)null!=(r=t[o])&&(n>r&&(n=r),i<r&&(i=r))}else{for(;++o<a;)if(null!=(r=e(t[o],o,t))&&r>=r){n=i=r;break}for(;++o<a;)null!=(r=e(t[o],o,t))&&(n>r&&(n=r),i<r&&(i=r))}return[n,i]}},function(t,e,n){\"use strict\";e.a=function(t,e){var n,r,i=-1,o=t.length;if(null==e){for(;++i<o;)if(null!=(r=t[i])&&r>=r){n=r;break}for(;++i<o;)null!=(r=t[i])&&n>r&&(n=r)}else{for(;++i<o;)if(null!=(r=e(t[i],i,t))&&r>=r){n=r;break}for(;++i<o;)null!=(r=e(t[i],i,t))&&n>r&&(n=r)}return n}},function(t,e,n){\"use strict\";e.a=function(t,e,n){t=+t,e=+e,n=(i=arguments.length)<2?(e=t,t=0,1):i<3?1:+n;for(var r=-1,i=0|Math.max(0,Math.ceil((e-t)/n)),o=new Array(i);++r<i;)o[r]=t+r*n;return o}},function(t,e,n){\"use strict\";e.a=function(t){return Math.ceil(Math.log(t.length)/Math.LN2)+1}},function(t,e,n){\"use strict\";function r(t,e,n){var r=Math.abs(e-t)/Math.max(0,n),i=Math.pow(10,Math.floor(Math.log(r)/Math.LN10)),c=r/i;return c>=o?i*=10:c>=a?i*=5:c>=u&&(i*=2),e<t?-i:i}var i=n(107);e.b=r;var o=Math.sqrt(50),a=Math.sqrt(10),u=Math.sqrt(2);e.a=function(t,e,o){var a=r(t,e,o);return n.i(i.a)(Math.ceil(t/a)*a,Math.floor(e/a)*a+a/2,a)}},function(t,e,n){\"use strict\";function r(t){return t.length}var i=n(106);e.a=function(t){if(!(u=t.length))return[];for(var e=-1,o=n.i(i.a)(t,r),a=new Array(o);++e<o;)for(var u,c=-1,s=a[e]=new Array(u);++c<u;)s[c]=t[c][e];return a}},function(t,e,n){\"use strict\";var r=n(29);e.a=function(t,e){var i,o,a=t.length,u=0,c=0,s=-1,l=0;if(null==e)for(;++s<a;)isNaN(i=n.i(r.a)(t[s]))||(o=i-u,u+=o/++l,c+=o*(i-u));else for(;++s<a;)isNaN(i=n.i(r.a)(e(t[s],s,t)))||(o=i-u,u+=o/++l,c+=o*(i-u));if(l>1)return c/(l-1)}},function(t,e,n){\"use strict\";Object.defineProperty(e,\"__esModule\",{value:!0});var r=n(201);n.d(e,\"axisTop\",function(){return r.a}),n.d(e,\"axisRight\",function(){return r.b}),n.d(e,\"axisBottom\",function(){return r.c}),n.d(e,\"axisLeft\",function(){return r.d})},function(t,e,n){\"use strict\";n.d(e,\"b\",function(){return r}),n.d(e,\"a\",function(){return i});var r=Math.PI/180,i=180/Math.PI},function(t,e,n){\"use strict\";var r=n(61);n.d(e,\"b\",function(){return i});var i;e.a=function(t,e){var o=n.i(r.a)(t,e);if(!o)return t+\"\";var a=o[0],u=o[1],c=u-(i=3*Math.max(-8,Math.min(8,Math.floor(u/3))))+1,s=a.length;return c===s?a:c>s?a+new Array(c-s+1).join(\"0\"):c>0?a.slice(0,c)+\".\"+a.slice(c):\"0.\"+new Array(1-c).join(\"0\")+n.i(r.a)(t,Math.max(0,e+c-1))[0]}},function(t,e,n){\"use strict\";function r(t){if(!(e=o.exec(t)))throw new Error(\"invalid format: \"+t);var e,n=e[1]||\" \",r=e[2]||\">\",a=e[3]||\"-\",u=e[4]||\"\",c=!!e[5],s=e[6]&&+e[6],l=!!e[7],f=e[8]&&+e[8].slice(1),p=e[9]||\"\";\"n\"===p?(l=!0,p=\"g\"):i.a[p]||(p=\"\"),(c||\"0\"===n&&\"=\"===r)&&(c=!0,n=\"0\",r=\"=\"),this.fill=n,this.align=r,this.sign=a,this.symbol=u,this.zero=c,this.width=s,this.comma=l,this.precision=f,this.type=p}var i=n(116),o=/^(?:(.)?([<>=^]))?([+\\-\\( ])?([$#])?(0)?(\\d+)?(,)?(\\.\\d+)?([a-z%])?$/i;e.a=function(t){return new r(t)},r.prototype.toString=function(){return this.fill+this.align+this.sign+this.symbol+(this.zero?\"0\":\"\")+(null==this.width?\"\":Math.max(1,0|this.width))+(this.comma?\",\":\"\")+(null==this.precision?\"\":\".\"+Math.max(0,0|this.precision))+this.type}},function(t,e,n){\"use strict\";var r=n(212),i=n(114),o=n(214);e.a={\"\":r.a,\"%\":function(t,e){return(100*t).toFixed(e)},b:function(t){return Math.round(t).toString(2)},c:function(t){return t+\"\"},d:function(t){return Math.round(t).toString(10)},e:function(t,e){return t.toExponential(e)},f:function(t,e){return t.toFixed(e)},g:function(t,e){return t.toPrecision(e)},o:function(t){return Math.round(t).toString(8)},p:function(t,e){return n.i(o.a)(100*t,e)},r:o.a,s:i.a,X:function(t){return Math.round(t).toString(16).toUpperCase()},x:function(t){return Math.round(t).toString(16)}}},function(t,e,n){\"use strict\";function r(t){return t}var i=n(42),o=n(213),a=n(115),u=n(116),c=n(114),s=[\"y\",\"z\",\"a\",\"f\",\"p\",\"n\",\"µ\",\"m\",\"\",\"k\",\"M\",\"G\",\"T\",\"P\",\"E\",\"Z\",\"Y\"];e.a=function(t){function e(t){function e(t){var e,n,a,u=_,l=b;if(\"c\"===y)l=x(t)+l,t=\"\";else{t=+t;var p=(t<0||1/t<0)&&(t*=-1,!0);if(t=x(t,m),p)for(e=-1,n=t.length,p=!1;++e<n;)if(a=t.charCodeAt(e),48<a&&a<58||\"x\"===y&&96<a&&a<103||\"X\"===y&&64<a&&a<71){p=!0;break}if(u=(p?\"(\"===o?o:\"-\":\"-\"===o||\"(\"===o?\"\":o)+u,l=l+(\"s\"===y?s[8+c.b/3]:\"\")+(p&&\"(\"===o?\")\":\"\"),w)for(e=-1,n=t.length;++e<n;)if(a=t.charCodeAt(e),48>a||a>57){l=(46===a?h+t.slice(e+1):t.slice(e))+l,t=t.slice(0,e);break}}g&&!d&&(t=f(t,1/0));var C=u.length+t.length+l.length,M=C<v?new Array(v-C+1).join(r):\"\";switch(g&&d&&(t=f(M+t,M.length?v-l.length:1/0),M=\"\"),i){case\"<\":return u+t+l+M;case\"=\":return u+M+t+l;case\"^\":return M.slice(0,C=M.length>>1)+u+t+l+M.slice(C)}return M+u+t+l}t=n.i(a.a)(t);var r=t.fill,i=t.align,o=t.sign,l=t.symbol,d=t.zero,v=t.width,g=t.comma,m=t.precision,y=t.type,_=\"$\"===l?p[0]:\"#\"===l&&/[boxX]/.test(y)?\"0\"+y.toLowerCase():\"\",b=\"$\"===l?p[1]:/[%p]/.test(y)?\"%\":\"\",x=u.a[y],w=!y||/[defgprs%]/.test(y);return m=null==m?y?6:12:/[gprs]/.test(y)?Math.max(1,Math.min(21,m)):Math.max(0,Math.min(20,m)),e.toString=function(){return t+\"\"},e}function l(t,r){var o=e((t=n.i(a.a)(t),t.type=\"f\",t)),u=3*Math.max(-8,Math.min(8,Math.floor(n.i(i.a)(r)/3))),c=Math.pow(10,-u),l=s[8+u/3];return function(t){return o(c*t)+l}}var f=t.grouping&&t.thousands?n.i(o.a)(t.grouping,t.thousands):r,p=t.currency,h=t.decimal;return{format:e,formatPrefix:l}}},function(t,e,n){\"use strict\";var r=n(63);e.a=function(t,e){var i,o=e?e.length:0,a=t?Math.min(o,t.length):0,u=new Array(o),c=new Array(o);for(i=0;i<a;++i)u[i]=n.i(r.a)(t[i],e[i]);for(;i<o;++i)c[i]=e[i];return function(t){for(i=0;i<a;++i)c[i]=u[i](t);return c}}},function(t,e,n){\"use strict\";var r=n(62);e.a=function(t){var e=t.length;return function(i){var o=Math.floor(((i%=1)<0?++i:i)*e),a=t[(o+e-1)%e],u=t[o%e],c=t[(o+1)%e],s=t[(o+2)%e];return n.i(r.b)((i-o/e)*e,a,u,c,s)}}},function(t,e,n){\"use strict\";e.a=function(t){return function(){return t}}},function(t,e,n){\"use strict\";e.a=function(t,e){var n=new Date;return t=+t,e-=t,function(r){return n.setTime(t+e*r),n}}},function(t,e,n){\"use strict\";var r=n(63);e.a=function(t,e){var i,o={},a={};null!==t&&\"object\"==typeof t||(t={}),null!==e&&\"object\"==typeof e||(e={});for(i in e)i in t?o[i]=n.i(r.a)(t[i],e[i]):a[i]=e[i];return function(t){for(i in o)a[i]=o[i](t);return a}}},function(t,e,n){\"use strict\";function r(t){return function(e){var r,o,a=e.length,u=new Array(a),c=new Array(a),s=new Array(a);for(r=0;r<a;++r)o=n.i(i.rgb)(e[r]),u[r]=o.r||0,c[r]=o.g||0,s[r]=o.b||0;return u=t(u),c=t(c),s=t(s),o.opacity=1,function(t){return o.r=u(t),o.g=c(t),o.b=s(t),o+\"\"}}}var i=n(10),o=n(62),a=n(119),u=n(32);e.a=function t(e){function r(t,e){var r=o((t=n.i(i.rgb)(t)).r,(e=n.i(i.rgb)(e)).r),a=o(t.g,e.g),c=o(t.b,e.b),s=n.i(u.a)(t.opacity,e.opacity);return function(e){return t.r=r(e),t.g=a(e),t.b=c(e),t.opacity=s(e),t+\"\"}}var o=n.i(u.c)(e);return r.gamma=t,r}(1);r(o.a),r(a.a)},function(t,e,n){\"use strict\";function r(t){return function(){return t}}function i(t){return function(e){return t(e)+\"\"}}var o=n(43),a=/[-+]?(?:\\d+\\.?\\d*|\\.?\\d+)(?:[eE][-+]?\\d+)?/g,u=new RegExp(a.source,\"g\");e.a=function(t,e){var c,s,l,f=a.lastIndex=u.lastIndex=0,p=-1,h=[],d=[];for(t+=\"\",e+=\"\";(c=a.exec(t))&&(s=u.exec(e));)(l=s.index)>f&&(l=e.slice(f,l),h[p]?h[p]+=l:h[++p]=l),(c=c[0])===(s=s[0])?h[p]?h[p]+=s:h[++p]=s:(h[++p]=null,d.push({i:p,x:n.i(o.a)(c,s)})),f=u.lastIndex;return f<e.length&&(l=e.slice(f),h[p]?h[p]+=l:h[++p]=l),h.length<2?d[0]?i(d[0].x):r(e):(e=d.length,function(t){for(var n,r=0;r<e;++r)h[(n=d[r]).i]=n.x(t);return h.join(\"\")})}},function(t,e,n){\"use strict\";e.a=function(t,e){t=t.slice();var n,r=0,i=t.length-1,o=t[r],a=t[i];return a<o&&(n=r,r=i,i=n,n=o,o=a,a=n),t[r]=e.floor(o),t[i]=e.ceil(a),t}},function(t,e,n){\"use strict\";e.a=function(t){return+t}},function(t,e,n){\"use strict\";function r(t){function e(e){var n=e+\"\",r=u.get(n);if(!r){if(s!==a)return s;u.set(n,r=c.push(e))}return t[(r-1)%t.length]}var u=n.i(i.a)(),c=[],s=a;return t=null==t?[]:o.b.call(t),e.domain=function(t){if(!arguments.length)return c.slice();c=[],u=n.i(i.a)();for(var r,o,a=-1,s=t.length;++a<s;)u.has(o=(r=t[a])+\"\")||u.set(o,c.push(r));return e},e.range=function(n){return arguments.length?(t=o.b.call(n),e):t.slice()},e.unknown=function(t){return arguments.length?(s=t,e):s},e.copy=function(){return r().domain(c).range(t).unknown(s)},e}var i=n(203),o=n(16);n.d(e,\"b\",function(){return a}),e.a=r;var a={name:\"implicit\"}},function(t,e,n){\"use strict\";function r(t){return new Date(t)}function i(t){return t instanceof Date?+t:+new Date(+t)}function o(t,e,c,s,b,x,w,C,M){function k(n){return(w(n)<n?N:x(n)<n?A:b(n)<n?O:s(n)<n?I:e(n)<n?c(n)<n?D:R:t(n)<n?L:U)(n)}function E(e,r,i,o){if(null==e&&(e=10),\"number\"==typeof e){var u=Math.abs(i-r)/e,c=n.i(a.d)(function(t){return t[2]}).right(F,u);c===F.length?(o=n.i(a.b)(r/_,i/_,e),e=t):c?(c=F[u/F[c-1][2]<F[c][2]/u?c-1:c],o=c[1],e=c[0]):(o=n.i(a.b)(r,i,e),e=C)}return null==o?e:e.every(o)}var T=n.i(f.a)(f.b,u.a),S=T.invert,P=T.domain,N=M(\".%L\"),A=M(\":%S\"),O=M(\"%I:%M\"),I=M(\"%I %p\"),D=M(\"%a %d\"),R=M(\"%b %d\"),L=M(\"%B\"),U=M(\"%Y\"),F=[[w,1,h],[w,5,5*h],[w,15,15*h],[w,30,30*h],[x,1,d],[x,5,5*d],[x,15,15*d],[x,30,30*d],[b,1,v],[b,3,3*v],[b,6,6*v],[b,12,12*v],[s,1,g],[s,2,2*g],[c,1,m],[e,1,y],[e,3,3*y],[t,1,_]];return T.invert=function(t){return new Date(S(t))},T.domain=function(t){return arguments.length?P(l.a.call(t,i)):P().map(r)},T.ticks=function(t,e){var n,r=P(),i=r[0],o=r[r.length-1],a=o<i;return a&&(n=i,i=o,o=n),n=E(t,i,o,e),n=n?n.range(i,o+1):[],a?n.reverse():n},T.tickFormat=function(t,e){return null==e?k:M(e)},T.nice=function(t,e){var r=P();return(t=E(t,r[0],r[r.length-1],e))?P(n.i(p.a)(r,t)):T},T.copy=function(){return n.i(f.c)(T,o(t,e,c,s,b,x,w,C,M))},T}var a=n(12),u=n(31),c=n(79),s=n(77),l=n(16),f=n(45),p=n(125);e.b=o;var h=1e3,d=60*h,v=60*d,g=24*v,m=7*g,y=30*g,_=365*g;e.a=function(){return o(c.b,c.o,c.p,c.a,c.q,c.r,c.s,c.t,s.timeFormat).domain([new Date(2e3,0,1),new Date(2e3,0,2)])}},function(t,e,n){\"use strict\";Object.defineProperty(e,\"__esModule\",{value:!0});var r=n(66);n.d(e,\"creator\",function(){return r.a});var i=n(247);n.d(e,\"local\",function(){return i.a});var o=n(130);n.d(e,\"matcher\",function(){return o.a});var a=n(248);n.d(e,\"mouse\",function(){return a.a});var u=n(67);n.d(e,\"namespace\",function(){return u.a});var c=n(68);n.d(e,\"namespaces\",function(){return c.a});var s=n(249);n.d(e,\"select\",function(){return s.a});var l=n(250);n.d(e,\"selectAll\",function(){return l.a});var f=n(7);n.d(e,\"selection\",function(){return f.a});var p=n(71);n.d(e,\"selector\",function(){return p.a});var h=n(133);n.d(e,\"selectorAll\",function(){return h.a});var d=n(278);n.d(e,\"touch\",function(){return d.a});var v=n(279);n.d(e,\"touches\",function(){return v.a});var g=n(73);n.d(e,\"window\",function(){return g.a});var m=n(70);n.d(e,\"event\",function(){return m.a}),n.d(e,\"customEvent\",function(){return m.b})},function(t,e,n){\"use strict\";var r=function(t){return function(){return this.matches(t)}};if(\"undefined\"!=typeof document){var i=document.documentElement;if(!i.matches){var o=i.webkitMatchesSelector||i.msMatchesSelector||i.mozMatchesSelector||i.oMatchesSelector;r=function(t){return function(){return o.call(this,t)}}}}e.a=r},function(t,e,n){\"use strict\";function r(t,e){this.ownerDocument=t.ownerDocument,this.namespaceURI=t.namespaceURI,this._next=null,this._parent=t,this.__data__=e}var i=n(132),o=n(7);e.b=r,e.a=function(){return new o.b(this._enter||this._groups.map(i.a),this._parents)},r.prototype={constructor:r,appendChild:function(t){return this._parent.insertBefore(t,this._next)},insertBefore:function(t,e){return this._parent.insertBefore(t,e)},querySelector:function(t){return this._parent.querySelector(t)},querySelectorAll:function(t){return this._parent.querySelectorAll(t)}}},function(t,e,n){\"use strict\";e.a=function(t){return new Array(t.length)}},function(t,e,n){\"use strict\";function r(){return[]}e.a=function(t){return null==t?r:function(){return this.querySelectorAll(t)}}},function(t,e,n){\"use strict\";Object.defineProperty(e,\"__esModule\",{value:!0});var r=n(280);n.d(e,\"arc\",function(){return r.a});var i=n(135);n.d(e,\"area\",function(){return i.a});var o=n(75);n.d(e,\"line\",function(){return o.a});var a=n(299);n.d(e,\"pie\",function(){return a.a});var u=n(300);n.d(e,\"radialArea\",function(){return u.a});var c=n(140);n.d(e,\"radialLine\",function(){return c.a});var s=n(302);n.d(e,\"symbol\",function(){return s.a}),n.d(e,\"symbols\",function(){return s.b});var l=n(141);n.d(e,\"symbolCircle\",function(){return l.a});var f=n(142);n.d(e,\"symbolCross\",function(){return f.a});var p=n(143);n.d(e,\"symbolDiamond\",function(){return p.a});var h=n(144);n.d(e,\"symbolSquare\",function(){return h.a});var d=n(145);n.d(e,\"symbolStar\",function(){return d.a});var v=n(146);n.d(e,\"symbolTriangle\",function(){return v.a});var g=n(147);n.d(e,\"symbolWye\",function(){return g.a});var m=n(282);n.d(e,\"curveBasisClosed\",function(){return m.a});var y=n(283);n.d(e,\"curveBasisOpen\",function(){return y.a});var _=n(46);n.d(e,\"curveBasis\",function(){return _.a});var b=n(284);n.d(e,\"curveBundle\",function(){return b.a});var x=n(136);n.d(e,\"curveCardinalClosed\",function(){return x.a});var w=n(137);n.d(e,\"curveCardinalOpen\",function(){return w.a});var C=n(47);n.d(e,\"curveCardinal\",function(){return C.a});var M=n(285);n.d(e,\"curveCatmullRomClosed\",function(){return M.a});var k=n(286);n.d(e,\"curveCatmullRomOpen\",function(){return k.a});var E=n(74);n.d(e,\"curveCatmullRom\",function(){return E.a});var T=n(287);n.d(e,\"curveLinearClosed\",function(){return T.a});var S=n(48);n.d(e,\"curveLinear\",function(){return S.a});var P=n(288);n.d(e,\"curveMonotoneX\",function(){return P.a}),n.d(e,\"curveMonotoneY\",function(){return P.b});var N=n(289);n.d(e,\"curveNatural\",function(){return N.a});var A=n(290);n.d(e,\"curveStep\",function(){return A.a}),n.d(e,\"curveStepAfter\",function(){return A.b}),n.d(e,\"curveStepBefore\",function(){return A.c});var O=n(301);n.d(e,\"stack\",function(){return O.a});var I=n(293);n.d(e,\"stackOffsetExpand\",function(){return I.a});var D=n(36);n.d(e,\"stackOffsetNone\",function(){return D.a});var R=n(294);n.d(e,\"stackOffsetSilhouette\",function(){return R.a});var L=n(295);n.d(e,\"stackOffsetWiggle\",function(){return L.a});var U=n(76);n.d(e,\"stackOrderAscending\",function(){return U.a});var F=n(296);n.d(e,\"stackOrderDescending\",function(){return F.a});var j=n(297);n.d(e,\"stackOrderInsideOut\",function(){return j.a});var B=n(37);n.d(e,\"stackOrderNone\",function(){return B.a});var W=n(298);n.d(e,\"stackOrderReverse\",function(){return W.a})},function(t,e,n){\"use strict\";var r=n(44),i=n(19),o=n(48),a=n(75),u=n(139);e.a=function(){function t(t){var e,i,o,a,u,g=t.length,m=!1,y=new Array(g),_=new Array(g);for(null==h&&(v=d(u=n.i(r.a)())),e=0;e<=g;++e){if(!(e<g&&p(a=t[e],e,t))===m)if(m=!m)i=e,v.areaStart(),v.lineStart();else{for(v.lineEnd(),v.lineStart(),o=e-1;o>=i;--o)v.point(y[o],_[o]);v.lineEnd(),v.areaEnd()}m&&(y[e]=+c(a,e,t),_[e]=+l(a,e,t),v.point(s?+s(a,e,t):y[e],f?+f(a,e,t):_[e]))}if(u)return v=null,u+\"\"||null}function e(){return n.i(a.a)().defined(p).curve(d).context(h)}var c=u.a,s=null,l=n.i(i.a)(0),f=u.b,p=n.i(i.a)(!0),h=null,d=o.a,v=null;return t.x=function(e){return arguments.length?(c=\"function\"==typeof e?e:n.i(i.a)(+e),s=null,t):c},t.x0=function(e){return arguments.length?(c=\"function\"==typeof e?e:n.i(i.a)(+e),t):c},t.x1=function(e){return arguments.length?(s=null==e?null:\"function\"==typeof e?e:n.i(i.a)(+e),t):s},t.y=function(e){return arguments.length?(l=\"function\"==typeof e?e:n.i(i.a)(+e),f=null,t):l},t.y0=function(e){return arguments.length?(l=\"function\"==typeof e?e:n.i(i.a)(+e),t):l},t.y1=function(e){return arguments.length?(f=null==e?null:\"function\"==typeof e?e:n.i(i.a)(+e),t):f},t.lineX0=t.lineY0=function(){return e().x(c).y(l)},t.lineY1=function(){return e().x(c).y(f)},t.lineX1=function(){return e().x(s).y(l)},t.defined=function(e){return arguments.length?(p=\"function\"==typeof e?e:n.i(i.a)(!!e),t):p},t.curve=function(e){return arguments.length?(d=e,null!=h&&(v=d(h)),t):d},t.context=function(e){return arguments.length?(null==e?h=v=null:v=d(h=e),t):h},t}},function(t,e,n){\"use strict\";function r(t,e){this._context=t,this._k=(1-e)/6}var i=n(49),o=n(47);e.b=r,r.prototype={areaStart:i.a,areaEnd:i.a,lineStart:function(){this._x0=this._x1=this._x2=this._x3=this._x4=this._x5=this._y0=this._y1=this._y2=this._y3=this._y4=this._y5=NaN,this._point=0},lineEnd:function(){switch(this._point){case 1:this._context.moveTo(this._x3,this._y3),this._context.closePath();break;case 2:this._context.lineTo(this._x3,this._y3),this._context.closePath();break;case 3:this.point(this._x3,this._y3),this.point(this._x4,this._y4),this.point(this._x5,this._y5)}},point:function(t,e){switch(t=+t,e=+e,this._point){case 0:this._point=1,this._x3=t,this._y3=e;break;case 1:this._point=2,this._context.moveTo(this._x4=t,this._y4=e);break;case 2:this._point=3,this._x5=t,this._y5=e;break;default:n.i(o.c)(this,t,e)}this._x0=this._x1,this._x1=this._x2,this._x2=t,this._y0=this._y1,this._y1=this._y2,this._y2=e}},e.a=function t(e){function n(t){return new r(t,e)}return n.tension=function(e){return t(+e)},n}(0)},function(t,e,n){\"use strict\";function r(t,e){this._context=t,this._k=(1-e)/6}var i=n(47);e.b=r,r.prototype={areaStart:function(){this._line=0},areaEnd:function(){this._line=NaN},lineStart:function(){this._x0=this._x1=this._x2=this._y0=this._y1=this._y2=NaN,this._point=0},lineEnd:function(){(this._line||0!==this._line&&3===this._point)&&this._context.closePath(),this._line=1-this._line},point:function(t,e){switch(t=+t,e=+e,this._point){case 0:this._point=1;break;case 1:this._point=2;break;case 2:this._point=3,this._line?this._context.lineTo(this._x2,this._y2):this._context.moveTo(this._x2,this._y2);break;case 3:this._point=4;default:n.i(i.c)(this,t,e)}this._x0=this._x1,this._x1=this._x2,this._x2=t,this._y0=this._y1,this._y1=this._y2,this._y2=e}},e.a=function t(e){function n(t){return new r(t,e)}return n.tension=function(e){return t(+e)},n}(0)},function(t,e,n){\"use strict\";function r(t){this._curve=t}function i(t){function e(e){return new r(t(e))}return e._curve=t,e}var o=n(48);n.d(e,\"b\",function(){return a}),e.a=i;var a=i(o.a);r.prototype={areaStart:function(){this._curve.areaStart()},areaEnd:function(){this._curve.areaEnd()},lineStart:function(){this._curve.lineStart()},lineEnd:function(){this._curve.lineEnd()},point:function(t,e){this._curve.point(e*Math.sin(t),e*-Math.cos(t))}}},function(t,e,n){\"use strict\";function r(t){return t[0]}function i(t){return t[1]}e.a=r,e.b=i},function(t,e,n){\"use strict\";function r(t){var e=t.curve;return t.angle=t.x,delete t.x,t.radius=t.y,delete t.y,t.curve=function(t){return arguments.length?e(n.i(i.a)(t)):e()._curve},t}var i=n(138),o=n(75);e.b=r,e.a=function(){return r(n.i(o.a)().curve(i.b))}},function(t,e,n){\"use strict\";var r=n(35);e.a={draw:function(t,e){var n=Math.sqrt(e/r.b);t.moveTo(n,0),t.arc(0,0,n,0,r.c)}}},function(t,e,n){\"use strict\";e.a={draw:function(t,e){var n=Math.sqrt(e/5)/2;t.moveTo(-3*n,-n),t.lineTo(-n,-n),t.lineTo(-n,-3*n),t.lineTo(n,-3*n),t.lineTo(n,-n),t.lineTo(3*n,-n),t.lineTo(3*n,n),t.lineTo(n,n),t.lineTo(n,3*n),t.lineTo(-n,3*n),t.lineTo(-n,n),t.lineTo(-3*n,n),t.closePath()}}},function(t,e,n){\"use strict\";var r=Math.sqrt(1/3),i=2*r;e.a={draw:function(t,e){var n=Math.sqrt(e/i),o=n*r;t.moveTo(0,-n),t.lineTo(o,0),t.lineTo(0,n),t.lineTo(-o,0),t.closePath()}}},function(t,e,n){\"use strict\";e.a={draw:function(t,e){var n=Math.sqrt(e),r=-n/2;t.rect(r,r,n,n)}}},function(t,e,n){\"use strict\";var r=n(35),i=.8908130915292852,o=Math.sin(r.b/10)/Math.sin(7*r.b/10),a=Math.sin(r.c/10)*o,u=-Math.cos(r.c/10)*o;e.a={draw:function(t,e){var n=Math.sqrt(e*i),o=a*n,c=u*n;t.moveTo(0,-n),t.lineTo(o,c);for(var s=1;s<5;++s){var l=r.c*s/5,f=Math.cos(l),p=Math.sin(l);t.lineTo(p*n,-f*n),t.lineTo(f*o-p*c,p*o+f*c)}t.closePath()}}},function(t,e,n){\"use strict\";var r=Math.sqrt(3);e.a={draw:function(t,e){var n=-Math.sqrt(e/(3*r));t.moveTo(0,2*n),t.lineTo(-r*n,-n),t.lineTo(r*n,-n),t.closePath()}}},function(t,e,n){\"use strict\";var r=-.5,i=Math.sqrt(3)/2,o=1/Math.sqrt(12),a=3*(o/2+1);e.a={draw:function(t,e){var n=Math.sqrt(e/a),u=n/2,c=n*o,s=u,l=n*o+n,f=-s,p=l;t.moveTo(u,c),t.lineTo(s,l),t.lineTo(f,p),t.lineTo(r*u-i*c,i*u+r*c),t.lineTo(r*s-i*l,i*s+r*l),t.lineTo(r*f-i*p,i*f+r*p),t.lineTo(r*u+i*c,r*c-i*u),t.lineTo(r*s+i*l,r*l-i*s),t.lineTo(r*f+i*p,r*p-i*f),t.closePath()}}},function(t,e,n){\"use strict\";function r(t){return t.toISOString()}var i=n(78);n.d(e,\"b\",function(){return o});var o=\"%Y-%m-%dT%H:%M:%S.%LZ\",a=Date.prototype.toISOString?r:n.i(i.d)(o);e.a=a},function(t,e,n){\"use strict\";function r(t){if(0<=t.y&&t.y<100){var e=new Date(-1,t.m,t.d,t.H,t.M,t.S,t.L);return e.setFullYear(t.y),e}return new Date(t.y,t.m,t.d,t.H,t.M,t.S,t.L)}function i(t){if(0<=t.y&&t.y<100){var e=new Date(Date.UTC(-1,t.m,t.d,t.H,t.M,t.S,t.L));return e.setUTCFullYear(t.y),e}return new Date(Date.UTC(t.y,t.m,t.d,t.H,t.M,t.S,t.L))}function o(t){return{y:t,m:0,d:1,H:0,M:0,S:0,L:0}}function a(t){function e(t,e){return function(n){var r,i,o,a=[],u=-1,c=0,s=t.length;for(n instanceof Date||(n=new Date(+n));++u<s;)37===t.charCodeAt(u)&&(a.push(t.slice(c,u)),null!=(i=et[r=t.charAt(++u)])?r=t.charAt(++u):i=\"e\"===r?\" \":\"0\",(o=e[r])&&(r=o(n,i)),a.push(r),c=u+1);return a.push(t.slice(c,u)),a.join(\"\")}}function n(t,e){return function(n){var r=o(1900),u=a(r,t,n+=\"\",0);if(u!=n.length)return null;if(\"p\"in r&&(r.H=r.H%12+12*r.p),\"W\"in r||\"U\"in r){\"w\"in r||(r.w=\"W\"in r?1:0);var c=\"Z\"in r?i(o(r.y)).getUTCDay():e(o(r.y)).getDay();r.m=0,r.d=\"W\"in r?(r.w+6)%7+7*r.W-(c+5)%7:r.w+7*r.U-(c+6)%7}return\"Z\"in r?(r.H+=r.Z/100|0,r.M+=r.Z%100,i(r)):e(r)}}function a(t,e,n,r){for(var i,o,a=0,u=e.length,c=n.length;a<u;){if(r>=c)return-1;if(i=e.charCodeAt(a++),37===i){if(i=e.charAt(a++),o=Ut[i in et?e.charAt(a++):i],!o||(r=o(t,n,r))<0)return-1}else if(i!=n.charCodeAt(r++))return-1}return r}function u(t,e,n){var r=kt.exec(e.slice(n));return r?(t.p=Et[r[0].toLowerCase()],n+r[0].length):-1}function c(t,e,n){var r=Pt.exec(e.slice(n));return r?(t.w=Nt[r[0].toLowerCase()],n+r[0].length):-1}function tt(t,e,n){var r=Tt.exec(e.slice(n));return r?(t.w=St[r[0].toLowerCase()],n+r[0].length):-1}function nt(t,e,n){var r=It.exec(e.slice(n));return r?(t.m=Dt[r[0].toLowerCase()],n+r[0].length):-1}function rt(t,e,n){var r=At.exec(e.slice(n));return r?(t.m=Ot[r[0].toLowerCase()],n+r[0].length):-1}function it(t,e,n){return a(t,mt,e,n)}function ot(t,e,n){return a(t,yt,e,n)}function at(t,e,n){return a(t,_t,e,n)}function ut(t){return wt[t.getDay()]}function ct(t){return xt[t.getDay()]}function st(t){return Mt[t.getMonth()]}function lt(t){return Ct[t.getMonth()]}function ft(t){return bt[+(t.getHours()>=12)]}function pt(t){return wt[t.getUTCDay()]}function ht(t){return xt[t.getUTCDay()]}function dt(t){return Mt[t.getUTCMonth()]}function vt(t){return Ct[t.getUTCMonth()]}function gt(t){return bt[+(t.getUTCHours()>=12)]}var mt=t.dateTime,yt=t.date,_t=t.time,bt=t.periods,xt=t.days,wt=t.shortDays,Ct=t.months,Mt=t.shortMonths,kt=s(bt),Et=l(bt),Tt=s(xt),St=l(xt),Pt=s(wt),Nt=l(wt),At=s(Ct),Ot=l(Ct),It=s(Mt),Dt=l(Mt),Rt={a:ut,A:ct,b:st,B:lt,c:null,d:k,e:k,H:E,I:T,j:S,L:P,m:N,M:A,p:ft,S:O,U:I,w:D,W:R,x:null,X:null,y:L,Y:U,Z:F,\"%\":J},Lt={a:pt,A:ht,b:dt,B:vt,c:null,d:j,e:j,H:B,I:W,j:V,L:z,m:H,M:q,p:gt,S:Y,U:K,w:G,W:$,x:null,X:null,y:X,Y:Z,Z:Q,\"%\":J},Ut={a:c,A:tt,b:nt,B:rt,c:it,d:y,e:y,H:b,I:b,j:_,L:C,m:m,M:x,p:u,S:w,U:p,w:f,W:h,x:ot,X:at,y:v,Y:d,Z:g,\"%\":M};return Rt.x=e(yt,Rt),Rt.X=e(_t,Rt),Rt.c=e(mt,Rt),Lt.x=e(yt,Lt),Lt.X=e(_t,Lt),Lt.c=e(mt,Lt),{format:function(t){var n=e(t+=\"\",Rt);return n.toString=function(){return t},n},parse:function(t){var e=n(t+=\"\",r);return e.toString=function(){return t},e},utcFormat:function(t){var n=e(t+=\"\",Lt);return n.toString=function(){return t},n},utcParse:function(t){var e=n(t,i);return e.toString=function(){return t},e}}}function u(t,e,n){var r=t<0?\"-\":\"\",i=(r?-t:t)+\"\",o=i.length;return r+(o<n?new Array(n-o+1).join(e)+i:i)}function c(t){return t.replace(it,\"\\\\$&\")}function s(t){return new RegExp(\"^(?:\"+t.map(c).join(\"|\")+\")\",\"i\")}function l(t){for(var e={},n=-1,r=t.length;++n<r;)e[t[n].toLowerCase()]=n;return e}function f(t,e,n){var r=nt.exec(e.slice(n,n+1));return r?(t.w=+r[0],n+r[0].length):-1}function p(t,e,n){var r=nt.exec(e.slice(n));return r?(t.U=+r[0],n+r[0].length):-1}function h(t,e,n){var r=nt.exec(e.slice(n));return r?(t.W=+r[0],n+r[0].length):-1}function d(t,e,n){var r=nt.exec(e.slice(n,n+4));return r?(t.y=+r[0],n+r[0].length):-1}function v(t,e,n){var r=nt.exec(e.slice(n,n+2));return r?(t.y=+r[0]+(+r[0]>68?1900:2e3),n+r[0].length):-1}function g(t,e,n){var r=/^(Z)|([+-]\\d\\d)(?:\\:?(\\d\\d))?/.exec(e.slice(n,n+6));return r?(t.Z=r[1]?0:-(r[2]+(r[3]||\"00\")),n+r[0].length):-1}function m(t,e,n){var r=nt.exec(e.slice(n,n+2));return r?(t.m=r[0]-1,n+r[0].length):-1}function y(t,e,n){var r=nt.exec(e.slice(n,n+2));return r?(t.d=+r[0],n+r[0].length):-1}function _(t,e,n){var r=nt.exec(e.slice(n,n+3));return r?(t.m=0,t.d=+r[0],n+r[0].length):-1}function b(t,e,n){var r=nt.exec(e.slice(n,n+2));return r?(t.H=+r[0],n+r[0].length):-1}function x(t,e,n){var r=nt.exec(e.slice(n,n+2));return r?(t.M=+r[0],n+r[0].length):-1}function w(t,e,n){var r=nt.exec(e.slice(n,n+2));return r?(t.S=+r[0],n+r[0].length):-1}function C(t,e,n){var r=nt.exec(e.slice(n,n+3));return r?(t.L=+r[0],n+r[0].length):-1}function M(t,e,n){var r=rt.exec(e.slice(n,n+1));return r?n+r[0].length:-1}function k(t,e){return u(t.getDate(),e,2)}function E(t,e){return u(t.getHours(),e,2)}function T(t,e){return u(t.getHours()%12||12,e,2)}function S(t,e){return u(1+tt.a.count(n.i(tt.b)(t),t),e,3)}function P(t,e){return u(t.getMilliseconds(),e,3)}function N(t,e){return u(t.getMonth()+1,e,2)}function A(t,e){return u(t.getMinutes(),e,2)}function O(t,e){return u(t.getSeconds(),e,2)}function I(t,e){return u(tt.c.count(n.i(tt.b)(t),t),e,2)}function D(t){return t.getDay()}function R(t,e){return u(tt.d.count(n.i(tt.b)(t),t),e,2)}function L(t,e){return u(t.getFullYear()%100,e,2)}function U(t,e){return u(t.getFullYear()%1e4,e,4)}function F(t){var e=t.getTimezoneOffset();return(e>0?\"-\":(e*=-1,\"+\"))+u(e/60|0,\"0\",2)+u(e%60,\"0\",2)}function j(t,e){return u(t.getUTCDate(),e,2)}function B(t,e){return u(t.getUTCHours(),e,2)}function W(t,e){return u(t.getUTCHours()%12||12,e,2)}function V(t,e){return u(1+tt.e.count(n.i(tt.f)(t),t),e,3)}function z(t,e){return u(t.getUTCMilliseconds(),e,3)}function H(t,e){return u(t.getUTCMonth()+1,e,2)}function q(t,e){return u(t.getUTCMinutes(),e,2)}function Y(t,e){return u(t.getUTCSeconds(),e,2)}function K(t,e){return u(tt.g.count(n.i(tt.f)(t),t),e,2)}function G(t){return t.getUTCDay()}function $(t,e){return u(tt.h.count(n.i(tt.f)(t),t),e,2)}function X(t,e){return u(t.getUTCFullYear()%100,e,2)}function Z(t,e){return u(t.getUTCFullYear()%1e4,e,4)}function Q(){return\"+0000\"}function J(){return\"%\"}var tt=n(79);e.a=a;var et={\"-\":\"\",_:\" \",0:\"0\"},nt=/^\\s*\\d+/,rt=/^%/,it=/[\\\\\\^\\$\\*\\+\\?\\|\\[\\]\\(\\)\\.\\{\\}]/g},function(t,e,n){\"use strict\";var r=n(8),i={listen:function(t,e,n){return t.addEventListener?(t.addEventListener(e,n,!1),{remove:function(){t.removeEventListener(e,n,!1)}}):t.attachEvent?(t.attachEvent(\"on\"+e,n),{remove:function(){t.detachEvent(\"on\"+e,n)}}):void 0},capture:function(t,e,n){return t.addEventListener?(t.addEventListener(e,n,!0),{remove:function(){t.removeEventListener(e,n,!0)}}):{remove:r}},registerDefault:function(){}};t.exports=i},function(t,e,n){\"use strict\";function r(t){try{t.focus()}catch(t){}}t.exports=r},function(t,e,n){\"use strict\";function r(){if(\"undefined\"==typeof document)return null;try{return document.activeElement||document.body}catch(t){return document.body}}t.exports=r},function(t,e){function n(){throw new Error(\"setTimeout has not been defined\")}function r(){throw new Error(\"clearTimeout has not been defined\")}function i(t){if(l===setTimeout)return setTimeout(t,0);if((l===n||!l)&&setTimeout)return l=setTimeout,setTimeout(t,0);try{return l(t,0)}catch(e){try{return l.call(null,t,0)}catch(e){return l.call(this,t,0)}}}function o(t){if(f===clearTimeout)return clearTimeout(t);if((f===r||!f)&&clearTimeout)return f=clearTimeout,clearTimeout(t);try{return f(t)}catch(e){try{return f.call(null,t)}catch(e){return f.call(this,t)}}}function a(){v&&h&&(v=!1,h.length?d=h.concat(d):g=-1,d.length&&u())}function u(){if(!v){var t=i(a);v=!0;for(var e=d.length;e;){for(h=d,d=[];++g<e;)h&&h[g].run();g=-1,e=d.length}h=null,v=!1,o(t)}}function c(t,e){this.fun=t,this.array=e}function s(){}var l,f,p=t.exports={};!function(){try{l=\"function\"==typeof setTimeout?setTimeout:n}catch(t){l=n}try{f=\"function\"==typeof clearTimeout?clearTimeout:r}catch(t){f=r}}();var h,d=[],v=!1,g=-1;p.nextTick=function(t){var e=new Array(arguments.length-1);if(arguments.length>1)for(var n=1;n<arguments.length;n++)e[n-1]=arguments[n];d.push(new c(t,e)),1!==d.length||v||i(u)},c.prototype.run=function(){this.fun.apply(null,this.array)},p.title=\"browser\",p.browser=!0,p.env={},p.argv=[],p.version=\"\",p.versions={},p.on=s,p.addListener=s,p.once=s,p.off=s,p.removeListener=s,p.removeAllListeners=s,p.emit=s,p.binding=function(t){throw new Error(\"process.binding is not supported\")},p.cwd=function(){return\"/\"},p.chdir=function(t){throw new Error(\"process.chdir is not supported\")},p.umask=function(){\n",
       "return 0}},function(t,e,n){\"use strict\";function r(t,e){return t+e.charAt(0).toUpperCase()+e.substring(1)}var i={animationIterationCount:!0,borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridRow:!0,gridColumn:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},o=[\"Webkit\",\"ms\",\"Moz\",\"O\"];Object.keys(i).forEach(function(t){o.forEach(function(e){i[r(e,t)]=i[t]})});var a={background:{backgroundAttachment:!0,backgroundColor:!0,backgroundImage:!0,backgroundPositionX:!0,backgroundPositionY:!0,backgroundRepeat:!0},backgroundPosition:{backgroundPositionX:!0,backgroundPositionY:!0},border:{borderWidth:!0,borderStyle:!0,borderColor:!0},borderBottom:{borderBottomWidth:!0,borderBottomStyle:!0,borderBottomColor:!0},borderLeft:{borderLeftWidth:!0,borderLeftStyle:!0,borderLeftColor:!0},borderRight:{borderRightWidth:!0,borderRightStyle:!0,borderRightColor:!0},borderTop:{borderTopWidth:!0,borderTopStyle:!0,borderTopColor:!0},font:{fontStyle:!0,fontVariant:!0,fontWeight:!0,fontSize:!0,lineHeight:!0,fontFamily:!0},outline:{outlineWidth:!0,outlineStyle:!0,outlineColor:!0}},u={isUnitlessNumber:i,shorthandPropertyExpansions:a};t.exports=u},function(t,e,n){\"use strict\";function r(t,e){if(!(t instanceof e))throw new TypeError(\"Cannot call a class as a function\")}var i=n(2),o=n(17),a=(n(0),function(){function t(e){r(this,t),this._callbacks=null,this._contexts=null,this._arg=e}return t.prototype.enqueue=function(t,e){this._callbacks=this._callbacks||[],this._callbacks.push(t),this._contexts=this._contexts||[],this._contexts.push(e)},t.prototype.notifyAll=function(){var t=this._callbacks,e=this._contexts,n=this._arg;if(t&&e){t.length!==e.length?i(\"24\"):void 0,this._callbacks=null,this._contexts=null;for(var r=0;r<t.length;r++)t[r].call(e[r],n);t.length=0,e.length=0}},t.prototype.checkpoint=function(){return this._callbacks?this._callbacks.length:0},t.prototype.rollback=function(t){this._callbacks&&this._contexts&&(this._callbacks.length=t,this._contexts.length=t)},t.prototype.reset=function(){this._callbacks=null,this._contexts=null},t.prototype.destructor=function(){this.reset()},t}());t.exports=o.addPoolingTo(a)},function(t,e,n){\"use strict\";function r(t){return!!s.hasOwnProperty(t)||!c.hasOwnProperty(t)&&(u.test(t)?(s[t]=!0,!0):(c[t]=!0,!1))}function i(t,e){return null==e||t.hasBooleanValue&&!e||t.hasNumericValue&&isNaN(e)||t.hasPositiveNumericValue&&e<1||t.hasOverloadedBooleanValue&&e===!1}var o=n(21),a=(n(4),n(9),n(394)),u=(n(1),new RegExp(\"^[\"+o.ATTRIBUTE_NAME_START_CHAR+\"][\"+o.ATTRIBUTE_NAME_CHAR+\"]*$\")),c={},s={},l={createMarkupForID:function(t){return o.ID_ATTRIBUTE_NAME+\"=\"+a(t)},setAttributeForID:function(t,e){t.setAttribute(o.ID_ATTRIBUTE_NAME,e)},createMarkupForRoot:function(){return o.ROOT_ATTRIBUTE_NAME+'=\"\"'},setAttributeForRoot:function(t){t.setAttribute(o.ROOT_ATTRIBUTE_NAME,\"\")},createMarkupForProperty:function(t,e){var n=o.properties.hasOwnProperty(t)?o.properties[t]:null;if(n){if(i(n,e))return\"\";var r=n.attributeName;return n.hasBooleanValue||n.hasOverloadedBooleanValue&&e===!0?r+'=\"\"':r+\"=\"+a(e)}return o.isCustomAttribute(t)?null==e?\"\":t+\"=\"+a(e):null},createMarkupForCustomAttribute:function(t,e){return r(t)&&null!=e?t+\"=\"+a(e):\"\"},setValueForProperty:function(t,e,n){var r=o.properties.hasOwnProperty(e)?o.properties[e]:null;if(r){var a=r.mutationMethod;if(a)a(t,n);else{if(i(r,n))return void this.deleteValueForProperty(t,e);if(r.mustUseProperty)t[r.propertyName]=n;else{var u=r.attributeName,c=r.attributeNamespace;c?t.setAttributeNS(c,u,\"\"+n):r.hasBooleanValue||r.hasOverloadedBooleanValue&&n===!0?t.setAttribute(u,\"\"):t.setAttribute(u,\"\"+n)}}}else if(o.isCustomAttribute(e))return void l.setValueForAttribute(t,e,n)},setValueForAttribute:function(t,e,n){if(r(e)){null==n?t.removeAttribute(e):t.setAttribute(e,\"\"+n)}},deleteValueForAttribute:function(t,e){t.removeAttribute(e)},deleteValueForProperty:function(t,e){var n=o.properties.hasOwnProperty(e)?o.properties[e]:null;if(n){var r=n.mutationMethod;if(r)r(t,void 0);else if(n.mustUseProperty){var i=n.propertyName;n.hasBooleanValue?t[i]=!1:t[i]=\"\"}else t.removeAttribute(n.attributeName)}else o.isCustomAttribute(e)&&t.removeAttribute(e)}};t.exports=l},function(t,e,n){\"use strict\";var r={hasCachedChildNodes:1};t.exports=r},function(t,e,n){\"use strict\";function r(){if(this._rootNodeID&&this._wrapperState.pendingUpdate){this._wrapperState.pendingUpdate=!1;var t=this._currentElement.props,e=u.getValue(t);null!=e&&i(this,Boolean(t.multiple),e)}}function i(t,e,n){var r,i,o=c.getNodeFromInstance(t).options;if(e){for(r={},i=0;i<n.length;i++)r[\"\"+n[i]]=!0;for(i=0;i<o.length;i++){var a=r.hasOwnProperty(o[i].value);o[i].selected!==a&&(o[i].selected=a)}}else{for(r=\"\"+n,i=0;i<o.length;i++)if(o[i].value===r)return void(o[i].selected=!0);o.length&&(o[0].selected=!0)}}function o(t){var e=this._currentElement.props,n=u.executeOnChange(e,t);return this._rootNodeID&&(this._wrapperState.pendingUpdate=!0),s.asap(r,this),n}var a=n(3),u=n(85),c=n(4),s=n(11),l=(n(1),!1),f={getHostProps:function(t,e){return a({},e,{onChange:t._wrapperState.onChange,value:void 0})},mountWrapper:function(t,e){var n=u.getValue(e);t._wrapperState={pendingUpdate:!1,initialValue:null!=n?n:e.defaultValue,listeners:null,onChange:o.bind(t),wasMultiple:Boolean(e.multiple)},void 0===e.value||void 0===e.defaultValue||l||(l=!0)},getSelectValueContext:function(t){return t._wrapperState.initialValue},postUpdateWrapper:function(t){var e=t._currentElement.props;t._wrapperState.initialValue=void 0;var n=t._wrapperState.wasMultiple;t._wrapperState.wasMultiple=Boolean(e.multiple);var r=u.getValue(e);null!=r?(t._wrapperState.pendingUpdate=!1,i(t,Boolean(e.multiple),r)):n!==Boolean(e.multiple)&&(null!=e.defaultValue?i(t,Boolean(e.multiple),e.defaultValue):i(t,Boolean(e.multiple),e.multiple?[]:\"\"))}};t.exports=f},function(t,e,n){\"use strict\";var r,i={injectEmptyComponentFactory:function(t){r=t}},o={create:function(t){return r(t)}};o.injection=i,t.exports=o},function(t,e,n){\"use strict\";var r={logTopLevelRenders:!1};t.exports=r},function(t,e,n){\"use strict\";function r(t){return u?void 0:a(\"111\",t.type),new u(t)}function i(t){return new c(t)}function o(t){return t instanceof c}var a=n(2),u=(n(0),null),c=null,s={injectGenericComponentClass:function(t){u=t},injectTextComponentClass:function(t){c=t}},l={createInternalComponent:r,createInstanceForText:i,isTextComponent:o,injection:s};t.exports=l},function(t,e,n){\"use strict\";function r(t){return o(document.documentElement,t)}var i=n(353),o=n(320),a=n(151),u=n(152),c={hasSelectionCapabilities:function(t){var e=t&&t.nodeName&&t.nodeName.toLowerCase();return e&&(\"input\"===e&&\"text\"===t.type||\"textarea\"===e||\"true\"===t.contentEditable)},getSelectionInformation:function(){var t=u();return{focusedElem:t,selectionRange:c.hasSelectionCapabilities(t)?c.getSelection(t):null}},restoreSelection:function(t){var e=u(),n=t.focusedElem,i=t.selectionRange;e!==n&&r(n)&&(c.hasSelectionCapabilities(n)&&c.setSelection(n,i),a(n))},getSelection:function(t){var e;if(\"selectionStart\"in t)e={start:t.selectionStart,end:t.selectionEnd};else if(document.selection&&t.nodeName&&\"input\"===t.nodeName.toLowerCase()){var n=document.selection.createRange();n.parentElement()===t&&(e={start:-n.moveStart(\"character\",-t.value.length),end:-n.moveEnd(\"character\",-t.value.length)})}else e=i.getOffsets(t);return e||{start:0,end:0}},setSelection:function(t,e){var n=e.start,r=e.end;if(void 0===r&&(r=n),\"selectionStart\"in t)t.selectionStart=n,t.selectionEnd=Math.min(r,t.value.length);else if(document.selection&&t.nodeName&&\"input\"===t.nodeName.toLowerCase()){var o=t.createTextRange();o.collapse(!0),o.moveStart(\"character\",n),o.moveEnd(\"character\",r-n),o.select()}else i.setOffsets(t,e)}};t.exports=c},function(t,e,n){\"use strict\";function r(t,e){for(var n=Math.min(t.length,e.length),r=0;r<n;r++)if(t.charAt(r)!==e.charAt(r))return r;return t.length===e.length?-1:n}function i(t){return t?t.nodeType===D?t.documentElement:t.firstChild:null}function o(t){return t.getAttribute&&t.getAttribute(A)||\"\"}function a(t,e,n,r,i){var o;if(x.logTopLevelRenders){var a=t._currentElement.props.child,u=a.type;o=\"React mount: \"+(\"string\"==typeof u?u:u.displayName||u.name),console.time(o)}var c=M.mountComponent(t,n,null,_(t,e),i,0);o&&console.timeEnd(o),t._renderedComponent._topLevelWrapper=t,j._mountImageIntoNode(c,e,t,r,n)}function u(t,e,n,r){var i=E.ReactReconcileTransaction.getPooled(!n&&b.useCreateElement);i.perform(a,null,t,e,i,n,r),E.ReactReconcileTransaction.release(i)}function c(t,e,n){for(M.unmountComponent(t,n),e.nodeType===D&&(e=e.documentElement);e.lastChild;)e.removeChild(e.lastChild)}function s(t){var e=i(t);if(e){var n=y.getInstanceFromNode(e);return!(!n||!n._hostParent)}}function l(t){return!(!t||t.nodeType!==I&&t.nodeType!==D&&t.nodeType!==R)}function f(t){var e=i(t),n=e&&y.getInstanceFromNode(e);return n&&!n._hostParent?n:null}function p(t){var e=f(t);return e?e._hostContainerInfo._topLevelWrapper:null}var h=n(2),d=n(20),v=n(21),g=n(26),m=n(51),y=(n(15),n(4)),_=n(347),b=n(349),x=n(160),w=n(40),C=(n(9),n(363)),M=n(24),k=n(88),E=n(11),T=n(38),S=n(169),P=(n(0),n(55)),N=n(95),A=(n(1),v.ID_ATTRIBUTE_NAME),O=v.ROOT_ATTRIBUTE_NAME,I=1,D=9,R=11,L={},U=1,F=function(){this.rootID=U++};F.prototype.isReactComponent={},F.prototype.render=function(){return this.props.child},F.isReactTopLevelWrapper=!0;var j={TopLevelWrapper:F,_instancesByReactRootID:L,scrollMonitor:function(t,e){e()},_updateRootComponent:function(t,e,n,r,i){return j.scrollMonitor(r,function(){k.enqueueElementInternal(t,e,n),i&&k.enqueueCallbackInternal(t,i)}),t},_renderNewRootComponent:function(t,e,n,r){l(e)?void 0:h(\"37\"),m.ensureScrollValueMonitoring();var i=S(t,!1);E.batchedUpdates(u,i,e,n,r);var o=i._instance.rootID;return L[o]=i,i},renderSubtreeIntoContainer:function(t,e,n,r){return null!=t&&w.has(t)?void 0:h(\"38\"),j._renderSubtreeIntoContainer(t,e,n,r)},_renderSubtreeIntoContainer:function(t,e,n,r){k.validateCallback(r,\"ReactDOM.render\"),g.isValidElement(e)?void 0:h(\"39\",\"string\"==typeof e?\" Instead of passing a string like 'div', pass React.createElement('div') or <div />.\":\"function\"==typeof e?\" Instead of passing a class like Foo, pass React.createElement(Foo) or <Foo />.\":null!=e&&void 0!==e.props?\" This may be caused by unintentionally loading two independent copies of React.\":\"\");var a,u=g.createElement(F,{child:e});if(t){var c=w.get(t);a=c._processChildContext(c._context)}else a=T;var l=p(n);if(l){var f=l._currentElement,d=f.props.child;if(N(d,e)){var v=l._renderedComponent.getPublicInstance(),m=r&&function(){r.call(v)};return j._updateRootComponent(l,u,a,n,m),v}j.unmountComponentAtNode(n)}var y=i(n),_=y&&!!o(y),b=s(n),x=_&&!l&&!b,C=j._renderNewRootComponent(u,n,x,a)._renderedComponent.getPublicInstance();return r&&r.call(C),C},render:function(t,e,n){return j._renderSubtreeIntoContainer(null,t,e,n)},unmountComponentAtNode:function(t){l(t)?void 0:h(\"40\");var e=p(t);if(!e){s(t),1===t.nodeType&&t.hasAttribute(O);return!1}return delete L[e._instance.rootID],E.batchedUpdates(c,e,t,!1),!0},_mountImageIntoNode:function(t,e,n,o,a){if(l(e)?void 0:h(\"41\"),o){var u=i(e);if(C.canReuseMarkup(t,u))return void y.precacheNode(n,u);var c=u.getAttribute(C.CHECKSUM_ATTR_NAME);u.removeAttribute(C.CHECKSUM_ATTR_NAME);var s=u.outerHTML;u.setAttribute(C.CHECKSUM_ATTR_NAME,c);var f=t,p=r(f,s),v=\" (client) \"+f.substring(p-20,p+20)+\"\\n (server) \"+s.substring(p-20,p+20);e.nodeType===D?h(\"42\",v):void 0}if(e.nodeType===D?h(\"43\"):void 0,a.useCreateElement){for(;e.lastChild;)e.removeChild(e.lastChild);d.insertTreeBefore(e,t,null)}else P(e,t),y.precacheNode(n,e.firstChild)}};t.exports=j},function(t,e,n){\"use strict\";var r=n(2),i=n(26),o=(n(0),{HOST:0,COMPOSITE:1,EMPTY:2,getType:function(t){return null===t||t===!1?o.EMPTY:i.isValidElement(t)?\"function\"==typeof t.type?o.COMPOSITE:o.HOST:void r(\"26\",t)}});t.exports=o},function(t,e,n){\"use strict\";function r(t,e){return null==e?i(\"30\"):void 0,null==t?e:Array.isArray(t)?Array.isArray(e)?(t.push.apply(t,e),t):(t.push(e),t):Array.isArray(e)?[t].concat(e):[t,e]}var i=n(2);n(0);t.exports=r},function(t,e,n){\"use strict\";function r(t,e,n){Array.isArray(t)?t.forEach(e,n):t&&e.call(n,t)}t.exports=r},function(t,e,n){\"use strict\";function r(t){for(var e;(e=t._renderedNodeType)===i.COMPOSITE;)t=t._renderedComponent;return e===i.HOST?t._renderedComponent:e===i.EMPTY?null:void 0}var i=n(164);t.exports=r},function(t,e,n){\"use strict\";function r(){return!o&&i.canUseDOM&&(o=\"textContent\"in document.documentElement?\"textContent\":\"innerText\"),o}var i=n(6),o=null;t.exports=r},function(t,e,n){\"use strict\";function r(t){if(t){var e=t.getName();if(e)return\" Check the render method of `\"+e+\"`.\"}return\"\"}function i(t){return\"function\"==typeof t&&\"undefined\"!=typeof t.prototype&&\"function\"==typeof t.prototype.mountComponent&&\"function\"==typeof t.prototype.receiveComponent}function o(t,e){var n;if(null===t||t===!1)n=s.create(o);else if(\"object\"==typeof t){var u=t,c=u.type;if(\"function\"!=typeof c&&\"string\"!=typeof c){var p=\"\";p+=r(u._owner),a(\"130\",null==c?c:typeof c,p)}\"string\"==typeof u.type?n=l.createInternalComponent(u):i(u.type)?(n=new u.type(u),n.getHostNode||(n.getHostNode=n.getNativeNode)):n=new f(u)}else\"string\"==typeof t||\"number\"==typeof t?n=l.createInstanceForText(t):a(\"131\",typeof t);return n._mountIndex=0,n._mountImage=null,n}var a=n(2),u=n(3),c=n(344),s=n(159),l=n(161),f=(n(391),n(0),n(1),function(t){this.construct(t)});u(f.prototype,c,{_instantiateReactComponent:o}),t.exports=o},function(t,e,n){\"use strict\";function r(t){var e=t&&t.nodeName&&t.nodeName.toLowerCase();return\"input\"===e?!!i[t.type]:\"textarea\"===e}var i={color:!0,date:!0,datetime:!0,\"datetime-local\":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};t.exports=r},function(t,e,n){\"use strict\";var r=n(6),i=n(54),o=n(55),a=function(t,e){if(e){var n=t.firstChild;if(n&&n===t.lastChild&&3===n.nodeType)return void(n.nodeValue=e)}t.textContent=e};r.canUseDOM&&(\"textContent\"in document.documentElement||(a=function(t,e){return 3===t.nodeType?void(t.nodeValue=e):void o(t,i(e))})),t.exports=a},function(t,e,n){\"use strict\";function r(t,e){return t&&\"object\"==typeof t&&null!=t.key?s.escape(t.key):e.toString(36)}function i(t,e,n,o){var p=typeof t;if(\"undefined\"!==p&&\"boolean\"!==p||(t=null),null===t||\"string\"===p||\"number\"===p||\"object\"===p&&t.$$typeof===u)return n(o,t,\"\"===e?l+r(t,0):e),1;var h,d,v=0,g=\"\"===e?l:e+f;if(Array.isArray(t))for(var m=0;m<t.length;m++)h=t[m],d=g+r(h,m),v+=i(h,d,n,o);else{var y=c(t);if(y){var _,b=y.call(t);if(y!==t.entries)for(var x=0;!(_=b.next()).done;)h=_.value,d=g+r(h,x++),v+=i(h,d,n,o);else for(;!(_=b.next()).done;){var w=_.value;w&&(h=w[1],d=g+s.escape(w[0])+f+r(h,0),v+=i(h,d,n,o))}}else if(\"object\"===p){var C=\"\",M=String(t);a(\"31\",\"[object Object]\"===M?\"object with keys {\"+Object.keys(t).join(\", \")+\"}\":M,C)}}return v}function o(t,e,n){return null==t?0:i(t,\"\",e,n)}var a=n(2),u=(n(15),n(359)),c=n(390),s=(n(0),n(84)),l=(n(1),\".\"),f=\":\";t.exports=o},function(t,e,n){\"use strict\";function r(t){var e=Function.prototype.toString,n=Object.prototype.hasOwnProperty,r=RegExp(\"^\"+e.call(n).replace(/[\\\\^$.*+?()[\\]{}|]/g,\"\\\\$&\").replace(/hasOwnProperty|(function).*?(?=\\\\\\()| for .+?(?=\\\\\\])/g,\"$1.*?\")+\"$\");try{var i=e.call(t);return r.test(i)}catch(t){return!1}}function i(t){var e=s(t);if(e){var n=e.childIDs;l(t),n.forEach(i)}}function o(t,e,n){return\"\\n    in \"+(t||\"Unknown\")+(e?\" (at \"+e.fileName.replace(/^.*[\\\\\\/]/,\"\")+\":\"+e.lineNumber+\")\":n?\" (created by \"+n+\")\":\"\")}function a(t){return null==t?\"#empty\":\"string\"==typeof t||\"number\"==typeof t?\"#text\":\"string\"==typeof t.type?t.type:t.type.displayName||t.type.name||\"Unknown\"}function u(t){var e,n=k.getDisplayName(t),r=k.getElement(t),i=k.getOwnerID(t);return i&&(e=k.getDisplayName(i)),o(n,r&&r._source,e)}var c,s,l,f,p,h,d,v=n(28),g=n(15),m=(n(0),n(1),\"function\"==typeof Array.from&&\"function\"==typeof Map&&r(Map)&&null!=Map.prototype&&\"function\"==typeof Map.prototype.keys&&r(Map.prototype.keys)&&\"function\"==typeof Set&&r(Set)&&null!=Set.prototype&&\"function\"==typeof Set.prototype.keys&&r(Set.prototype.keys));if(m){var y=new Map,_=new Set;c=function(t,e){y.set(t,e)},s=function(t){return y.get(t)},l=function(t){y.delete(t)},f=function(){return Array.from(y.keys())},p=function(t){_.add(t)},h=function(t){_.delete(t)},d=function(){return Array.from(_.keys())}}else{var b={},x={},w=function(t){return\".\"+t},C=function(t){return parseInt(t.substr(1),10)};c=function(t,e){var n=w(t);b[n]=e},s=function(t){var e=w(t);return b[e]},l=function(t){var e=w(t);delete b[e]},f=function(){return Object.keys(b).map(C)},p=function(t){var e=w(t);x[e]=!0},h=function(t){var e=w(t);delete x[e]},d=function(){return Object.keys(x).map(C)}}var M=[],k={onSetChildren:function(t,e){var n=s(t);n?void 0:v(\"144\"),n.childIDs=e;for(var r=0;r<e.length;r++){var i=e[r],o=s(i);o?void 0:v(\"140\"),null==o.childIDs&&\"object\"==typeof o.element&&null!=o.element?v(\"141\"):void 0,o.isMounted?void 0:v(\"71\"),null==o.parentID&&(o.parentID=t),o.parentID!==t?v(\"142\",i,o.parentID,t):void 0}},onBeforeMountComponent:function(t,e,n){var r={element:e,parentID:n,text:null,childIDs:[],isMounted:!1,updateCount:0};c(t,r)},onBeforeUpdateComponent:function(t,e){var n=s(t);n&&n.isMounted&&(n.element=e)},onMountComponent:function(t){var e=s(t);e?void 0:v(\"144\"),e.isMounted=!0;var n=0===e.parentID;n&&p(t)},onUpdateComponent:function(t){var e=s(t);e&&e.isMounted&&e.updateCount++},onUnmountComponent:function(t){var e=s(t);if(e){e.isMounted=!1;var n=0===e.parentID;n&&h(t)}M.push(t)},purgeUnmountedComponents:function(){if(!k._preventPurging){for(var t=0;t<M.length;t++){var e=M[t];i(e)}M.length=0}},isMounted:function(t){var e=s(t);return!!e&&e.isMounted},getCurrentStackAddendum:function(t){var e=\"\";if(t){var n=a(t),r=t._owner;e+=o(n,t._source,r&&r.getName())}var i=g.current,u=i&&i._debugID;return e+=k.getStackAddendumByID(u)},getStackAddendumByID:function(t){for(var e=\"\";t;)e+=u(t),t=k.getParentID(t);return e},getChildIDs:function(t){var e=s(t);return e?e.childIDs:[]},getDisplayName:function(t){var e=k.getElement(t);return e?a(e):null},getElement:function(t){var e=s(t);return e?e.element:null},getOwnerID:function(t){var e=k.getElement(t);return e&&e._owner?e._owner._debugID:null},getParentID:function(t){var e=s(t);return e?e.parentID:null},getSource:function(t){var e=s(t),n=e?e.element:null,r=null!=n?n._source:null;return r},getText:function(t){var e=k.getElement(t);return\"string\"==typeof e?e:\"number\"==typeof e?\"\"+e:null},getUpdateCount:function(t){var e=s(t);return e?e.updateCount:0},getRootIDs:d,getRegisteredIDs:f};t.exports=k},function(t,e,n){\"use strict\";var r=\"function\"==typeof Symbol&&Symbol.for&&Symbol.for(\"react.element\")||60103;t.exports=r},function(t,e,n){\"use strict\";var r={};t.exports=r},function(t,e,n){\"use strict\";var r=!1;t.exports=r},function(t,e,n){\"use strict\";function r(t){var e=t&&(i&&t[i]||t[o]);if(\"function\"==typeof e)return e}var i=\"function\"==typeof Symbol&&Symbol.iterator,o=\"@@iterator\";t.exports=r},,function(t,e,n){\"use strict\";function r(t){return t&&t.__esModule?t:{default:t}}function i(t,e){if(!(t instanceof e))throw new TypeError(\"Cannot call a class as a function\")}function o(t,e){if(!t)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return!e||\"object\"!=typeof e&&\"function\"!=typeof e?t:e}function a(t,e){if(\"function\"!=typeof e&&null!==e)throw new TypeError(\"Super expression must either be null or a function, not \"+typeof e);t.prototype=Object.create(e&&e.prototype,{constructor:{value:t,enumerable:!1,writable:!0,configurable:!0}}),e&&(Object.setPrototypeOf?Object.setPrototypeOf(t,e):t.__proto__=e)}Object.defineProperty(e,\"__esModule\",{value:!0});var u=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&\"function\"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?\"symbol\":typeof t},c=function(){function t(t,e){for(var n=0;n<e.length;n++){var r=e[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(t,r.key,r)}}return function(e,n,r){return n&&t(e.prototype,n),r&&t(e,r),e}}(),s=n(41),l=r(s),f=n(129),p=n(64),h=n(30),d=n(77),v=n(112),g=n(134),m=n(10),y=n(39),_=n(56),b=r(_),x=function(t){function e(){i(this,e);var t=o(this,(e.__proto__||Object.getPrototypeOf(e)).call(this));return window.lastAdditiveForceArrayVisualizer=t,t.topOffset=28,t.leftOffset=80,t.height=350,t.effectFormat=(0,h.format)(\".2\"),t.redraw=(0,y.debounce)(function(){return t.draw()},200),t}return a(e,t),c(e,[{key:\"componentDidMount\",value:function(){var t=this;this.mainGroup=this.svg.append(\"g\"),this.onTopGroup=this.svg.append(\"g\"),this.xaxisElement=this.onTopGroup.append(\"g\").attr(\"transform\",\"translate(0,35)\").attr(\"class\",\"force-bar-array-xaxis\"),this.yaxisElement=this.onTopGroup.append(\"g\").attr(\"transform\",\"translate(0,35)\").attr(\"class\",\"force-bar-array-yaxis\"),this.hoverGroup1=this.svg.append(\"g\"),this.hoverGroup2=this.svg.append(\"g\"),this.baseValueTitle=this.svg.append(\"text\"),this.hoverLine=this.svg.append(\"line\"),this.hoverxOutline=this.svg.append(\"text\").attr(\"text-anchor\",\"middle\").attr(\"font-weight\",\"bold\").attr(\"fill\",\"#fff\").attr(\"stroke\",\"#fff\").attr(\"stroke-width\",\"6\").attr(\"font-size\",\"12px\"),this.hoverx=this.svg.append(\"text\").attr(\"text-anchor\",\"middle\").attr(\"font-weight\",\"bold\").attr(\"fill\",\"#000\").attr(\"font-size\",\"12px\"),this.hoverxTitle=this.svg.append(\"text\").attr(\"text-anchor\",\"middle\").attr(\"opacity\",.6).attr(\"font-size\",\"12px\"),this.hoveryOutline=this.svg.append(\"text\").attr(\"text-anchor\",\"end\").attr(\"font-weight\",\"bold\").attr(\"fill\",\"#fff\").attr(\"stroke\",\"#fff\").attr(\"stroke-width\",\"6\").attr(\"font-size\",\"12px\"),this.hovery=this.svg.append(\"text\").attr(\"text-anchor\",\"end\").attr(\"font-weight\",\"bold\").attr(\"fill\",\"#000\").attr(\"font-size\",\"12px\"),this.xlabel=this.wrapper.select(\".additive-force-array-xlabel\"),this.ylabel=this.wrapper.select(\".additive-force-array-ylabel\");var e=void 0;\"string\"==typeof this.props.plot_cmap?this.props.plot_cmap in b.default.colors?e=b.default.colors[this.props.plot_cmap]:(console.log(\"Invalid color map name, reverting to default.\"),e=b.default.colors.RdBu):Array.isArray(this.props.plot_cmap)&&(e=this.props.plot_cmap),this.colors=e.map(function(t){return(0,m.hsl)(t)}),this.brighterColors=[1.45,1.6].map(function(e,n){return t.colors[n].brighter(e)});var n=(0,h.format)(\",.4\");if(null!=this.props.ordering_keys&&null!=this.props.ordering_keys_time_format){var r=function(t){return\"object\"==(\"undefined\"==typeof t?\"undefined\":u(t))?this.formatTime(t):n(t)};this.parseTime=(0,d.timeParse)(this.props.ordering_keys_time_format),this.formatTime=(0,d.timeFormat)(this.props.ordering_keys_time_format),this.xtickFormat=r}else this.parseTime=null,this.formatTime=null,this.xtickFormat=n;this.xscale=(0,p.scaleLinear)(),this.xaxis=(0,v.axisBottom)().scale(this.xscale).tickSizeInner(4).tickSizeOuter(0).tickFormat(function(e){return t.xtickFormat(e)}).tickPadding(-18),this.ytickFormat=n,this.yscale=(0,p.scaleLinear)(),this.yaxis=(0,v.axisLeft)().scale(this.yscale).tickSizeInner(4).tickSizeOuter(0).tickFormat(function(e){return t.ytickFormat(t.invLinkFunction(e))}).tickPadding(2),this.xlabel.node().onchange=function(){return t.internalDraw()},this.ylabel.node().onchange=function(){return t.internalDraw()},this.svg.on(\"mousemove\",function(e){return t.mouseMoved(e)}),this.svg.on(\"click\",function(){return alert(\"This original index of the sample you clicked is \"+t.nearestExpIndex)}),this.svg.on(\"mouseout\",function(e){return t.mouseOut(e)}),window.addEventListener(\"resize\",this.redraw),window.setTimeout(this.redraw,50)}},{key:\"componentDidUpdate\",value:function(){this.draw()}},{key:\"mouseOut\",value:function(){this.hoverLine.attr(\"display\",\"none\"),this.hoverx.attr(\"display\",\"none\"),this.hoverxOutline.attr(\"display\",\"none\"),this.hoverxTitle.attr(\"display\",\"none\"),this.hovery.attr(\"display\",\"none\"),this.hoveryOutline.attr(\"display\",\"none\"),this.hoverGroup1.attr(\"display\",\"none\"),this.hoverGroup2.attr(\"display\",\"none\")}},{key:\"mouseMoved\",value:function(){var t=this,e=void 0,n=void 0;this.hoverLine.attr(\"display\",\"\"),this.hoverx.attr(\"display\",\"\"),this.hoverxOutline.attr(\"display\",\"\"),this.hoverxTitle.attr(\"display\",\"\"),this.hovery.attr(\"display\",\"\"),this.hoveryOutline.attr(\"display\",\"\"),this.hoverGroup1.attr(\"display\",\"\"),this.hoverGroup2.attr(\"display\",\"\");var r=(0,f.mouse)(this.svg.node())[0];if(this.props.explanations){for(e=0;e<this.currExplanations.length;++e)(!n||Math.abs(n.xmapScaled-r)>Math.abs(this.currExplanations[e].xmapScaled-r))&&(n=this.currExplanations[e]);this.nearestExpIndex=n.origInd,this.hoverLine.attr(\"x1\",n.xmapScaled).attr(\"x2\",n.xmapScaled).attr(\"y1\",0+this.topOffset).attr(\"y2\",this.height),this.hoverx.attr(\"x\",n.xmapScaled).attr(\"y\",this.topOffset-5).text(this.xtickFormat(n.xmap)),this.hoverxOutline.attr(\"x\",n.xmapScaled).attr(\"y\",this.topOffset-5).text(this.xtickFormat(n.xmap)),this.hoverxTitle.attr(\"x\",n.xmapScaled).attr(\"y\",this.topOffset-18).text(n.count>1?n.count+\" averaged samples\":\"\"),this.hovery.attr(\"x\",this.leftOffset-6).attr(\"y\",n.joinPointy).text(this.ytickFormat(this.invLinkFunction(n.joinPoint))),this.hoveryOutline.attr(\"x\",this.leftOffset-6).attr(\"y\",n.joinPointy).text(this.ytickFormat(this.invLinkFunction(n.joinPoint)));for(var i=[],o=void 0,a=void 0,u=this.currPosOrderedFeatures.length-1;u>=0;--u){var c=this.currPosOrderedFeatures[u],s=n.features[c];a=5+(s.posyTop+s.posyBottom)/2,(!o||a-o>=15)&&s.posyTop-s.posyBottom>=6&&(i.push(s),o=a)}var l=[];o=void 0;var p=!0,h=!1,d=void 0;try{for(var v,g=this.currNegOrderedFeatures[Symbol.iterator]();!(p=(v=g.next()).done);p=!0){var m=v.value,y=n.features[m];a=5+(y.negyTop+y.negyBottom)/2,(!o||o-a>=15)&&y.negyTop-y.negyBottom>=6&&(l.push(y),o=a)}}catch(t){h=!0,d=t}finally{try{!p&&g.return&&g.return()}finally{if(h)throw d}}var _=function(e){var r=\"\";return null!==e.value&&void 0!==e.value&&(r=\" = \"+(isNaN(e.value)?e.value:t.ytickFormat(e.value))),n.count>1?\"mean(\"+t.props.featureNames[e.ind]+\")\"+r:t.props.featureNames[e.ind]+r},b=this.hoverGroup1.selectAll(\".pos-values\").data(i);b.enter().append(\"text\").attr(\"class\",\"pos-values\").merge(b).attr(\"x\",n.xmapScaled+5).attr(\"y\",function(t){return 4+(t.posyTop+t.posyBottom)/2}).attr(\"text-anchor\",\"start\").attr(\"font-size\",12).attr(\"stroke\",\"#fff\").attr(\"fill\",\"#fff\").attr(\"stroke-width\",\"4\").attr(\"stroke-linejoin\",\"round\").attr(\"opacity\",1).text(_),b.exit().remove();var x=this.hoverGroup2.selectAll(\".pos-values\").data(i);x.enter().append(\"text\").attr(\"class\",\"pos-values\").merge(x).attr(\"x\",n.xmapScaled+5).attr(\"y\",function(t){return 4+(t.posyTop+t.posyBottom)/2}).attr(\"text-anchor\",\"start\").attr(\"font-size\",12).attr(\"fill\",this.colors[0]).text(_),x.exit().remove();var w=this.hoverGroup1.selectAll(\".neg-values\").data(l);w.enter().append(\"text\").attr(\"class\",\"neg-values\").merge(w).attr(\"x\",n.xmapScaled+5).attr(\"y\",function(t){return 4+(t.negyTop+t.negyBottom)/2}).attr(\"text-anchor\",\"start\").attr(\"font-size\",12).attr(\"stroke\",\"#fff\").attr(\"fill\",\"#fff\").attr(\"stroke-width\",\"4\").attr(\"stroke-linejoin\",\"round\").attr(\"opacity\",1).text(_),w.exit().remove();var C=this.hoverGroup2.selectAll(\".neg-values\").data(l);C.enter().append(\"text\").attr(\"class\",\"neg-values\").merge(C).attr(\"x\",n.xmapScaled+5).attr(\"y\",function(t){return 4+(t.negyTop+t.negyBottom)/2}).attr(\"text-anchor\",\"start\").attr(\"font-size\",12).attr(\"fill\",this.colors[1]).text(_),C.exit().remove()}}},{key:\"draw\",value:function(){var t=this;if(this.props.explanations&&0!==this.props.explanations.length){(0,y.each)(this.props.explanations,function(t,e){return t.origInd=e});var e={},n={},r={},i=!0,o=!1,a=void 0;try{for(var u,c=this.props.explanations[Symbol.iterator]();!(i=(u=c.next()).done);i=!0){var s=u.value;for(var l in s.features)void 0===e[l]&&(e[l]=0,n[l]=0,r[l]=0),s.features[l].effect>0?e[l]+=s.features[l].effect:n[l]-=s.features[l].effect,null!==s.features[l].value&&void 0!==s.features[l].value&&(r[l]+=1)}}catch(t){o=!0,a=t}finally{try{!i&&c.return&&c.return()}finally{if(o)throw a}}this.usedFeatures=(0,y.sortBy)((0,y.keys)(e),function(t){return-(e[t]+n[t])}),console.log(\"found \",this.usedFeatures.length,\" used features\"),this.posOrderedFeatures=(0,y.sortBy)(this.usedFeatures,function(t){return e[t]}),this.negOrderedFeatures=(0,y.sortBy)(this.usedFeatures,function(t){return-n[t]}),this.singleValueFeatures=(0,y.filter)(this.usedFeatures,function(t){return r[t]>0});var f=[\"sample order by similarity\",\"sample order by output value\",\"original sample ordering\"].concat(this.singleValueFeatures.map(function(e){return t.props.featureNames[e]}));null!=this.props.ordering_keys&&f.unshift(\"sample order by key\");var p=this.xlabel.selectAll(\"option\").data(f);p.enter().append(\"option\").merge(p).attr(\"value\",function(t){return t}).text(function(t){return t}),p.exit().remove();var h=this.props.outNames[0]?this.props.outNames[0]:\"model output value\";f=(0,y.map)(this.usedFeatures,function(e){return[t.props.featureNames[e],t.props.featureNames[e]+\" effects\"]}),f.unshift([\"model output value\",h]);var d=this.ylabel.selectAll(\"option\").data(f);d.enter().append(\"option\").merge(d).attr(\"value\",function(t){return t[0]}).text(function(t){return t[1]}),d.exit().remove(),this.ylabel.style(\"top\",(this.height-10-this.topOffset)/2+this.topOffset+\"px\").style(\"left\",10-this.ylabel.node().offsetWidth/2+\"px\"),this.internalDraw()}}},{key:\"internalDraw\",value:function(){var t=this,e=!0,n=!1,r=void 0;try{for(var i,o=this.props.explanations[Symbol.iterator]();!(e=(i=o.next()).done);e=!0){var a=i.value,c=!0,s=!1,l=void 0;try{for(var f,h=this.usedFeatures[Symbol.iterator]();!(c=(f=h.next()).done);c=!0){var d=f.value;a.features.hasOwnProperty(d)||(a.features[d]={effect:0,value:0}),a.features[d].ind=d}}catch(t){s=!0,l=t}finally{try{!c&&h.return&&h.return()}finally{if(s)throw l}}}}catch(t){n=!0,r=t}finally{try{!e&&o.return&&o.return()}finally{if(n)throw r}}var v=void 0,m=this.xlabel.node().value,_=\"sample order by key\"===m&&null!=this.props.ordering_keys_time_format;if(_?this.xscale=(0,p.scaleTime)():this.xscale=(0,p.scaleLinear)(),this.xaxis.scale(this.xscale),\"sample order by similarity\"===m)v=(0,y.sortBy)(this.props.explanations,function(t){return t.simIndex}),(0,y.each)(v,function(t,e){return t.xmap=e});else if(\"sample order by output value\"===m)v=(0,y.sortBy)(this.props.explanations,function(t){return-t.outValue}),(0,y.each)(v,function(t,e){return t.xmap=e});else if(\"original sample ordering\"===m)v=(0,y.sortBy)(this.props.explanations,function(t){return t.origInd}),(0,y.each)(v,function(t,e){return t.xmap=e});else if(\"sample order by key\"===m)v=this.props.explanations,_?(0,y.each)(v,function(e,n){return e.xmap=t.parseTime(t.props.ordering_keys[n])}):(0,y.each)(v,function(e,n){return e.xmap=t.props.ordering_keys[n]}),v=(0,y.sortBy)(v,function(t){return t.xmap});else{var b=function(){var e=(0,y.findKey)(t.props.featureNames,function(t){return t===m});(0,y.each)(t.props.explanations,function(t,n){return t.xmap=t.features[e].value});var n=(0,y.sortBy)(t.props.explanations,function(t){return t.xmap}),r=(0,y.map)(n,function(t){return t.xmap});if(\"string\"==typeof r[0])return alert(\"Ordering by category names is not yet supported.\"),{v:void 0};var i=(0,y.min)(r),o=(0,y.max)(r),a=(o-i)/100;v=[];for(var u=void 0,c=void 0,s=0;s<n.length;++s){var l=n[s];if(u&&!c&&l.xmap-u.xmap<=a||c&&l.xmap-c.xmap<=a){c||(c=(0,y.cloneDeep)(u),c.count=1);var f=!0,p=!1,h=void 0;try{for(var d,g=t.usedFeatures[Symbol.iterator]();!(f=(d=g.next()).done);f=!0){var _=d.value;c.features[_].effect+=l.features[_].effect,c.features[_].value+=l.features[_].value;\n",
       "}}catch(t){p=!0,h=t}finally{try{!f&&g.return&&g.return()}finally{if(p)throw h}}c.count+=1}else if(u)if(c){var b=!0,x=!1,w=void 0;try{for(var C,M=t.usedFeatures[Symbol.iterator]();!(b=(C=M.next()).done);b=!0){var k=C.value;c.features[k].effect/=c.count,c.features[k].value/=c.count}}catch(t){x=!0,w=t}finally{try{!b&&M.return&&M.return()}finally{if(x)throw w}}v.push(c),c=void 0}else v.push(u);u=l}u.xmap-v[v.length-1].xmap>a&&v.push(u)}();if(\"object\"===(\"undefined\"==typeof b?\"undefined\":u(b)))return b.v}this.currUsedFeatures=this.usedFeatures,this.currPosOrderedFeatures=this.posOrderedFeatures,this.currNegOrderedFeatures=this.negOrderedFeatures;var x=this.ylabel.node().value;if(\"model output value\"!==x){var w=v;v=(0,y.cloneDeep)(v);for(var C=(0,y.findKey)(this.props.featureNames,function(t){return t===x}),M=0;M<v.length;++M){var k=v[M].features[C];v[M].features={},v[M].features[C]=k,w[M].remapped_version=v[M]}this.currUsedFeatures=[C],this.currPosOrderedFeatures=[C],this.currNegOrderedFeatures=[C]}this.currExplanations=v,\"identity\"===this.props.link?this.invLinkFunction=function(e){return t.props.baseValue+e}:\"logit\"===this.props.link?this.invLinkFunction=function(e){return 1/(1+Math.exp(-(t.props.baseValue+e)))}:console.log(\"ERROR: Unrecognized link function: \",this.props.link),this.predValues=(0,y.map)(v,function(t){return(0,y.sum)((0,y.map)(t.features,function(t){return t.effect}))});var E=this.wrapper.node().offsetWidth;if(0==E)return setTimeout(function(){return t.draw(v)},500);this.svg.style(\"height\",this.height+\"px\"),this.svg.style(\"width\",E+\"px\");var T=(0,y.map)(v,function(t){return t.xmap});this.xscale.domain([(0,y.min)(T),(0,y.max)(T)]).range([this.leftOffset,E]).clamp(!0),this.xaxisElement.attr(\"transform\",\"translate(0,\"+this.topOffset+\")\").call(this.xaxis);for(var S=0;S<this.currExplanations.length;++S)this.currExplanations[S].xmapScaled=this.xscale(this.currExplanations[S].xmap);for(var P=v.length,N=0,A=0;A<P;++A){var O=v[A].features,I=(0,y.sum)((0,y.map)((0,y.filter)(O,function(t){return t.effect>0}),function(t){return t.effect}))||0,D=(0,y.sum)((0,y.map)((0,y.filter)(O,function(t){return t.effect<0}),function(t){return-t.effect}))||0;N=Math.max(N,2.2*Math.max(I,D))}this.yscale.domain([-N/2,N/2]).range([this.height-10,this.topOffset]),this.yaxisElement.attr(\"transform\",\"translate(\"+this.leftOffset+\",0)\").call(this.yaxis);for(var R=0;R<P;++R){var L=v[R].features,U=(0,y.sum)((0,y.map)((0,y.filter)(L,function(t){return t.effect<0}),function(t){return-t.effect}))||0,F=-U,j=void 0,B=!0,W=!1,V=void 0;try{for(var z,H=this.currPosOrderedFeatures[Symbol.iterator]();!(B=(z=H.next()).done);B=!0)j=z.value,L[j].posyTop=this.yscale(F),L[j].effect>0&&(F+=L[j].effect),L[j].posyBottom=this.yscale(F),L[j].ind=j}catch(t){W=!0,V=t}finally{try{!B&&H.return&&H.return()}finally{if(W)throw V}}var q=F,Y=!0,K=!1,G=void 0;try{for(var $,X=this.currNegOrderedFeatures[Symbol.iterator]();!(Y=($=X.next()).done);Y=!0)j=$.value,L[j].negyTop=this.yscale(F),L[j].effect<0&&(F-=L[j].effect),L[j].negyBottom=this.yscale(F)}catch(t){K=!0,G=t}finally{try{!Y&&X.return&&X.return()}finally{if(K)throw G}}v[R].joinPoint=q,v[R].joinPointy=this.yscale(q)}var Z=(0,g.line)().x(function(t){return t[0]}).y(function(t){return t[1]}),Q=this.mainGroup.selectAll(\".force-bar-array-area-pos\").data(this.currUsedFeatures);Q.enter().append(\"path\").attr(\"class\",\"force-bar-array-area-pos\").merge(Q).attr(\"d\",function(t){var e=(0,y.map)((0,y.range)(P),function(e){return[v[e].xmapScaled,v[e].features[t].posyTop]}),n=(0,y.map)((0,y.rangeRight)(P),function(e){return[v[e].xmapScaled,v[e].features[t].posyBottom]});return Z(e.concat(n))}).attr(\"fill\",this.colors[0]),Q.exit().remove();var J=this.mainGroup.selectAll(\".force-bar-array-area-neg\").data(this.currUsedFeatures);J.enter().append(\"path\").attr(\"class\",\"force-bar-array-area-neg\").merge(J).attr(\"d\",function(t){var e=(0,y.map)((0,y.range)(P),function(e){return[v[e].xmapScaled,v[e].features[t].negyTop]}),n=(0,y.map)((0,y.rangeRight)(P),function(e){return[v[e].xmapScaled,v[e].features[t].negyBottom]});return Z(e.concat(n))}).attr(\"fill\",this.colors[1]),J.exit().remove();var tt=this.mainGroup.selectAll(\".force-bar-array-divider-pos\").data(this.currUsedFeatures);tt.enter().append(\"path\").attr(\"class\",\"force-bar-array-divider-pos\").merge(tt).attr(\"d\",function(t){var e=(0,y.map)((0,y.range)(P),function(e){return[v[e].xmapScaled,v[e].features[t].posyBottom]});return Z(e)}).attr(\"fill\",\"none\").attr(\"stroke-width\",1).attr(\"stroke\",function(){return t.colors[0].brighter(1.2)}),tt.exit().remove();var et=this.mainGroup.selectAll(\".force-bar-array-divider-neg\").data(this.currUsedFeatures);et.enter().append(\"path\").attr(\"class\",\"force-bar-array-divider-neg\").merge(et).attr(\"d\",function(t){var e=(0,y.map)((0,y.range)(P),function(e){return[v[e].xmapScaled,v[e].features[t].negyTop]});return Z(e)}).attr(\"fill\",\"none\").attr(\"stroke-width\",1).attr(\"stroke\",function(){return t.colors[1].brighter(1.5)}),et.exit().remove();for(var nt=function(t,e,n,r,i){var o=void 0,a=void 0;\"pos\"===i?(o=t[n].features[e].posyBottom,a=t[n].features[e].posyTop):(o=t[n].features[e].negyBottom,a=t[n].features[e].negyTop);for(var u=void 0,c=void 0,s=n+1;s<=r;++s)\"pos\"===i?(u=t[s].features[e].posyBottom,c=t[s].features[e].posyTop):(u=t[s].features[e].negyBottom,c=t[s].features[e].negyTop),u>o&&(o=u),c<a&&(a=c);return{top:o,bottom:a}},rt=100,it=20,ot=100,at=[],ut=[\"pos\",\"neg\"],ct=0;ct<ut.length;ct++){var st=ut[ct],lt=!0,ft=!1,pt=void 0;try{for(var ht,dt=this.currUsedFeatures[Symbol.iterator]();!(lt=(ht=dt.next()).done);lt=!0)for(var vt=ht.value,gt=0,mt=0,yt=0,_t={top:0,bottom:0},bt=void 0;mt<P-1;){for(;yt<rt&&mt<P-1;)++mt,yt=v[mt].xmapScaled-v[gt].xmapScaled;for(_t=nt(v,vt,gt,mt,st);_t.bottom-_t.top<it&&gt<mt;)++gt,_t=nt(v,vt,gt,mt,st);if(yt=v[mt].xmapScaled-v[gt].xmapScaled,_t.bottom-_t.top>=it&&yt>=rt){for(;mt<P-1;){if(++mt,bt=nt(v,vt,gt,mt,st),!(bt.bottom-bt.top>it)){--mt;break}_t=bt}yt=v[mt].xmapScaled-v[gt].xmapScaled,at.push([(v[mt].xmapScaled+v[gt].xmapScaled)/2,(_t.top+_t.bottom)/2,this.props.featureNames[vt]]);var xt=v[mt].xmapScaled;for(gt=mt;xt+ot>v[gt].xmapScaled&&gt<P-1;)++gt;mt=gt}}}catch(t){ft=!0,pt=t}finally{try{!lt&&dt.return&&dt.return()}finally{if(ft)throw pt}}}var wt=this.onTopGroup.selectAll(\".force-bar-array-flabels\").data(at);wt.enter().append(\"text\").attr(\"class\",\"force-bar-array-flabels\").merge(wt).attr(\"x\",function(t){return t[0]}).attr(\"y\",function(t){return t[1]+4}).text(function(t){return t[2]}),wt.exit().remove()}},{key:\"componentWillUnmount\",value:function(){window.removeEventListener(\"resize\",this.redraw)}},{key:\"render\",value:function(){var t=this;return l.default.createElement(\"div\",{ref:function(e){return t.wrapper=(0,f.select)(e)},style:{textAlign:\"center\"}},l.default.createElement(\"style\",{dangerouslySetInnerHTML:{__html:\"\\n          .force-bar-array-wrapper {\\n            text-align: center;\\n          }\\n          .force-bar-array-xaxis path {\\n            fill: none;\\n            opacity: 0.4;\\n          }\\n          .force-bar-array-xaxis .domain {\\n            opacity: 0;\\n          }\\n          .force-bar-array-xaxis paths {\\n            display: none;\\n          }\\n          .force-bar-array-yaxis path {\\n            fill: none;\\n            opacity: 0.4;\\n          }\\n          .force-bar-array-yaxis paths {\\n            display: none;\\n          }\\n          .tick line {\\n            stroke: #000;\\n            stroke-width: 1px;\\n            opacity: 0.4;\\n          }\\n          .tick text {\\n            fill: #000;\\n            opacity: 0.5;\\n            font-size: 12px;\\n            padding: 0px;\\n          }\\n          .force-bar-array-flabels {\\n            font-size: 12px;\\n            fill: #fff;\\n            text-anchor: middle;\\n          }\\n          .additive-force-array-xlabel {\\n            background: none;\\n            border: 1px solid #ccc;\\n            opacity: 0.5;\\n            margin-bottom: 0px;\\n            font-size: 12px;\\n            font-family: arial;\\n            margin-left: 80px;\\n            max-width: 300px;\\n          }\\n          .additive-force-array-xlabel:focus {\\n            outline: none;\\n          }\\n          .additive-force-array-ylabel {\\n            position: relative;\\n            top: 0px;\\n            left: 0px;\\n            transform: rotate(-90deg);\\n            background: none;\\n            border: 1px solid #ccc;\\n            opacity: 0.5;\\n            margin-bottom: 0px;\\n            font-size: 12px;\\n            font-family: arial;\\n            max-width: 150px;\\n          }\\n          .additive-force-array-ylabel:focus {\\n            outline: none;\\n          }\\n          .additive-force-array-hoverLine {\\n            stroke-width: 1px;\\n            stroke: #fff;\\n            opacity: 1;\\n          }\"}}),l.default.createElement(\"select\",{className:\"additive-force-array-xlabel\"}),l.default.createElement(\"div\",{style:{height:\"0px\",textAlign:\"left\"}},l.default.createElement(\"select\",{className:\"additive-force-array-ylabel\"})),l.default.createElement(\"svg\",{ref:function(e){return t.svg=(0,f.select)(e)},style:{userSelect:\"none\",display:\"block\",fontFamily:\"arial\",sansSerif:!0}}))}}]),e}(l.default.Component);x.defaultProps={plot_cmap:\"RdBu\",ordering_keys:null,ordering_keys_time_format:null},e.default=x},function(t,e,n){\"use strict\";function r(t){return t&&t.__esModule?t:{default:t}}function i(t,e){if(!(t instanceof e))throw new TypeError(\"Cannot call a class as a function\")}function o(t,e){if(!t)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return!e||\"object\"!=typeof e&&\"function\"!=typeof e?t:e}function a(t,e){if(\"function\"!=typeof e&&null!==e)throw new TypeError(\"Super expression must either be null or a function, not \"+typeof e);t.prototype=Object.create(e&&e.prototype,{constructor:{value:t,enumerable:!1,writable:!0,configurable:!0}}),e&&(Object.setPrototypeOf?Object.setPrototypeOf(t,e):t.__proto__=e)}Object.defineProperty(e,\"__esModule\",{value:!0});var u=function(){function t(t,e){for(var n=0;n<e.length;n++){var r=e[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(t,r.key,r)}}return function(e,n,r){return n&&t(e.prototype,n),r&&t(e,r),e}}(),c=n(41),s=r(c),l=n(129),f=n(64),p=n(30),h=n(112),d=n(134),v=n(10),g=n(39),m=n(56),y=r(m),b=function(t){function e(){i(this,e);var t=o(this,(e.__proto__||Object.getPrototypeOf(e)).call(this));return window.lastAdditiveForceVisualizer=t,t.effectFormat=(0,p.format)(\".2\"),t.redraw=(0,g.debounce)(function(){return t.draw()},200),t}return a(e,t),u(e,[{key:\"componentDidMount\",value:function(){var t=this;this.mainGroup=this.svg.append(\"g\"),this.axisElement=this.mainGroup.append(\"g\").attr(\"transform\",\"translate(0,35)\").attr(\"class\",\"force-bar-axis\"),this.onTopGroup=this.svg.append(\"g\"),this.baseValueTitle=this.svg.append(\"text\"),this.joinPointLine=this.svg.append(\"line\"),this.joinPointLabelOutline=this.svg.append(\"text\"),this.joinPointLabel=this.svg.append(\"text\"),this.joinPointTitleLeft=this.svg.append(\"text\"),this.joinPointTitleLeftArrow=this.svg.append(\"text\"),this.joinPointTitle=this.svg.append(\"text\"),this.joinPointTitleRightArrow=this.svg.append(\"text\"),this.joinPointTitleRight=this.svg.append(\"text\"),this.hoverLabelBacking=this.svg.append(\"text\").attr(\"x\",10).attr(\"y\",20).attr(\"text-anchor\",\"middle\").attr(\"font-size\",12).attr(\"stroke\",\"#fff\").attr(\"fill\",\"#fff\").attr(\"stroke-width\",\"4\").attr(\"stroke-linejoin\",\"round\").text(\"\").on(\"mouseover\",function(){t.hoverLabel.attr(\"opacity\",1),t.hoverLabelBacking.attr(\"opacity\",1)}).on(\"mouseout\",function(){t.hoverLabel.attr(\"opacity\",0),t.hoverLabelBacking.attr(\"opacity\",0)}),this.hoverLabel=this.svg.append(\"text\").attr(\"x\",10).attr(\"y\",20).attr(\"text-anchor\",\"middle\").attr(\"font-size\",12).attr(\"fill\",\"#0f0\").text(\"\").on(\"mouseover\",function(){t.hoverLabel.attr(\"opacity\",1),t.hoverLabelBacking.attr(\"opacity\",1)}).on(\"mouseout\",function(){t.hoverLabel.attr(\"opacity\",0),t.hoverLabelBacking.attr(\"opacity\",0)});var e=void 0;\"string\"==typeof this.props.plot_cmap?this.props.plot_cmap in y.default.colors?e=y.default.colors[this.props.plot_cmap]:(console.log(\"Invalid color map name, reverting to default.\"),e=y.default.colors.RdBu):Array.isArray(this.props.plot_cmap)&&(e=this.props.plot_cmap),this.colors=e.map(function(t){return(0,v.hsl)(t)}),this.brighterColors=[1.45,1.6].map(function(e,n){return t.colors[n].brighter(e)}),this.colors.map(function(e,n){var r=t.svg.append(\"linearGradient\").attr(\"id\",\"linear-grad-\"+n).attr(\"x1\",\"0%\").attr(\"y1\",\"0%\").attr(\"x2\",\"0%\").attr(\"y2\",\"100%\");r.append(\"stop\").attr(\"offset\",\"0%\").attr(\"stop-color\",e).attr(\"stop-opacity\",.6),r.append(\"stop\").attr(\"offset\",\"100%\").attr(\"stop-color\",e).attr(\"stop-opacity\",0);var i=t.svg.append(\"linearGradient\").attr(\"id\",\"linear-backgrad-\"+n).attr(\"x1\",\"0%\").attr(\"y1\",\"0%\").attr(\"x2\",\"0%\").attr(\"y2\",\"100%\");i.append(\"stop\").attr(\"offset\",\"0%\").attr(\"stop-color\",e).attr(\"stop-opacity\",.5),i.append(\"stop\").attr(\"offset\",\"100%\").attr(\"stop-color\",e).attr(\"stop-opacity\",0)}),this.tickFormat=(0,p.format)(\",.4\"),this.scaleCentered=(0,f.scaleLinear)(),this.axis=(0,h.axisBottom)().scale(this.scaleCentered).tickSizeInner(4).tickSizeOuter(0).tickFormat(function(e){return t.tickFormat(t.invLinkFunction(e))}).tickPadding(-18),window.addEventListener(\"resize\",this.redraw),window.setTimeout(this.redraw,50)}},{key:\"componentDidUpdate\",value:function(){this.draw()}},{key:\"draw\",value:function(){var t=this;(0,g.each)(this.props.featureNames,function(e,n){t.props.features[n]&&(t.props.features[n].name=e)}),\"identity\"===this.props.link?this.invLinkFunction=function(e){return t.props.baseValue+e}:\"logit\"===this.props.link?this.invLinkFunction=function(e){return 1/(1+Math.exp(-(t.props.baseValue+e)))}:console.log(\"ERROR: Unrecognized link function: \",this.props.link);var e=this.svg.node().parentNode.offsetWidth;if(0==e)return setTimeout(function(){return t.draw(t.props)},500);this.svg.style(\"height\",\"150px\"),this.svg.style(\"width\",e+\"px\");var n=50,r=(0,g.sortBy)(this.props.features,function(t){return-1/(t.effect+1e-10)}),i=(0,g.sum)((0,g.map)(r,function(t){return Math.abs(t.effect)})),o=(0,g.sum)((0,g.map)((0,g.filter)(r,function(t){return t.effect>0}),function(t){return t.effect}))||0,a=(0,g.sum)((0,g.map)((0,g.filter)(r,function(t){return t.effect<0}),function(t){return-t.effect}))||0;this.domainSize=3*Math.max(o,a);var u=(0,f.scaleLinear)().domain([0,this.domainSize]).range([0,e]),c=e/2-u(a);this.scaleCentered.domain([-this.domainSize/2,this.domainSize/2]).range([0,e]).clamp(!0),this.axisElement.attr(\"transform\",\"translate(0,\"+n+\")\").call(this.axis);var s=0,l=void 0,h=void 0,v=void 0;for(l=0;l<r.length;++l)r[l].x=s,r[l].effect<0&&void 0===h&&(h=s,v=l),s+=Math.abs(r[l].effect);void 0===h&&(h=s,v=l);var m=(0,d.line)().x(function(t){return t[0]}).y(function(t){return t[1]}),y=function(e){return void 0!==e.value&&null!==e.value&&\"\"!==e.value?e.name+\" = \"+(isNaN(e.value)?e.value:t.tickFormat(e.value)):e.name};r=this.props.hideBars?[]:r;var b=this.mainGroup.selectAll(\".force-bar-blocks\").data(r);b.enter().append(\"path\").attr(\"class\",\"force-bar-blocks\").merge(b).attr(\"d\",function(t,e){var r=u(t.x)+c,i=u(Math.abs(t.effect)),o=t.effect<0?-4:4,a=o;return e===v&&(o=0),e===v-1&&(a=0),m([[r,6+n],[r+i,6+n],[r+i+a,14.5+n],[r+i,23+n],[r,23+n],[r+o,14.5+n]])}).attr(\"fill\",function(e){return e.effect>0?t.colors[0]:t.colors[1]}).on(\"mouseover\",function(e){if(u(Math.abs(e.effect))<u(i)/50||u(Math.abs(e.effect))<10){var r=u(e.x)+c,o=u(Math.abs(e.effect));t.hoverLabel.attr(\"opacity\",1).attr(\"x\",r+o/2).attr(\"y\",n+.5).attr(\"fill\",e.effect>0?t.colors[0]:t.colors[1]).text(y(e)),t.hoverLabelBacking.attr(\"opacity\",1).attr(\"x\",r+o/2).attr(\"y\",n+.5).text(y(e))}}).on(\"mouseout\",function(){t.hoverLabel.attr(\"opacity\",0),t.hoverLabelBacking.attr(\"opacity\",0)}),b.exit().remove();var x=_.filter(r,function(t){return u(Math.abs(t.effect))>u(i)/50&&u(Math.abs(t.effect))>10}),w=this.onTopGroup.selectAll(\".force-bar-labels\").data(x);if(w.exit().remove(),w=w.enter().append(\"text\").attr(\"class\",\"force-bar-labels\").attr(\"font-size\",\"12px\").attr(\"y\",48+n).merge(w).text(function(e){return void 0!==e.value&&null!==e.value&&\"\"!==e.value?e.name+\" = \"+(isNaN(e.value)?e.value:t.tickFormat(e.value)):e.name}).attr(\"fill\",function(e){return e.effect>0?t.colors[0]:t.colors[1]}).attr(\"stroke\",function(t){return t.textWidth=Math.max(this.getComputedTextLength(),u(Math.abs(t.effect))-10),t.innerTextWidth=this.getComputedTextLength(),\"none\"}),this.filteredData=x,r.length>0){s=h+u.invert(5);for(var C=v;C<r.length;++C)r[C].textx=s,s+=u.invert(r[C].textWidth+10);s=h-u.invert(5);for(var M=v-1;M>=0;--M)r[M].textx=s,s-=u.invert(r[M].textWidth+10)}w.attr(\"x\",function(t){return u(t.textx)+c+(t.effect>0?-t.textWidth/2:t.textWidth/2)}).attr(\"text-anchor\",\"middle\"),x=(0,g.filter)(x,function(n){return u(n.textx)+c>t.props.labelMargin&&u(n.textx)+c<e-t.props.labelMargin}),this.filteredData2=x;var k=x.slice(),E=(0,g.findIndex)(r,x[0])-1;E>=0&&k.unshift(r[E]);var T=this.mainGroup.selectAll(\".force-bar-labelBacking\").data(x);T.enter().append(\"path\").attr(\"class\",\"force-bar-labelBacking\").attr(\"stroke\",\"none\").attr(\"opacity\",.2).merge(T).attr(\"d\",function(t){return m([[u(t.x)+u(Math.abs(t.effect))+c,23+n],[(t.effect>0?u(t.textx):u(t.textx)+t.textWidth)+c+5,33+n],[(t.effect>0?u(t.textx):u(t.textx)+t.textWidth)+c+5,54+n],[(t.effect>0?u(t.textx)-t.textWidth:u(t.textx))+c-5,54+n],[(t.effect>0?u(t.textx)-t.textWidth:u(t.textx))+c-5,33+n],[u(t.x)+c,23+n]])}).attr(\"fill\",function(t){return\"url(#linear-backgrad-\"+(t.effect>0?0:1)+\")\"}),T.exit().remove();var S=this.mainGroup.selectAll(\".force-bar-labelDividers\").data(x.slice(0,-1));S.enter().append(\"rect\").attr(\"class\",\"force-bar-labelDividers\").attr(\"height\",\"21px\").attr(\"width\",\"1px\").attr(\"y\",33+n).merge(S).attr(\"x\",function(t){return(t.effect>0?u(t.textx):u(t.textx)+t.textWidth)+c+4.5}).attr(\"fill\",function(t){return\"url(#linear-grad-\"+(t.effect>0?0:1)+\")\"}),S.exit().remove();var P=this.mainGroup.selectAll(\".force-bar-labelLinks\").data(x.slice(0,-1));P.enter().append(\"line\").attr(\"class\",\"force-bar-labelLinks\").attr(\"y1\",23+n).attr(\"y2\",33+n).attr(\"stroke-opacity\",.5).attr(\"stroke-width\",1).merge(P).attr(\"x1\",function(t){return u(t.x)+u(Math.abs(t.effect))+c}).attr(\"x2\",function(t){return(t.effect>0?u(t.textx):u(t.textx)+t.textWidth)+c+5}).attr(\"stroke\",function(e){return e.effect>0?t.colors[0]:t.colors[1]}),P.exit().remove();var N=this.mainGroup.selectAll(\".force-bar-blockDividers\").data(r.slice(0,-1));N.enter().append(\"path\").attr(\"class\",\"force-bar-blockDividers\").attr(\"stroke-width\",2).attr(\"fill\",\"none\").merge(N).attr(\"d\",function(t){var e=u(t.x)+u(Math.abs(t.effect))+c;return m([[e,6+n],[e+(t.effect<0?-4:4),14.5+n],[e,23+n]])}).attr(\"stroke\",function(e,n){return v===n+1||Math.abs(e.effect)<1e-8?\"#rgba(0,0,0,0)\":e.effect>0?t.brighterColors[0]:t.brighterColors[1]}),N.exit().remove(),this.joinPointLine.attr(\"x1\",u(h)+c).attr(\"x2\",u(h)+c).attr(\"y1\",0+n).attr(\"y2\",6+n).attr(\"stroke\",\"#F2F2F2\").attr(\"stroke-width\",1).attr(\"opacity\",1),this.joinPointLabelOutline.attr(\"x\",u(h)+c).attr(\"y\",-5+n).attr(\"color\",\"#fff\").attr(\"text-anchor\",\"middle\").attr(\"font-weight\",\"bold\").attr(\"stroke\",\"#fff\").attr(\"stroke-width\",6).text((0,p.format)(\",.2f\")(this.invLinkFunction(h-a))).attr(\"opacity\",1),console.log(\"joinPoint\",h,c,n,a),this.joinPointLabel.attr(\"x\",u(h)+c).attr(\"y\",-5+n).attr(\"text-anchor\",\"middle\").attr(\"font-weight\",\"bold\").attr(\"fill\",\"#000\").text((0,p.format)(\",.2f\")(this.invLinkFunction(h-a))).attr(\"opacity\",1),this.joinPointTitle.attr(\"x\",u(h)+c).attr(\"y\",-22+n).attr(\"text-anchor\",\"middle\").attr(\"font-size\",\"12\").attr(\"fill\",\"#000\").text(this.props.outNames[0]).attr(\"opacity\",.5),this.props.hideBars||(this.joinPointTitleLeft.attr(\"x\",u(h)+c-16).attr(\"y\",-38+n).attr(\"text-anchor\",\"end\").attr(\"font-size\",\"13\").attr(\"fill\",this.colors[0]).text(\"higher\").attr(\"opacity\",1),this.joinPointTitleRight.attr(\"x\",u(h)+c+16).attr(\"y\",-38+n).attr(\"text-anchor\",\"start\").attr(\"font-size\",\"13\").attr(\"fill\",this.colors[1]).text(\"lower\").attr(\"opacity\",1),this.joinPointTitleLeftArrow.attr(\"x\",u(h)+c+7).attr(\"y\",-42+n).attr(\"text-anchor\",\"end\").attr(\"font-size\",\"13\").attr(\"fill\",this.colors[0]).text(\"→\").attr(\"opacity\",1),this.joinPointTitleRightArrow.attr(\"x\",u(h)+c-7).attr(\"y\",-36+n).attr(\"text-anchor\",\"start\").attr(\"font-size\",\"13\").attr(\"fill\",this.colors[1]).text(\"←\").attr(\"opacity\",1)),this.props.hideBaseValueLabel||this.baseValueTitle.attr(\"x\",this.scaleCentered(0)).attr(\"y\",-22+n).attr(\"text-anchor\",\"middle\").attr(\"font-size\",\"12\").attr(\"fill\",\"#000\").text(\"base value\").attr(\"opacity\",.5)}},{key:\"componentWillUnmount\",value:function(){window.removeEventListener(\"resize\",this.redraw)}},{key:\"render\",value:function(){var t=this;return s.default.createElement(\"svg\",{ref:function(e){return t.svg=(0,l.select)(e)},style:{userSelect:\"none\",display:\"block\",fontFamily:\"arial\",sansSerif:!0}},s.default.createElement(\"style\",{dangerouslySetInnerHTML:{__html:\"\\n          .force-bar-axis path {\\n            fill: none;\\n            opacity: 0.4;\\n          }\\n          .force-bar-axis paths {\\n            display: none;\\n          }\\n          .tick line {\\n            stroke: #000;\\n            stroke-width: 1px;\\n            opacity: 0.4;\\n          }\\n          .tick text {\\n            fill: #000;\\n            opacity: 0.5;\\n            font-size: 12px;\\n            padding: 0px;\\n          }\"}}))}}]),e}(s.default.Component);b.defaultProps={plot_cmap:\"RdBu\"},e.default=b},function(t,e,n){\"use strict\";function r(t){return t&&t.__esModule?t:{default:t}}function i(t,e){if(!(t instanceof e))throw new TypeError(\"Cannot call a class as a function\")}function o(t,e){if(!t)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return!e||\"object\"!=typeof e&&\"function\"!=typeof e?t:e}function a(t,e){if(\"function\"!=typeof e&&null!==e)throw new TypeError(\"Super expression must either be null or a function, not \"+typeof e);t.prototype=Object.create(e&&e.prototype,{constructor:{value:t,enumerable:!1,writable:!0,configurable:!0}}),e&&(Object.setPrototypeOf?Object.setPrototypeOf(t,e):t.__proto__=e)}Object.defineProperty(e,\"__esModule\",{value:!0});var u=function(){function t(t,e){for(var n=0;n<e.length;n++){var r=e[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(t,r.key,r)}}return function(e,n,r){return n&&t(e.prototype,n),r&&t(e,r),e}}(),c=n(41),s=r(c),l=n(64),f=n(30),p=n(39),h=n(56),d=r(h),v=function(t){function e(){i(this,e);var t=o(this,(e.__proto__||Object.getPrototypeOf(e)).call(this));return t.width=100,window.lastSimpleListInstance=t,t.effectFormat=(0,f.format)(\".2\"),t}return a(e,t),u(e,[{key:\"render\",value:function(){var t=this,e=void 0;\"string\"==typeof this.props.plot_cmap?this.props.plot_cmap in d.default.colors?e=d.default.colors[this.props.plot_cmap]:(console.log(\"Invalid color map name, reverting to default.\"),e=d.default.colors.RdBu):Array.isArray(this.props.plot_cmap)&&(e=this.props.plot_cmap),console.log(this.props.features,this.props.features),this.scale=(0,l.scaleLinear)().domain([0,(0,p.max)((0,p.map)(this.props.features,function(t){return Math.abs(t.effect)}))]).range([0,this.width]);var n=(0,p.reverse)((0,p.sortBy)(Object.keys(this.props.features),function(e){return Math.abs(t.props.features[e].effect)})),r=n.map(function(n){var r=t.props.features[n],i=t.props.featureNames[n],o={width:t.scale(Math.abs(r.effect)),height:\"20px\",background:r.effect<0?e[0]:e[1],display:\"inline-block\"},a=void 0,u=void 0,c={lineHeight:\"20px\",display:\"inline-block\",width:t.width+40,verticalAlign:\"top\",marginRight:\"5px\",textAlign:\"right\"},l={lineHeight:\"20px\",display:\"inline-block\",width:t.width+40,verticalAlign:\"top\",marginLeft:\"5px\"};return r.effect<0?(u=s.default.createElement(\"span\",{style:l},i),c.width=40+t.width-t.scale(Math.abs(r.effect)),c.textAlign=\"right\",c.color=\"#999\",c.fontSize=\"13px\",a=s.default.createElement(\"span\",{style:c},t.effectFormat(r.effect))):(c.textAlign=\"right\",a=s.default.createElement(\"span\",{style:c},i),l.width=40,l.textAlign=\"left\",l.color=\"#999\",l.fontSize=\"13px\",u=s.default.createElement(\"span\",{style:l},t.effectFormat(r.effect))),s.default.createElement(\"div\",{key:n,style:{marginTop:\"2px\"}},a,s.default.createElement(\"div\",{style:o}),u)});return s.default.createElement(\"span\",null,r)}}]),e}(s.default.Component);v.defaultProps={plot_cmap:\"RdBu\"},e.default=v},function(t,e,n){\"use strict\";t.exports=n(345)},function(t,e,n){var r=(n(0),n(398)),i=!1;t.exports=function(t){t=t||{};var e=t.shouldRejectClick||r;i=!0,n(22).injection.injectEventPluginsByName({TapEventPlugin:n(396)(e)})}},function(t,e,n){\"use strict\";e.a=function(t){return function(){return t}}},function(t,e,n){\"use strict\"},function(t,e,n){\"use strict\";n(101),n(102),n(184),n(105),n(187),n(109),n(108)},function(t,e,n){\"use strict\";e.a=function(t){return t}},function(t,e,n){\"use strict\"},function(t,e,n){\"use strict\";n(29)},function(t,e,n){\"use strict\";n(18),n(29),n(57)},function(t,e,n){\"use strict\"},function(t,e,n){\"use strict\"},function(t,e,n){\"use strict\"},function(t,e,n){\"use strict\";n(18)},function(t,e,n){\"use strict\"},function(t,e,n){\"use strict\"},function(t,e,n){\"use strict\";n(101),n(18),n(29),n(57)},function(t,e,n){\"use strict\";n(104)},function(t,e,n){\"use strict\";n(110)},function(t,e,n){\"use strict\";n.d(e,\"a\",function(){return r});var r=Array.prototype.slice},function(t,e,n){\"use strict\";function r(t,e,n){var r=t(n);return\"translate(\"+(isFinite(r)?r:e(n))+\",0)\"}function i(t,e,n){var r=t(n);return\"translate(0,\"+(isFinite(r)?r:e(n))+\")\"}function o(t){var e=t.bandwidth()/2;return t.round()&&(e=Math.round(e)),function(n){return t(n)+e}}function a(){return!this.__axis}function u(t,e){function n(n){var p,b=null==c?e.ticks?e.ticks.apply(e,u):e.domain():c,x=null==s?e.tickFormat?e.tickFormat.apply(e,u):h.a:s,w=Math.max(l,0)+_,C=t===d||t===g?r:i,M=e.range(),k=M[0]+.5,E=M[M.length-1]+.5,T=(e.bandwidth?o:h.a)(e.copy()),S=n.selection?n.selection():n,P=S.selectAll(\".domain\").data([null]),N=S.selectAll(\".tick\").data(b,e).order(),A=N.exit(),O=N.enter().append(\"g\").attr(\"class\",\"tick\"),I=N.select(\"line\"),D=N.select(\"text\"),R=t===d||t===m?-1:1,L=t===m||t===v?(p=\"x\",\"y\"):(p=\"y\",\"x\");P=P.merge(P.enter().insert(\"path\",\".tick\").attr(\"class\",\"domain\").attr(\"stroke\",\"#000\")),N=N.merge(O),I=I.merge(O.append(\"line\").attr(\"stroke\",\"#000\").attr(p+\"2\",R*l).attr(L+\"1\",.5).attr(L+\"2\",.5)),D=D.merge(O.append(\"text\").attr(\"fill\",\"#000\").attr(p,R*w).attr(L,.5).attr(\"dy\",t===d?\"0em\":t===g?\"0.71em\":\"0.32em\")),n!==S&&(P=P.transition(n),N=N.transition(n),I=I.transition(n),D=D.transition(n),A=A.transition(n).attr(\"opacity\",y).attr(\"transform\",function(t){return C(T,this.parentNode.__axis||T,t)}),O.attr(\"opacity\",y).attr(\"transform\",function(t){return C(this.parentNode.__axis||T,T,t)})),A.remove(),P.attr(\"d\",t===m||t==v?\"M\"+R*f+\",\"+k+\"H0.5V\"+E+\"H\"+R*f:\"M\"+k+\",\"+R*f+\"V0.5H\"+E+\"V\"+R*f),N.attr(\"opacity\",1).attr(\"transform\",function(t){return C(T,T,t)}),I.attr(p+\"2\",R*l),D.attr(p,R*w).text(x),S.filter(a).attr(\"fill\",\"none\").attr(\"font-size\",10).attr(\"font-family\",\"sans-serif\").attr(\"text-anchor\",t===v?\"start\":t===m?\"end\":\"middle\"),S.each(function(){this.__axis=T})}var u=[],c=null,s=null,l=6,f=6,_=3;return n.scale=function(t){return arguments.length?(e=t,n):e},n.ticks=function(){return u=p.a.call(arguments),n},n.tickArguments=function(t){return arguments.length?(u=null==t?[]:p.a.call(t),n):u.slice()},n.tickValues=function(t){return arguments.length?(c=null==t?null:p.a.call(t),n):c&&c.slice()},n.tickFormat=function(t){return arguments.length?(s=t,n):s},n.tickSize=function(t){return arguments.length?(l=f=+t,n):l},n.tickSizeInner=function(t){return arguments.length?(l=+t,n):l},n.tickSizeOuter=function(t){return arguments.length?(f=+t,n):f},n.tickPadding=function(t){return arguments.length?(_=+t,n):_},n}function c(t){return u(d,t)}function s(t){return u(v,t)}function l(t){return u(g,t)}function f(t){return u(m,t)}var p=n(200),h=n(202);e.a=c,e.b=s,e.c=l,e.d=f;var d=1,v=2,g=3,m=4,y=1e-6},function(t,e,n){\"use strict\";e.a=function(t){return t}},function(t,e,n){\"use strict\";var r=(n(206),n(207),n(58));n.d(e,\"a\",function(){return r.a});n(205),n(208),n(204)},function(t,e,n){\"use strict\"},function(t,e,n){\"use strict\"},function(t,e,n){\"use strict\";n(58)},function(t,e,n){\"use strict\";function r(){}function i(t,e){var n=new r;if(t instanceof r)t.each(function(t){n.add(t)});else if(t){var i=-1,o=t.length;if(null==e)for(;++i<o;)n.add(t[i]);else for(;++i<o;)n.add(e(t[i],i,t))}return n}var o=n(58),a=o.a.prototype;r.prototype=i.prototype={constructor:r,has:a.has,add:function(t){return t+=\"\",this[o.b+t]=t,this},remove:a.remove,clear:a.clear,values:a.keys,size:a.size,empty:a.empty,each:a.each}},function(t,e,n){\"use strict\"},function(t,e,n){\"use strict\";function r(t){if(t instanceof o)return new o(t.h,t.s,t.l,t.opacity);t instanceof u.d||(t=n.i(u.e)(t));var e=t.r/255,r=t.g/255,i=t.b/255,a=(g*i+d*e-v*r)/(g+d-v),s=i-a,l=(h*(r-a)-f*s)/p,m=Math.sqrt(l*l+s*s)/(h*a*(1-a)),y=m?Math.atan2(l,s)*c.a-120:NaN;return new o(y<0?y+360:y,m,a,t.opacity)}function i(t,e,n,i){return 1===arguments.length?r(t):new o(t,e,n,null==i?1:i)}function o(t,e,n,r){this.h=+t,this.s=+e,this.l=+n,this.opacity=+r}var a=n(60),u=n(59),c=n(113);e.a=i;var s=-.14861,l=1.78277,f=-.29227,p=-.90649,h=1.97294,d=h*p,v=h*l,g=l*f-p*s;n.i(a.a)(o,i,n.i(a.b)(u.f,{brighter:function(t){return t=null==t?u.g:Math.pow(u.g,t),new o(this.h,this.s,this.l*t,this.opacity)},darker:function(t){return t=null==t?u.h:Math.pow(u.h,t),new o(this.h,this.s,this.l*t,this.opacity)},rgb:function(){var t=isNaN(this.h)?0:(this.h+120)*c.b,e=+this.l,n=isNaN(this.s)?0:this.s*e*(1-e),r=Math.cos(t),i=Math.sin(t);return new u.d(255*(e+n*(s*r+l*i)),255*(e+n*(f*r+p*i)),255*(e+n*(h*r)),this.opacity)}}))},function(t,e,n){\"use strict\";function r(t){if(t instanceof o)return new o(t.l,t.a,t.b,t.opacity);if(t instanceof p){var e=t.h*v.b;return new o(t.l,Math.cos(e)*t.c,Math.sin(e)*t.c,t.opacity)}t instanceof d.d||(t=n.i(d.e)(t));var r=s(t.r),i=s(t.g),u=s(t.b),c=a((.4124564*r+.3575761*i+.1804375*u)/m),l=a((.2126729*r+.7151522*i+.072175*u)/y),f=a((.0193339*r+.119192*i+.9503041*u)/_);return new o(116*l-16,500*(c-l),200*(l-f),t.opacity)}function i(t,e,n,i){return 1===arguments.length?r(t):new o(t,e,n,null==i?1:i)}function o(t,e,n,r){this.l=+t,this.a=+e,this.b=+n,this.opacity=+r}function a(t){return t>C?Math.pow(t,1/3):t/w+b}function u(t){return t>x?t*t*t:w*(t-b)}function c(t){return 255*(t<=.0031308?12.92*t:1.055*Math.pow(t,1/2.4)-.055)}function s(t){return(t/=255)<=.04045?t/12.92:Math.pow((t+.055)/1.055,2.4)}function l(t){if(t instanceof p)return new p(t.h,t.c,t.l,t.opacity);t instanceof o||(t=r(t));var e=Math.atan2(t.b,t.a)*v.a;return new p(e<0?e+360:e,Math.sqrt(t.a*t.a+t.b*t.b),t.l,t.opacity)}function f(t,e,n,r){return 1===arguments.length?l(t):new p(t,e,n,null==r?1:r)}function p(t,e,n,r){this.h=+t,this.c=+e,this.l=+n,this.opacity=+r}var h=n(60),d=n(59),v=n(113);e.a=i,e.b=f;var g=18,m=.95047,y=1,_=1.08883,b=4/29,x=6/29,w=3*x*x,C=x*x*x;n.i(h.a)(o,i,n.i(h.b)(d.f,{brighter:function(t){return new o(this.l+g*(null==t?1:t),this.a,this.b,this.opacity)},darker:function(t){return new o(this.l-g*(null==t?1:t),this.a,this.b,this.opacity)},rgb:function(){var t=(this.l+16)/116,e=isNaN(this.a)?t:t+this.a/500,n=isNaN(this.b)?t:t-this.b/200;return t=y*u(t),e=m*u(e),n=_*u(n),new d.d(c(3.2404542*e-1.5371385*t-.4985314*n),c(-.969266*e+1.8760108*t+.041556*n),c(.0556434*e-.2040259*t+1.0572252*n),this.opacity)}})),n.i(h.a)(p,f,n.i(h.b)(d.f,{brighter:function(t){return new p(this.h,this.c,this.l+g*(null==t?1:t),this.opacity)},darker:function(t){return new p(this.h,this.c,this.l-g*(null==t?1:t),this.opacity)},rgb:function(){return r(this).rgb()}}))},function(t,e,n){\"use strict\";function r(t){return o=n.i(i.a)(t),a=o.format,u=o.formatPrefix,o}var i=n(117);n.d(e,\"b\",function(){return a}),n.d(e,\"c\",function(){\n",
       "return u}),e.a=r;var o,a,u;r({decimal:\".\",thousands:\",\",grouping:[3],currency:[\"$\",\"\"]})},function(t,e,n){\"use strict\";e.a=function(t,e){t=t.toPrecision(e);t:for(var n,r=t.length,i=1,o=-1;i<r;++i)switch(t[i]){case\".\":o=n=i;break;case\"0\":0===o&&(o=i),n=i;break;case\"e\":break t;default:o>0&&(o=0)}return o>0?t.slice(0,o)+t.slice(n+1):t}},function(t,e,n){\"use strict\";e.a=function(t,e){return function(n,r){for(var i=n.length,o=[],a=0,u=t[0],c=0;i>0&&u>0&&(c+u+1>r&&(u=Math.max(1,r-c)),o.push(n.substring(i-=u,i+u)),!((c+=u+1)>r));)u=t[a=(a+1)%t.length];return o.reverse().join(e)}}},function(t,e,n){\"use strict\";var r=n(61);e.a=function(t,e){var i=n.i(r.a)(t,e);if(!i)return t+\"\";var o=i[0],a=i[1];return a<0?\"0.\"+new Array(-a).join(\"0\")+o:o.length>a+1?o.slice(0,a+1)+\".\"+o.slice(a+1):o+new Array(a-o.length+2).join(\"0\")}},function(t,e,n){\"use strict\";var r=n(42);e.a=function(t){return Math.max(0,-n.i(r.a)(Math.abs(t)))}},function(t,e,n){\"use strict\";var r=n(42);e.a=function(t,e){return Math.max(0,3*Math.max(-8,Math.min(8,Math.floor(n.i(r.a)(e)/3)))-n.i(r.a)(Math.abs(t)))}},function(t,e,n){\"use strict\";var r=n(42);e.a=function(t,e){return t=Math.abs(t),e=Math.abs(e)-t,Math.max(0,n.i(r.a)(e)-n.i(r.a)(t))+1}},function(t,e,n){\"use strict\";function r(t){return function e(r){function a(e,a){var u=t((e=n.i(i.cubehelix)(e)).h,(a=n.i(i.cubehelix)(a)).h),c=n.i(o.a)(e.s,a.s),s=n.i(o.a)(e.l,a.l),l=n.i(o.a)(e.opacity,a.opacity);return function(t){return e.h=u(t),e.s=c(t),e.l=s(Math.pow(t,r)),e.opacity=l(t),e+\"\"}}return r=+r,a.gamma=e,a}(1)}var i=n(10),o=n(32);n.d(e,\"a\",function(){return a});var a=(r(o.b),r(o.a))},function(t,e,n){\"use strict\";function r(t){return function(e,r){var a=t((e=n.i(i.hcl)(e)).h,(r=n.i(i.hcl)(r)).h),u=n.i(o.a)(e.c,r.c),c=n.i(o.a)(e.l,r.l),s=n.i(o.a)(e.opacity,r.opacity);return function(t){return e.h=a(t),e.c=u(t),e.l=c(t),e.opacity=s(t),e+\"\"}}}var i=n(10),o=n(32);r(o.b),r(o.a)},function(t,e,n){\"use strict\";function r(t){return function(e,r){var a=t((e=n.i(i.hsl)(e)).h,(r=n.i(i.hsl)(r)).h),u=n.i(o.a)(e.s,r.s),c=n.i(o.a)(e.l,r.l),s=n.i(o.a)(e.opacity,r.opacity);return function(t){return e.h=a(t),e.s=u(t),e.l=c(t),e.opacity=s(t),e+\"\"}}}var i=n(10),o=n(32);r(o.b),r(o.a)},function(t,e,n){\"use strict\";n(10),n(32)},function(t,e,n){\"use strict\"},function(t,e,n){\"use strict\";e.a=function(t,e){return t=+t,e-=t,function(n){return Math.round(t+e*n)}}},function(t,e,n){\"use strict\";n.d(e,\"a\",function(){return i});var r=180/Math.PI,i={translateX:0,translateY:0,rotate:0,skewX:0,scaleX:1,scaleY:1};e.b=function(t,e,n,i,o,a){var u,c,s;return(u=Math.sqrt(t*t+e*e))&&(t/=u,e/=u),(s=t*n+e*i)&&(n-=t*s,i-=e*s),(c=Math.sqrt(n*n+i*i))&&(n/=c,i/=c,s/=c),t*i<e*n&&(t=-t,e=-e,s=-s,u=-u),{translateX:o,translateY:a,rotate:Math.atan2(e,t)*r,skewX:Math.atan(s)*r,scaleX:u,scaleY:c}}},function(t,e,n){\"use strict\";function r(t,e,r,o){function a(t){return t.length?t.pop()+\" \":\"\"}function u(t,o,a,u,c,s){if(t!==a||o!==u){var l=c.push(\"translate(\",null,e,null,r);s.push({i:l-4,x:n.i(i.a)(t,a)},{i:l-2,x:n.i(i.a)(o,u)})}else(a||u)&&c.push(\"translate(\"+a+e+u+r)}function c(t,e,r,u){t!==e?(t-e>180?e+=360:e-t>180&&(t+=360),u.push({i:r.push(a(r)+\"rotate(\",null,o)-2,x:n.i(i.a)(t,e)})):e&&r.push(a(r)+\"rotate(\"+e+o)}function s(t,e,r,u){t!==e?u.push({i:r.push(a(r)+\"skewX(\",null,o)-2,x:n.i(i.a)(t,e)}):e&&r.push(a(r)+\"skewX(\"+e+o)}function l(t,e,r,o,u,c){if(t!==r||e!==o){var s=u.push(a(u)+\"scale(\",null,\",\",null,\")\");c.push({i:s-4,x:n.i(i.a)(t,r)},{i:s-2,x:n.i(i.a)(e,o)})}else 1===r&&1===o||u.push(a(u)+\"scale(\"+r+\",\"+o+\")\")}return function(e,n){var r=[],i=[];return e=t(e),n=t(n),u(e.translateX,e.translateY,n.translateX,n.translateY,r,i),c(e.rotate,n.rotate,r,i),s(e.skewX,n.skewX,r,i),l(e.scaleX,e.scaleY,n.scaleX,n.scaleY,r,i),e=n=null,function(t){for(var e,n=-1,o=i.length;++n<o;)r[(e=i[n]).i]=e.x(t);return r.join(\"\")}}}var i=n(43),o=n(226);r(o.a,\"px, \",\"px)\",\"deg)\"),r(o.b,\", \",\")\",\")\")},function(t,e,n){\"use strict\";function r(t){return\"none\"===t?o.a:(a||(a=document.createElement(\"DIV\"),u=document.documentElement,c=document.defaultView),a.style.transform=t,t=c.getComputedStyle(u.appendChild(a),null).getPropertyValue(\"transform\"),u.removeChild(a),t=t.slice(7,-1).split(\",\"),n.i(o.b)(+t[0],+t[1],+t[2],+t[3],+t[4],+t[5]))}function i(t){return null==t?o.a:(s||(s=document.createElementNS(\"http://www.w3.org/2000/svg\",\"g\")),s.setAttribute(\"transform\",t),(t=s.transform.baseVal.consolidate())?(t=t.matrix,n.i(o.b)(t.a,t.b,t.c,t.d,t.e,t.f)):o.a)}var o=n(224);e.a=r,e.b=i;var a,u,c,s},function(t,e,n){\"use strict\";Math.SQRT2},function(t,e,n){\"use strict\";function r(){this._x0=this._y0=this._x1=this._y1=null,this._=\"\"}function i(){return new r}var o=Math.PI,a=2*o,u=1e-6,c=a-u;r.prototype=i.prototype={constructor:r,moveTo:function(t,e){this._+=\"M\"+(this._x0=this._x1=+t)+\",\"+(this._y0=this._y1=+e)},closePath:function(){null!==this._x1&&(this._x1=this._x0,this._y1=this._y0,this._+=\"Z\")},lineTo:function(t,e){this._+=\"L\"+(this._x1=+t)+\",\"+(this._y1=+e)},quadraticCurveTo:function(t,e,n,r){this._+=\"Q\"+ +t+\",\"+ +e+\",\"+(this._x1=+n)+\",\"+(this._y1=+r)},bezierCurveTo:function(t,e,n,r,i,o){this._+=\"C\"+ +t+\",\"+ +e+\",\"+ +n+\",\"+ +r+\",\"+(this._x1=+i)+\",\"+(this._y1=+o)},arcTo:function(t,e,n,r,i){t=+t,e=+e,n=+n,r=+r,i=+i;var a=this._x1,c=this._y1,s=n-t,l=r-e,f=a-t,p=c-e,h=f*f+p*p;if(i<0)throw new Error(\"negative radius: \"+i);if(null===this._x1)this._+=\"M\"+(this._x1=t)+\",\"+(this._y1=e);else if(h>u)if(Math.abs(p*s-l*f)>u&&i){var d=n-a,v=r-c,g=s*s+l*l,m=d*d+v*v,y=Math.sqrt(g),_=Math.sqrt(h),b=i*Math.tan((o-Math.acos((g+h-m)/(2*y*_)))/2),x=b/_,w=b/y;Math.abs(x-1)>u&&(this._+=\"L\"+(t+x*f)+\",\"+(e+x*p)),this._+=\"A\"+i+\",\"+i+\",0,0,\"+ +(p*d>f*v)+\",\"+(this._x1=t+w*s)+\",\"+(this._y1=e+w*l)}else this._+=\"L\"+(this._x1=t)+\",\"+(this._y1=e);else;},arc:function(t,e,n,r,i,s){t=+t,e=+e,n=+n;var l=n*Math.cos(r),f=n*Math.sin(r),p=t+l,h=e+f,d=1^s,v=s?r-i:i-r;if(n<0)throw new Error(\"negative radius: \"+n);null===this._x1?this._+=\"M\"+p+\",\"+h:(Math.abs(this._x1-p)>u||Math.abs(this._y1-h)>u)&&(this._+=\"L\"+p+\",\"+h),n&&(v>c?this._+=\"A\"+n+\",\"+n+\",0,1,\"+d+\",\"+(t-l)+\",\"+(e-f)+\"A\"+n+\",\"+n+\",0,1,\"+d+\",\"+(this._x1=p)+\",\"+(this._y1=h):(v<0&&(v=v%a+a),this._+=\"A\"+n+\",\"+n+\",0,\"+ +(v>=o)+\",\"+d+\",\"+(this._x1=t+n*Math.cos(i))+\",\"+(this._y1=e+n*Math.sin(i))))},rect:function(t,e,n,r){this._+=\"M\"+(this._x0=this._x1=+t)+\",\"+(this._y0=this._y1=+e)+\"h\"+ +n+\"v\"+ +r+\"h\"+-n+\"Z\"},toString:function(){return this._}},e.a=i},function(t,e,n){\"use strict\";function r(){function t(){var t=c().length,r=l[1]<l[0],o=l[r-0],u=l[1-r];e=(u-o)/Math.max(1,t-p+2*h),f&&(e=Math.floor(e)),o+=(u-o-e*(t-p))*d,i=e*(1-p),f&&(o=Math.round(o),i=Math.round(i));var v=n.i(a.g)(t).map(function(t){return o+e*t});return s(r?v.reverse():v)}var e,i,o=n.i(u.a)().unknown(void 0),c=o.domain,s=o.range,l=[0,1],f=!1,p=0,h=0,d=.5;return delete o.unknown,o.domain=function(e){return arguments.length?(c(e),t()):c()},o.range=function(e){return arguments.length?(l=[+e[0],+e[1]],t()):l.slice()},o.rangeRound=function(e){return l=[+e[0],+e[1]],f=!0,t()},o.bandwidth=function(){return i},o.step=function(){return e},o.round=function(e){return arguments.length?(f=!!e,t()):f},o.padding=function(e){return arguments.length?(p=h=Math.max(0,Math.min(1,e)),t()):p},o.paddingInner=function(e){return arguments.length?(p=Math.max(0,Math.min(1,e)),t()):p},o.paddingOuter=function(e){return arguments.length?(h=Math.max(0,Math.min(1,e)),t()):h},o.align=function(e){return arguments.length?(d=Math.max(0,Math.min(1,e)),t()):d},o.copy=function(){return r().domain(c()).range(l).round(f).paddingInner(p).paddingOuter(h).align(d)},t()}function i(t){var e=t.copy;return t.padding=t.paddingOuter,delete t.paddingInner,delete t.paddingOuter,t.copy=function(){return i(e())},t}function o(){return i(r().paddingInner(1))}var a=n(12),u=n(127);e.a=r,e.b=o},function(t,e,n){\"use strict\";var r=n(33);e.a=n.i(r.a)(\"1f77b4ff7f0e2ca02cd627289467bd8c564be377c27f7f7fbcbd2217becf\")},function(t,e,n){\"use strict\";var r=n(33);e.a=n.i(r.a)(\"1f77b4aec7e8ff7f0effbb782ca02c98df8ad62728ff98969467bdc5b0d58c564bc49c94e377c2f7b6d27f7f7fc7c7c7bcbd22dbdb8d17becf9edae5\")},function(t,e,n){\"use strict\";var r=n(33);e.a=n.i(r.a)(\"393b795254a36b6ecf9c9ede6379398ca252b5cf6bcedb9c8c6d31bd9e39e7ba52e7cb94843c39ad494ad6616be7969c7b4173a55194ce6dbdde9ed6\")},function(t,e,n){\"use strict\";var r=n(33);e.a=n.i(r.a)(\"3182bd6baed69ecae1c6dbefe6550dfd8d3cfdae6bfdd0a231a35474c476a1d99bc7e9c0756bb19e9ac8bcbddcdadaeb636363969696bdbdbdd9d9d9\")},function(t,e,n){\"use strict\";var r=n(10),i=n(31);e.a=n.i(i.d)(n.i(r.cubehelix)(300,.5,0),n.i(r.cubehelix)(-240,.5,1))},function(t,e,n){\"use strict\";function r(){function t(t){return+t}var e=[0,1];return t.invert=t,t.domain=t.range=function(n){return arguments.length?(e=i.a.call(n,a.a),t):e.slice()},t.copy=function(){return r().domain(e)},n.i(o.b)(t)}var i=n(16),o=n(34),a=n(126);e.a=r},function(t,e,n){\"use strict\";function r(t,e){return(e=Math.log(e/t))?function(n){return Math.log(n/t)/e}:n.i(p.a)(e)}function i(t,e){return t<0?function(n){return-Math.pow(-e,n)*Math.pow(-t,1-n)}:function(n){return Math.pow(e,n)*Math.pow(t,1-n)}}function o(t){return isFinite(t)?+(\"1e\"+t):t<0?0:t}function a(t){return 10===t?o:t===Math.E?Math.exp:function(e){return Math.pow(t,e)}}function u(t){return t===Math.E?Math.log:10===t&&Math.log10||2===t&&Math.log2||(t=Math.log(t),function(e){return Math.log(e)/t})}function c(t){return function(e){return-t(-e)}}function s(){function t(){return v=u(p),g=a(p),o()[0]<0&&(v=c(v),g=c(g)),e}var e=n.i(d.a)(r,i).domain([1,10]),o=e.domain,p=10,v=u(10),g=a(10);return e.base=function(e){return arguments.length?(p=+e,t()):p},e.domain=function(e){return arguments.length?(o(e),t()):o()},e.ticks=function(t){var e,r=o(),i=r[0],a=r[r.length-1];(e=a<i)&&(f=i,i=a,a=f);var u,c,s,f=v(i),h=v(a),d=null==t?10:+t,m=[];if(!(p%1)&&h-f<d){if(f=Math.round(f)-1,h=Math.round(h)+1,i>0){for(;f<h;++f)for(c=1,u=g(f);c<p;++c)if(s=u*c,!(s<i)){if(s>a)break;m.push(s)}}else for(;f<h;++f)for(c=p-1,u=g(f);c>=1;--c)if(s=u*c,!(s<i)){if(s>a)break;m.push(s)}}else m=n.i(l.a)(f,h,Math.min(h-f,d)).map(g);return e?m.reverse():m},e.tickFormat=function(t,r){if(null==r&&(r=10===p?\".0e\":\",\"),\"function\"!=typeof r&&(r=n.i(f.format)(r)),t===1/0)return r;null==t&&(t=10);var i=Math.max(1,p*t/e.ticks().length);return function(t){var e=t/g(Math.round(v(t)));return e*p<p-.5&&(e*=p),e<=i?r(t):\"\"}},e.nice=function(){return o(n.i(h.a)(o(),{floor:function(t){return g(Math.floor(v(t)))},ceil:function(t){return g(Math.ceil(v(t)))}}))},e.copy=function(){return n.i(d.c)(e,s().base(p))},e}var l=n(12),f=n(30),p=n(65),h=n(125),d=n(45);e.a=s},function(t,e,n){\"use strict\";function r(t,e){return t<0?-Math.pow(-t,e):Math.pow(t,e)}function i(){function t(t,e){return(e=r(e,o)-(t=r(t,o)))?function(n){return(r(n,o)-t)/e}:n.i(a.a)(e)}function e(t,e){return e=r(e,o)-(t=r(t,o)),function(n){return r(t+e*n,1/o)}}var o=1,s=n.i(c.a)(t,e),l=s.domain;return s.exponent=function(t){return arguments.length?(o=+t,l(l())):o},s.copy=function(){return n.i(c.c)(s,i().exponent(o))},n.i(u.b)(s)}function o(){return i().exponent(.5)}var a=n(65),u=n(34),c=n(45);e.a=i,e.b=o},function(t,e,n){\"use strict\";function r(){function t(){var t=0,r=Math.max(1,u.length);for(c=new Array(r-1);++t<r;)c[t-1]=n.i(i.e)(a,t/r);return e}function e(t){if(!isNaN(t=+t))return u[n.i(i.c)(c,t)]}var a=[],u=[],c=[];return e.invertExtent=function(t){var e=u.indexOf(t);return e<0?[NaN,NaN]:[e>0?c[e-1]:a[0],e<c.length?c[e]:a[a.length-1]]},e.domain=function(e){if(!arguments.length)return a.slice();a=[];for(var n,r=0,o=e.length;r<o;++r)n=e[r],null==n||isNaN(n=+n)||a.push(n);return a.sort(i.f),t()},e.range=function(e){return arguments.length?(u=o.b.call(e),t()):u.slice()},e.quantiles=function(){return c.slice()},e.copy=function(){return r().domain(a).range(u)},e}var i=n(12),o=n(16);e.a=r},function(t,e,n){\"use strict\";function r(){function t(t){if(t<=t)return f[n.i(i.c)(l,t,0,s)]}function e(){var e=-1;for(l=new Array(s);++e<s;)l[e]=((e+1)*c-(e-s)*u)/(s+1);return t}var u=0,c=1,s=1,l=[.5],f=[0,1];return t.domain=function(t){return arguments.length?(u=+t[0],c=+t[1],e()):[u,c]},t.range=function(t){return arguments.length?(s=(f=o.b.call(t)).length-1,e()):f.slice()},t.invertExtent=function(t){var e=f.indexOf(t);return e<0?[NaN,NaN]:e<1?[u,l[0]]:e>=s?[l[s-1],c]:[l[e-1],l[e]]},t.copy=function(){return r().domain([u,c]).range(f)},n.i(a.b)(t)}var i=n(12),o=n(16),a=n(34);e.a=r},function(t,e,n){\"use strict\";var r=n(10),i=n(31);n.d(e,\"b\",function(){return o}),n.d(e,\"c\",function(){return a});var o=n.i(i.d)(n.i(r.cubehelix)(-100,.75,.35),n.i(r.cubehelix)(80,1.5,.8)),a=n.i(i.d)(n.i(r.cubehelix)(260,.75,.35),n.i(r.cubehelix)(80,1.5,.8)),u=n.i(r.cubehelix)();e.a=function(t){(t<0||t>1)&&(t-=Math.floor(t));var e=Math.abs(t-.5);return u.h=360*t-100,u.s=1.5-1.5*e,u.l=.8-.9*e,u+\"\"}},function(t,e,n){\"use strict\";function r(t){function e(e){var n=(e-o)/(a-o);return t(u?Math.max(0,Math.min(1,n)):n)}var o=0,a=1,u=!1;return e.domain=function(t){return arguments.length?(o=+t[0],a=+t[1],e):[o,a]},e.clamp=function(t){return arguments.length?(u=!!t,e):u},e.interpolator=function(n){return arguments.length?(t=n,e):t},e.copy=function(){return r(t).domain([o,a]).clamp(u)},n.i(i.b)(e)}var i=n(34);e.a=r},function(t,e,n){\"use strict\";function r(){function t(t){if(t<=t)return a[n.i(i.c)(e,t,0,u)]}var e=[.5],a=[0,1],u=1;return t.domain=function(n){return arguments.length?(e=o.b.call(n),u=Math.min(e.length,a.length-1),t):e.slice()},t.range=function(n){return arguments.length?(a=o.b.call(n),u=Math.min(e.length,a.length-1),t):a.slice()},t.invertExtent=function(t){var n=a.indexOf(t);return[e[n-1],e[n]]},t.copy=function(){return r().domain(e).range(a)},t}var i=n(12),o=n(16);e.a=r},function(t,e,n){\"use strict\";var r=n(12),i=n(30);e.a=function(t,e,o){var a,u=t[0],c=t[t.length-1],s=n.i(r.b)(u,c,null==e?10:e);switch(o=n.i(i.formatSpecifier)(null==o?\",f\":o),o.type){case\"s\":var l=Math.max(Math.abs(u),Math.abs(c));return null!=o.precision||isNaN(a=n.i(i.precisionPrefix)(s,l))||(o.precision=a),n.i(i.formatPrefix)(o,l);case\"\":case\"e\":case\"g\":case\"p\":case\"r\":null!=o.precision||isNaN(a=n.i(i.precisionRound)(s,Math.max(Math.abs(u),Math.abs(c))))||(o.precision=a-(\"e\"===o.type));break;case\"f\":case\"%\":null!=o.precision||isNaN(a=n.i(i.precisionFixed)(s))||(o.precision=a-2*(\"%\"===o.type))}return n.i(i.format)(o)}},function(t,e,n){\"use strict\";var r=n(128),i=n(77),o=n(79);e.a=function(){return n.i(r.b)(o.f,o.i,o.j,o.e,o.k,o.l,o.m,o.n,i.utcFormat).domain([Date.UTC(2e3,0,1),Date.UTC(2e3,0,2)])}},function(t,e,n){\"use strict\";function r(t){var e=t.length;return function(n){return t[Math.max(0,Math.min(e-1,Math.floor(n*e)))]}}var i=n(33);n.d(e,\"b\",function(){return o}),n.d(e,\"c\",function(){return a}),n.d(e,\"d\",function(){return u}),e.a=r(n.i(i.a)(\"44015444025645045745055946075a46085c460a5d460b5e470d60470e6147106347116447136548146748166848176948186a481a6c481b6d481c6e481d6f481f70482071482173482374482475482576482677482878482979472a7a472c7a472d7b472e7c472f7d46307e46327e46337f463480453581453781453882443983443a83443b84433d84433e85423f854240864241864142874144874045884046883f47883f48893e49893e4a893e4c8a3d4d8a3d4e8a3c4f8a3c508b3b518b3b528b3a538b3a548c39558c39568c38588c38598c375a8c375b8d365c8d365d8d355e8d355f8d34608d34618d33628d33638d32648e32658e31668e31678e31688e30698e306a8e2f6b8e2f6c8e2e6d8e2e6e8e2e6f8e2d708e2d718e2c718e2c728e2c738e2b748e2b758e2a768e2a778e2a788e29798e297a8e297b8e287c8e287d8e277e8e277f8e27808e26818e26828e26828e25838e25848e25858e24868e24878e23888e23898e238a8d228b8d228c8d228d8d218e8d218f8d21908d21918c20928c20928c20938c1f948c1f958b1f968b1f978b1f988b1f998a1f9a8a1e9b8a1e9c891e9d891f9e891f9f881fa0881fa1881fa1871fa28720a38620a48621a58521a68522a78522a88423a98324aa8325ab8225ac8226ad8127ad8128ae8029af7f2ab07f2cb17e2db27d2eb37c2fb47c31b57b32b67a34b67935b77937b87838b9773aba763bbb753dbc743fbc7340bd7242be7144bf7046c06f48c16e4ac16d4cc26c4ec36b50c46a52c56954c56856c66758c7655ac8645cc8635ec96260ca6063cb5f65cb5e67cc5c69cd5b6ccd5a6ece5870cf5773d05675d05477d1537ad1517cd2507fd34e81d34d84d44b86d54989d5488bd6468ed64590d74393d74195d84098d83e9bd93c9dd93ba0da39a2da37a5db36a8db34aadc32addc30b0dd2fb2dd2db5de2bb8de29bade28bddf26c0df25c2df23c5e021c8e020cae11fcde11dd0e11cd2e21bd5e21ad8e219dae319dde318dfe318e2e418e5e419e7e419eae51aece51befe51cf1e51df4e61ef6e620f8e621fbe723fde725\"));var o=r(n.i(i.a)(\"00000401000501010601010802010902020b02020d03030f03031204041405041606051806051a07061c08071e0907200a08220b09240c09260d0a290e0b2b100b2d110c2f120d31130d34140e36150e38160f3b180f3d19103f1a10421c10441d11471e114920114b21114e22115024125325125527125829115a2a115c2c115f2d11612f116331116533106734106936106b38106c390f6e3b0f703d0f713f0f72400f74420f75440f764510774710784910784a10794c117a4e117b4f127b51127c52137c54137d56147d57157e59157e5a167e5c167f5d177f5f187f601880621980641a80651a80671b80681c816a1c816b1d816d1d816e1e81701f81721f817320817521817621817822817922827b23827c23827e24828025828125818326818426818627818827818928818b29818c29818e2a81902a81912b81932b80942c80962c80982d80992d809b2e7f9c2e7f9e2f7fa02f7fa1307ea3307ea5317ea6317da8327daa337dab337cad347cae347bb0357bb2357bb3367ab5367ab73779b83779ba3878bc3978bd3977bf3a77c03a76c23b75c43c75c53c74c73d73c83e73ca3e72cc3f71cd4071cf4070d0416fd2426fd3436ed5446dd6456cd8456cd9466bdb476adc4869de4968df4a68e04c67e24d66e34e65e44f64e55064e75263e85362e95462ea5661eb5760ec5860ed5a5fee5b5eef5d5ef05f5ef1605df2625df2645cf3655cf4675cf4695cf56b5cf66c5cf66e5cf7705cf7725cf8745cf8765cf9785df9795df97b5dfa7d5efa7f5efa815ffb835ffb8560fb8761fc8961fc8a62fc8c63fc8e64fc9065fd9266fd9467fd9668fd9869fd9a6afd9b6bfe9d6cfe9f6dfea16efea36ffea571fea772fea973feaa74feac76feae77feb078feb27afeb47bfeb67cfeb77efeb97ffebb81febd82febf84fec185fec287fec488fec68afec88cfeca8dfecc8ffecd90fecf92fed194fed395fed597fed799fed89afdda9cfddc9efddea0fde0a1fde2a3fde3a5fde5a7fde7a9fde9aafdebacfcecaefceeb0fcf0b2fcf2b4fcf4b6fcf6b8fcf7b9fcf9bbfcfbbdfcfdbf\")),a=r(n.i(i.a)(\"00000401000501010601010802010a02020c02020e03021004031204031405041706041907051b08051d09061f0a07220b07240c08260d08290e092b10092d110a30120a32140b34150b37160b39180c3c190c3e1b0c411c0c431e0c451f0c48210c4a230c4c240c4f260c51280b53290b552b0b572d0b592f0a5b310a5c320a5e340a5f3609613809623909633b09643d09653e0966400a67420a68440a68450a69470b6a490b6a4a0c6b4c0c6b4d0d6c4f0d6c510e6c520e6d540f6d550f6d57106e59106e5a116e5c126e5d126e5f136e61136e62146e64156e65156e67166e69166e6a176e6c186e6d186e6f196e71196e721a6e741a6e751b6e771c6d781c6d7a1d6d7c1d6d7d1e6d7f1e6c801f6c82206c84206b85216b87216b88226a8a226a8c23698d23698f24699025689225689326679526679727669827669a28659b29649d29649f2a63a02a63a22b62a32c61a52c60a62d60a82e5fa92e5eab2f5ead305dae305cb0315bb1325ab3325ab43359b63458b73557b93556ba3655bc3754bd3853bf3952c03a51c13a50c33b4fc43c4ec63d4dc73e4cc83f4bca404acb4149cc4248ce4347cf4446d04545d24644d34743d44842d54a41d74b3fd84c3ed94d3dda4e3cdb503bdd513ade5238df5337e05536e15635e25734e35933e45a31e55c30e65d2fe75e2ee8602de9612bea632aeb6429eb6628ec6726ed6925ee6a24ef6c23ef6e21f06f20f1711ff1731df2741cf3761bf37819f47918f57b17f57d15f67e14f68013f78212f78410f8850ff8870ef8890cf98b0bf98c0af98e09fa9008fa9207fa9407fb9606fb9706fb9906fb9b06fb9d07fc9f07fca108fca309fca50afca60cfca80dfcaa0ffcac11fcae12fcb014fcb216fcb418fbb61afbb81dfbba1ffbbc21fbbe23fac026fac228fac42afac62df9c72ff9c932f9cb35f8cd37f8cf3af7d13df7d340f6d543f6d746f5d949f5db4cf4dd4ff4df53f4e156f3e35af3e55df2e661f2e865f2ea69f1ec6df1ed71f1ef75f1f179f2f27df2f482f3f586f3f68af4f88ef5f992f6fa96f8fb9af9fc9dfafda1fcffa4\")),u=r(n.i(i.a)(\"0d088710078813078916078a19068c1b068d1d068e20068f2206902406912605912805922a05932c05942e05952f059631059733059735049837049938049a3a049a3c049b3e049c3f049c41049d43039e44039e46039f48039f4903a04b03a14c02a14e02a25002a25102a35302a35502a45601a45801a45901a55b01a55c01a65e01a66001a66100a76300a76400a76600a76700a86900a86a00a86c00a86e00a86f00a87100a87201a87401a87501a87701a87801a87a02a87b02a87d03a87e03a88004a88104a78305a78405a78606a68707a68808a68a09a58b0aa58d0ba58e0ca48f0da4910ea3920fa39410a29511a19613a19814a099159f9a169f9c179e9d189d9e199da01a9ca11b9ba21d9aa31e9aa51f99a62098a72197a82296aa2395ab2494ac2694ad2793ae2892b02991b12a90b22b8fb32c8eb42e8db52f8cb6308bb7318ab83289ba3388bb3488bc3587bd3786be3885bf3984c03a83c13b82c23c81c33d80c43e7fc5407ec6417dc7427cc8437bc9447aca457acb4679cc4778cc4977cd4a76ce4b75cf4c74d04d73d14e72d24f71d35171d45270d5536fd5546ed6556dd7566cd8576bd9586ada5a6ada5b69db5c68dc5d67dd5e66de5f65de6164df6263e06363e16462e26561e26660e3685fe4695ee56a5de56b5de66c5ce76e5be76f5ae87059e97158e97257ea7457eb7556eb7655ec7754ed7953ed7a52ee7b51ef7c51ef7e50f07f4ff0804ef1814df1834cf2844bf3854bf3874af48849f48948f58b47f58c46f68d45f68f44f79044f79143f79342f89441f89540f9973ff9983ef99a3efa9b3dfa9c3cfa9e3bfb9f3afba139fba238fca338fca537fca636fca835fca934fdab33fdac33fdae32fdaf31fdb130fdb22ffdb42ffdb52efeb72dfeb82cfeba2cfebb2bfebd2afebe2afec029fdc229fdc328fdc527fdc627fdc827fdca26fdcb26fccd25fcce25fcd025fcd225fbd324fbd524fbd724fad824fada24f9dc24f9dd25f8df25f8e125f7e225f7e425f6e626f6e826f5e926f5eb27f4ed27f3ee27f3f027f2f227f1f426f1f525f0f724f0f921\"))},function(t,e,n){\"use strict\";e.a=function(t){return function(){return t}}},function(t,e,n){\"use strict\";function r(){return new i}function i(){this._=\"@\"+(++o).toString(36)}e.a=r;var o=0;i.prototype=r.prototype={constructor:i,get:function(t){for(var e=this._;!(e in t);)if(!(t=t.parentNode))return;return t[e]},set:function(t,e){return t[this._]=e},remove:function(t){return this._ in t&&delete t[this._]},toString:function(){return this._}}},function(t,e,n){\"use strict\";var r=n(72),i=n(69);e.a=function(t){var e=n.i(r.a)();return e.changedTouches&&(e=e.changedTouches[0]),n.i(i.a)(t,e)}},function(t,e,n){\"use strict\";var r=n(7);e.a=function(t){return\"string\"==typeof t?new r.b([[document.querySelector(t)]],[document.documentElement]):new r.b([[t]],r.c)}},function(t,e,n){\"use strict\";var r=n(7);e.a=function(t){return\"string\"==typeof t?new r.b([document.querySelectorAll(t)],[document.documentElement]):new r.b([null==t?[]:t],r.c)}},function(t,e,n){\"use strict\";var r=n(66);e.a=function(t){var e=\"function\"==typeof t?t:n.i(r.a)(t);return this.select(function(){return this.appendChild(e.apply(this,arguments))})}},function(t,e,n){\"use strict\";function r(t){return function(){this.removeAttribute(t)}}function i(t){return function(){this.removeAttributeNS(t.space,t.local)}}function o(t,e){return function(){this.setAttribute(t,e)}}function a(t,e){return function(){this.setAttributeNS(t.space,t.local,e)}}function u(t,e){return function(){var n=e.apply(this,arguments);null==n?this.removeAttribute(t):this.setAttribute(t,n)}}function c(t,e){return function(){var n=e.apply(this,arguments);null==n?this.removeAttributeNS(t.space,t.local):this.setAttributeNS(t.space,t.local,n)}}var s=n(67);e.a=function(t,e){var l=n.i(s.a)(t);if(arguments.length<2){var f=this.node();return l.local?f.getAttributeNS(l.space,l.local):f.getAttribute(l)}return this.each((null==e?l.local?i:r:\"function\"==typeof e?l.local?c:u:l.local?a:o)(l,e))}},function(t,e,n){\"use strict\";e.a=function(){var t=arguments[0];return arguments[0]=this,t.apply(null,arguments),this}},function(t,e,n){\"use strict\";function r(t){return t.trim().split(/^|\\s+/)}function i(t){return t.classList||new o(t)}function o(t){this._node=t,this._names=r(t.getAttribute(\"class\")||\"\")}function a(t,e){for(var n=i(t),r=-1,o=e.length;++r<o;)n.add(e[r])}function u(t,e){for(var n=i(t),r=-1,o=e.length;++r<o;)n.remove(e[r])}function c(t){return function(){a(this,t)}}function s(t){return function(){u(this,t)}}function l(t,e){return function(){(e.apply(this,arguments)?a:u)(this,t)}}o.prototype={add:function(t){var e=this._names.indexOf(t);e<0&&(this._names.push(t),this._node.setAttribute(\"class\",this._names.join(\" \")))},remove:function(t){var e=this._names.indexOf(t);e>=0&&(this._names.splice(e,1),this._node.setAttribute(\"class\",this._names.join(\" \")))},contains:function(t){return this._names.indexOf(t)>=0}},e.a=function(t,e){var n=r(t+\"\");if(arguments.length<2){for(var o=i(this.node()),a=-1,u=n.length;++a<u;)if(!o.contains(n[a]))return!1;return!0}return this.each((\"function\"==typeof e?l:e?c:s)(n,e))}},function(t,e,n){\"use strict\";function r(t,e,n,r,i,o){for(var u,c=0,s=e.length,l=o.length;c<l;++c)(u=e[c])?(u.__data__=o[c],r[c]=u):n[c]=new a.b(t,o[c]);for(;c<s;++c)(u=e[c])&&(i[c]=u)}function i(t,e,n,r,i,o,u){var s,l,f,p={},h=e.length,d=o.length,v=new Array(h);for(s=0;s<h;++s)(l=e[s])&&(v[s]=f=c+u.call(l,l.__data__,s,e),f in p?i[s]=l:p[f]=l);for(s=0;s<d;++s)f=c+u.call(t,o[s],s,o),(l=p[f])?(r[s]=l,l.__data__=o[s],p[f]=null):n[s]=new a.b(t,o[s]);for(s=0;s<h;++s)(l=e[s])&&p[v[s]]===l&&(i[s]=l)}var o=n(7),a=n(131),u=n(246),c=\"$\";e.a=function(t,e){if(!t)return y=new Array(this.size()),d=-1,this.each(function(t){y[++d]=t}),y;var a=e?i:r,c=this._parents,s=this._groups;\"function\"!=typeof t&&(t=n.i(u.a)(t));for(var l=s.length,f=new Array(l),p=new Array(l),h=new Array(l),d=0;d<l;++d){var v=c[d],g=s[d],m=g.length,y=t.call(v,v&&v.__data__,d,c),_=y.length,b=p[d]=new Array(_),x=f[d]=new Array(_),w=h[d]=new Array(m);a(v,g,b,x,w,y,e);for(var C,M,k=0,E=0;k<_;++k)if(C=b[k]){for(k>=E&&(E=k+1);!(M=x[E])&&++E<_;);C._next=M||null}}return f=new o.b(f,c),f._enter=p,f._exit=h,f}},function(t,e,n){\"use strict\";e.a=function(t){return arguments.length?this.property(\"__data__\",t):this.node().__data__}},function(t,e,n){\"use strict\";function r(t,e,r){var i=n.i(a.a)(t),o=i.CustomEvent;o?o=new o(e,r):(o=i.document.createEvent(\"Event\"),r?(o.initEvent(e,r.bubbles,r.cancelable),o.detail=r.detail):o.initEvent(e,!1,!1)),t.dispatchEvent(o)}function i(t,e){return function(){return r(this,t,e)}}function o(t,e){return function(){return r(this,t,e.apply(this,arguments))}}var a=n(73);e.a=function(t,e){return this.each((\"function\"==typeof e?o:i)(t,e))}},function(t,e,n){\"use strict\";e.a=function(t){for(var e=this._groups,n=0,r=e.length;n<r;++n)for(var i,o=e[n],a=0,u=o.length;a<u;++a)(i=o[a])&&t.call(i,i.__data__,a,o);return this}},function(t,e,n){\"use strict\";e.a=function(){return!this.node()}},function(t,e,n){\"use strict\";var r=n(132),i=n(7);e.a=function(){return new i.b(this._exit||this._groups.map(r.a),this._parents)}},function(t,e,n){\"use strict\";var r=n(7),i=n(130);e.a=function(t){\"function\"!=typeof t&&(t=n.i(i.a)(t));for(var e=this._groups,o=e.length,a=new Array(o),u=0;u<o;++u)for(var c,s=e[u],l=s.length,f=a[u]=[],p=0;p<l;++p)(c=s[p])&&t.call(c,c.__data__,p,s)&&f.push(c);return new r.b(a,this._parents)}},function(t,e,n){\"use strict\";function r(){this.innerHTML=\"\"}function i(t){return function(){this.innerHTML=t}}function o(t){return function(){var e=t.apply(this,arguments);this.innerHTML=null==e?\"\":e}}e.a=function(t){return arguments.length?this.each(null==t?r:(\"function\"==typeof t?o:i)(t)):this.node().innerHTML}},function(t,e,n){\"use strict\";function r(){return null}var i=n(66),o=n(71);e.a=function(t,e){var a=\"function\"==typeof t?t:n.i(i.a)(t),u=null==e?r:\"function\"==typeof e?e:n.i(o.a)(e);return this.select(function(){return this.insertBefore(a.apply(this,arguments),u.apply(this,arguments)||null)})}},function(t,e,n){\"use strict\";function r(){this.previousSibling&&this.parentNode.insertBefore(this,this.parentNode.firstChild)}e.a=function(){return this.each(r)}},function(t,e,n){\"use strict\";var r=n(7);e.a=function(t){for(var e=this._groups,n=t._groups,i=e.length,o=n.length,a=Math.min(i,o),u=new Array(i),c=0;c<a;++c)for(var s,l=e[c],f=n[c],p=l.length,h=u[c]=new Array(p),d=0;d<p;++d)(s=l[d]||f[d])&&(h[d]=s);for(;c<i;++c)u[c]=e[c];return new r.b(u,this._parents)}},function(t,e,n){\"use strict\";e.a=function(){for(var t=this._groups,e=0,n=t.length;e<n;++e)for(var r=t[e],i=0,o=r.length;i<o;++i){var a=r[i];if(a)return a}return null}},function(t,e,n){\"use strict\";e.a=function(){var t=new Array(this.size()),e=-1;return this.each(function(){t[++e]=this}),t}},function(t,e,n){\"use strict\";e.a=function(){for(var t=this._groups,e=-1,n=t.length;++e<n;)for(var r,i=t[e],o=i.length-1,a=i[o];--o>=0;)(r=i[o])&&(a&&a!==r.nextSibling&&a.parentNode.insertBefore(r,a),a=r);return this}},function(t,e,n){\"use strict\";function r(t){return function(){delete this[t]}}function i(t,e){return function(){this[t]=e}}function o(t,e){return function(){var n=e.apply(this,arguments);null==n?delete this[t]:this[t]=n}}e.a=function(t,e){return arguments.length>1?this.each((null==e?r:\"function\"==typeof e?o:i)(t,e)):this.node()[t]}},function(t,e,n){\"use strict\";function r(){this.nextSibling&&this.parentNode.appendChild(this)}e.a=function(){return this.each(r)}},function(t,e,n){\"use strict\";function r(){var t=this.parentNode;t&&t.removeChild(this)}e.a=function(){return this.each(r)}},function(t,e,n){\"use strict\";var r=n(7),i=n(71);e.a=function(t){\"function\"!=typeof t&&(t=n.i(i.a)(t));for(var e=this._groups,o=e.length,a=new Array(o),u=0;u<o;++u)for(var c,s,l=e[u],f=l.length,p=a[u]=new Array(f),h=0;h<f;++h)(c=l[h])&&(s=t.call(c,c.__data__,h,l))&&(\"__data__\"in c&&(s.__data__=c.__data__),p[h]=s);return new r.b(a,this._parents)}},function(t,e,n){\"use strict\";var r=n(7),i=n(133);e.a=function(t){\"function\"!=typeof t&&(t=n.i(i.a)(t));for(var e=this._groups,o=e.length,a=[],u=[],c=0;c<o;++c)for(var s,l=e[c],f=l.length,p=0;p<f;++p)(s=l[p])&&(a.push(t.call(s,s.__data__,p,l)),u.push(s));return new r.b(a,u)}},function(t,e,n){\"use strict\";e.a=function(){var t=0;return this.each(function(){++t}),t}},function(t,e,n){\"use strict\";function r(t,e){return t<e?-1:t>e?1:t>=e?0:NaN}var i=n(7);e.a=function(t){function e(e,n){return e&&n?t(e.__data__,n.__data__):!e-!n}t||(t=r);for(var n=this._groups,o=n.length,a=new Array(o),u=0;u<o;++u){for(var c,s=n[u],l=s.length,f=a[u]=new Array(l),p=0;p<l;++p)(c=s[p])&&(f[p]=c);f.sort(e)}return new i.b(a,this._parents).order()}},function(t,e,n){\"use strict\";function r(t){return function(){this.style.removeProperty(t)}}function i(t,e,n){return function(){this.style.setProperty(t,e,n)}}function o(t,e,n){return function(){var r=e.apply(this,arguments);null==r?this.style.removeProperty(t):this.style.setProperty(t,r,n)}}var a=n(73);e.a=function(t,e,u){var c;return arguments.length>1?this.each((null==e?r:\"function\"==typeof e?o:i)(t,e,null==u?\"\":u)):n.i(a.a)(c=this.node()).getComputedStyle(c,null).getPropertyValue(t)}},function(t,e,n){\"use strict\";function r(){this.textContent=\"\"}function i(t){return function(){this.textContent=t}}function o(t){return function(){var e=t.apply(this,arguments);this.textContent=null==e?\"\":e}}e.a=function(t){return arguments.length?this.each(null==t?r:(\"function\"==typeof t?o:i)(t)):this.node().textContent}},function(t,e,n){\"use strict\";var r=n(72),i=n(69);e.a=function(t,e,o){arguments.length<3&&(o=e,e=n.i(r.a)().changedTouches);for(var a,u=0,c=e?e.length:0;u<c;++u)if((a=e[u]).identifier===o)return n.i(i.a)(t,a);return null}},function(t,e,n){\"use strict\";var r=n(72),i=n(69);e.a=function(t,e){null==e&&(e=n.i(r.a)().touches);for(var o=0,a=e?e.length:0,u=new Array(a);o<a;++o)u[o]=n.i(i.a)(t,e[o]);return u}},function(t,e,n){\"use strict\";function r(t){return t.innerRadius}function i(t){return t.outerRadius}function o(t){return t.startAngle}function a(t){return t.endAngle}function u(t){return t&&t.padAngle}function c(t){return t>=1?h.d:t<=-1?-h.d:Math.asin(t)}function s(t,e,n,r,i,o,a,u){var c=n-t,s=r-e,l=a-i,f=u-o,p=(l*(e-o)-f*(t-i))/(f*c-l*s);return[t+p*c,e+p*s]}function l(t,e,n,r,i,o,a){var u=t-n,c=e-r,s=(a?o:-o)/Math.sqrt(u*u+c*c),l=s*c,f=-s*u,p=t+l,h=e+f,d=n+l,v=r+f,g=(p+d)/2,m=(h+v)/2,y=d-p,_=v-h,b=y*y+_*_,x=i-o,w=p*v-d*h,C=(_<0?-1:1)*Math.sqrt(Math.max(0,x*x*b-w*w)),M=(w*_-y*C)/b,k=(-w*y-_*C)/b,E=(w*_+y*C)/b,T=(-w*y+_*C)/b,S=M-g,P=k-m,N=E-g,A=T-m;return S*S+P*P>N*N+A*A&&(M=E,k=T),{cx:M,cy:k,x01:-l,y01:-f,x11:M*(i/x-1),y11:k*(i/x-1)}}var f=n(44),p=n(19),h=n(35);e.a=function(){function t(){var t,r,i=+e.apply(this,arguments),o=+d.apply(this,arguments),a=m.apply(this,arguments)-h.d,u=y.apply(this,arguments)-h.d,p=Math.abs(u-a),x=u>a;if(b||(b=t=n.i(f.a)()),o<i&&(r=o,o=i,i=r),o>h.a)if(p>h.c-h.a)b.moveTo(o*Math.cos(a),o*Math.sin(a)),b.arc(0,0,o,a,u,!x),i>h.a&&(b.moveTo(i*Math.cos(u),i*Math.sin(u)),b.arc(0,0,i,u,a,x));else{var w,C,M=a,k=u,E=a,T=u,S=p,P=p,N=_.apply(this,arguments)/2,A=N>h.a&&(g?+g.apply(this,arguments):Math.sqrt(i*i+o*o)),O=Math.min(Math.abs(o-i)/2,+v.apply(this,arguments)),I=O,D=O;\n",
       "if(A>h.a){var R=c(A/i*Math.sin(N)),L=c(A/o*Math.sin(N));(S-=2*R)>h.a?(R*=x?1:-1,E+=R,T-=R):(S=0,E=T=(a+u)/2),(P-=2*L)>h.a?(L*=x?1:-1,M+=L,k-=L):(P=0,M=k=(a+u)/2)}var U=o*Math.cos(M),F=o*Math.sin(M),j=i*Math.cos(T),B=i*Math.sin(T);if(O>h.a){var W=o*Math.cos(k),V=o*Math.sin(k),z=i*Math.cos(E),H=i*Math.sin(E);if(p<h.b){var q=S>h.a?s(U,F,z,H,W,V,j,B):[j,B],Y=U-q[0],K=F-q[1],G=W-q[0],$=V-q[1],X=1/Math.sin(Math.acos((Y*G+K*$)/(Math.sqrt(Y*Y+K*K)*Math.sqrt(G*G+$*$)))/2),Z=Math.sqrt(q[0]*q[0]+q[1]*q[1]);I=Math.min(O,(i-Z)/(X-1)),D=Math.min(O,(o-Z)/(X+1))}}P>h.a?D>h.a?(w=l(z,H,U,F,o,D,x),C=l(W,V,j,B,o,D,x),b.moveTo(w.cx+w.x01,w.cy+w.y01),D<O?b.arc(w.cx,w.cy,D,Math.atan2(w.y01,w.x01),Math.atan2(C.y01,C.x01),!x):(b.arc(w.cx,w.cy,D,Math.atan2(w.y01,w.x01),Math.atan2(w.y11,w.x11),!x),b.arc(0,0,o,Math.atan2(w.cy+w.y11,w.cx+w.x11),Math.atan2(C.cy+C.y11,C.cx+C.x11),!x),b.arc(C.cx,C.cy,D,Math.atan2(C.y11,C.x11),Math.atan2(C.y01,C.x01),!x))):(b.moveTo(U,F),b.arc(0,0,o,M,k,!x)):b.moveTo(U,F),i>h.a&&S>h.a?I>h.a?(w=l(j,B,W,V,i,-I,x),C=l(U,F,z,H,i,-I,x),b.lineTo(w.cx+w.x01,w.cy+w.y01),I<O?b.arc(w.cx,w.cy,I,Math.atan2(w.y01,w.x01),Math.atan2(C.y01,C.x01),!x):(b.arc(w.cx,w.cy,I,Math.atan2(w.y01,w.x01),Math.atan2(w.y11,w.x11),!x),b.arc(0,0,i,Math.atan2(w.cy+w.y11,w.cx+w.x11),Math.atan2(C.cy+C.y11,C.cx+C.x11),x),b.arc(C.cx,C.cy,I,Math.atan2(C.y11,C.x11),Math.atan2(C.y01,C.x01),!x))):b.arc(0,0,i,T,E,x):b.lineTo(j,B)}else b.moveTo(0,0);if(b.closePath(),t)return b=null,t+\"\"||null}var e=r,d=i,v=n.i(p.a)(0),g=null,m=o,y=a,_=u,b=null;return t.centroid=function(){var t=(+e.apply(this,arguments)+ +d.apply(this,arguments))/2,n=(+m.apply(this,arguments)+ +y.apply(this,arguments))/2-h.b/2;return[Math.cos(n)*t,Math.sin(n)*t]},t.innerRadius=function(r){return arguments.length?(e=\"function\"==typeof r?r:n.i(p.a)(+r),t):e},t.outerRadius=function(e){return arguments.length?(d=\"function\"==typeof e?e:n.i(p.a)(+e),t):d},t.cornerRadius=function(e){return arguments.length?(v=\"function\"==typeof e?e:n.i(p.a)(+e),t):v},t.padRadius=function(e){return arguments.length?(g=null==e?null:\"function\"==typeof e?e:n.i(p.a)(+e),t):g},t.startAngle=function(e){return arguments.length?(m=\"function\"==typeof e?e:n.i(p.a)(+e),t):m},t.endAngle=function(e){return arguments.length?(y=\"function\"==typeof e?e:n.i(p.a)(+e),t):y},t.padAngle=function(e){return arguments.length?(_=\"function\"==typeof e?e:n.i(p.a)(+e),t):_},t.context=function(e){return arguments.length?(b=null==e?null:e,t):b},t}},function(t,e,n){\"use strict\";n.d(e,\"a\",function(){return r});var r=Array.prototype.slice},function(t,e,n){\"use strict\";function r(t){this._context=t}var i=n(49),o=n(46);r.prototype={areaStart:i.a,areaEnd:i.a,lineStart:function(){this._x0=this._x1=this._x2=this._x3=this._x4=this._y0=this._y1=this._y2=this._y3=this._y4=NaN,this._point=0},lineEnd:function(){switch(this._point){case 1:this._context.moveTo(this._x2,this._y2),this._context.closePath();break;case 2:this._context.moveTo((this._x2+2*this._x3)/3,(this._y2+2*this._y3)/3),this._context.lineTo((this._x3+2*this._x2)/3,(this._y3+2*this._y2)/3),this._context.closePath();break;case 3:this.point(this._x2,this._y2),this.point(this._x3,this._y3),this.point(this._x4,this._y4)}},point:function(t,e){switch(t=+t,e=+e,this._point){case 0:this._point=1,this._x2=t,this._y2=e;break;case 1:this._point=2,this._x3=t,this._y3=e;break;case 2:this._point=3,this._x4=t,this._y4=e,this._context.moveTo((this._x0+4*this._x1+t)/6,(this._y0+4*this._y1+e)/6);break;default:n.i(o.c)(this,t,e)}this._x0=this._x1,this._x1=t,this._y0=this._y1,this._y1=e}},e.a=function(t){return new r(t)}},function(t,e,n){\"use strict\";function r(t){this._context=t}var i=n(46);r.prototype={areaStart:function(){this._line=0},areaEnd:function(){this._line=NaN},lineStart:function(){this._x0=this._x1=this._y0=this._y1=NaN,this._point=0},lineEnd:function(){(this._line||0!==this._line&&3===this._point)&&this._context.closePath(),this._line=1-this._line},point:function(t,e){switch(t=+t,e=+e,this._point){case 0:this._point=1;break;case 1:this._point=2;break;case 2:this._point=3;var r=(this._x0+4*this._x1+t)/6,o=(this._y0+4*this._y1+e)/6;this._line?this._context.lineTo(r,o):this._context.moveTo(r,o);break;case 3:this._point=4;default:n.i(i.c)(this,t,e)}this._x0=this._x1,this._x1=t,this._y0=this._y1,this._y1=e}},e.a=function(t){return new r(t)}},function(t,e,n){\"use strict\";function r(t,e){this._basis=new i.b(t),this._beta=e}var i=n(46);r.prototype={lineStart:function(){this._x=[],this._y=[],this._basis.lineStart()},lineEnd:function(){var t=this._x,e=this._y,n=t.length-1;if(n>0)for(var r,i=t[0],o=e[0],a=t[n]-i,u=e[n]-o,c=-1;++c<=n;)r=c/n,this._basis.point(this._beta*t[c]+(1-this._beta)*(i+r*a),this._beta*e[c]+(1-this._beta)*(o+r*u));this._x=this._y=null,this._basis.lineEnd()},point:function(t,e){this._x.push(+t),this._y.push(+e)}},e.a=function t(e){function n(t){return 1===e?new i.b(t):new r(t,e)}return n.beta=function(e){return t(+e)},n}(.85)},function(t,e,n){\"use strict\";function r(t,e){this._context=t,this._alpha=e}var i=n(136),o=n(49),a=n(74);r.prototype={areaStart:o.a,areaEnd:o.a,lineStart:function(){this._x0=this._x1=this._x2=this._x3=this._x4=this._x5=this._y0=this._y1=this._y2=this._y3=this._y4=this._y5=NaN,this._l01_a=this._l12_a=this._l23_a=this._l01_2a=this._l12_2a=this._l23_2a=this._point=0},lineEnd:function(){switch(this._point){case 1:this._context.moveTo(this._x3,this._y3),this._context.closePath();break;case 2:this._context.lineTo(this._x3,this._y3),this._context.closePath();break;case 3:this.point(this._x3,this._y3),this.point(this._x4,this._y4),this.point(this._x5,this._y5)}},point:function(t,e){if(t=+t,e=+e,this._point){var r=this._x2-t,i=this._y2-e;this._l23_a=Math.sqrt(this._l23_2a=Math.pow(r*r+i*i,this._alpha))}switch(this._point){case 0:this._point=1,this._x3=t,this._y3=e;break;case 1:this._point=2,this._context.moveTo(this._x4=t,this._y4=e);break;case 2:this._point=3,this._x5=t,this._y5=e;break;default:n.i(a.b)(this,t,e)}this._l01_a=this._l12_a,this._l12_a=this._l23_a,this._l01_2a=this._l12_2a,this._l12_2a=this._l23_2a,this._x0=this._x1,this._x1=this._x2,this._x2=t,this._y0=this._y1,this._y1=this._y2,this._y2=e}},e.a=function t(e){function n(t){return e?new r(t,e):new i.b(t,0)}return n.alpha=function(e){return t(+e)},n}(.5)},function(t,e,n){\"use strict\";function r(t,e){this._context=t,this._alpha=e}var i=n(137),o=n(74);r.prototype={areaStart:function(){this._line=0},areaEnd:function(){this._line=NaN},lineStart:function(){this._x0=this._x1=this._x2=this._y0=this._y1=this._y2=NaN,this._l01_a=this._l12_a=this._l23_a=this._l01_2a=this._l12_2a=this._l23_2a=this._point=0},lineEnd:function(){(this._line||0!==this._line&&3===this._point)&&this._context.closePath(),this._line=1-this._line},point:function(t,e){if(t=+t,e=+e,this._point){var r=this._x2-t,i=this._y2-e;this._l23_a=Math.sqrt(this._l23_2a=Math.pow(r*r+i*i,this._alpha))}switch(this._point){case 0:this._point=1;break;case 1:this._point=2;break;case 2:this._point=3,this._line?this._context.lineTo(this._x2,this._y2):this._context.moveTo(this._x2,this._y2);break;case 3:this._point=4;default:n.i(o.b)(this,t,e)}this._l01_a=this._l12_a,this._l12_a=this._l23_a,this._l01_2a=this._l12_2a,this._l12_2a=this._l23_2a,this._x0=this._x1,this._x1=this._x2,this._x2=t,this._y0=this._y1,this._y1=this._y2,this._y2=e}},e.a=function t(e){function n(t){return e?new r(t,e):new i.b(t,0)}return n.alpha=function(e){return t(+e)},n}(.5)},function(t,e,n){\"use strict\";function r(t){this._context=t}var i=n(49);r.prototype={areaStart:i.a,areaEnd:i.a,lineStart:function(){this._point=0},lineEnd:function(){this._point&&this._context.closePath()},point:function(t,e){t=+t,e=+e,this._point?this._context.lineTo(t,e):(this._point=1,this._context.moveTo(t,e))}},e.a=function(t){return new r(t)}},function(t,e,n){\"use strict\";function r(t){return t<0?-1:1}function i(t,e,n){var i=t._x1-t._x0,o=e-t._x1,a=(t._y1-t._y0)/(i||o<0&&-0),u=(n-t._y1)/(o||i<0&&-0),c=(a*o+u*i)/(i+o);return(r(a)+r(u))*Math.min(Math.abs(a),Math.abs(u),.5*Math.abs(c))||0}function o(t,e){var n=t._x1-t._x0;return n?(3*(t._y1-t._y0)/n-e)/2:e}function a(t,e,n){var r=t._x0,i=t._y0,o=t._x1,a=t._y1,u=(o-r)/3;t._context.bezierCurveTo(r+u,i+u*e,o-u,a-u*n,o,a)}function u(t){this._context=t}function c(t){this._context=new s(t)}function s(t){this._context=t}function l(t){return new u(t)}function f(t){return new c(t)}e.a=l,e.b=f,u.prototype={areaStart:function(){this._line=0},areaEnd:function(){this._line=NaN},lineStart:function(){this._x0=this._x1=this._y0=this._y1=this._t0=NaN,this._point=0},lineEnd:function(){switch(this._point){case 2:this._context.lineTo(this._x1,this._y1);break;case 3:a(this,this._t0,o(this,this._t0))}(this._line||0!==this._line&&1===this._point)&&this._context.closePath(),this._line=1-this._line},point:function(t,e){var n=NaN;if(t=+t,e=+e,t!==this._x1||e!==this._y1){switch(this._point){case 0:this._point=1,this._line?this._context.lineTo(t,e):this._context.moveTo(t,e);break;case 1:this._point=2;break;case 2:this._point=3,a(this,o(this,n=i(this,t,e)),n);break;default:a(this,this._t0,n=i(this,t,e))}this._x0=this._x1,this._x1=t,this._y0=this._y1,this._y1=e,this._t0=n}}},(c.prototype=Object.create(u.prototype)).point=function(t,e){u.prototype.point.call(this,e,t)},s.prototype={moveTo:function(t,e){this._context.moveTo(e,t)},closePath:function(){this._context.closePath()},lineTo:function(t,e){this._context.lineTo(e,t)},bezierCurveTo:function(t,e,n,r,i,o){this._context.bezierCurveTo(e,t,r,n,o,i)}}},function(t,e,n){\"use strict\";function r(t){this._context=t}function i(t){var e,n,r=t.length-1,i=new Array(r),o=new Array(r),a=new Array(r);for(i[0]=0,o[0]=2,a[0]=t[0]+2*t[1],e=1;e<r-1;++e)i[e]=1,o[e]=4,a[e]=4*t[e]+2*t[e+1];for(i[r-1]=2,o[r-1]=7,a[r-1]=8*t[r-1]+t[r],e=1;e<r;++e)n=i[e]/o[e-1],o[e]-=n,a[e]-=n*a[e-1];for(i[r-1]=a[r-1]/o[r-1],e=r-2;e>=0;--e)i[e]=(a[e]-i[e+1])/o[e];for(o[r-1]=(t[r]+i[r-1])/2,e=0;e<r-1;++e)o[e]=2*t[e+1]-i[e+1];return[i,o]}r.prototype={areaStart:function(){this._line=0},areaEnd:function(){this._line=NaN},lineStart:function(){this._x=[],this._y=[]},lineEnd:function(){var t=this._x,e=this._y,n=t.length;if(n)if(this._line?this._context.lineTo(t[0],e[0]):this._context.moveTo(t[0],e[0]),2===n)this._context.lineTo(t[1],e[1]);else for(var r=i(t),o=i(e),a=0,u=1;u<n;++a,++u)this._context.bezierCurveTo(r[0][a],o[0][a],r[1][a],o[1][a],t[u],e[u]);(this._line||0!==this._line&&1===n)&&this._context.closePath(),this._line=1-this._line,this._x=this._y=null},point:function(t,e){this._x.push(+t),this._y.push(+e)}},e.a=function(t){return new r(t)}},function(t,e,n){\"use strict\";function r(t,e){this._context=t,this._t=e}function i(t){return new r(t,0)}function o(t){return new r(t,1)}e.c=i,e.b=o,r.prototype={areaStart:function(){this._line=0},areaEnd:function(){this._line=NaN},lineStart:function(){this._x=this._y=NaN,this._point=0},lineEnd:function(){0<this._t&&this._t<1&&2===this._point&&this._context.lineTo(this._x,this._y),(this._line||0!==this._line&&1===this._point)&&this._context.closePath(),this._line>=0&&(this._t=1-this._t,this._line=1-this._line)},point:function(t,e){switch(t=+t,e=+e,this._point){case 0:this._point=1,this._line?this._context.lineTo(t,e):this._context.moveTo(t,e);break;case 1:this._point=2;default:if(this._t<=0)this._context.lineTo(this._x,e),this._context.lineTo(t,e);else{var n=this._x*(1-this._t)+t*this._t;this._context.lineTo(n,this._y),this._context.lineTo(n,e)}}this._x=t,this._y=e}},e.a=function(t){return new r(t,.5)}},function(t,e,n){\"use strict\";e.a=function(t,e){return e<t?-1:e>t?1:e>=t?0:NaN}},function(t,e,n){\"use strict\";e.a=function(t){return t}},function(t,e,n){\"use strict\";var r=n(36);e.a=function(t,e){if((o=t.length)>0){for(var i,o,a,u=0,c=t[0].length;u<c;++u){for(a=i=0;i<o;++i)a+=t[i][u][1]||0;if(a)for(i=0;i<o;++i)t[i][u][1]/=a}n.i(r.a)(t,e)}}},function(t,e,n){\"use strict\";var r=n(36);e.a=function(t,e){if((i=t.length)>0){for(var i,o=0,a=t[e[0]],u=a.length;o<u;++o){for(var c=0,s=0;c<i;++c)s+=t[c][o][1]||0;a[o][1]+=a[o][0]=-s/2}n.i(r.a)(t,e)}}},function(t,e,n){\"use strict\";var r=n(36);e.a=function(t,e){if((a=t.length)>0&&(o=(i=t[e[0]]).length)>0){for(var i,o,a,u=0,c=1;c<o;++c){for(var s=0,l=0,f=0;s<a;++s){for(var p=t[e[s]],h=p[c][1]||0,d=p[c-1][1]||0,v=(h-d)/2,g=0;g<s;++g){var m=t[e[g]],y=m[c][1]||0,_=m[c-1][1]||0;v+=y-_}l+=h,f+=v*h}i[c-1][1]+=i[c-1][0]=u,l&&(u-=f/l)}i[c-1][1]+=i[c-1][0]=u,n.i(r.a)(t,e)}}},function(t,e,n){\"use strict\";var r=n(76);e.a=function(t){return n.i(r.a)(t).reverse()}},function(t,e,n){\"use strict\";var r=n(37),i=n(76);e.a=function(t){var e,o,a=t.length,u=t.map(i.b),c=n.i(r.a)(t).sort(function(t,e){return u[e]-u[t]}),s=0,l=0,f=[],p=[];for(e=0;e<a;++e)o=c[e],s<l?(s+=u[o],f.push(o)):(l+=u[o],p.push(o));return p.reverse().concat(f)}},function(t,e,n){\"use strict\";var r=n(37);e.a=function(t){return n.i(r.a)(t).reverse()}},function(t,e,n){\"use strict\";var r=n(19),i=n(291),o=n(292),a=n(35);e.a=function(){function t(t){var n,r,i,o,p,h=t.length,d=0,v=new Array(h),g=new Array(h),m=+s.apply(this,arguments),y=Math.min(a.c,Math.max(-a.c,l.apply(this,arguments)-m)),_=Math.min(Math.abs(y)/h,f.apply(this,arguments)),b=_*(y<0?-1:1);for(n=0;n<h;++n)(p=g[v[n]=n]=+e(t[n],n,t))>0&&(d+=p);for(null!=u?v.sort(function(t,e){return u(g[t],g[e])}):null!=c&&v.sort(function(e,n){return c(t[e],t[n])}),n=0,i=d?(y-h*b)/d:0;n<h;++n,m=o)r=v[n],p=g[r],o=m+(p>0?p*i:0)+b,g[r]={data:t[r],index:n,value:p,startAngle:m,endAngle:o,padAngle:_};return g}var e=o.a,u=i.a,c=null,s=n.i(r.a)(0),l=n.i(r.a)(a.c),f=n.i(r.a)(0);return t.value=function(i){return arguments.length?(e=\"function\"==typeof i?i:n.i(r.a)(+i),t):e},t.sortValues=function(e){return arguments.length?(u=e,c=null,t):u},t.sort=function(e){return arguments.length?(c=e,u=null,t):c},t.startAngle=function(e){return arguments.length?(s=\"function\"==typeof e?e:n.i(r.a)(+e),t):s},t.endAngle=function(e){return arguments.length?(l=\"function\"==typeof e?e:n.i(r.a)(+e),t):l},t.padAngle=function(e){return arguments.length?(f=\"function\"==typeof e?e:n.i(r.a)(+e),t):f},t}},function(t,e,n){\"use strict\";var r=n(138),i=n(135),o=n(140);e.a=function(){var t=n.i(i.a)().curve(r.b),e=t.curve,a=t.lineX0,u=t.lineX1,c=t.lineY0,s=t.lineY1;return t.angle=t.x,delete t.x,t.startAngle=t.x0,delete t.x0,t.endAngle=t.x1,delete t.x1,t.radius=t.y,delete t.y,t.innerRadius=t.y0,delete t.y0,t.outerRadius=t.y1,delete t.y1,t.lineStartAngle=function(){return n.i(o.b)(a())},delete t.lineX0,t.lineEndAngle=function(){return n.i(o.b)(u())},delete t.lineX1,t.lineInnerRadius=function(){return n.i(o.b)(c())},delete t.lineY0,t.lineOuterRadius=function(){return n.i(o.b)(s())},delete t.lineY1,t.curve=function(t){return arguments.length?e(n.i(r.a)(t)):e()._curve},t}},function(t,e,n){\"use strict\";function r(t,e){return t[e]}var i=n(281),o=n(19),a=n(36),u=n(37);e.a=function(){function t(t){var n,r,i=e.apply(this,arguments),o=t.length,a=i.length,u=new Array(a);for(n=0;n<a;++n){for(var f,p=i[n],h=u[n]=new Array(o),d=0;d<o;++d)h[d]=f=[0,+l(t[d],p,d,t)],f.data=t[d];h.key=p}for(n=0,r=c(u);n<a;++n)u[r[n]].index=n;return s(u,r),u}var e=n.i(o.a)([]),c=u.a,s=a.a,l=r;return t.keys=function(r){return arguments.length?(e=\"function\"==typeof r?r:n.i(o.a)(i.a.call(r)),t):e},t.value=function(e){return arguments.length?(l=\"function\"==typeof e?e:n.i(o.a)(+e),t):l},t.order=function(e){return arguments.length?(c=null==e?u.a:\"function\"==typeof e?e:n.i(o.a)(i.a.call(e)),t):c},t.offset=function(e){return arguments.length?(s=null==e?a.a:e,t):s},t}},function(t,e,n){\"use strict\";var r=n(44),i=n(141),o=n(142),a=n(143),u=n(145),c=n(144),s=n(146),l=n(147),f=n(19);n.d(e,\"b\",function(){return p});var p=[i.a,o.a,a.a,c.a,u.a,s.a,l.a];e.a=function(){function t(){var t;if(a||(a=t=n.i(r.a)()),e.apply(this,arguments).draw(a,+o.apply(this,arguments)),t)return a=null,t+\"\"||null}var e=n.i(f.a)(i.a),o=n.i(f.a)(64),a=null;return t.type=function(r){return arguments.length?(e=\"function\"==typeof r?r:n.i(f.a)(r),t):e},t.size=function(e){return arguments.length?(o=\"function\"==typeof e?e:n.i(f.a)(+e),t):o},t.context=function(e){return arguments.length?(a=null==e?null:e,t):a},t}},function(t,e,n){\"use strict\";function r(t){var e=new Date(t);return isNaN(e)?null:e}var i=n(148),o=n(78),a=+new Date(\"2000-01-01T00:00:00.000Z\")?r:n.i(o.e)(i.b);e.a=a},function(t,e,n){\"use strict\";var r=n(5),i=n(13),o=n.i(r.a)(function(t){t.setHours(0,0,0,0)},function(t,e){t.setDate(t.getDate()+e)},function(t,e){return(e-t-(e.getTimezoneOffset()-t.getTimezoneOffset())*i.d)/i.b},function(t){return t.getDate()-1});e.a=o;o.range},function(t,e,n){\"use strict\";var r=n(5),i=n(13),o=n.i(r.a)(function(t){var e=t.getTimezoneOffset()*i.d%i.c;e<0&&(e+=i.c),t.setTime(Math.floor((+t-e)/i.c)*i.c+e)},function(t,e){t.setTime(+t+e*i.c)},function(t,e){return(e-t)/i.c},function(t){return t.getHours()});e.a=o;o.range},function(t,e,n){\"use strict\";var r=n(5),i=n.i(r.a)(function(){},function(t,e){t.setTime(+t+e)},function(t,e){return e-t});i.every=function(t){return t=Math.floor(t),isFinite(t)&&t>0?t>1?n.i(r.a)(function(e){e.setTime(Math.floor(e/t)*t)},function(e,n){e.setTime(+e+n*t)},function(e,n){return(n-e)/t}):i:null},e.a=i;i.range},function(t,e,n){\"use strict\";var r=n(5),i=n(13),o=n.i(r.a)(function(t){t.setTime(Math.floor(t/i.d)*i.d)},function(t,e){t.setTime(+t+e*i.d)},function(t,e){return(e-t)/i.d},function(t){return t.getMinutes()});e.a=o;o.range},function(t,e,n){\"use strict\";var r=n(5),i=n.i(r.a)(function(t){t.setDate(1),t.setHours(0,0,0,0)},function(t,e){t.setMonth(t.getMonth()+e)},function(t,e){return e.getMonth()-t.getMonth()+12*(e.getFullYear()-t.getFullYear())},function(t){return t.getMonth()});e.a=i;i.range},function(t,e,n){\"use strict\";var r=n(5),i=n(13),o=n.i(r.a)(function(t){t.setTime(Math.floor(t/i.e)*i.e)},function(t,e){t.setTime(+t+e*i.e)},function(t,e){return(e-t)/i.e},function(t){return t.getUTCSeconds()});e.a=o;o.range},function(t,e,n){\"use strict\";var r=n(5),i=n(13),o=n.i(r.a)(function(t){t.setUTCHours(0,0,0,0)},function(t,e){t.setUTCDate(t.getUTCDate()+e)},function(t,e){return(e-t)/i.b},function(t){return t.getUTCDate()-1});e.a=o;o.range},function(t,e,n){\"use strict\";var r=n(5),i=n(13),o=n.i(r.a)(function(t){t.setUTCMinutes(0,0,0)},function(t,e){t.setTime(+t+e*i.c)},function(t,e){return(e-t)/i.c},function(t){return t.getUTCHours()});e.a=o;o.range},function(t,e,n){\"use strict\";var r=n(5),i=n(13),o=n.i(r.a)(function(t){t.setUTCSeconds(0,0)},function(t,e){t.setTime(+t+e*i.d)},function(t,e){return(e-t)/i.d},function(t){return t.getUTCMinutes()});e.a=o;o.range},function(t,e,n){\"use strict\";var r=n(5),i=n.i(r.a)(function(t){t.setUTCDate(1),t.setUTCHours(0,0,0,0)},function(t,e){t.setUTCMonth(t.getUTCMonth()+e)},function(t,e){return e.getUTCMonth()-t.getUTCMonth()+12*(e.getUTCFullYear()-t.getUTCFullYear())},function(t){return t.getUTCMonth()});e.a=i;i.range},function(t,e,n){\"use strict\";function r(t){return n.i(i.a)(function(e){e.setUTCDate(e.getUTCDate()-(e.getUTCDay()+7-t)%7),e.setUTCHours(0,0,0,0)},function(t,e){t.setUTCDate(t.getUTCDate()+7*e)},function(t,e){return(e-t)/o.a})}var i=n(5),o=n(13);n.d(e,\"a\",function(){return a}),n.d(e,\"b\",function(){return u});var a=r(0),u=r(1),c=r(2),s=r(3),l=r(4),f=r(5),p=r(6);a.range,u.range,c.range,s.range,l.range,f.range,p.range},function(t,e,n){\"use strict\";var r=n(5),i=n.i(r.a)(function(t){t.setUTCMonth(0,1),t.setUTCHours(0,0,0,0)},function(t,e){t.setUTCFullYear(t.getUTCFullYear()+e)},function(t,e){return e.getUTCFullYear()-t.getUTCFullYear()},function(t){return t.getUTCFullYear()});i.every=function(t){return isFinite(t=Math.floor(t))&&t>0?n.i(r.a)(function(e){e.setUTCFullYear(Math.floor(e.getUTCFullYear()/t)*t),e.setUTCMonth(0,1),e.setUTCHours(0,0,0,0)},function(e,n){e.setUTCFullYear(e.getUTCFullYear()+n*t)}):null},e.a=i;i.range},function(t,e,n){\"use strict\";function r(t){return n.i(i.a)(function(e){e.setDate(e.getDate()-(e.getDay()+7-t)%7),e.setHours(0,0,0,0)},function(t,e){t.setDate(t.getDate()+7*e)},function(t,e){return(e-t-(e.getTimezoneOffset()-t.getTimezoneOffset())*o.d)/o.a})}var i=n(5),o=n(13);n.d(e,\"a\",function(){return a}),n.d(e,\"b\",function(){return u});var a=r(0),u=r(1),c=r(2),s=r(3),l=r(4),f=r(5),p=r(6);a.range,u.range,c.range,s.range,l.range,f.range,p.range},function(t,e,n){\"use strict\";var r=n(5),i=n.i(r.a)(function(t){t.setMonth(0,1),t.setHours(0,0,0,0)},function(t,e){t.setFullYear(t.getFullYear()+e)},function(t,e){return e.getFullYear()-t.getFullYear()},function(t){return t.getFullYear()});i.every=function(t){return isFinite(t=Math.floor(t))&&t>0?n.i(r.a)(function(e){e.setFullYear(Math.floor(e.getFullYear()/t)*t),e.setMonth(0,1),e.setHours(0,0,0,0)},function(e,n){e.setFullYear(e.getFullYear()+n*t)}):null},e.a=i;i.range},function(t,e,n){\"use strict\";function r(t){return t.replace(i,function(t,e){return e.toUpperCase()})}var i=/-(.)/g;t.exports=r},function(t,e,n){\"use strict\";function r(t){return i(t.replace(o,\"ms-\"))}var i=n(318),o=/^-ms-/;t.exports=r},function(t,e,n){\"use strict\";function r(t,e){return!(!t||!e)&&(t===e||!i(t)&&(i(e)?r(t,e.parentNode):\"contains\"in t?t.contains(e):!!t.compareDocumentPosition&&!!(16&t.compareDocumentPosition(e))))}var i=n(328);t.exports=r},function(t,e,n){\"use strict\";function r(t){var e=t.length;if(Array.isArray(t)||\"object\"!=typeof t&&\"function\"!=typeof t?a(!1):void 0,\"number\"!=typeof e?a(!1):void 0,0===e||e-1 in t?void 0:a(!1),\"function\"==typeof t.callee?a(!1):void 0,t.hasOwnProperty)try{return Array.prototype.slice.call(t)}catch(t){}for(var n=Array(e),r=0;r<e;r++)n[r]=t[r];return n}function i(t){return!!t&&(\"object\"==typeof t||\"function\"==typeof t)&&\"length\"in t&&!(\"setInterval\"in t)&&\"number\"!=typeof t.nodeType&&(Array.isArray(t)||\"callee\"in t||\"item\"in t)}function o(t){return i(t)?Array.isArray(t)?t.slice():r(t):[t]}var a=n(0);t.exports=o},function(t,e,n){\"use strict\";function r(t){var e=t.match(l);return e&&e[1].toLowerCase()}function i(t,e){var n=s;s?void 0:c(!1);var i=r(t),o=i&&u(i);if(o){n.innerHTML=o[1]+t+o[2];for(var l=o[0];l--;)n=n.lastChild}else n.innerHTML=t;var f=n.getElementsByTagName(\"script\");f.length&&(e?void 0:c(!1),a(f).forEach(e));for(var p=Array.from(n.childNodes);n.lastChild;)n.removeChild(n.lastChild);return p}var o=n(6),a=n(321),u=n(323),c=n(0),s=o.canUseDOM?document.createElement(\"div\"):null,l=/^\\s*<(\\w+)/;t.exports=i},function(t,e,n){\"use strict\";function r(t){return a?void 0:o(!1),p.hasOwnProperty(t)||(t=\"*\"),u.hasOwnProperty(t)||(\"*\"===t?a.innerHTML=\"<link />\":a.innerHTML=\"<\"+t+\"></\"+t+\">\",u[t]=!a.firstChild),u[t]?p[t]:null}var i=n(6),o=n(0),a=i.canUseDOM?document.createElement(\"div\"):null,u={},c=[1,'<select multiple=\"true\">',\"</select>\"],s=[1,\"<table>\",\"</table>\"],l=[3,\"<table><tbody><tr>\",\"</tr></tbody></table>\"],f=[1,'<svg xmlns=\"http://www.w3.org/2000/svg\">',\"</svg>\"],p={\"*\":[1,\"?<div>\",\"</div>\"],area:[1,\"<map>\",\"</map>\"],col:[2,\"<table><tbody></tbody><colgroup>\",\"</colgroup></table>\"],legend:[1,\"<fieldset>\",\"</fieldset>\"],param:[1,\"<object>\",\"</object>\"],tr:[2,\"<table><tbody>\",\"</tbody></table>\"],optgroup:c,option:c,caption:s,colgroup:s,tbody:s,tfoot:s,thead:s,td:l,th:l},h=[\"circle\",\"clipPath\",\"defs\",\"ellipse\",\"g\",\"image\",\"line\",\"linearGradient\",\"mask\",\"path\",\"pattern\",\"polygon\",\"polyline\",\"radialGradient\",\"rect\",\"stop\",\"text\",\"tspan\"];h.forEach(function(t){p[t]=f,u[t]=!0}),t.exports=r},function(t,e,n){\"use strict\";function r(t){return t===window?{x:window.pageXOffset||document.documentElement.scrollLeft,y:window.pageYOffset||document.documentElement.scrollTop}:{x:t.scrollLeft,y:t.scrollTop}}t.exports=r},function(t,e,n){\"use strict\";function r(t){return t.replace(i,\"-$1\").toLowerCase()}var i=/([A-Z])/g;t.exports=r},function(t,e,n){\"use strict\";function r(t){return i(t).replace(o,\"-ms-\")}var i=n(325),o=/^ms-/;t.exports=r},function(t,e,n){\"use strict\";function r(t){return!(!t||!(\"function\"==typeof Node?t instanceof Node:\"object\"==typeof t&&\"number\"==typeof t.nodeType&&\"string\"==typeof t.nodeName))}t.exports=r},function(t,e,n){\"use strict\";function r(t){return i(t)&&3==t.nodeType}var i=n(327);t.exports=r},function(t,e,n){\"use strict\";var r=function(t){var e;for(e in t)if(t.hasOwnProperty(e))return e;return null};t.exports=r},function(t,e,n){\"use strict\";function r(t){var e={};return function(n){return e.hasOwnProperty(n)||(e[n]=t.call(this,n)),e[n]}}t.exports=r},function(t,e,n){\"use strict\";var r={Properties:{\"aria-current\":0,\"aria-details\":0,\"aria-disabled\":0,\"aria-hidden\":0,\"aria-invalid\":0,\"aria-keyshortcuts\":0,\"aria-label\":0,\"aria-roledescription\":0,\"aria-autocomplete\":0,\"aria-checked\":0,\"aria-expanded\":0,\"aria-haspopup\":0,\"aria-level\":0,\"aria-modal\":0,\"aria-multiline\":0,\"aria-multiselectable\":0,\"aria-orientation\":0,\"aria-placeholder\":0,\"aria-pressed\":0,\"aria-readonly\":0,\"aria-required\":0,\"aria-selected\":0,\"aria-sort\":0,\"aria-valuemax\":0,\"aria-valuemin\":0,\"aria-valuenow\":0,\"aria-valuetext\":0,\"aria-atomic\":0,\"aria-busy\":0,\"aria-live\":0,\"aria-relevant\":0,\"aria-dropeffect\":0,\"aria-grabbed\":0,\"aria-activedescendant\":0,\"aria-colcount\":0,\"aria-colindex\":0,\"aria-colspan\":0,\"aria-controls\":0,\"aria-describedby\":0,\"aria-errormessage\":0,\"aria-flowto\":0,\"aria-labelledby\":0,\"aria-owns\":0,\"aria-posinset\":0,\"aria-rowcount\":0,\"aria-rowindex\":0,\"aria-rowspan\":0,\"aria-setsize\":0},DOMAttributeNames:{},DOMPropertyNames:{}};t.exports=r},function(t,e,n){\"use strict\";var r=n(4),i=n(151),o={focusDOMComponent:function(){i(r.getNodeFromInstance(this))}};t.exports=o},function(t,e,n){\"use strict\";function r(){var t=window.opera;return\"object\"==typeof t&&\"function\"==typeof t.version&&parseInt(t.version(),10)<=12}function i(t){return(t.ctrlKey||t.altKey||t.metaKey)&&!(t.ctrlKey&&t.altKey)}function o(t){switch(t){case\"topCompositionStart\":return E.compositionStart;case\"topCompositionEnd\":return E.compositionEnd;case\"topCompositionUpdate\":return E.compositionUpdate}}function a(t,e){return\"topKeyDown\"===t&&e.keyCode===_}function u(t,e){switch(t){case\"topKeyUp\":return y.indexOf(e.keyCode)!==-1;case\"topKeyDown\":return e.keyCode!==_;case\"topKeyPress\":case\"topMouseDown\":case\"topBlur\":return!0;default:return!1}}function c(t){var e=t.detail;return\"object\"==typeof e&&\"data\"in e?e.data:null}function s(t,e,n,r){var i,s;if(b?i=o(t):S?u(t,n)&&(i=E.compositionEnd):a(t,n)&&(i=E.compositionStart),!i)return null;C&&(S||i!==E.compositionStart?i===E.compositionEnd&&S&&(s=S.getData()):S=v.getPooled(r));var l=g.getPooled(i,e,n,r);if(s)l.data=s;else{var f=c(n);null!==f&&(l.data=f)}return h.accumulateTwoPhaseDispatches(l),l}function l(t,e){switch(t){case\"topCompositionEnd\":return c(e);case\"topKeyPress\":var n=e.which;return n!==M?null:(T=!0,k);case\"topTextInput\":var r=e.data;return r===k&&T?null:r;default:return null}}function f(t,e){if(S){if(\"topCompositionEnd\"===t||!b&&u(t,e)){var n=S.getData();return v.release(S),S=null,n}return null}switch(t){case\"topPaste\":return null;case\"topKeyPress\":return e.which&&!i(e)?String.fromCharCode(e.which):null;case\"topCompositionEnd\":return C?null:e.data;default:return null}}function p(t,e,n,r){var i;if(i=w?l(t,n):f(t,n),!i)return null;var o=m.getPooled(E.beforeInput,e,n,r);return o.data=i,h.accumulateTwoPhaseDispatches(o),o}var h=n(23),d=n(6),v=n(340),g=n(377),m=n(380),y=[9,13,27,32],_=229,b=d.canUseDOM&&\"CompositionEvent\"in window,x=null;d.canUseDOM&&\"documentMode\"in document&&(x=document.documentMode);var w=d.canUseDOM&&\"TextEvent\"in window&&!x&&!r(),C=d.canUseDOM&&(!b||x&&x>8&&x<=11),M=32,k=String.fromCharCode(M),E={beforeInput:{phasedRegistrationNames:{bubbled:\"onBeforeInput\",captured:\"onBeforeInputCapture\"},dependencies:[\"topCompositionEnd\",\"topKeyPress\",\"topTextInput\",\"topPaste\"]},compositionEnd:{phasedRegistrationNames:{bubbled:\"onCompositionEnd\",captured:\"onCompositionEndCapture\"},dependencies:[\"topBlur\",\"topCompositionEnd\",\"topKeyDown\",\"topKeyPress\",\"topKeyUp\",\"topMouseDown\"]},compositionStart:{phasedRegistrationNames:{bubbled:\"onCompositionStart\",captured:\"onCompositionStartCapture\"},dependencies:[\"topBlur\",\"topCompositionStart\",\"topKeyDown\",\"topKeyPress\",\"topKeyUp\",\"topMouseDown\"]},compositionUpdate:{phasedRegistrationNames:{bubbled:\"onCompositionUpdate\",captured:\"onCompositionUpdateCapture\"},dependencies:[\"topBlur\",\"topCompositionUpdate\",\"topKeyDown\",\"topKeyPress\",\"topKeyUp\",\"topMouseDown\"]}},T=!1,S=null,P={eventTypes:E,extractEvents:function(t,e,n,r){return[s(t,e,n,r),p(t,e,n,r)]}};t.exports=P},function(t,e,n){\"use strict\";var r=n(154),i=n(6),o=(n(9),n(319),n(386)),a=n(326),u=n(330),c=(n(1),u(function(t){return a(t)})),s=!1,l=\"cssFloat\";if(i.canUseDOM){var f=document.createElement(\"div\").style;try{f.font=\"\"}catch(t){s=!0}void 0===document.documentElement.style.cssFloat&&(l=\"styleFloat\")}var p={createMarkupForStyles:function(t,e){var n=\"\";for(var r in t)if(t.hasOwnProperty(r)){var i=t[r];null!=i&&(n+=c(r)+\":\",n+=o(r,i,e)+\";\")}return n||null},setValueForStyles:function(t,e,n){var i=t.style;for(var a in e)if(e.hasOwnProperty(a)){var u=o(a,e[a],n);if(\"float\"!==a&&\"cssFloat\"!==a||(a=l),u)i[a]=u;else{var c=s&&r.shorthandPropertyExpansions[a];if(c)for(var f in c)i[f]=\"\";else i[a]=\"\"}}}};t.exports=p},function(t,e,n){\"use strict\";function r(t){var e=t.nodeName&&t.nodeName.toLowerCase();return\"select\"===e||\"input\"===e&&\"file\"===t.type}function i(t){var e=C.getPooled(T.change,P,t,M(t));_.accumulateTwoPhaseDispatches(e),w.batchedUpdates(o,e)}function o(t){y.enqueueEvents(t),y.processEventQueue(!1)}function a(t,e){S=t,P=e,S.attachEvent(\"onchange\",i)}function u(){S&&(S.detachEvent(\"onchange\",i),S=null,P=null)}function c(t,e){if(\"topChange\"===t)return e}function s(t,e,n){\"topFocus\"===t?(u(),a(e,n)):\"topBlur\"===t&&u()}function l(t,e){S=t,P=e,N=t.value,A=Object.getOwnPropertyDescriptor(t.constructor.prototype,\"value\"),Object.defineProperty(S,\"value\",D),S.attachEvent?S.attachEvent(\"onpropertychange\",p):S.addEventListener(\"propertychange\",p,!1)}function f(){S&&(delete S.value,S.detachEvent?S.detachEvent(\"onpropertychange\",p):S.removeEventListener(\"propertychange\",p,!1),S=null,P=null,N=null,A=null)}function p(t){if(\"value\"===t.propertyName){var e=t.srcElement.value;e!==N&&(N=e,i(t))}}function h(t,e){if(\"topInput\"===t)return e}function d(t,e,n){\"topFocus\"===t?(f(),l(e,n)):\"topBlur\"===t&&f()}function v(t,e){if((\"topSelectionChange\"===t||\"topKeyUp\"===t||\"topKeyDown\"===t)&&S&&S.value!==N)return N=S.value,P}function g(t){return t.nodeName&&\"input\"===t.nodeName.toLowerCase()&&(\"checkbox\"===t.type||\"radio\"===t.type)}function m(t,e){if(\"topClick\"===t)return e}var y=n(22),_=n(23),b=n(6),x=n(4),w=n(11),C=n(14),M=n(93),k=n(94),E=n(170),T={change:{phasedRegistrationNames:{bubbled:\"onChange\",captured:\"onChangeCapture\"},dependencies:[\"topBlur\",\"topChange\",\"topClick\",\"topFocus\",\"topInput\",\"topKeyDown\",\"topKeyUp\",\"topSelectionChange\"]}},S=null,P=null,N=null,A=null,O=!1;b.canUseDOM&&(O=k(\"change\")&&(!document.documentMode||document.documentMode>8));var I=!1;b.canUseDOM&&(I=k(\"input\")&&(!document.documentMode||document.documentMode>11));var D={get:function(){return A.get.call(this)},set:function(t){N=\"\"+t,A.set.call(this,t)}},R={eventTypes:T,extractEvents:function(t,e,n,i){var o,a,u=e?x.getNodeFromInstance(e):window;if(r(u)?O?o=c:a=s:E(u)?I?o=h:(o=v,a=d):g(u)&&(o=m),o){var l=o(t,e);if(l){var f=C.getPooled(T.change,l,n,i);return f.type=\"change\",_.accumulateTwoPhaseDispatches(f),f}}a&&a(t,u,e)}};t.exports=R},function(t,e,n){\"use strict\";var r=n(2),i=n(20),o=n(6),a=n(322),u=n(8),c=(n(0),{dangerouslyReplaceNodeWithMarkup:function(t,e){if(o.canUseDOM?void 0:r(\"56\"),e?void 0:r(\"57\"),\"HTML\"===t.nodeName?r(\"58\"):void 0,\"string\"==typeof e){var n=a(e,u)[0];t.parentNode.replaceChild(n,t)}else i.replaceChildWithTree(t,e)}});t.exports=c},function(t,e,n){\"use strict\";var r=[\"ResponderEventPlugin\",\"SimpleEventPlugin\",\"TapEventPlugin\",\"EnterLeaveEventPlugin\",\"ChangeEventPlugin\",\"SelectEventPlugin\",\"BeforeInputEventPlugin\"];t.exports=r},function(t,e,n){\"use strict\";var r=n(23),i=n(4),o=n(52),a={mouseEnter:{registrationName:\"onMouseEnter\",dependencies:[\"topMouseOut\",\"topMouseOver\"]},mouseLeave:{registrationName:\"onMouseLeave\",dependencies:[\"topMouseOut\",\"topMouseOver\"]}},u={eventTypes:a,extractEvents:function(t,e,n,u){if(\"topMouseOver\"===t&&(n.relatedTarget||n.fromElement))return null;\n",
       "if(\"topMouseOut\"!==t&&\"topMouseOver\"!==t)return null;var c;if(u.window===u)c=u;else{var s=u.ownerDocument;c=s?s.defaultView||s.parentWindow:window}var l,f;if(\"topMouseOut\"===t){l=e;var p=n.relatedTarget||n.toElement;f=p?i.getClosestInstanceFromNode(p):null}else l=null,f=e;if(l===f)return null;var h=null==l?c:i.getNodeFromInstance(l),d=null==f?c:i.getNodeFromInstance(f),v=o.getPooled(a.mouseLeave,l,n,u);v.type=\"mouseleave\",v.target=h,v.relatedTarget=d;var g=o.getPooled(a.mouseEnter,f,n,u);return g.type=\"mouseenter\",g.target=d,g.relatedTarget=h,r.accumulateEnterLeaveDispatches(v,g,l,f),[v,g]}};t.exports=u},function(t,e,n){\"use strict\";var r={topAbort:null,topAnimationEnd:null,topAnimationIteration:null,topAnimationStart:null,topBlur:null,topCanPlay:null,topCanPlayThrough:null,topChange:null,topClick:null,topCompositionEnd:null,topCompositionStart:null,topCompositionUpdate:null,topContextMenu:null,topCopy:null,topCut:null,topDoubleClick:null,topDrag:null,topDragEnd:null,topDragEnter:null,topDragExit:null,topDragLeave:null,topDragOver:null,topDragStart:null,topDrop:null,topDurationChange:null,topEmptied:null,topEncrypted:null,topEnded:null,topError:null,topFocus:null,topInput:null,topInvalid:null,topKeyDown:null,topKeyPress:null,topKeyUp:null,topLoad:null,topLoadedData:null,topLoadedMetadata:null,topLoadStart:null,topMouseDown:null,topMouseMove:null,topMouseOut:null,topMouseOver:null,topMouseUp:null,topPaste:null,topPause:null,topPlay:null,topPlaying:null,topProgress:null,topRateChange:null,topReset:null,topScroll:null,topSeeked:null,topSeeking:null,topSelectionChange:null,topStalled:null,topSubmit:null,topSuspend:null,topTextInput:null,topTimeUpdate:null,topTouchCancel:null,topTouchEnd:null,topTouchMove:null,topTouchStart:null,topTransitionEnd:null,topVolumeChange:null,topWaiting:null,topWheel:null},i={topLevelTypes:r};t.exports=i},function(t,e,n){\"use strict\";function r(t){this._root=t,this._startText=this.getText(),this._fallbackText=null}var i=n(3),o=n(17),a=n(168);i(r.prototype,{destructor:function(){this._root=null,this._startText=null,this._fallbackText=null},getText:function(){return\"value\"in this._root?this._root.value:this._root[a()]},getData:function(){if(this._fallbackText)return this._fallbackText;var t,e,n=this._startText,r=n.length,i=this.getText(),o=i.length;for(t=0;t<r&&n[t]===i[t];t++);var a=r-t;for(e=1;e<=a&&n[r-e]===i[o-e];e++);var u=e>1?1-e:void 0;return this._fallbackText=i.slice(t,u),this._fallbackText}}),o.addPoolingTo(r),t.exports=r},function(t,e,n){\"use strict\";var r=n(21),i=r.injection.MUST_USE_PROPERTY,o=r.injection.HAS_BOOLEAN_VALUE,a=r.injection.HAS_NUMERIC_VALUE,u=r.injection.HAS_POSITIVE_NUMERIC_VALUE,c=r.injection.HAS_OVERLOADED_BOOLEAN_VALUE,s={isCustomAttribute:RegExp.prototype.test.bind(new RegExp(\"^(data|aria)-[\"+r.ATTRIBUTE_NAME_CHAR+\"]*$\")),Properties:{accept:0,acceptCharset:0,accessKey:0,action:0,allowFullScreen:o,allowTransparency:0,alt:0,as:0,async:o,autoComplete:0,autoPlay:o,capture:o,cellPadding:0,cellSpacing:0,charSet:0,challenge:0,checked:i|o,cite:0,classID:0,className:0,cols:u,colSpan:0,content:0,contentEditable:0,contextMenu:0,controls:o,coords:0,crossOrigin:0,data:0,dateTime:0,default:o,defer:o,dir:0,disabled:o,download:c,draggable:0,encType:0,form:0,formAction:0,formEncType:0,formMethod:0,formNoValidate:o,formTarget:0,frameBorder:0,headers:0,height:0,hidden:o,high:0,href:0,hrefLang:0,htmlFor:0,httpEquiv:0,icon:0,id:0,inputMode:0,integrity:0,is:0,keyParams:0,keyType:0,kind:0,label:0,lang:0,list:0,loop:o,low:0,manifest:0,marginHeight:0,marginWidth:0,max:0,maxLength:0,media:0,mediaGroup:0,method:0,min:0,minLength:0,multiple:i|o,muted:i|o,name:0,nonce:0,noValidate:o,open:o,optimum:0,pattern:0,placeholder:0,playsInline:o,poster:0,preload:0,profile:0,radioGroup:0,readOnly:o,referrerPolicy:0,rel:0,required:o,reversed:o,role:0,rows:u,rowSpan:a,sandbox:0,scope:0,scoped:o,scrolling:0,seamless:o,selected:i|o,shape:0,size:u,sizes:0,span:u,spellCheck:0,src:0,srcDoc:0,srcLang:0,srcSet:0,start:a,step:0,style:0,summary:0,tabIndex:0,target:0,title:0,type:0,useMap:0,value:0,width:0,wmode:0,wrap:0,about:0,datatype:0,inlist:0,prefix:0,property:0,resource:0,typeof:0,vocab:0,autoCapitalize:0,autoCorrect:0,autoSave:0,color:0,itemProp:0,itemScope:o,itemType:0,itemID:0,itemRef:0,results:0,security:0,unselectable:0},DOMAttributeNames:{acceptCharset:\"accept-charset\",className:\"class\",htmlFor:\"for\",httpEquiv:\"http-equiv\"},DOMPropertyNames:{}};t.exports=s},function(t,e,n){\"use strict\";(function(e){function r(t,e,n,r){var i=void 0===t[n];null!=e&&i&&(t[n]=o(e,!0))}var i=n(24),o=n(169),a=(n(84),n(95)),u=n(172);n(1);\"undefined\"!=typeof e&&e.env,1;var c={instantiateChildren:function(t,e,n,i){if(null==t)return null;var o={};return u(t,r,o),o},updateChildren:function(t,e,n,r,u,c,s,l,f){if(e||t){var p,h;for(p in e)if(e.hasOwnProperty(p)){h=t&&t[p];var d=h&&h._currentElement,v=e[p];if(null!=h&&a(d,v))i.receiveComponent(h,v,u,l),e[p]=h;else{h&&(r[p]=i.getHostNode(h),i.unmountComponent(h,!1));var g=o(v,!0);e[p]=g;var m=i.mountComponent(g,u,c,s,l,f);n.push(m)}}for(p in t)!t.hasOwnProperty(p)||e&&e.hasOwnProperty(p)||(h=t[p],r[p]=i.getHostNode(h),i.unmountComponent(h,!1))}},unmountChildren:function(t,e){for(var n in t)if(t.hasOwnProperty(n)){var r=t[n];i.unmountComponent(r,e)}}};t.exports=c}).call(e,n(153))},function(t,e,n){\"use strict\";var r=n(81),i=n(350),o={processChildrenUpdates:i.dangerouslyProcessChildrenUpdates,replaceNodeWithMarkup:r.dangerouslyReplaceNodeWithMarkup};t.exports=o},function(t,e,n){\"use strict\";function r(t){}function i(t,e){}function o(t){return!(!t.prototype||!t.prototype.isReactComponent)}function a(t){return!(!t.prototype||!t.prototype.isPureReactComponent)}var u=n(2),c=n(3),s=n(26),l=n(86),f=n(15),p=n(87),h=n(40),d=(n(9),n(164)),v=n(24),g=n(38),m=(n(0),n(80)),y=n(95),_=(n(1),{ImpureClass:0,PureClass:1,StatelessFunctional:2});r.prototype.render=function(){var t=h.get(this)._currentElement.type,e=t(this.props,this.context,this.updater);return i(t,e),e};var b=1,x={construct:function(t){this._currentElement=t,this._rootNodeID=0,this._compositeType=null,this._instance=null,this._hostParent=null,this._hostContainerInfo=null,this._updateBatchNumber=null,this._pendingElement=null,this._pendingStateQueue=null,this._pendingReplaceState=!1,this._pendingForceUpdate=!1,this._renderedNodeType=null,this._renderedComponent=null,this._context=null,this._mountOrder=0,this._topLevelWrapper=null,this._pendingCallbacks=null,this._calledComponentWillUnmount=!1},mountComponent:function(t,e,n,c){this._context=c,this._mountOrder=b++,this._hostParent=e,this._hostContainerInfo=n;var l,f=this._currentElement.props,p=this._processContext(c),d=this._currentElement.type,v=t.getUpdateQueue(),m=o(d),y=this._constructComponent(m,f,p,v);m||null!=y&&null!=y.render?a(d)?this._compositeType=_.PureClass:this._compositeType=_.ImpureClass:(l=y,i(d,l),null===y||y===!1||s.isValidElement(y)?void 0:u(\"105\",d.displayName||d.name||\"Component\"),y=new r(d),this._compositeType=_.StatelessFunctional);y.props=f,y.context=p,y.refs=g,y.updater=v,this._instance=y,h.set(y,this);var x=y.state;void 0===x&&(y.state=x=null),\"object\"!=typeof x||Array.isArray(x)?u(\"106\",this.getName()||\"ReactCompositeComponent\"):void 0,this._pendingStateQueue=null,this._pendingReplaceState=!1,this._pendingForceUpdate=!1;var w;return w=y.unstable_handleError?this.performInitialMountWithErrorHandling(l,e,n,t,c):this.performInitialMount(l,e,n,t,c),y.componentDidMount&&t.getReactMountReady().enqueue(y.componentDidMount,y),w},_constructComponent:function(t,e,n,r){return this._constructComponentWithoutOwner(t,e,n,r)},_constructComponentWithoutOwner:function(t,e,n,r){var i=this._currentElement.type;return t?new i(e,n,r):i(e,n,r)},performInitialMountWithErrorHandling:function(t,e,n,r,i){var o,a=r.checkpoint();try{o=this.performInitialMount(t,e,n,r,i)}catch(u){r.rollback(a),this._instance.unstable_handleError(u),this._pendingStateQueue&&(this._instance.state=this._processPendingState(this._instance.props,this._instance.context)),a=r.checkpoint(),this._renderedComponent.unmountComponent(!0),r.rollback(a),o=this.performInitialMount(t,e,n,r,i)}return o},performInitialMount:function(t,e,n,r,i){var o=this._instance,a=0;o.componentWillMount&&(o.componentWillMount(),this._pendingStateQueue&&(o.state=this._processPendingState(o.props,o.context))),void 0===t&&(t=this._renderValidatedComponent());var u=d.getType(t);this._renderedNodeType=u;var c=this._instantiateReactComponent(t,u!==d.EMPTY);this._renderedComponent=c;var s=v.mountComponent(c,r,e,n,this._processChildContext(i),a);return s},getHostNode:function(){return v.getHostNode(this._renderedComponent)},unmountComponent:function(t){if(this._renderedComponent){var e=this._instance;if(e.componentWillUnmount&&!e._calledComponentWillUnmount)if(e._calledComponentWillUnmount=!0,t){var n=this.getName()+\".componentWillUnmount()\";p.invokeGuardedCallback(n,e.componentWillUnmount.bind(e))}else e.componentWillUnmount();this._renderedComponent&&(v.unmountComponent(this._renderedComponent,t),this._renderedNodeType=null,this._renderedComponent=null,this._instance=null),this._pendingStateQueue=null,this._pendingReplaceState=!1,this._pendingForceUpdate=!1,this._pendingCallbacks=null,this._pendingElement=null,this._context=null,this._rootNodeID=0,this._topLevelWrapper=null,h.remove(e)}},_maskContext:function(t){var e=this._currentElement.type,n=e.contextTypes;if(!n)return g;var r={};for(var i in n)r[i]=t[i];return r},_processContext:function(t){var e=this._maskContext(t);return e},_processChildContext:function(t){var e,n=this._currentElement.type,r=this._instance;if(r.getChildContext&&(e=r.getChildContext()),e){\"object\"!=typeof n.childContextTypes?u(\"107\",this.getName()||\"ReactCompositeComponent\"):void 0;for(var i in e)i in n.childContextTypes?void 0:u(\"108\",this.getName()||\"ReactCompositeComponent\",i);return c({},t,e)}return t},_checkContextTypes:function(t,e,n){},receiveComponent:function(t,e,n){var r=this._currentElement,i=this._context;this._pendingElement=null,this.updateComponent(e,r,t,i,n)},performUpdateIfNecessary:function(t){null!=this._pendingElement?v.receiveComponent(this,this._pendingElement,t,this._context):null!==this._pendingStateQueue||this._pendingForceUpdate?this.updateComponent(t,this._currentElement,this._currentElement,this._context,this._context):this._updateBatchNumber=null},updateComponent:function(t,e,n,r,i){var o=this._instance;null==o?u(\"136\",this.getName()||\"ReactCompositeComponent\"):void 0;var a,c=!1;this._context===i?a=o.context:(a=this._processContext(i),c=!0);var s=e.props,l=n.props;e!==n&&(c=!0),c&&o.componentWillReceiveProps&&o.componentWillReceiveProps(l,a);var f=this._processPendingState(l,a),p=!0;this._pendingForceUpdate||(o.shouldComponentUpdate?p=o.shouldComponentUpdate(l,f,a):this._compositeType===_.PureClass&&(p=!m(s,l)||!m(o.state,f))),this._updateBatchNumber=null,p?(this._pendingForceUpdate=!1,this._performComponentUpdate(n,l,f,a,t,i)):(this._currentElement=n,this._context=i,o.props=l,o.state=f,o.context=a)},_processPendingState:function(t,e){var n=this._instance,r=this._pendingStateQueue,i=this._pendingReplaceState;if(this._pendingReplaceState=!1,this._pendingStateQueue=null,!r)return n.state;if(i&&1===r.length)return r[0];for(var o=c({},i?r[0]:n.state),a=i?1:0;a<r.length;a++){var u=r[a];c(o,\"function\"==typeof u?u.call(n,o,t,e):u)}return o},_performComponentUpdate:function(t,e,n,r,i,o){var a,u,c,s=this._instance,l=Boolean(s.componentDidUpdate);l&&(a=s.props,u=s.state,c=s.context),s.componentWillUpdate&&s.componentWillUpdate(e,n,r),this._currentElement=t,this._context=o,s.props=e,s.state=n,s.context=r,this._updateRenderedComponent(i,o),l&&i.getReactMountReady().enqueue(s.componentDidUpdate.bind(s,a,u,c),s)},_updateRenderedComponent:function(t,e){var n=this._renderedComponent,r=n._currentElement,i=this._renderValidatedComponent(),o=0;if(y(r,i))v.receiveComponent(n,i,t,this._processChildContext(e));else{var a=v.getHostNode(n);v.unmountComponent(n,!1);var u=d.getType(i);this._renderedNodeType=u;var c=this._instantiateReactComponent(i,u!==d.EMPTY);this._renderedComponent=c;var s=v.mountComponent(c,t,this._hostParent,this._hostContainerInfo,this._processChildContext(e),o);this._replaceNodeWithMarkup(a,s,n)}},_replaceNodeWithMarkup:function(t,e,n){l.replaceNodeWithMarkup(t,e,n)},_renderValidatedComponentWithoutOwnerOrContext:function(){var t,e=this._instance;return t=e.render()},_renderValidatedComponent:function(){var t;if(this._compositeType!==_.StatelessFunctional){f.current=this;try{t=this._renderValidatedComponentWithoutOwnerOrContext()}finally{f.current=null}}else t=this._renderValidatedComponentWithoutOwnerOrContext();return null===t||t===!1||s.isValidElement(t)?void 0:u(\"109\",this.getName()||\"ReactCompositeComponent\"),t},attachRef:function(t,e){var n=this.getPublicInstance();null==n?u(\"110\"):void 0;var r=e.getPublicInstance(),i=n.refs===g?n.refs={}:n.refs;i[t]=r},detachRef:function(t){var e=this.getPublicInstance().refs;delete e[t]},getName:function(){var t=this._currentElement.type,e=this._instance&&this._instance.constructor;return t.displayName||e&&e.displayName||t.name||e&&e.name||null},getPublicInstance:function(){var t=this._instance;return this._compositeType===_.StatelessFunctional?null:t},_instantiateReactComponent:null};t.exports=x},function(t,e,n){\"use strict\";var r=n(4),i=n(358),o=n(163),a=n(24),u=n(11),c=n(371),s=n(387),l=n(167),f=n(395);n(1);i.inject();var p={findDOMNode:s,render:o.render,unmountComponentAtNode:o.unmountComponentAtNode,version:c,unstable_batchedUpdates:u.batchedUpdates,unstable_renderSubtreeIntoContainer:f};\"undefined\"!=typeof __REACT_DEVTOOLS_GLOBAL_HOOK__&&\"function\"==typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.inject&&__REACT_DEVTOOLS_GLOBAL_HOOK__.inject({ComponentTree:{getClosestInstanceFromNode:r.getClosestInstanceFromNode,getNodeFromInstance:function(t){return t._renderedComponent&&(t=l(t)),t?r.getNodeFromInstance(t):null}},Mount:o,Reconciler:a});t.exports=p},function(t,e,n){\"use strict\";function r(t){if(t){var e=t._currentElement._owner||null;if(e){var n=e.getName();if(n)return\" This DOM node was rendered by `\"+n+\"`.\"}}return\"\"}function i(t,e){e&&(G[t._tag]&&(null!=e.children||null!=e.dangerouslySetInnerHTML?v(\"137\",t._tag,t._currentElement._owner?\" Check the render method of \"+t._currentElement._owner.getName()+\".\":\"\"):void 0),null!=e.dangerouslySetInnerHTML&&(null!=e.children?v(\"60\"):void 0,\"object\"==typeof e.dangerouslySetInnerHTML&&V in e.dangerouslySetInnerHTML?void 0:v(\"61\")),null!=e.style&&\"object\"!=typeof e.style?v(\"62\",r(t)):void 0)}function o(t,e,n,r){if(!(r instanceof I)){var i=t._hostContainerInfo,o=i._node&&i._node.nodeType===H,u=o?i._node:i._ownerDocument;F(e,u),r.getReactMountReady().enqueue(a,{inst:t,registrationName:e,listener:n})}}function a(){var t=this;C.putListener(t.inst,t.registrationName,t.listener)}function u(){var t=this;S.postMountWrapper(t)}function c(){var t=this;A.postMountWrapper(t)}function s(){var t=this;P.postMountWrapper(t)}function l(){var t=this;t._rootNodeID?void 0:v(\"63\");var e=U(t);switch(e?void 0:v(\"64\"),t._tag){case\"iframe\":case\"object\":t._wrapperState.listeners=[k.trapBubbledEvent(\"topLoad\",\"load\",e)];break;case\"video\":case\"audio\":t._wrapperState.listeners=[];for(var n in q)q.hasOwnProperty(n)&&t._wrapperState.listeners.push(k.trapBubbledEvent(n,q[n],e));break;case\"source\":t._wrapperState.listeners=[k.trapBubbledEvent(\"topError\",\"error\",e)];break;case\"img\":t._wrapperState.listeners=[k.trapBubbledEvent(\"topError\",\"error\",e),k.trapBubbledEvent(\"topLoad\",\"load\",e)];break;case\"form\":t._wrapperState.listeners=[k.trapBubbledEvent(\"topReset\",\"reset\",e),k.trapBubbledEvent(\"topSubmit\",\"submit\",e)];break;case\"input\":case\"select\":case\"textarea\":t._wrapperState.listeners=[k.trapBubbledEvent(\"topInvalid\",\"invalid\",e)]}}function f(){N.postUpdateWrapper(this)}function p(t){Z.call(X,t)||($.test(t)?void 0:v(\"65\",t),X[t]=!0)}function h(t,e){return t.indexOf(\"-\")>=0||null!=e.is}function d(t){var e=t.type;p(e),this._currentElement=t,this._tag=e.toLowerCase(),this._namespaceURI=null,this._renderedChildren=null,this._previousStyle=null,this._previousStyleCopy=null,this._hostNode=null,this._hostParent=null,this._rootNodeID=0,this._domID=0,this._hostContainerInfo=null,this._wrapperState=null,this._topLevelWrapper=null,this._flags=0}var v=n(2),g=n(3),m=n(332),y=n(334),_=n(20),b=n(82),x=n(21),w=n(156),C=n(22),M=n(83),k=n(51),E=n(157),T=n(4),S=n(351),P=n(352),N=n(158),A=n(355),O=(n(9),n(364)),I=n(369),D=(n(8),n(54)),R=(n(0),n(94),n(80),n(96),n(1),E),L=C.deleteListener,U=T.getNodeFromInstance,F=k.listenTo,j=M.registrationNameModules,B={string:!0,number:!0},W=\"style\",V=\"__html\",z={children:null,dangerouslySetInnerHTML:null,suppressContentEditableWarning:null},H=11,q={topAbort:\"abort\",topCanPlay:\"canplay\",topCanPlayThrough:\"canplaythrough\",topDurationChange:\"durationchange\",topEmptied:\"emptied\",topEncrypted:\"encrypted\",topEnded:\"ended\",topError:\"error\",topLoadedData:\"loadeddata\",topLoadedMetadata:\"loadedmetadata\",topLoadStart:\"loadstart\",topPause:\"pause\",topPlay:\"play\",topPlaying:\"playing\",topProgress:\"progress\",topRateChange:\"ratechange\",topSeeked:\"seeked\",topSeeking:\"seeking\",topStalled:\"stalled\",topSuspend:\"suspend\",topTimeUpdate:\"timeupdate\",topVolumeChange:\"volumechange\",topWaiting:\"waiting\"},Y={area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0},K={listing:!0,pre:!0,textarea:!0},G=g({menuitem:!0},Y),$=/^[a-zA-Z][a-zA-Z:_\\.\\-\\d]*$/,X={},Z={}.hasOwnProperty,Q=1;d.displayName=\"ReactDOMComponent\",d.Mixin={mountComponent:function(t,e,n,r){this._rootNodeID=Q++,this._domID=n._idCounter++,this._hostParent=e,this._hostContainerInfo=n;var o=this._currentElement.props;switch(this._tag){case\"audio\":case\"form\":case\"iframe\":case\"img\":case\"link\":case\"object\":case\"source\":case\"video\":this._wrapperState={listeners:null},t.getReactMountReady().enqueue(l,this);break;case\"input\":S.mountWrapper(this,o,e),o=S.getHostProps(this,o),t.getReactMountReady().enqueue(l,this);break;case\"option\":P.mountWrapper(this,o,e),o=P.getHostProps(this,o);break;case\"select\":N.mountWrapper(this,o,e),o=N.getHostProps(this,o),t.getReactMountReady().enqueue(l,this);break;case\"textarea\":A.mountWrapper(this,o,e),o=A.getHostProps(this,o),t.getReactMountReady().enqueue(l,this)}i(this,o);var a,f;null!=e?(a=e._namespaceURI,f=e._tag):n._tag&&(a=n._namespaceURI,f=n._tag),(null==a||a===b.svg&&\"foreignobject\"===f)&&(a=b.html),a===b.html&&(\"svg\"===this._tag?a=b.svg:\"math\"===this._tag&&(a=b.mathml)),this._namespaceURI=a;var p;if(t.useCreateElement){var h,d=n._ownerDocument;if(a===b.html)if(\"script\"===this._tag){var v=d.createElement(\"div\"),g=this._currentElement.type;v.innerHTML=\"<\"+g+\"></\"+g+\">\",h=v.removeChild(v.firstChild)}else h=o.is?d.createElement(this._currentElement.type,o.is):d.createElement(this._currentElement.type);else h=d.createElementNS(a,this._currentElement.type);T.precacheNode(this,h),this._flags|=R.hasCachedChildNodes,this._hostParent||w.setAttributeForRoot(h),this._updateDOMProperties(null,o,t);var y=_(h);this._createInitialChildren(t,o,r,y),p=y}else{var x=this._createOpenTagMarkupAndPutListeners(t,o),C=this._createContentMarkup(t,o,r);p=!C&&Y[this._tag]?x+\"/>\":x+\">\"+C+\"</\"+this._currentElement.type+\">\"}switch(this._tag){case\"input\":t.getReactMountReady().enqueue(u,this),o.autoFocus&&t.getReactMountReady().enqueue(m.focusDOMComponent,this);break;case\"textarea\":t.getReactMountReady().enqueue(c,this),o.autoFocus&&t.getReactMountReady().enqueue(m.focusDOMComponent,this);break;case\"select\":o.autoFocus&&t.getReactMountReady().enqueue(m.focusDOMComponent,this);break;case\"button\":o.autoFocus&&t.getReactMountReady().enqueue(m.focusDOMComponent,this);break;case\"option\":t.getReactMountReady().enqueue(s,this)}return p},_createOpenTagMarkupAndPutListeners:function(t,e){var n=\"<\"+this._currentElement.type;for(var r in e)if(e.hasOwnProperty(r)){var i=e[r];if(null!=i)if(j.hasOwnProperty(r))i&&o(this,r,i,t);else{r===W&&(i&&(i=this._previousStyleCopy=g({},e.style)),i=y.createMarkupForStyles(i,this));var a=null;null!=this._tag&&h(this._tag,e)?z.hasOwnProperty(r)||(a=w.createMarkupForCustomAttribute(r,i)):a=w.createMarkupForProperty(r,i),a&&(n+=\" \"+a)}}return t.renderToStaticMarkup?n:(this._hostParent||(n+=\" \"+w.createMarkupForRoot()),n+=\" \"+w.createMarkupForID(this._domID))},_createContentMarkup:function(t,e,n){var r=\"\",i=e.dangerouslySetInnerHTML;if(null!=i)null!=i.__html&&(r=i.__html);else{var o=B[typeof e.children]?e.children:null,a=null!=o?null:e.children;if(null!=o)r=D(o);else if(null!=a){var u=this.mountChildren(a,t,n);r=u.join(\"\")}}return K[this._tag]&&\"\\n\"===r.charAt(0)?\"\\n\"+r:r},_createInitialChildren:function(t,e,n,r){var i=e.dangerouslySetInnerHTML;if(null!=i)null!=i.__html&&_.queueHTML(r,i.__html);else{var o=B[typeof e.children]?e.children:null,a=null!=o?null:e.children;if(null!=o)\"\"!==o&&_.queueText(r,o);else if(null!=a)for(var u=this.mountChildren(a,t,n),c=0;c<u.length;c++)_.queueChild(r,u[c])}},receiveComponent:function(t,e,n){var r=this._currentElement;this._currentElement=t,this.updateComponent(e,r,t,n)},updateComponent:function(t,e,n,r){var o=e.props,a=this._currentElement.props;switch(this._tag){case\"input\":o=S.getHostProps(this,o),a=S.getHostProps(this,a);break;case\"option\":o=P.getHostProps(this,o),a=P.getHostProps(this,a);break;case\"select\":o=N.getHostProps(this,o),a=N.getHostProps(this,a);break;case\"textarea\":o=A.getHostProps(this,o),a=A.getHostProps(this,a)}switch(i(this,a),this._updateDOMProperties(o,a,t),this._updateDOMChildren(o,a,t,r),this._tag){case\"input\":S.updateWrapper(this);break;case\"textarea\":A.updateWrapper(this);break;case\"select\":t.getReactMountReady().enqueue(f,this)}},_updateDOMProperties:function(t,e,n){var r,i,a;for(r in t)if(!e.hasOwnProperty(r)&&t.hasOwnProperty(r)&&null!=t[r])if(r===W){var u=this._previousStyleCopy;for(i in u)u.hasOwnProperty(i)&&(a=a||{},a[i]=\"\");this._previousStyleCopy=null}else j.hasOwnProperty(r)?t[r]&&L(this,r):h(this._tag,t)?z.hasOwnProperty(r)||w.deleteValueForAttribute(U(this),r):(x.properties[r]||x.isCustomAttribute(r))&&w.deleteValueForProperty(U(this),r);for(r in e){var c=e[r],s=r===W?this._previousStyleCopy:null!=t?t[r]:void 0;if(e.hasOwnProperty(r)&&c!==s&&(null!=c||null!=s))if(r===W)if(c?c=this._previousStyleCopy=g({},c):this._previousStyleCopy=null,s){for(i in s)!s.hasOwnProperty(i)||c&&c.hasOwnProperty(i)||(a=a||{},a[i]=\"\");for(i in c)c.hasOwnProperty(i)&&s[i]!==c[i]&&(a=a||{},a[i]=c[i])}else a=c;else if(j.hasOwnProperty(r))c?o(this,r,c,n):s&&L(this,r);else if(h(this._tag,e))z.hasOwnProperty(r)||w.setValueForAttribute(U(this),r,c);else if(x.properties[r]||x.isCustomAttribute(r)){var l=U(this);null!=c?w.setValueForProperty(l,r,c):w.deleteValueForProperty(l,r)}}a&&y.setValueForStyles(U(this),a,this)},_updateDOMChildren:function(t,e,n,r){var i=B[typeof t.children]?t.children:null,o=B[typeof e.children]?e.children:null,a=t.dangerouslySetInnerHTML&&t.dangerouslySetInnerHTML.__html,u=e.dangerouslySetInnerHTML&&e.dangerouslySetInnerHTML.__html,c=null!=i?null:t.children,s=null!=o?null:e.children,l=null!=i||null!=a,f=null!=o||null!=u;null!=c&&null==s?this.updateChildren(null,n,r):l&&!f&&this.updateTextContent(\"\"),null!=o?i!==o&&this.updateTextContent(\"\"+o):null!=u?a!==u&&this.updateMarkup(\"\"+u):null!=s&&this.updateChildren(s,n,r)},getHostNode:function(){return U(this)},unmountComponent:function(t){switch(this._tag){case\"audio\":case\"form\":case\"iframe\":case\"img\":case\"link\":case\"object\":case\"source\":case\"video\":var e=this._wrapperState.listeners;if(e)for(var n=0;n<e.length;n++)e[n].remove();break;case\"html\":case\"head\":case\"body\":v(\"66\",this._tag)}this.unmountChildren(t),T.uncacheNode(this),C.deleteAllListeners(this),this._rootNodeID=0,this._domID=0,this._wrapperState=null},getPublicInstance:function(){return U(this)}},g(d.prototype,d.Mixin,O.Mixin),t.exports=d},function(t,e,n){\"use strict\";function r(t,e){var n={_topLevelWrapper:t,_idCounter:1,_ownerDocument:e?e.nodeType===i?e:e.ownerDocument:null,_node:e,_tag:e?e.nodeName.toLowerCase():null,_namespaceURI:e?e.namespaceURI:null};return n}var i=(n(96),9);t.exports=r},function(t,e,n){\"use strict\";var r=n(3),i=n(20),o=n(4),a=function(t){this._currentElement=null,this._hostNode=null,this._hostParent=null,this._hostContainerInfo=null,this._domID=0};r(a.prototype,{mountComponent:function(t,e,n,r){var a=n._idCounter++;this._domID=a,this._hostParent=e,this._hostContainerInfo=n;var u=\" react-empty: \"+this._domID+\" \";if(t.useCreateElement){var c=n._ownerDocument,s=c.createComment(u);return o.precacheNode(this,s),i(s)}return t.renderToStaticMarkup?\"\":\"<!--\"+u+\"-->\"},receiveComponent:function(){},getHostNode:function(){return o.getNodeFromInstance(this)},unmountComponent:function(){o.uncacheNode(this)}}),t.exports=a},function(t,e,n){\"use strict\";var r={useCreateElement:!0,useFiber:!1};t.exports=r},function(t,e,n){\"use strict\";var r=n(81),i=n(4),o={dangerouslyProcessChildrenUpdates:function(t,e){var n=i.getNodeFromInstance(t);r.processUpdates(n,e)}};t.exports=o},function(t,e,n){\"use strict\";function r(){this._rootNodeID&&f.updateWrapper(this)}function i(t){var e=this._currentElement.props,n=c.executeOnChange(e,t);l.asap(r,this);var i=e.name;if(\"radio\"===e.type&&null!=i){for(var a=s.getNodeFromInstance(this),u=a;u.parentNode;)u=u.parentNode;for(var f=u.querySelectorAll(\"input[name=\"+JSON.stringify(\"\"+i)+'][type=\"radio\"]'),p=0;p<f.length;p++){var h=f[p];if(h!==a&&h.form===a.form){var d=s.getInstanceFromNode(h);d?void 0:o(\"90\"),l.asap(r,d)}}}return n}var o=n(2),a=n(3),u=n(156),c=n(85),s=n(4),l=n(11),f=(n(0),n(1),{getHostProps:function(t,e){var n=c.getValue(e),r=c.getChecked(e),i=a({type:void 0,step:void 0,min:void 0,max:void 0},e,{defaultChecked:void 0,defaultValue:void 0,value:null!=n?n:t._wrapperState.initialValue,checked:null!=r?r:t._wrapperState.initialChecked,onChange:t._wrapperState.onChange});return i},mountWrapper:function(t,e){var n=e.defaultValue;t._wrapperState={initialChecked:null!=e.checked?e.checked:e.defaultChecked,initialValue:null!=e.value?e.value:n,listeners:null,onChange:i.bind(t)}},updateWrapper:function(t){var e=t._currentElement.props,n=e.checked;null!=n&&u.setValueForProperty(s.getNodeFromInstance(t),\"checked\",n||!1);var r=s.getNodeFromInstance(t),i=c.getValue(e);if(null!=i){var o=\"\"+i;o!==r.value&&(r.value=o)}else null==e.value&&null!=e.defaultValue&&r.defaultValue!==\"\"+e.defaultValue&&(r.defaultValue=\"\"+e.defaultValue),null==e.checked&&null!=e.defaultChecked&&(r.defaultChecked=!!e.defaultChecked)},postMountWrapper:function(t){var e=t._currentElement.props,n=s.getNodeFromInstance(t);switch(e.type){case\"submit\":case\"reset\":break;case\"color\":case\"date\":case\"datetime\":case\"datetime-local\":case\"month\":case\"time\":case\"week\":n.value=\"\",n.value=n.defaultValue;break;default:n.value=n.value}var r=n.name;\"\"!==r&&(n.name=\"\"),n.defaultChecked=!n.defaultChecked,n.defaultChecked=!n.defaultChecked,\"\"!==r&&(n.name=r)}});t.exports=f},function(t,e,n){\"use strict\";function r(t){var e=\"\";return o.Children.forEach(t,function(t){null!=t&&(\"string\"==typeof t||\"number\"==typeof t?e+=t:c||(c=!0))}),e}var i=n(3),o=n(26),a=n(4),u=n(158),c=(n(1),!1),s={mountWrapper:function(t,e,n){var i=null;if(null!=n){var o=n;\"optgroup\"===o._tag&&(o=o._hostParent),null!=o&&\"select\"===o._tag&&(i=u.getSelectValueContext(o))}var a=null;if(null!=i){var c;if(c=null!=e.value?e.value+\"\":r(e.children),a=!1,Array.isArray(i)){for(var s=0;s<i.length;s++)if(\"\"+i[s]===c){a=!0;break}}else a=\"\"+i===c}t._wrapperState={selected:a}},postMountWrapper:function(t){var e=t._currentElement.props;if(null!=e.value){var n=a.getNodeFromInstance(t);n.setAttribute(\"value\",e.value)}},getHostProps:function(t,e){var n=i({selected:void 0,children:void 0},e);null!=t._wrapperState.selected&&(n.selected=t._wrapperState.selected);var o=r(e.children);return o&&(n.children=o),n}};t.exports=s},function(t,e,n){\"use strict\";function r(t,e,n,r){return t===n&&e===r}function i(t){var e=document.selection,n=e.createRange(),r=n.text.length,i=n.duplicate();i.moveToElementText(t),i.setEndPoint(\"EndToStart\",n);var o=i.text.length,a=o+r;return{start:o,end:a}}function o(t){var e=window.getSelection&&window.getSelection();if(!e||0===e.rangeCount)return null;var n=e.anchorNode,i=e.anchorOffset,o=e.focusNode,a=e.focusOffset,u=e.getRangeAt(0);try{u.startContainer.nodeType,u.endContainer.nodeType}catch(t){return null}var c=r(e.anchorNode,e.anchorOffset,e.focusNode,e.focusOffset),s=c?0:u.toString().length,l=u.cloneRange();l.selectNodeContents(t),l.setEnd(u.startContainer,u.startOffset);var f=r(l.startContainer,l.startOffset,l.endContainer,l.endOffset),p=f?0:l.toString().length,h=p+s,d=document.createRange();d.setStart(n,i),d.setEnd(o,a);var v=d.collapsed;return{start:v?h:p,end:v?p:h}}function a(t,e){var n,r,i=document.selection.createRange().duplicate();void 0===e.end?(n=e.start,r=n):e.start>e.end?(n=e.end,r=e.start):(n=e.start,r=e.end),i.moveToElementText(t),i.moveStart(\"character\",n),i.setEndPoint(\"EndToStart\",i),i.moveEnd(\"character\",r-n),i.select()}function u(t,e){if(window.getSelection){var n=window.getSelection(),r=t[l()].length,i=Math.min(e.start,r),o=void 0===e.end?i:Math.min(e.end,r);if(!n.extend&&i>o){var a=o;o=i,i=a}var u=s(t,i),c=s(t,o);if(u&&c){var f=document.createRange();f.setStart(u.node,u.offset),n.removeAllRanges(),i>o?(n.addRange(f),n.extend(c.node,c.offset)):(f.setEnd(c.node,c.offset),n.addRange(f))}}}var c=n(6),s=n(392),l=n(168),f=c.canUseDOM&&\"selection\"in document&&!(\"getSelection\"in window),p={getOffsets:f?i:o,setOffsets:f?a:u};t.exports=p},function(t,e,n){\"use strict\";var r=n(2),i=n(3),o=n(81),a=n(20),u=n(4),c=n(54),s=(n(0),n(96),function(t){this._currentElement=t,this._stringText=\"\"+t,this._hostNode=null,this._hostParent=null,this._domID=0,this._mountIndex=0,this._closingComment=null,this._commentNodes=null});i(s.prototype,{mountComponent:function(t,e,n,r){var i=n._idCounter++,o=\" react-text: \"+i+\" \",s=\" /react-text \";if(this._domID=i,this._hostParent=e,t.useCreateElement){var l=n._ownerDocument,f=l.createComment(o),p=l.createComment(s),h=a(l.createDocumentFragment());return a.queueChild(h,a(f)),this._stringText&&a.queueChild(h,a(l.createTextNode(this._stringText))),a.queueChild(h,a(p)),u.precacheNode(this,f),this._closingComment=p,h}var d=c(this._stringText);return t.renderToStaticMarkup?d:\"<!--\"+o+\"-->\"+d+\"<!--\"+s+\"-->\"},receiveComponent:function(t,e){if(t!==this._currentElement){this._currentElement=t;var n=\"\"+t;if(n!==this._stringText){this._stringText=n;var r=this.getHostNode();o.replaceDelimitedText(r[0],r[1],n)}}},getHostNode:function(){var t=this._commentNodes;if(t)return t;if(!this._closingComment)for(var e=u.getNodeFromInstance(this),n=e.nextSibling;;){if(null==n?r(\"67\",this._domID):void 0,8===n.nodeType&&\" /react-text \"===n.nodeValue){this._closingComment=n;break}n=n.nextSibling}return t=[this._hostNode,this._closingComment],this._commentNodes=t,t},unmountComponent:function(){this._closingComment=null,this._commentNodes=null,u.uncacheNode(this)}}),t.exports=s},function(t,e,n){\"use strict\";function r(){this._rootNodeID&&l.updateWrapper(this)}function i(t){var e=this._currentElement.props,n=u.executeOnChange(e,t);return s.asap(r,this),n}var o=n(2),a=n(3),u=n(85),c=n(4),s=n(11),l=(n(0),n(1),{getHostProps:function(t,e){null!=e.dangerouslySetInnerHTML?o(\"91\"):void 0;var n=a({},e,{value:void 0,defaultValue:void 0,children:\"\"+t._wrapperState.initialValue,onChange:t._wrapperState.onChange});return n},mountWrapper:function(t,e){var n=u.getValue(e),r=n;if(null==n){var a=e.defaultValue,c=e.children;null!=c&&(null!=a?o(\"92\"):void 0,Array.isArray(c)&&(c.length<=1?void 0:o(\"93\"),c=c[0]),a=\"\"+c),null==a&&(a=\"\"),r=a}t._wrapperState={initialValue:\"\"+r,listeners:null,onChange:i.bind(t)}},updateWrapper:function(t){var e=t._currentElement.props,n=c.getNodeFromInstance(t),r=u.getValue(e);if(null!=r){var i=\"\"+r;i!==n.value&&(n.value=i),null==e.defaultValue&&(n.defaultValue=i)}null!=e.defaultValue&&(n.defaultValue=e.defaultValue)},postMountWrapper:function(t){var e=c.getNodeFromInstance(t),n=e.textContent;\n",
       "n===t._wrapperState.initialValue&&(e.value=n)}});t.exports=l},function(t,e,n){\"use strict\";function r(t,e){\"_hostNode\"in t?void 0:c(\"33\"),\"_hostNode\"in e?void 0:c(\"33\");for(var n=0,r=t;r;r=r._hostParent)n++;for(var i=0,o=e;o;o=o._hostParent)i++;for(;n-i>0;)t=t._hostParent,n--;for(;i-n>0;)e=e._hostParent,i--;for(var a=n;a--;){if(t===e)return t;t=t._hostParent,e=e._hostParent}return null}function i(t,e){\"_hostNode\"in t?void 0:c(\"35\"),\"_hostNode\"in e?void 0:c(\"35\");for(;e;){if(e===t)return!0;e=e._hostParent}return!1}function o(t){return\"_hostNode\"in t?void 0:c(\"36\"),t._hostParent}function a(t,e,n){for(var r=[];t;)r.push(t),t=t._hostParent;var i;for(i=r.length;i-- >0;)e(r[i],\"captured\",n);for(i=0;i<r.length;i++)e(r[i],\"bubbled\",n)}function u(t,e,n,i,o){for(var a=t&&e?r(t,e):null,u=[];t&&t!==a;)u.push(t),t=t._hostParent;for(var c=[];e&&e!==a;)c.push(e),e=e._hostParent;var s;for(s=0;s<u.length;s++)n(u[s],\"bubbled\",i);for(s=c.length;s-- >0;)n(c[s],\"captured\",o)}var c=n(2);n(0);t.exports={isAncestor:i,getLowestCommonAncestor:r,getParentInstance:o,traverseTwoPhase:a,traverseEnterLeave:u}},function(t,e,n){\"use strict\";function r(){this.reinitializeTransaction()}var i=n(3),o=n(11),a=n(53),u=n(8),c={initialize:u,close:function(){p.isBatchingUpdates=!1}},s={initialize:u,close:o.flushBatchedUpdates.bind(o)},l=[s,c];i(r.prototype,a,{getTransactionWrappers:function(){return l}});var f=new r,p={isBatchingUpdates:!1,batchedUpdates:function(t,e,n,r,i,o){var a=p.isBatchingUpdates;return p.isBatchingUpdates=!0,a?t(e,n,r,i,o):f.perform(t,null,e,n,r,i,o)}};t.exports=p},function(t,e,n){\"use strict\";function r(){C||(C=!0,y.EventEmitter.injectReactEventListener(m),y.EventPluginHub.injectEventPluginOrder(u),y.EventPluginUtils.injectComponentTree(p),y.EventPluginUtils.injectTreeTraversal(d),y.EventPluginHub.injectEventPluginsByName({SimpleEventPlugin:w,EnterLeaveEventPlugin:c,ChangeEventPlugin:a,SelectEventPlugin:x,BeforeInputEventPlugin:o}),y.HostComponent.injectGenericComponentClass(f),y.HostComponent.injectTextComponentClass(v),y.DOMProperty.injectDOMPropertyConfig(i),y.DOMProperty.injectDOMPropertyConfig(s),y.DOMProperty.injectDOMPropertyConfig(b),y.EmptyComponent.injectEmptyComponentFactory(function(t){return new h(t)}),y.Updates.injectReconcileTransaction(_),y.Updates.injectBatchingStrategy(g),y.Component.injectEnvironment(l))}var i=n(331),o=n(333),a=n(335),u=n(337),c=n(338),s=n(341),l=n(343),f=n(346),p=n(4),h=n(348),d=n(356),v=n(354),g=n(357),m=n(361),y=n(362),_=n(367),b=n(372),x=n(373),w=n(374),C=!1;t.exports={inject:r}},function(t,e,n){\"use strict\";var r=\"function\"==typeof Symbol&&Symbol.for&&Symbol.for(\"react.element\")||60103;t.exports=r},function(t,e,n){\"use strict\";function r(t){i.enqueueEvents(t),i.processEventQueue(!1)}var i=n(22),o={handleTopLevel:function(t,e,n,o){var a=i.extractEvents(t,e,n,o);r(a)}};t.exports=o},function(t,e,n){\"use strict\";function r(t){for(;t._hostParent;)t=t._hostParent;var e=f.getNodeFromInstance(t),n=e.parentNode;return f.getClosestInstanceFromNode(n)}function i(t,e){this.topLevelType=t,this.nativeEvent=e,this.ancestors=[]}function o(t){var e=h(t.nativeEvent),n=f.getClosestInstanceFromNode(e),i=n;do t.ancestors.push(i),i=i&&r(i);while(i);for(var o=0;o<t.ancestors.length;o++)n=t.ancestors[o],v._handleTopLevel(t.topLevelType,n,t.nativeEvent,h(t.nativeEvent))}function a(t){var e=d(window);t(e)}var u=n(3),c=n(150),s=n(6),l=n(17),f=n(4),p=n(11),h=n(93),d=n(324);u(i.prototype,{destructor:function(){this.topLevelType=null,this.nativeEvent=null,this.ancestors.length=0}}),l.addPoolingTo(i,l.twoArgumentPooler);var v={_enabled:!0,_handleTopLevel:null,WINDOW_HANDLE:s.canUseDOM?window:null,setHandleTopLevel:function(t){v._handleTopLevel=t},setEnabled:function(t){v._enabled=!!t},isEnabled:function(){return v._enabled},trapBubbledEvent:function(t,e,n){return n?c.listen(n,e,v.dispatchEvent.bind(null,t)):null},trapCapturedEvent:function(t,e,n){return n?c.capture(n,e,v.dispatchEvent.bind(null,t)):null},monitorScrollValue:function(t){var e=a.bind(null,t);c.listen(window,\"scroll\",e)},dispatchEvent:function(t,e){if(v._enabled){var n=i.getPooled(t,e);try{p.batchedUpdates(o,n)}finally{i.release(n)}}}};t.exports=v},function(t,e,n){\"use strict\";var r=n(21),i=n(22),o=n(50),a=n(86),u=n(159),c=n(51),s=n(161),l=n(11),f={Component:a.injection,DOMProperty:r.injection,EmptyComponent:u.injection,EventPluginHub:i.injection,EventPluginUtils:o.injection,EventEmitter:c.injection,HostComponent:s.injection,Updates:l.injection};t.exports=f},function(t,e,n){\"use strict\";var r=n(385),i=/\\/?>/,o=/^<\\!\\-\\-/,a={CHECKSUM_ATTR_NAME:\"data-react-checksum\",addChecksumToMarkup:function(t){var e=r(t);return o.test(t)?t:t.replace(i,\" \"+a.CHECKSUM_ATTR_NAME+'=\"'+e+'\"$&')},canReuseMarkup:function(t,e){var n=e.getAttribute(a.CHECKSUM_ATTR_NAME);n=n&&parseInt(n,10);var i=r(t);return i===n}};t.exports=a},function(t,e,n){\"use strict\";function r(t,e,n){return{type:\"INSERT_MARKUP\",content:t,fromIndex:null,fromNode:null,toIndex:n,afterNode:e}}function i(t,e,n){return{type:\"MOVE_EXISTING\",content:null,fromIndex:t._mountIndex,fromNode:p.getHostNode(t),toIndex:n,afterNode:e}}function o(t,e){return{type:\"REMOVE_NODE\",content:null,fromIndex:t._mountIndex,fromNode:e,toIndex:null,afterNode:null}}function a(t){return{type:\"SET_MARKUP\",content:t,fromIndex:null,fromNode:null,toIndex:null,afterNode:null}}function u(t){return{type:\"TEXT_CONTENT\",content:t,fromIndex:null,fromNode:null,toIndex:null,afterNode:null}}function c(t,e){return e&&(t=t||[],t.push(e)),t}function s(t,e){f.processChildrenUpdates(t,e)}var l=n(2),f=n(86),p=(n(40),n(9),n(15),n(24)),h=n(342),d=(n(8),n(388)),v=(n(0),{Mixin:{_reconcilerInstantiateChildren:function(t,e,n){return h.instantiateChildren(t,e,n)},_reconcilerUpdateChildren:function(t,e,n,r,i,o){var a,u=0;return a=d(e,u),h.updateChildren(t,a,n,r,i,this,this._hostContainerInfo,o,u),a},mountChildren:function(t,e,n){var r=this._reconcilerInstantiateChildren(t,e,n);this._renderedChildren=r;var i=[],o=0;for(var a in r)if(r.hasOwnProperty(a)){var u=r[a],c=0,s=p.mountComponent(u,e,this,this._hostContainerInfo,n,c);u._mountIndex=o++,i.push(s)}return i},updateTextContent:function(t){var e=this._renderedChildren;h.unmountChildren(e,!1);for(var n in e)e.hasOwnProperty(n)&&l(\"118\");var r=[u(t)];s(this,r)},updateMarkup:function(t){var e=this._renderedChildren;h.unmountChildren(e,!1);for(var n in e)e.hasOwnProperty(n)&&l(\"118\");var r=[a(t)];s(this,r)},updateChildren:function(t,e,n){this._updateChildren(t,e,n)},_updateChildren:function(t,e,n){var r=this._renderedChildren,i={},o=[],a=this._reconcilerUpdateChildren(r,t,o,i,e,n);if(a||r){var u,l=null,f=0,h=0,d=0,v=null;for(u in a)if(a.hasOwnProperty(u)){var g=r&&r[u],m=a[u];g===m?(l=c(l,this.moveChild(g,v,f,h)),h=Math.max(g._mountIndex,h),g._mountIndex=f):(g&&(h=Math.max(g._mountIndex,h)),l=c(l,this._mountChildAtIndex(m,o[d],v,f,e,n)),d++),f++,v=p.getHostNode(m)}for(u in i)i.hasOwnProperty(u)&&(l=c(l,this._unmountChild(r[u],i[u])));l&&s(this,l),this._renderedChildren=a}},unmountChildren:function(t){var e=this._renderedChildren;h.unmountChildren(e,t),this._renderedChildren=null},moveChild:function(t,e,n,r){if(t._mountIndex<r)return i(t,e,n)},createChild:function(t,e,n){return r(n,e,t._mountIndex)},removeChild:function(t,e){return o(t,e)},_mountChildAtIndex:function(t,e,n,r,i,o){return t._mountIndex=r,this.createChild(t,n,e)},_unmountChild:function(t,e){var n=this.removeChild(t,e);return t._mountIndex=null,n}}});t.exports=v},function(t,e,n){\"use strict\";function r(t){return!(!t||\"function\"!=typeof t.attachRef||\"function\"!=typeof t.detachRef)}var i=n(2),o=(n(0),{addComponentAsRefTo:function(t,e,n){r(n)?void 0:i(\"119\"),n.attachRef(e,t)},removeComponentAsRefFrom:function(t,e,n){r(n)?void 0:i(\"120\");var o=n.getPublicInstance();o&&o.refs[e]===t.getPublicInstance()&&n.detachRef(e)}});t.exports=o},function(t,e,n){\"use strict\";var r=\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\";t.exports=r},function(t,e,n){\"use strict\";function r(t){this.reinitializeTransaction(),this.renderToStaticMarkup=!1,this.reactMountReady=o.getPooled(null),this.useCreateElement=t}var i=n(3),o=n(155),a=n(17),u=n(51),c=n(162),s=(n(9),n(53)),l=n(88),f={initialize:c.getSelectionInformation,close:c.restoreSelection},p={initialize:function(){var t=u.isEnabled();return u.setEnabled(!1),t},close:function(t){u.setEnabled(t)}},h={initialize:function(){this.reactMountReady.reset()},close:function(){this.reactMountReady.notifyAll()}},d=[f,p,h],v={getTransactionWrappers:function(){return d},getReactMountReady:function(){return this.reactMountReady},getUpdateQueue:function(){return l},checkpoint:function(){return this.reactMountReady.checkpoint()},rollback:function(t){this.reactMountReady.rollback(t)},destructor:function(){o.release(this.reactMountReady),this.reactMountReady=null}};i(r.prototype,s,v),a.addPoolingTo(r),t.exports=r},function(t,e,n){\"use strict\";function r(t,e,n){\"function\"==typeof t?t(e.getPublicInstance()):o.addComponentAsRefTo(e,t,n)}function i(t,e,n){\"function\"==typeof t?t(null):o.removeComponentAsRefFrom(e,t,n)}var o=n(365),a={};a.attachRefs=function(t,e){if(null!==e&&\"object\"==typeof e){var n=e.ref;null!=n&&r(n,t,e._owner)}},a.shouldUpdateRefs=function(t,e){var n=null,r=null;null!==t&&\"object\"==typeof t&&(n=t.ref,r=t._owner);var i=null,o=null;return null!==e&&\"object\"==typeof e&&(i=e.ref,o=e._owner),n!==i||\"string\"==typeof i&&o!==r},a.detachRefs=function(t,e){if(null!==e&&\"object\"==typeof e){var n=e.ref;null!=n&&i(n,t,e._owner)}},t.exports=a},function(t,e,n){\"use strict\";function r(t){this.reinitializeTransaction(),this.renderToStaticMarkup=t,this.useCreateElement=!1,this.updateQueue=new u(this)}var i=n(3),o=n(17),a=n(53),u=(n(9),n(370)),c=[],s={enqueue:function(){}},l={getTransactionWrappers:function(){return c},getReactMountReady:function(){return s},getUpdateQueue:function(){return this.updateQueue},destructor:function(){},checkpoint:function(){},rollback:function(){}};i(r.prototype,a,l),o.addPoolingTo(r),t.exports=r},function(t,e,n){\"use strict\";function r(t,e){if(!(t instanceof e))throw new TypeError(\"Cannot call a class as a function\")}function i(t,e){}var o=n(88),a=(n(1),function(){function t(e){r(this,t),this.transaction=e}return t.prototype.isMounted=function(t){return!1},t.prototype.enqueueCallback=function(t,e,n){this.transaction.isInTransaction()&&o.enqueueCallback(t,e,n)},t.prototype.enqueueForceUpdate=function(t){this.transaction.isInTransaction()?o.enqueueForceUpdate(t):i(t,\"forceUpdate\")},t.prototype.enqueueReplaceState=function(t,e){this.transaction.isInTransaction()?o.enqueueReplaceState(t,e):i(t,\"replaceState\")},t.prototype.enqueueSetState=function(t,e){this.transaction.isInTransaction()?o.enqueueSetState(t,e):i(t,\"setState\")},t}());t.exports=a},function(t,e,n){\"use strict\";t.exports=\"15.4.2\"},function(t,e,n){\"use strict\";var r={xlink:\"http://www.w3.org/1999/xlink\",xml:\"http://www.w3.org/XML/1998/namespace\"},i={accentHeight:\"accent-height\",accumulate:0,additive:0,alignmentBaseline:\"alignment-baseline\",allowReorder:\"allowReorder\",alphabetic:0,amplitude:0,arabicForm:\"arabic-form\",ascent:0,attributeName:\"attributeName\",attributeType:\"attributeType\",autoReverse:\"autoReverse\",azimuth:0,baseFrequency:\"baseFrequency\",baseProfile:\"baseProfile\",baselineShift:\"baseline-shift\",bbox:0,begin:0,bias:0,by:0,calcMode:\"calcMode\",capHeight:\"cap-height\",clip:0,clipPath:\"clip-path\",clipRule:\"clip-rule\",clipPathUnits:\"clipPathUnits\",colorInterpolation:\"color-interpolation\",colorInterpolationFilters:\"color-interpolation-filters\",colorProfile:\"color-profile\",colorRendering:\"color-rendering\",contentScriptType:\"contentScriptType\",contentStyleType:\"contentStyleType\",cursor:0,cx:0,cy:0,d:0,decelerate:0,descent:0,diffuseConstant:\"diffuseConstant\",direction:0,display:0,divisor:0,dominantBaseline:\"dominant-baseline\",dur:0,dx:0,dy:0,edgeMode:\"edgeMode\",elevation:0,enableBackground:\"enable-background\",end:0,exponent:0,externalResourcesRequired:\"externalResourcesRequired\",fill:0,fillOpacity:\"fill-opacity\",fillRule:\"fill-rule\",filter:0,filterRes:\"filterRes\",filterUnits:\"filterUnits\",floodColor:\"flood-color\",floodOpacity:\"flood-opacity\",focusable:0,fontFamily:\"font-family\",fontSize:\"font-size\",fontSizeAdjust:\"font-size-adjust\",fontStretch:\"font-stretch\",fontStyle:\"font-style\",fontVariant:\"font-variant\",fontWeight:\"font-weight\",format:0,from:0,fx:0,fy:0,g1:0,g2:0,glyphName:\"glyph-name\",glyphOrientationHorizontal:\"glyph-orientation-horizontal\",glyphOrientationVertical:\"glyph-orientation-vertical\",glyphRef:\"glyphRef\",gradientTransform:\"gradientTransform\",gradientUnits:\"gradientUnits\",hanging:0,horizAdvX:\"horiz-adv-x\",horizOriginX:\"horiz-origin-x\",ideographic:0,imageRendering:\"image-rendering\",in:0,in2:0,intercept:0,k:0,k1:0,k2:0,k3:0,k4:0,kernelMatrix:\"kernelMatrix\",kernelUnitLength:\"kernelUnitLength\",kerning:0,keyPoints:\"keyPoints\",keySplines:\"keySplines\",keyTimes:\"keyTimes\",lengthAdjust:\"lengthAdjust\",letterSpacing:\"letter-spacing\",lightingColor:\"lighting-color\",limitingConeAngle:\"limitingConeAngle\",local:0,markerEnd:\"marker-end\",markerMid:\"marker-mid\",markerStart:\"marker-start\",markerHeight:\"markerHeight\",markerUnits:\"markerUnits\",markerWidth:\"markerWidth\",mask:0,maskContentUnits:\"maskContentUnits\",maskUnits:\"maskUnits\",mathematical:0,mode:0,numOctaves:\"numOctaves\",offset:0,opacity:0,operator:0,order:0,orient:0,orientation:0,origin:0,overflow:0,overlinePosition:\"overline-position\",overlineThickness:\"overline-thickness\",paintOrder:\"paint-order\",panose1:\"panose-1\",pathLength:\"pathLength\",patternContentUnits:\"patternContentUnits\",patternTransform:\"patternTransform\",patternUnits:\"patternUnits\",pointerEvents:\"pointer-events\",points:0,pointsAtX:\"pointsAtX\",pointsAtY:\"pointsAtY\",pointsAtZ:\"pointsAtZ\",preserveAlpha:\"preserveAlpha\",preserveAspectRatio:\"preserveAspectRatio\",primitiveUnits:\"primitiveUnits\",r:0,radius:0,refX:\"refX\",refY:\"refY\",renderingIntent:\"rendering-intent\",repeatCount:\"repeatCount\",repeatDur:\"repeatDur\",requiredExtensions:\"requiredExtensions\",requiredFeatures:\"requiredFeatures\",restart:0,result:0,rotate:0,rx:0,ry:0,scale:0,seed:0,shapeRendering:\"shape-rendering\",slope:0,spacing:0,specularConstant:\"specularConstant\",specularExponent:\"specularExponent\",speed:0,spreadMethod:\"spreadMethod\",startOffset:\"startOffset\",stdDeviation:\"stdDeviation\",stemh:0,stemv:0,stitchTiles:\"stitchTiles\",stopColor:\"stop-color\",stopOpacity:\"stop-opacity\",strikethroughPosition:\"strikethrough-position\",strikethroughThickness:\"strikethrough-thickness\",string:0,stroke:0,strokeDasharray:\"stroke-dasharray\",strokeDashoffset:\"stroke-dashoffset\",strokeLinecap:\"stroke-linecap\",strokeLinejoin:\"stroke-linejoin\",strokeMiterlimit:\"stroke-miterlimit\",strokeOpacity:\"stroke-opacity\",strokeWidth:\"stroke-width\",surfaceScale:\"surfaceScale\",systemLanguage:\"systemLanguage\",tableValues:\"tableValues\",targetX:\"targetX\",targetY:\"targetY\",textAnchor:\"text-anchor\",textDecoration:\"text-decoration\",textRendering:\"text-rendering\",textLength:\"textLength\",to:0,transform:0,u1:0,u2:0,underlinePosition:\"underline-position\",underlineThickness:\"underline-thickness\",unicode:0,unicodeBidi:\"unicode-bidi\",unicodeRange:\"unicode-range\",unitsPerEm:\"units-per-em\",vAlphabetic:\"v-alphabetic\",vHanging:\"v-hanging\",vIdeographic:\"v-ideographic\",vMathematical:\"v-mathematical\",values:0,vectorEffect:\"vector-effect\",version:0,vertAdvY:\"vert-adv-y\",vertOriginX:\"vert-origin-x\",vertOriginY:\"vert-origin-y\",viewBox:\"viewBox\",viewTarget:\"viewTarget\",visibility:0,widths:0,wordSpacing:\"word-spacing\",writingMode:\"writing-mode\",x:0,xHeight:\"x-height\",x1:0,x2:0,xChannelSelector:\"xChannelSelector\",xlinkActuate:\"xlink:actuate\",xlinkArcrole:\"xlink:arcrole\",xlinkHref:\"xlink:href\",xlinkRole:\"xlink:role\",xlinkShow:\"xlink:show\",xlinkTitle:\"xlink:title\",xlinkType:\"xlink:type\",xmlBase:\"xml:base\",xmlns:0,xmlnsXlink:\"xmlns:xlink\",xmlLang:\"xml:lang\",xmlSpace:\"xml:space\",y:0,y1:0,y2:0,yChannelSelector:\"yChannelSelector\",z:0,zoomAndPan:\"zoomAndPan\"},o={Properties:{},DOMAttributeNamespaces:{xlinkActuate:r.xlink,xlinkArcrole:r.xlink,xlinkHref:r.xlink,xlinkRole:r.xlink,xlinkShow:r.xlink,xlinkTitle:r.xlink,xlinkType:r.xlink,xmlBase:r.xml,xmlLang:r.xml,xmlSpace:r.xml},DOMAttributeNames:{}};Object.keys(i).forEach(function(t){o.Properties[t]=0,i[t]&&(o.DOMAttributeNames[t]=i[t])}),t.exports=o},function(t,e,n){\"use strict\";function r(t){if(\"selectionStart\"in t&&c.hasSelectionCapabilities(t))return{start:t.selectionStart,end:t.selectionEnd};if(window.getSelection){var e=window.getSelection();return{anchorNode:e.anchorNode,anchorOffset:e.anchorOffset,focusNode:e.focusNode,focusOffset:e.focusOffset}}if(document.selection){var n=document.selection.createRange();return{parentElement:n.parentElement(),text:n.text,top:n.boundingTop,left:n.boundingLeft}}}function i(t,e){if(y||null==v||v!==l())return null;var n=r(v);if(!m||!p(m,n)){m=n;var i=s.getPooled(d.select,g,t,e);return i.type=\"select\",i.target=v,o.accumulateTwoPhaseDispatches(i),i}return null}var o=n(23),a=n(6),u=n(4),c=n(162),s=n(14),l=n(152),f=n(170),p=n(80),h=a.canUseDOM&&\"documentMode\"in document&&document.documentMode<=11,d={select:{phasedRegistrationNames:{bubbled:\"onSelect\",captured:\"onSelectCapture\"},dependencies:[\"topBlur\",\"topContextMenu\",\"topFocus\",\"topKeyDown\",\"topKeyUp\",\"topMouseDown\",\"topMouseUp\",\"topSelectionChange\"]}},v=null,g=null,m=null,y=!1,_=!1,b={eventTypes:d,extractEvents:function(t,e,n,r){if(!_)return null;var o=e?u.getNodeFromInstance(e):window;switch(t){case\"topFocus\":(f(o)||\"true\"===o.contentEditable)&&(v=o,g=e,m=null);break;case\"topBlur\":v=null,g=null,m=null;break;case\"topMouseDown\":y=!0;break;case\"topContextMenu\":case\"topMouseUp\":return y=!1,i(n,r);case\"topSelectionChange\":if(h)break;case\"topKeyDown\":case\"topKeyUp\":return i(n,r)}return null},didPutListener:function(t,e,n){\"onSelect\"===e&&(_=!0)}};t.exports=b},function(t,e,n){\"use strict\";function r(t){return\".\"+t._rootNodeID}function i(t){return\"button\"===t||\"input\"===t||\"select\"===t||\"textarea\"===t}var o=n(2),a=n(150),u=n(23),c=n(4),s=n(375),l=n(376),f=n(14),p=n(379),h=n(381),d=n(52),v=n(378),g=n(382),m=n(383),y=n(25),_=n(384),b=n(8),x=n(91),w=(n(0),{}),C={};[\"abort\",\"animationEnd\",\"animationIteration\",\"animationStart\",\"blur\",\"canPlay\",\"canPlayThrough\",\"click\",\"contextMenu\",\"copy\",\"cut\",\"doubleClick\",\"drag\",\"dragEnd\",\"dragEnter\",\"dragExit\",\"dragLeave\",\"dragOver\",\"dragStart\",\"drop\",\"durationChange\",\"emptied\",\"encrypted\",\"ended\",\"error\",\"focus\",\"input\",\"invalid\",\"keyDown\",\"keyPress\",\"keyUp\",\"load\",\"loadedData\",\"loadedMetadata\",\"loadStart\",\"mouseDown\",\"mouseMove\",\"mouseOut\",\"mouseOver\",\"mouseUp\",\"paste\",\"pause\",\"play\",\"playing\",\"progress\",\"rateChange\",\"reset\",\"scroll\",\"seeked\",\"seeking\",\"stalled\",\"submit\",\"suspend\",\"timeUpdate\",\"touchCancel\",\"touchEnd\",\"touchMove\",\"touchStart\",\"transitionEnd\",\"volumeChange\",\"waiting\",\"wheel\"].forEach(function(t){var e=t[0].toUpperCase()+t.slice(1),n=\"on\"+e,r=\"top\"+e,i={phasedRegistrationNames:{bubbled:n,captured:n+\"Capture\"},dependencies:[r]};w[t]=i,C[r]=i});var M={},k={eventTypes:w,extractEvents:function(t,e,n,r){var i=C[t];if(!i)return null;var a;switch(t){case\"topAbort\":case\"topCanPlay\":case\"topCanPlayThrough\":case\"topDurationChange\":case\"topEmptied\":case\"topEncrypted\":case\"topEnded\":case\"topError\":case\"topInput\":case\"topInvalid\":case\"topLoad\":case\"topLoadedData\":case\"topLoadedMetadata\":case\"topLoadStart\":case\"topPause\":case\"topPlay\":case\"topPlaying\":case\"topProgress\":case\"topRateChange\":case\"topReset\":case\"topSeeked\":case\"topSeeking\":case\"topStalled\":case\"topSubmit\":case\"topSuspend\":case\"topTimeUpdate\":case\"topVolumeChange\":case\"topWaiting\":a=f;break;case\"topKeyPress\":if(0===x(n))return null;case\"topKeyDown\":case\"topKeyUp\":a=h;break;case\"topBlur\":case\"topFocus\":a=p;break;case\"topClick\":if(2===n.button)return null;case\"topDoubleClick\":case\"topMouseDown\":case\"topMouseMove\":case\"topMouseUp\":case\"topMouseOut\":case\"topMouseOver\":case\"topContextMenu\":a=d;break;case\"topDrag\":case\"topDragEnd\":case\"topDragEnter\":case\"topDragExit\":case\"topDragLeave\":case\"topDragOver\":case\"topDragStart\":case\"topDrop\":a=v;break;case\"topTouchCancel\":case\"topTouchEnd\":case\"topTouchMove\":case\"topTouchStart\":a=g;break;case\"topAnimationEnd\":case\"topAnimationIteration\":case\"topAnimationStart\":a=s;break;case\"topTransitionEnd\":a=m;break;case\"topScroll\":a=y;break;case\"topWheel\":a=_;break;case\"topCopy\":case\"topCut\":case\"topPaste\":a=l}a?void 0:o(\"86\",t);var c=a.getPooled(i,e,n,r);return u.accumulateTwoPhaseDispatches(c),c},didPutListener:function(t,e,n){if(\"onClick\"===e&&!i(t._tag)){var o=r(t),u=c.getNodeFromInstance(t);M[o]||(M[o]=a.listen(u,\"click\",b))}},willDeleteListener:function(t,e){if(\"onClick\"===e&&!i(t._tag)){var n=r(t);M[n].remove(),delete M[n]}}};t.exports=k},function(t,e,n){\"use strict\";function r(t,e,n,r){return i.call(this,t,e,n,r)}var i=n(14),o={animationName:null,elapsedTime:null,pseudoElement:null};i.augmentClass(r,o),t.exports=r},function(t,e,n){\"use strict\";function r(t,e,n,r){return i.call(this,t,e,n,r)}var i=n(14),o={clipboardData:function(t){return\"clipboardData\"in t?t.clipboardData:window.clipboardData}};i.augmentClass(r,o),t.exports=r},function(t,e,n){\"use strict\";function r(t,e,n,r){return i.call(this,t,e,n,r)}var i=n(14),o={data:null};i.augmentClass(r,o),t.exports=r},function(t,e,n){\"use strict\";function r(t,e,n,r){return i.call(this,t,e,n,r)}var i=n(52),o={dataTransfer:null};i.augmentClass(r,o),t.exports=r},function(t,e,n){\"use strict\";function r(t,e,n,r){return i.call(this,t,e,n,r)}var i=n(25),o={relatedTarget:null};i.augmentClass(r,o),t.exports=r},function(t,e,n){\"use strict\";function r(t,e,n,r){return i.call(this,t,e,n,r)}var i=n(14),o={data:null};i.augmentClass(r,o),t.exports=r},function(t,e,n){\"use strict\";function r(t,e,n,r){return i.call(this,t,e,n,r)}var i=n(25),o=n(91),a=n(389),u=n(92),c={key:a,location:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,repeat:null,locale:null,getModifierState:u,charCode:function(t){return\"keypress\"===t.type?o(t):0},keyCode:function(t){return\"keydown\"===t.type||\"keyup\"===t.type?t.keyCode:0},which:function(t){return\"keypress\"===t.type?o(t):\"keydown\"===t.type||\"keyup\"===t.type?t.keyCode:0}};i.augmentClass(r,c),t.exports=r},function(t,e,n){\"use strict\";function r(t,e,n,r){return i.call(this,t,e,n,r)}var i=n(25),o=n(92),a={touches:null,targetTouches:null,changedTouches:null,altKey:null,metaKey:null,ctrlKey:null,shiftKey:null,getModifierState:o};i.augmentClass(r,a),t.exports=r},function(t,e,n){\"use strict\";function r(t,e,n,r){return i.call(this,t,e,n,r)}var i=n(14),o={propertyName:null,elapsedTime:null,pseudoElement:null};i.augmentClass(r,o),t.exports=r},function(t,e,n){\"use strict\";function r(t,e,n,r){return i.call(this,t,e,n,r)}var i=n(52),o={deltaX:function(t){return\"deltaX\"in t?t.deltaX:\"wheelDeltaX\"in t?-t.wheelDeltaX:0},deltaY:function(t){return\"deltaY\"in t?t.deltaY:\"wheelDeltaY\"in t?-t.wheelDeltaY:\"wheelDelta\"in t?-t.wheelDelta:0},deltaZ:null,deltaMode:null};i.augmentClass(r,o),t.exports=r},function(t,e,n){\"use strict\";function r(t){for(var e=1,n=0,r=0,o=t.length,a=o&-4;r<a;){for(var u=Math.min(r+4096,a);r<u;r+=4)n+=(e+=t.charCodeAt(r))+(e+=t.charCodeAt(r+1))+(e+=t.charCodeAt(r+2))+(e+=t.charCodeAt(r+3));e%=i,n%=i}for(;r<o;r++)n+=e+=t.charCodeAt(r);return e%=i,n%=i,e|n<<16}var i=65521;t.exports=r},function(t,e,n){\"use strict\";function r(t,e,n){var r=null==e||\"boolean\"==typeof e||\"\"===e;if(r)return\"\";var i=isNaN(e);if(i||0===e||o.hasOwnProperty(t)&&o[t])return\"\"+e;if(\"string\"==typeof e){e=e.trim()}return e+\"px\"}var i=n(154),o=(n(1),i.isUnitlessNumber);t.exports=r},function(t,e,n){\"use strict\";function r(t){if(null==t)return null;if(1===t.nodeType)return t;var e=a.get(t);return e?(e=u(e),e?o.getNodeFromInstance(e):null):void(\"function\"==typeof t.render?i(\"44\"):i(\"45\",Object.keys(t)))}var i=n(2),o=(n(15),n(4)),a=n(40),u=n(167);n(0),n(1);t.exports=r},function(t,e,n){\"use strict\";(function(e){function r(t,e,n,r){if(t&&\"object\"==typeof t){var i=t,o=void 0===i[n];o&&null!=e&&(i[n]=e)}}function i(t,e){if(null==t)return t;var n={};return o(t,r,n),n}var o=(n(84),n(172));n(1);\"undefined\"!=typeof e&&e.env,1,t.exports=i}).call(e,n(153))},function(t,e,n){\"use strict\";function r(t){if(t.key){var e=o[t.key]||t.key;if(\"Unidentified\"!==e)return e}if(\"keypress\"===t.type){var n=i(t);return 13===n?\"Enter\":String.fromCharCode(n)}return\"keydown\"===t.type||\"keyup\"===t.type?a[t.keyCode]||\"Unidentified\":\"\"}var i=n(91),o={Esc:\"Escape\",Spacebar:\" \",Left:\"ArrowLeft\",Up:\"ArrowUp\",Right:\"ArrowRight\",Down:\"ArrowDown\",Del:\"Delete\",Win:\"OS\",Menu:\"ContextMenu\",Apps:\"ContextMenu\",Scroll:\"ScrollLock\",MozPrintableKey:\"Unidentified\"},a={8:\"Backspace\",9:\"Tab\",12:\"Clear\",13:\"Enter\",16:\"Shift\",17:\"Control\",18:\"Alt\",19:\"Pause\",20:\"CapsLock\",27:\"Escape\",32:\" \",33:\"PageUp\",34:\"PageDown\",35:\"End\",36:\"Home\",37:\"ArrowLeft\",38:\"ArrowUp\",39:\"ArrowRight\",40:\"ArrowDown\",45:\"Insert\",46:\"Delete\",112:\"F1\",113:\"F2\",114:\"F3\",115:\"F4\",116:\"F5\",117:\"F6\",118:\"F7\",119:\"F8\",120:\"F9\",121:\"F10\",122:\"F11\",123:\"F12\",144:\"NumLock\",145:\"ScrollLock\",224:\"Meta\"};t.exports=r},function(t,e,n){\"use strict\";function r(t){var e=t&&(i&&t[i]||t[o]);if(\"function\"==typeof e)return e}var i=\"function\"==typeof Symbol&&Symbol.iterator,o=\"@@iterator\";t.exports=r},function(t,e,n){\"use strict\";function r(){return i++}var i=1;t.exports=r},function(t,e,n){\"use strict\";function r(t){for(;t&&t.firstChild;)t=t.firstChild;return t}function i(t){for(;t;){if(t.nextSibling)return t.nextSibling;t=t.parentNode}}function o(t,e){for(var n=r(t),o=0,a=0;n;){if(3===n.nodeType){if(a=o+n.textContent.length,o<=e&&a>=e)return{node:n,offset:e-o};o=a}n=r(i(n))}}t.exports=o},function(t,e,n){\"use strict\";function r(t,e){var n={};return n[t.toLowerCase()]=e.toLowerCase(),n[\"Webkit\"+t]=\"webkit\"+e,n[\"Moz\"+t]=\"moz\"+e,n[\"ms\"+t]=\"MS\"+e,n[\"O\"+t]=\"o\"+e.toLowerCase(),n}function i(t){if(u[t])return u[t];if(!a[t])return t;var e=a[t];for(var n in e)if(e.hasOwnProperty(n)&&n in c)return u[t]=e[n];return\"\"}var o=n(6),a={animationend:r(\"Animation\",\"AnimationEnd\"),animationiteration:r(\"Animation\",\"AnimationIteration\"),animationstart:r(\"Animation\",\"AnimationStart\"),transitionend:r(\"Transition\",\"TransitionEnd\")},u={},c={};o.canUseDOM&&(c=document.createElement(\"div\").style,\"AnimationEvent\"in window||(delete a.animationend.animation,delete a.animationiteration.animation,delete a.animationstart.animation),\"TransitionEvent\"in window||delete a.transitionend.transition),t.exports=i},function(t,e,n){\"use strict\";function r(t){return'\"'+i(t)+'\"'}var i=n(54);t.exports=r},function(t,e,n){\"use strict\";var r=n(163);t.exports=r.renderSubtreeIntoContainer},function(t,e,n){\"use strict\";function r(t,e){var n=l.extractSingleTouch(e);return n?n[t.page]:t.page in e?e[t.page]:e[t.client]+f[t.envScroll]}function i(t,e){var n=r(b.x,e),i=r(b.y,e);return Math.pow(Math.pow(n-t.x,2)+Math.pow(i-t.y,2),.5)}function o(t){return{tapMoveThreshold:g,ignoreMouseThreshold:m,eventTypes:C,extractEvents:function(e,n,o,a){if(!h(e)&&!d(e))return null;if(v(e))_=M();else if(t(_,M()))return null;var u=null,l=i(y,o);return d(e)&&l<g&&(u=s.getPooled(C.touchTap,n,o,a)),h(e)?(y.x=r(b.x,o),y.y=r(b.y,o)):d(e)&&(y.x=0,y.y=0),c.accumulateTwoPhaseDispatches(u),u}}}var a=n(339),u=n(50),c=n(23),s=n(25),l=n(397),f=n(89),p=n(329),h=(a.topLevelTypes,u.isStartish),d=u.isEndish,v=function(t){var e=[\"topTouchCancel\",\"topTouchEnd\",\"topTouchStart\",\"topTouchMove\"];return e.indexOf(t)>=0},g=10,m=750,y={x:null,y:null},_=null,b={x:{page:\"pageX\",client:\"clientX\",envScroll:\"currentPageScrollLeft\"},y:{page:\"pageY\",client:\"clientY\",envScroll:\"currentPageScrollTop\"}},x=[\"topTouchStart\",\"topTouchCancel\",\"topTouchEnd\",\"topTouchMove\"],w=[\"topMouseDown\",\"topMouseMove\",\"topMouseUp\"].concat(x),C={touchTap:{phasedRegistrationNames:{bubbled:p({onTouchTap:null}),captured:p({onTouchTapCapture:null})},dependencies:w}},M=function(){return Date.now?Date.now:function(){return+new Date}}();t.exports=o},function(t,e){var n={extractSingleTouch:function(t){var e=t.touches,n=t.changedTouches,r=e&&e.length>0,i=n&&n.length>0;return!r&&i?n[0]:r?e[0]:t}};t.exports=n},function(t,e){t.exports=function(t,e){if(t&&e-t<750)return!0}},function(t,e,n){\"use strict\";function r(t){var e=/[=:]/g,n={\"=\":\"=0\",\":\":\"=2\"},r=(\"\"+t).replace(e,function(t){return n[t]});return\"$\"+r}function i(t){var e=/(=0|=2)/g,n={\"=0\":\"=\",\"=2\":\":\"},r=\".\"===t[0]&&\"$\"===t[1]?t.substring(2):t.substring(1);return(\"\"+r).replace(e,function(t){return n[t]})}var o={escape:r,unescape:i};t.exports=o},function(t,e,n){\"use strict\";var r=n(28),i=(n(0),function(t){var e=this;if(e.instancePool.length){var n=e.instancePool.pop();return e.call(n,t),n}return new e(t)}),o=function(t,e){var n=this;if(n.instancePool.length){var r=n.instancePool.pop();return n.call(r,t,e),r}return new n(t,e)},a=function(t,e,n){var r=this;if(r.instancePool.length){var i=r.instancePool.pop();return r.call(i,t,e,n),i}return new r(t,e,n)},u=function(t,e,n,r){var i=this;if(i.instancePool.length){var o=i.instancePool.pop();return i.call(o,t,e,n,r),o}return new i(t,e,n,r)},c=function(t){var e=this;t instanceof e?void 0:r(\"25\"),t.destructor(),e.instancePool.length<e.poolSize&&e.instancePool.push(t)},s=10,l=i,f=function(t,e){var n=t;return n.instancePool=[],n.getPooled=e||l,n.poolSize||(n.poolSize=s),n.release=c,n},p={addPoolingTo:f,oneArgumentPooler:i,twoArgumentPooler:o,threeArgumentPooler:a,fourArgumentPooler:u};t.exports=p},function(t,e,n){\"use strict\";function r(t){return(\"\"+t).replace(b,\"$&/\")}function i(t,e){this.func=t,this.context=e,this.count=0}function o(t,e,n){var r=t.func,i=t.context;r.call(i,e,t.count++)}function a(t,e,n){if(null==t)return t;var r=i.getPooled(e,n);m(t,o,r),i.release(r)}function u(t,e,n,r){this.result=t,this.keyPrefix=e,this.func=n,this.context=r,this.count=0}function c(t,e,n){var i=t.result,o=t.keyPrefix,a=t.func,u=t.context,c=a.call(u,e,t.count++);Array.isArray(c)?s(c,i,n,g.thatReturnsArgument):null!=c&&(v.isValidElement(c)&&(c=v.cloneAndReplaceKey(c,o+(!c.key||e&&e.key===c.key?\"\":r(c.key)+\"/\")+n)),i.push(c))}function s(t,e,n,i,o){var a=\"\";null!=n&&(a=r(n)+\"/\");var s=u.getPooled(e,a,i,o);m(t,c,s),u.release(s)}function l(t,e,n){if(null==t)return t;var r=[];return s(t,r,null,e,n),r}function f(t,e,n){return null}function p(t,e){return m(t,f,null)}function h(t){var e=[];return s(t,e,null,g.thatReturnsArgument),e}var d=n(400),v=n(27),g=n(8),m=n(409),y=d.twoArgumentPooler,_=d.fourArgumentPooler,b=/\\/+/g;i.prototype.destructor=function(){this.func=null,this.context=null,this.count=0},d.addPoolingTo(i,y),u.prototype.destructor=function(){this.result=null,this.keyPrefix=null,this.func=null,this.context=null,this.count=0},d.addPoolingTo(u,_);var x={forEach:a,map:l,mapIntoWithKeyPrefixInternal:s,count:p,toArray:h};t.exports=x},function(t,e,n){\"use strict\";function r(t){return t}function i(t,e){var n=b.hasOwnProperty(e)?b[e]:null;w.hasOwnProperty(e)&&(\"OVERRIDE_BASE\"!==n?p(\"73\",e):void 0),t&&(\"DEFINE_MANY\"!==n&&\"DEFINE_MANY_MERGED\"!==n?p(\"74\",e):void 0)}function o(t,e){if(e){\"function\"==typeof e?p(\"75\"):void 0,v.isValidElement(e)?p(\"76\"):void 0;var n=t.prototype,r=n.__reactAutoBindPairs;e.hasOwnProperty(y)&&x.mixins(t,e.mixins);for(var o in e)if(e.hasOwnProperty(o)&&o!==y){var a=e[o],u=n.hasOwnProperty(o);if(i(u,o),x.hasOwnProperty(o))x[o](t,a);else{var l=b.hasOwnProperty(o),f=\"function\"==typeof a,h=f&&!l&&!u&&e.autobind!==!1;if(h)r.push(o,a),n[o]=a;else if(u){var d=b[o];!l||\"DEFINE_MANY_MERGED\"!==d&&\"DEFINE_MANY\"!==d?p(\"77\",d,o):void 0,\"DEFINE_MANY_MERGED\"===d?n[o]=c(n[o],a):\"DEFINE_MANY\"===d&&(n[o]=s(n[o],a))}else n[o]=a}}}else;}function a(t,e){if(e)for(var n in e){var r=e[n];if(e.hasOwnProperty(n)){var i=n in x;i?p(\"78\",n):void 0;var o=n in t;o?p(\"79\",n):void 0,t[n]=r}}}function u(t,e){t&&e&&\"object\"==typeof t&&\"object\"==typeof e?void 0:p(\"80\");for(var n in e)e.hasOwnProperty(n)&&(void 0!==t[n]?p(\"81\",n):void 0,t[n]=e[n]);return t}function c(t,e){return function(){var n=t.apply(this,arguments),r=e.apply(this,arguments);if(null==n)return r;if(null==r)return n;var i={};return u(i,n),u(i,r),i}}function s(t,e){return function(){t.apply(this,arguments),e.apply(this,arguments)}}function l(t,e){var n=e.bind(t);return n;\n",
       "}function f(t){for(var e=t.__reactAutoBindPairs,n=0;n<e.length;n+=2){var r=e[n],i=e[n+1];t[r]=l(t,i)}}var p=n(28),h=n(3),d=n(97),v=n(27),g=(n(175),n(98)),m=n(38),y=(n(0),n(1),\"mixins\"),_=[],b={mixins:\"DEFINE_MANY\",statics:\"DEFINE_MANY\",propTypes:\"DEFINE_MANY\",contextTypes:\"DEFINE_MANY\",childContextTypes:\"DEFINE_MANY\",getDefaultProps:\"DEFINE_MANY_MERGED\",getInitialState:\"DEFINE_MANY_MERGED\",getChildContext:\"DEFINE_MANY_MERGED\",render:\"DEFINE_ONCE\",componentWillMount:\"DEFINE_MANY\",componentDidMount:\"DEFINE_MANY\",componentWillReceiveProps:\"DEFINE_MANY\",shouldComponentUpdate:\"DEFINE_ONCE\",componentWillUpdate:\"DEFINE_MANY\",componentDidUpdate:\"DEFINE_MANY\",componentWillUnmount:\"DEFINE_MANY\",updateComponent:\"OVERRIDE_BASE\"},x={displayName:function(t,e){t.displayName=e},mixins:function(t,e){if(e)for(var n=0;n<e.length;n++)o(t,e[n])},childContextTypes:function(t,e){t.childContextTypes=h({},t.childContextTypes,e)},contextTypes:function(t,e){t.contextTypes=h({},t.contextTypes,e)},getDefaultProps:function(t,e){t.getDefaultProps?t.getDefaultProps=c(t.getDefaultProps,e):t.getDefaultProps=e},propTypes:function(t,e){t.propTypes=h({},t.propTypes,e)},statics:function(t,e){a(t,e)},autobind:function(){}},w={replaceState:function(t,e){this.updater.enqueueReplaceState(this,t),e&&this.updater.enqueueCallback(this,e,\"replaceState\")},isMounted:function(){return this.updater.isMounted(this)}},C=function(){};h(C.prototype,d.prototype,w);var M={createClass:function(t){var e=r(function(t,n,r){this.__reactAutoBindPairs.length&&f(this),this.props=t,this.context=n,this.refs=m,this.updater=r||g,this.state=null;var i=this.getInitialState?this.getInitialState():null;\"object\"!=typeof i||Array.isArray(i)?p(\"82\",e.displayName||\"ReactCompositeComponent\"):void 0,this.state=i});e.prototype=new C,e.prototype.constructor=e,e.prototype.__reactAutoBindPairs=[],_.forEach(o.bind(null,e)),o(e,t),e.getDefaultProps&&(e.defaultProps=e.getDefaultProps()),e.prototype.render?void 0:p(\"83\");for(var n in b)e.prototype[n]||(e.prototype[n]=null);return e},injection:{injectMixin:function(t){_.push(t)}}};t.exports=M},function(t,e,n){\"use strict\";var r=n(27),i=r.createFactory,o={a:i(\"a\"),abbr:i(\"abbr\"),address:i(\"address\"),area:i(\"area\"),article:i(\"article\"),aside:i(\"aside\"),audio:i(\"audio\"),b:i(\"b\"),base:i(\"base\"),bdi:i(\"bdi\"),bdo:i(\"bdo\"),big:i(\"big\"),blockquote:i(\"blockquote\"),body:i(\"body\"),br:i(\"br\"),button:i(\"button\"),canvas:i(\"canvas\"),caption:i(\"caption\"),cite:i(\"cite\"),code:i(\"code\"),col:i(\"col\"),colgroup:i(\"colgroup\"),data:i(\"data\"),datalist:i(\"datalist\"),dd:i(\"dd\"),del:i(\"del\"),details:i(\"details\"),dfn:i(\"dfn\"),dialog:i(\"dialog\"),div:i(\"div\"),dl:i(\"dl\"),dt:i(\"dt\"),em:i(\"em\"),embed:i(\"embed\"),fieldset:i(\"fieldset\"),figcaption:i(\"figcaption\"),figure:i(\"figure\"),footer:i(\"footer\"),form:i(\"form\"),h1:i(\"h1\"),h2:i(\"h2\"),h3:i(\"h3\"),h4:i(\"h4\"),h5:i(\"h5\"),h6:i(\"h6\"),head:i(\"head\"),header:i(\"header\"),hgroup:i(\"hgroup\"),hr:i(\"hr\"),html:i(\"html\"),i:i(\"i\"),iframe:i(\"iframe\"),img:i(\"img\"),input:i(\"input\"),ins:i(\"ins\"),kbd:i(\"kbd\"),keygen:i(\"keygen\"),label:i(\"label\"),legend:i(\"legend\"),li:i(\"li\"),link:i(\"link\"),main:i(\"main\"),map:i(\"map\"),mark:i(\"mark\"),menu:i(\"menu\"),menuitem:i(\"menuitem\"),meta:i(\"meta\"),meter:i(\"meter\"),nav:i(\"nav\"),noscript:i(\"noscript\"),object:i(\"object\"),ol:i(\"ol\"),optgroup:i(\"optgroup\"),option:i(\"option\"),output:i(\"output\"),p:i(\"p\"),param:i(\"param\"),picture:i(\"picture\"),pre:i(\"pre\"),progress:i(\"progress\"),q:i(\"q\"),rp:i(\"rp\"),rt:i(\"rt\"),ruby:i(\"ruby\"),s:i(\"s\"),samp:i(\"samp\"),script:i(\"script\"),section:i(\"section\"),select:i(\"select\"),small:i(\"small\"),source:i(\"source\"),span:i(\"span\"),strong:i(\"strong\"),style:i(\"style\"),sub:i(\"sub\"),summary:i(\"summary\"),sup:i(\"sup\"),table:i(\"table\"),tbody:i(\"tbody\"),td:i(\"td\"),textarea:i(\"textarea\"),tfoot:i(\"tfoot\"),th:i(\"th\"),thead:i(\"thead\"),time:i(\"time\"),title:i(\"title\"),tr:i(\"tr\"),track:i(\"track\"),u:i(\"u\"),ul:i(\"ul\"),var:i(\"var\"),video:i(\"video\"),wbr:i(\"wbr\"),circle:i(\"circle\"),clipPath:i(\"clipPath\"),defs:i(\"defs\"),ellipse:i(\"ellipse\"),g:i(\"g\"),image:i(\"image\"),line:i(\"line\"),linearGradient:i(\"linearGradient\"),mask:i(\"mask\"),path:i(\"path\"),pattern:i(\"pattern\"),polygon:i(\"polygon\"),polyline:i(\"polyline\"),radialGradient:i(\"radialGradient\"),rect:i(\"rect\"),stop:i(\"stop\"),svg:i(\"svg\"),text:i(\"text\"),tspan:i(\"tspan\")};t.exports=o},function(t,e,n){\"use strict\";function r(t,e){return t===e?0!==t||1/t===1/e:t!==t&&e!==e}function i(t){this.message=t,this.stack=\"\"}function o(t){function e(e,n,r,o,a,u,c){o=o||E,u=u||r;if(null==n[r]){var s=w[a];return e?new i(null===n[r]?\"The \"+s+\" `\"+u+\"` is marked as required \"+(\"in `\"+o+\"`, but its value is `null`.\"):\"The \"+s+\" `\"+u+\"` is marked as required in \"+(\"`\"+o+\"`, but its value is `undefined`.\")):null}return t(n,r,o,a,u)}var n=e.bind(null,!1);return n.isRequired=e.bind(null,!0),n}function a(t){function e(e,n,r,o,a,u){var c=e[n],s=y(c);if(s!==t){var l=w[o],f=_(c);return new i(\"Invalid \"+l+\" `\"+a+\"` of type \"+(\"`\"+f+\"` supplied to `\"+r+\"`, expected \")+(\"`\"+t+\"`.\"))}return null}return o(e)}function u(){return o(M.thatReturns(null))}function c(t){function e(e,n,r,o,a){if(\"function\"!=typeof t)return new i(\"Property `\"+a+\"` of component `\"+r+\"` has invalid PropType notation inside arrayOf.\");var u=e[n];if(!Array.isArray(u)){var c=w[o],s=y(u);return new i(\"Invalid \"+c+\" `\"+a+\"` of type \"+(\"`\"+s+\"` supplied to `\"+r+\"`, expected an array.\"))}for(var l=0;l<u.length;l++){var f=t(u,l,r,o,a+\"[\"+l+\"]\",C);if(f instanceof Error)return f}return null}return o(e)}function s(){function t(t,e,n,r,o){var a=t[e];if(!x.isValidElement(a)){var u=w[r],c=y(a);return new i(\"Invalid \"+u+\" `\"+o+\"` of type \"+(\"`\"+c+\"` supplied to `\"+n+\"`, expected a single ReactElement.\"))}return null}return o(t)}function l(t){function e(e,n,r,o,a){if(!(e[n]instanceof t)){var u=w[o],c=t.name||E,s=b(e[n]);return new i(\"Invalid \"+u+\" `\"+a+\"` of type \"+(\"`\"+s+\"` supplied to `\"+r+\"`, expected \")+(\"instance of `\"+c+\"`.\"))}return null}return o(e)}function f(t){function e(e,n,o,a,u){for(var c=e[n],s=0;s<t.length;s++)if(r(c,t[s]))return null;var l=w[a],f=JSON.stringify(t);return new i(\"Invalid \"+l+\" `\"+u+\"` of value `\"+c+\"` \"+(\"supplied to `\"+o+\"`, expected one of \"+f+\".\"))}return Array.isArray(t)?o(e):M.thatReturnsNull}function p(t){function e(e,n,r,o,a){if(\"function\"!=typeof t)return new i(\"Property `\"+a+\"` of component `\"+r+\"` has invalid PropType notation inside objectOf.\");var u=e[n],c=y(u);if(\"object\"!==c){var s=w[o];return new i(\"Invalid \"+s+\" `\"+a+\"` of type \"+(\"`\"+c+\"` supplied to `\"+r+\"`, expected an object.\"))}for(var l in u)if(u.hasOwnProperty(l)){var f=t(u,l,r,o,a+\".\"+l,C);if(f instanceof Error)return f}return null}return o(e)}function h(t){function e(e,n,r,o,a){for(var u=0;u<t.length;u++){var c=t[u];if(null==c(e,n,r,o,a,C))return null}var s=w[o];return new i(\"Invalid \"+s+\" `\"+a+\"` supplied to \"+(\"`\"+r+\"`.\"))}return Array.isArray(t)?o(e):M.thatReturnsNull}function d(){function t(t,e,n,r,o){if(!g(t[e])){var a=w[r];return new i(\"Invalid \"+a+\" `\"+o+\"` supplied to \"+(\"`\"+n+\"`, expected a ReactNode.\"))}return null}return o(t)}function v(t){function e(e,n,r,o,a){var u=e[n],c=y(u);if(\"object\"!==c){var s=w[o];return new i(\"Invalid \"+s+\" `\"+a+\"` of type `\"+c+\"` \"+(\"supplied to `\"+r+\"`, expected `object`.\"))}for(var l in t){var f=t[l];if(f){var p=f(u,l,r,o,a+\".\"+l,C);if(p)return p}}return null}return o(e)}function g(t){switch(typeof t){case\"number\":case\"string\":case\"undefined\":return!0;case\"boolean\":return!t;case\"object\":if(Array.isArray(t))return t.every(g);if(null===t||x.isValidElement(t))return!0;var e=k(t);if(!e)return!1;var n,r=e.call(t);if(e!==t.entries){for(;!(n=r.next()).done;)if(!g(n.value))return!1}else for(;!(n=r.next()).done;){var i=n.value;if(i&&!g(i[1]))return!1}return!0;default:return!1}}function m(t,e){return\"symbol\"===t||(\"Symbol\"===e[\"@@toStringTag\"]||\"function\"==typeof Symbol&&e instanceof Symbol)}function y(t){var e=typeof t;return Array.isArray(t)?\"array\":t instanceof RegExp?\"object\":m(e,t)?\"symbol\":e}function _(t){var e=y(t);if(\"object\"===e){if(t instanceof Date)return\"date\";if(t instanceof RegExp)return\"regexp\"}return e}function b(t){return t.constructor&&t.constructor.name?t.constructor.name:E}var x=n(27),w=n(175),C=n(405),M=n(8),k=n(177),E=(n(1),\"<<anonymous>>\"),T={array:a(\"array\"),bool:a(\"boolean\"),func:a(\"function\"),number:a(\"number\"),object:a(\"object\"),string:a(\"string\"),symbol:a(\"symbol\"),any:u(),arrayOf:c,element:s(),instanceOf:l,node:d(),objectOf:p,oneOf:f,oneOfType:h,shape:v};i.prototype=Error.prototype,t.exports=T},function(t,e,n){\"use strict\";var r=\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\";t.exports=r},function(t,e,n){\"use strict\";function r(t,e,n){this.props=t,this.context=e,this.refs=c,this.updater=n||u}function i(){}var o=n(3),a=n(97),u=n(98),c=n(38);i.prototype=a.prototype,r.prototype=new i,r.prototype.constructor=r,o(r.prototype,a.prototype),r.prototype.isPureReactComponent=!0,t.exports=r},function(t,e,n){\"use strict\";t.exports=\"15.4.2\"},function(t,e,n){\"use strict\";function r(t){return o.isValidElement(t)?void 0:i(\"143\"),t}var i=n(28),o=n(27);n(0);t.exports=r},function(t,e,n){\"use strict\";function r(t,e){return t&&\"object\"==typeof t&&null!=t.key?s.escape(t.key):e.toString(36)}function i(t,e,n,o){var p=typeof t;if(\"undefined\"!==p&&\"boolean\"!==p||(t=null),null===t||\"string\"===p||\"number\"===p||\"object\"===p&&t.$$typeof===u)return n(o,t,\"\"===e?l+r(t,0):e),1;var h,d,v=0,g=\"\"===e?l:e+f;if(Array.isArray(t))for(var m=0;m<t.length;m++)h=t[m],d=g+r(h,m),v+=i(h,d,n,o);else{var y=c(t);if(y){var _,b=y.call(t);if(y!==t.entries)for(var x=0;!(_=b.next()).done;)h=_.value,d=g+r(h,x++),v+=i(h,d,n,o);else for(;!(_=b.next()).done;){var w=_.value;w&&(h=w[1],d=g+s.escape(w[0])+f+r(h,0),v+=i(h,d,n,o))}}else if(\"object\"===p){var C=\"\",M=String(t);a(\"31\",\"[object Object]\"===M?\"object with keys {\"+Object.keys(t).join(\", \")+\"}\":M,C)}}return v}function o(t,e,n){return null==t?0:i(t,\"\",e,n)}var a=n(28),u=(n(15),n(174)),c=n(177),s=(n(0),n(399)),l=(n(1),\".\"),f=\":\";t.exports=o},function(t,e,n){\"use strict\";function r(t){return t&&t.__esModule?t:{default:t}}var i=n(41),o=r(i),a=n(182),u=r(a),c=n(183),s=r(c),l=n(181),f=r(l),p=n(180),h=r(p),d=n(179),v=r(d);(0,s.default)(),window.SHAP={SimpleListVisualizer:f.default,AdditiveForceVisualizer:h.default,AdditiveForceArrayVisualizer:v.default,React:o.default,ReactDom:u.default}}]);</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import random, os, datetime, pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# xgbr에 내장된 함수 이용\n",
    "xgbr = xgb.XGBRegressor()\n",
    "#xgbr.load_model('result/outliers+w+h/size/acc1-0.621_acc3-0.982_XGBR_5inputs_10fold/model.model')\n",
    "xgbr.load_model('result/outliers+w+h/size/acc1-0.622_acc3-0.981_XGBR_4inputs_10fold/model.model')\n",
    "\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(xgbr)\n",
    "shap_values = explainer.shap_values(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f64c8af5-4c2c-483e-b15e-4f7b5a4fd04c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T07:19:45.070237Z",
     "iopub.status.busy": "2023-06-05T07:19:45.069735Z",
     "iopub.status.idle": "2023-06-05T07:19:48.932016Z",
     "shell.execute_reply": "2023-06-05T07:19:48.931347Z",
     "shell.execute_reply.started": "2023-06-05T07:19:45.070175Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x223.2 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "INPUT_VARS = ['age','weight','height', 'cuffed']\n",
    "\n",
    "X_train = pd.DataFrame(x_train, columns=INPUT_VARS)\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "fig.set_facecolor('white')\n",
    "ax = fig.add_subplot()\n",
    "shap.summary_plot(shap_values, X_train, show=False)\n",
    "ax.set_xlabel('SHAP Value')\n",
    "#ax.set_title('SHAP Dot Plot', fontsize=20)\n",
    "plt.savefig(\"figures/Fig4_a_size_SHAP.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a9b519-2baa-472d-87d7-bda9ffa43a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(x_train, columns=INPUT_VARS)\n",
    "shap.summary_plot(shap_values, X_train, plot_type = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd039b16-eae0-4880-bf7e-5887aa67cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_interaction_values = explainer.shap_interaction_values(X_train)\n",
    "shap.summary_plot(shap_interaction_values, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2455a7cb-3ae1-4572-841b-d2e12a426214",
   "metadata": {},
   "source": [
    "[Documentation of shap plot](https://shap-lrjball.readthedocs.io/en/latest/example_notebooks/plots/scatter.html?highlight=scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f17b0f6-16a1-484b-96bd-445c6a3e26d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T05:53:02.878631Z",
     "iopub.status.busy": "2023-06-08T05:53:02.878370Z",
     "iopub.status.idle": "2023-06-08T05:53:03.495501Z",
     "shell.execute_reply": "2023-06-08T05:53:03.494943Z",
     "shell.execute_reply.started": "2023-06-08T05:53:02.878597Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap_xgb = explainer(X_train)\n",
    "shap.plots.scatter(shap_xgb[:,'age'], dot_size = None, show=False)\n",
    "#plt.rcParams['font.family'] = 'AvenirNextLTPro'\n",
    "plt.savefig(\"figures/Fig5_a_age.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92061a40-c230-42e2-903b-a52c1c62cc7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T04:08:06.228526Z",
     "iopub.status.busy": "2023-06-06T04:08:06.228022Z",
     "iopub.status.idle": "2023-06-06T04:08:08.187489Z",
     "shell.execute_reply": "2023-06-06T04:08:08.186842Z",
     "shell.execute_reply.started": "2023-06-06T04:08:06.228466Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap.plots.scatter(shap_xgb[:,'cuffed'], x_jitter=0.1, dot_size = None, show=False)\n",
    "plt.xticks([0,1], ['uncuffed', 'cuffed'])\n",
    "plt.savefig(\"figures/Fig5_d_cuffed.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d9b54a7-011d-405e-a9b6-bafab3b207e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T03:32:43.180794Z",
     "iopub.status.busy": "2023-04-19T03:32:43.180349Z",
     "iopub.status.idle": "2023-04-19T03:32:44.362168Z",
     "shell.execute_reply": "2023-04-19T03:32:44.361591Z",
     "shell.execute_reply.started": "2023-04-19T03:32:43.180741Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ind = 20\n",
    "\n",
    "fig,ax = shap.partial_dependence_plot(\n",
    "    \"age\", xgbr.predict, X_train, model_expected_value=True,\n",
    "    feature_expected_value=True, show=False, ice=False,\n",
    "    shap_values=shap_xgb[sample_ind:sample_ind+1,:]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numba",
   "language": "python",
   "name": "numbaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
