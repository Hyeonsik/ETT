{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d6614ff-760e-44c5-82f4-625243c70792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T06:01:09.116491Z",
     "iopub.status.busy": "2023-02-22T06:01:09.115912Z",
     "iopub.status.idle": "2023-02-22T06:01:15.412445Z",
     "shell.execute_reply": "2023-02-22T06:01:15.411812Z",
     "shell.execute_reply.started": "2023-02-22T06:01:09.116434Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 전체 opid: 155719\n",
      "airway tube type이 plain이 아닌 경우 제외: 78848\n",
      "age, airway_tube_size 결측치 제외: 78502\n",
      "10세 이상 제외 : 59352\n",
      "cuffed data가 없는 경우는 제외: 45067\n",
      "중복되는 hid는 첫번째 수술 외 제외: 34042\n",
      "depth가 없는 경우 제외: 30515\n",
      "depth가 8cm 미만인 경우 제외: 30223\n",
      "x_train: (24179, 6), x_test: (6044, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import random, os, datetime, pickle\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv, math\n",
    "\n",
    "\n",
    "# tube.csv에 수술 시점 weight, height 추가\n",
    "df0 = pd.read_csv('tube.csv')\n",
    "df0.drop(columns=['weight', 'height'], inplace=True)\n",
    "df = pd.read_csv('demography_revised.csv')\n",
    "df = df0.merge(df[['opid', 'weight', 'height']], how='left', on='opid', suffixes=('_o',''))\n",
    "\n",
    "df.loc[df['weight'] <= 1, 'weight'] = None\n",
    "df.loc[df['weight'] > 200, 'weight'] = None\n",
    "df.loc[df['height'] <= 30, 'height'] = None\n",
    "df.loc[df['height'] > 230, 'height'] = None\n",
    "df['age'] = df['age'].astype(int)\n",
    "df = df.loc[df['age'] < 19]\n",
    "print(f'초기 전체 opid: {len(df)}')\n",
    "df = df.loc[df['airway_tube_type'] == 'plain']\n",
    "# [nan 'plain' 'RAE(oral)' 'reinforced' 'LMA' 'T-tube' 'CobraPLA', 'double lumen tube' 'RAE(nasal)' 'laser' 'combitube' 'univent']\n",
    "\n",
    "print(f'airway tube type이 plain이 아닌 경우 제외: {len(df)}')\n",
    "# age, sex, airway tube size 값이 없는 경우는 제외\n",
    "\n",
    "df.dropna(subset=['age', 'airway_tube_size'], inplace=True)  # sex는 모든 데이터 다 있음\n",
    "df['sex'] = (df['sex'] == 'M')\n",
    "print(f'age, airway_tube_size 결측치 제외: {len(df)}')\n",
    "\n",
    "# 나이 계산 -> age_cal 열에 추가\n",
    "df_b = pd.read_csv('birth_sex.csv')\n",
    "df_b.rename(columns={'생년월일':'birth_date'}, inplace=True)\n",
    "df_b['birth_date'] = df_b['birth_date'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "\n",
    "df_o = pd.read_csv('opdates.csv')\n",
    "df_o['opdate'] = df_o['opdate'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "\n",
    "df1 = pd.merge(df_o, df_b, how='inner', on='hid')\n",
    "df1['age_cal'] = (df1['opdate'] - df1['birth_date'])/pd.Timedelta(days=365.2425)\n",
    "\n",
    "df = pd.merge(df, df1[['opid', 'age_cal', 'opdate', 'birth_date']], how='inner', on='opid')\n",
    "#df3 = pd.merge(df, df1[['opid', 'age_cal', 'opdate', 'birth_date']], how='left', on='opid')\n",
    "\n",
    "# inclusion criteria : 소아 10세 미만\n",
    "#df['age_cal'] = df['age_cal'] + 0.01 # 생일이랑 수술날 같은 경우\n",
    "df = df.loc[df['age_cal'] < 10-0.01]  # 생일이랑 수술날 같은 경우\n",
    "df = df.loc[df['age_cal'] > 0]\n",
    "print(f'10세 이상 제외 : {len(df)}')\n",
    "\n",
    "# cuffed 여부와 fixed depth 추가\n",
    "df_t = pd.read_csv('tube_type.csv')\n",
    "df_t['cuffed'] = (df_t['cuffed'] == 1)\n",
    "\n",
    "df_f = pd.read_csv('tube_fixed.csv')\n",
    "\n",
    "# merge 하면서 cuffed 데이터가 없는 경우는 제외\n",
    "df = df.merge(df_f, how='left', on='opid')\n",
    "df = df.merge(df_t[['opid', 'cuffed']], how='inner', on='opid')\n",
    "print(f'cuffed data가 없는 경우는 제외: {len(df)}')\n",
    "\n",
    "\n",
    "# 중복되는 hid 경우 제외 (첫번째 수술기록만 가져오기)\n",
    "df = df.merge(df_o[['opid','hid']], how='inner', on='opid')\n",
    "df = df.loc[df[['hid', 'opid']].groupby('hid')['opid'].idxmin()]\n",
    "print(f'중복되는 hid는 첫번째 수술 외 제외: {len(df)}')\n",
    "\n",
    "\n",
    "# age-based formula에 따른 ETT size\n",
    "OLD_VAR = 'old_tube_size'\n",
    "# df[OLD_VAR] = np.round((df['age'] / 4 + 4) * 2) / 2\n",
    "df[OLD_VAR] = df['age'].apply(lambda x: np.round((x / 4 + 4) * 2) / 2 if x >= 2 else (3.5 if x < 1 else 4)) \n",
    "df[OLD_VAR] = df.apply(lambda x: x[OLD_VAR] - 0.5 if x['cuffed'] else x[OLD_VAR], axis=1)\n",
    "\n",
    "\n",
    "# fixed depth를 output\n",
    "INPUT_VARS = ['age_cal','sex','weight','height', 'cuffed', 'airway_tube_size']\n",
    "TARGET_VAR = 'fixed'\n",
    "\n",
    "\n",
    "random.seed(98)\n",
    "#seed_everything(SEED)\n",
    "df.dropna(subset=['fixed'], inplace=True)\n",
    "print(f'depth가 없는 경우 제외: {len(df)}')\n",
    "df = df.loc[df['fixed']>=8]\n",
    "print(f'depth가 8cm 미만인 경우 제외: {len(df)}')\n",
    "df = shuffle(df)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "y = df[[TARGET_VAR]].values.flatten().astype(float)\n",
    "x = df.loc[:, INPUT_VARS].values.astype(float)\n",
    "c = df['opid'].values.flatten().astype(int)\n",
    "\n",
    "# 저장하기\n",
    "pickle.dump(df, open(f'dataset/ETT_depth_8','wb'))\n",
    "np.savez(f'dataset/ETT_depth_8.npz', x=x, y=y, c=c)\n",
    "\n",
    "\n",
    "# training set의 뒤쪽 20%를 test set 으로 사용\n",
    "nsamp = len(y)\n",
    "ntest = int(nsamp * 0.2)\n",
    "ntrain = nsamp - ntest\n",
    "x_test = x[-ntest:, :]\n",
    "y_test = y[-ntest:]\n",
    "#y_test_old = y_old[-ntest:]\n",
    "x_train = x[:ntrain, :]\n",
    "y_train = y[:ntrain]\n",
    "\n",
    "print(f'x_train: {(x_train).shape}, x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb3ff492-f04e-43c5-8da4-f77dca49a257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T13:41:27.346879Z",
     "iopub.status.busy": "2023-02-22T13:41:27.346542Z",
     "iopub.status.idle": "2023-02-22T13:41:27.357593Z",
     "shell.execute_reply": "2023-02-22T13:41:27.356996Z",
     "shell.execute_reply.started": "2023-02-22T13:41:27.346849Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (24179, 5), x_test: (6044, 5)\n"
     ]
    }
   ],
   "source": [
    "dat = np.load(f'dataset/ETT_depth_8.npz')\n",
    "x, y = dat['x'], dat['y']\n",
    "x = x[:,0:5]\n",
    "\n",
    "\n",
    "nsamp = len(y)\n",
    "ntest = int(nsamp * 0.2)\n",
    "ntrain = nsamp - ntest\n",
    "x_test = x[-ntest:, :]\n",
    "y_test = y[-ntest:]\n",
    "x_train = x[:ntrain, :]\n",
    "y_train = y[:ntrain]\n",
    "\n",
    "print(f'x_train: {(x_train).shape}, x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0eacf293-820f-406e-8ed4-7303d05c3d7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T13:41:28.976384Z",
     "iopub.status.busy": "2023-02-22T13:41:28.975907Z",
     "iopub.status.idle": "2023-02-22T13:41:28.983049Z",
     "shell.execute_reply": "2023-02-22T13:41:28.981888Z",
     "shell.execute_reply.started": "2023-02-22T13:41:28.976334Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (24179, 4), x_test: (6044, 4)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train[:,0:4]\n",
    "x_test = x_test[:,0:4]\n",
    "\n",
    "print(f'x_train: {(x_train).shape}, x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f78b54d-0f13-4a0e-835b-25d5311eb18c",
   "metadata": {},
   "source": [
    "# PALS guideline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d3cf94-a0ee-40fe-becd-1de488b12cbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T06:01:15.414194Z",
     "iopub.status.busy": "2023-02-22T06:01:15.413930Z",
     "iopub.status.idle": "2023-02-22T06:01:15.424228Z",
     "shell.execute_reply": "2023-02-22T06:01:15.423633Z",
     "shell.execute_reply.started": "2023-02-22T06:01:15.414169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model = 3 * age-based ETT size\n",
      "--------------\n",
      "explained_variance_score: 0.605\n",
      "mean_squared_errors: 3.043\n",
      "mean_absolute_errors: 1.283\n",
      "r2_score: 0.588\n"
     ]
    }
   ],
   "source": [
    "OLD_VAR = 'old_depth1'\n",
    "df[OLD_VAR] = 3 * df['old_tube_size']\n",
    "#df[OLD_VAR] = df['airway_tube_size'].apply(lambda x: np.round((x / 4 + 4) * 2) / 2 if x >= 2 else (3.5 if x < 1 else 4)) \n",
    "#df[OLD_VAR] = df.apply(lambda x: x[OLD_VAR] - 0.5 if x['cuffed'] else x[OLD_VAR], axis=1)\n",
    "y_old = df[[OLD_VAR]].values.flatten().astype(float)\n",
    "y_test_old = y_old[-ntest:]\n",
    "\n",
    "print('--------------')\n",
    "print('old model = 3 * age-based ETT size')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_test_old):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_test_old):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_test_old):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_test_old):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1cef3ca-5df5-4bef-b537-6d55e654ff81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T06:01:15.425209Z",
     "iopub.status.busy": "2023-02-22T06:01:15.424983Z",
     "iopub.status.idle": "2023-02-22T06:01:15.434097Z",
     "shell.execute_reply": "2023-02-22T06:01:15.433560Z",
     "shell.execute_reply.started": "2023-02-22T06:01:15.425186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model = 3 * ETT size\n",
      "--------------\n",
      "explained_variance_score: 0.617\n",
      "mean_squared_errors: 3.048\n",
      "mean_absolute_errors: 1.331\n",
      "r2_score: 0.587\n"
     ]
    }
   ],
   "source": [
    "OLD_VAR = 'old_depth1'\n",
    "# OLD_VAR = 'old_tube_size'\n",
    "df[OLD_VAR] = 3 * df['airway_tube_size']\n",
    "#df[OLD_VAR] = df['airway_tube_size'].apply(lambda x: np.round((x / 4 + 4) * 2) / 2 if x >= 2 else (3.5 if x < 1 else 4)) \n",
    "#df[OLD_VAR] = df.apply(lambda x: x[OLD_VAR] - 0.5 if x['cuffed'] else x[OLD_VAR], axis=1)\n",
    "y_old = df[[OLD_VAR]].values.flatten().astype(float)\n",
    "y_test_old = y_old[-ntest:]\n",
    "\n",
    "print('--------------')\n",
    "print('old model = 3 * ETT size')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_test_old):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_test_old):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_test_old):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_test_old):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6722a993-afd6-4402-8345-f83869bbbb5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T06:01:15.435397Z",
     "iopub.status.busy": "2023-02-22T06:01:15.435171Z",
     "iopub.status.idle": "2023-02-22T06:01:15.444357Z",
     "shell.execute_reply": "2023-02-22T06:01:15.443830Z",
     "shell.execute_reply.started": "2023-02-22T06:01:15.435374Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model = age / 2 + 12\n",
      "--------------\n",
      "explained_variance_score: 0.581\n",
      "mean_squared_errors: 3.133\n",
      "mean_absolute_errors: 1.361\n",
      "r2_score: 0.575\n"
     ]
    }
   ],
   "source": [
    "OLD_VAR = 'old_depth1'\n",
    "# OLD_VAR = 'old_tube_size'\n",
    "df[OLD_VAR] = df['age'] / 2 + 12\n",
    "#df[OLD_VAR] = df['airway_tube_size'].apply(lambda x: np.round((x / 4 + 4) * 2) / 2 if x >= 2 else (3.5 if x < 1 else 4)) \n",
    "#df[OLD_VAR] = df.apply(lambda x: x[OLD_VAR] - 0.5 if x['cuffed'] else x[OLD_VAR], axis=1)\n",
    "y_old = df[[OLD_VAR]].values.flatten().astype(float)\n",
    "y_test_old = y_old[-ntest:]\n",
    "\n",
    "print('--------------')\n",
    "print('old model = age / 2 + 12')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_test_old):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_test_old):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_test_old):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_test_old):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b287a-14a9-4c13-bf8d-7b28caf608d1",
   "metadata": {},
   "source": [
    "# XGBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94a696eb-7544-4fc7-9062-d877d87ef864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T12:04:03.406128Z",
     "iopub.status.busy": "2023-02-22T12:04:03.405620Z",
     "iopub.status.idle": "2023-02-22T12:05:07.175061Z",
     "shell.execute_reply": "2023-02-22T12:05:07.174323Z",
     "shell.execute_reply.started": "2023-02-22T12:04:03.406075Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 120 candidates, totalling 1200 fits\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'colsample_bytree': 0.8, 'max_depth': 4, 'n_estimators': 25, 'subsample': 1}\n",
      "0.8223467297039477\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.809\n",
      "mean_squared_errors: 1.408\n",
      "mean_absolute_errors: 0.845\n",
      "r2_score: 0.809\n",
      "acc(+-1cm): 0.692\n"
     ]
    }
   ],
   "source": [
    "# 길이 제한 없는 경우\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# age (일단위)\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.05, 0.07, 0.1], #[0.01, 0.03, 0.05],\n",
    "                'max_depth': [3, 4, 5, 7],#[3,4,5],\n",
    "                'n_estimators': [25, 50, 100, 200, 300],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                'subsample': [0.5, 0.8, 1], #[0.5, 0.8, 1],\n",
    "                'colsample_bytree': [0.5, 0.8], #[0.8, 1],\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=xgb.sklearn.XGBRegressor(),\n",
    "                    n_jobs=-1,\n",
    "                    verbose=3,\n",
    "                    param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train, y_train)\n",
    "model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = gs.predict(x_test).flatten()\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mse:.3f}')\n",
    "print(f'mean_absolute_errors: {mae:.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc3 = np.mean((y_pred >= y_test-1) & (y_pred <= y_test+1))\n",
    "print(f'acc(+-1cm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "686af611-364c-4e76-b340-48ae4334de67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T12:09:36.229776Z",
     "iopub.status.busy": "2023-02-22T12:09:36.229226Z",
     "iopub.status.idle": "2023-02-22T12:09:36.247976Z",
     "shell.execute_reply": "2023-02-22T12:09:36.246901Z",
     "shell.execute_reply.started": "2023-02-22T12:09:36.229714Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "odir_f = f'mae-{mae:.3f}_mse-{mse:.3f}_XGBR-4inputs_{nfold}fold'\n",
    "odir = f'result/depth_8cm/{odir_f}'\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "model.save_model(f'{odir}/model.model')\n",
    "pickle.dump(gs, open(f'{odir}/gridSearch','wb'))\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "pickle.dump(param_dict, open(f'{odir}/param_dict', 'wb'))\n",
    "f = open(f'{odir}/result.txt', 'w')\n",
    "f.write(f'classification model')\n",
    "f.write(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "f.write(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfda6e4-42db-4eb9-b678-1f0811ef00e8",
   "metadata": {},
   "source": [
    "# RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "564ae98a-b9bc-4834-a8ec-557185ae23ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T13:22:34.136727Z",
     "iopub.status.busy": "2023-02-22T13:22:34.136204Z",
     "iopub.status.idle": "2023-02-22T13:22:34.296471Z",
     "shell.execute_reply": "2023-02-22T13:22:34.295889Z",
     "shell.execute_reply.started": "2023-02-22T13:22:34.136673Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (24179, 5), x_test shape: (6044, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imp = IterativeImputer().fit(x_train)\n",
    "x_train_imputed = imp.transform(x_train)\n",
    "x_test_imputed = imp.transform(x_test)\n",
    "\n",
    "print(f'x_train shape: {x_train_imputed.shape}, x_test shape: {x_test_imputed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96f20dbc-4272-40c7-8390-430ebb86aaa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T13:22:42.770326Z",
     "iopub.status.busy": "2023-02-22T13:22:42.769825Z",
     "iopub.status.idle": "2023-02-22T13:32:01.814792Z",
     "shell.execute_reply": "2023-02-22T13:32:01.814086Z",
     "shell.execute_reply.started": "2023-02-22T13:22:42.770273Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 216 candidates, totalling 2160 fits\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.794 total time=   8.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.818 total time=  16.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.821 total time=  15.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.799 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.800 total time=   7.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.801 total time=  22.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.804 total time=  13.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.803 total time=   3.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.804 total time=   6.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.804 total time=  20.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.805 total time=   1.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.806 total time=   3.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.822 total time=   1.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.824 total time=   3.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.799 total time=   1.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.800 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.820 total time=   1.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.809 total time=   3.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.804 total time=  10.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.804 total time=   6.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.819 total time=   2.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.821 total time=   4.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.820 total time=   2.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.821 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.822 total time=   1.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.823 total time=   3.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.799 total time=   1.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.800 total time=   3.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.826 total time=   7.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.802 total time=   1.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.802 total time=   3.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.800 total time=   1.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.802 total time=   3.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.827 total time=  10.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.804 total time=   6.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.676 total time=  25.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.670 total time=   6.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.670 total time=  12.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.694 total time=   6.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.714 total time=  11.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.679 total time=   5.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.687 total time=  11.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.679 total time=  34.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.715 total time=  21.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.712 total time=   5.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.712 total time=  10.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.712 total time=  31.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.808 total time=   2.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.809 total time=   5.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.810 total time=   2.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.811 total time=   5.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.810 total time=   2.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.811 total time=   5.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.817 total time=  10.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.819 total time=   2.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.820 total time=   4.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.819 total time=   2.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.820 total time=   4.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.817 total time=  16.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.801 total time=   3.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.803 total time=   7.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.790 total time=   3.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.793 total time=   7.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.806 total time=  15.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.795 total time=   3.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.794 total time=   7.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.810 total time=   3.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.811 total time=   6.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.797 total time=  20.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.811 total time=  13.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.803 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.804 total time=   4.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.811 total time=  12.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.814 total time=   7.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.799 total time=   1.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.814 total time=   3.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.826 total time=  10.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.825 total time=   6.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.791 total time=   2.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.793 total time=   4.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.793 total time=   2.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.794 total time=   4.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.820 total time=   1.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.821 total time=   3.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.822 total time=   1.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.822 total time=   3.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.824 total time=   7.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.802 total time=   1.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.800 total time=   3.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.810 total time=   1.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.816 total time=   3.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.802 total time=  10.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.811 total time=   6.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.657 total time=  25.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.656 total time=   5.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.671 total time=  12.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.687 total time=   5.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.687 total time=  11.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.695 total time=   5.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.687 total time=  11.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.687 total time=  34.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.726 total time=  21.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.716 total time=   5.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.716 total time=  10.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.716 total time=  31.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.796 total time=   2.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.797 total time=   5.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.797 total time=   2.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.799 total time=   5.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.797 total time=   2.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.799 total time=   5.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.812 total time=  16.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.805 total time=   9.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.792 total time=   2.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.794 total time=   4.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.808 total time=  14.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.813 total time=   4.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.814 total time=   4.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.817 total time=   8.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.816 total time=  24.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.794 total time=   3.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.797 total time=   7.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.808 total time=  22.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.809 total time=  13.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.810 total time=   3.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.811 total time=   6.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.811 total time=  20.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.804 total time=   1.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.802 total time=   3.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.797 total time=   1.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.814 total time=   3.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.797 total time=   1.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.798 total time=   3.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.823 total time=  11.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.811 total time=  10.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.811 total time=   6.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.803 total time=  13.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.825 total time=   7.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.823 total time=   1.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.825 total time=   3.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.800 total time=  10.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.806 total time=   6.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.800 total time=   1.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.802 total time=   3.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.817 total time=   9.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.658 total time=   6.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.658 total time=  12.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.648 total time=  38.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.677 total time=   5.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.686 total time=  11.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.694 total time=  34.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.711 total time=  20.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.726 total time=   5.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.726 total time=  10.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.726 total time=  31.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.799 total time=   2.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.787 total time=   5.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.814 total time=   2.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.815 total time=   5.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.814 total time=   2.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.815 total time=   5.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.800 total time=   2.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.801 total time=   5.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.813 total time=  15.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.794 total time=   9.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.776 total time=   3.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.777 total time=   6.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.779 total time=   3.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.781 total time=   6.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.792 total time=   2.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.793 total time=   5.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.810 total time=   2.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.811 total time=   5.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.811 total time=  10.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.796 total time=  16.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.816 total time=   3.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.797 total time=   7.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.794 total time=   3.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.797 total time=   7.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.797 total time=  14.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.807 total time=   3.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.808 total time=   7.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.807 total time=   3.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.808 total time=   6.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.809 total time=  20.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.809 total time=  13.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.815 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.817 total time=   4.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.817 total time=  12.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.819 total time=   7.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.814 total time=  10.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.804 total time=   6.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.823 total time=   1.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.802 total time=   3.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.807 total time=   9.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.795 total time=   8.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.802 total time=   7.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.813 total time=   1.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.814 total time=   3.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.814 total time=  11.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.827 total time=   6.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.805 total time=   1.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.804 total time=   3.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.828 total time=   9.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.681 total time=   6.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.662 total time=  12.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.680 total time=  37.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.694 total time=  34.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.690 total time=  22.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.709 total time=   5.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.719 total time=  10.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.715 total time=   5.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.715 total time=  10.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.715 total time=  32.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.789 total time=   2.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.798 total time=  11.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.816 total time=  10.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.812 total time=   2.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.814 total time=   5.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.815 total time=  15.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.798 total time=   9.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.780 total time=   3.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.782 total time=   6.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.786 total time=   3.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.804 total time=   6.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.789 total time=   2.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.790 total time=   5.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.792 total time=   2.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.793 total time=   5.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.793 total time=  10.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.812 total time=   2.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.814 total time=   5.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.796 total time=  16.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.790 total time=   3.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.815 total time=   7.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.807 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.815 total time=   7.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.815 total time=  14.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.805 total time=   3.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.806 total time=   7.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.794 total time=   3.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.796 total time=   6.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.811 total time=  20.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.797 total time=  13.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.794 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.794 total time=   4.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.795 total time=  12.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.803 total time=   7.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.812 total time=   1.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.803 total time=   3.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.825 total time=  10.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.827 total time=   6.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.816 total time=   2.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.818 total time=   4.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.819 total time=   2.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.820 total time=   4.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.795 total time=   1.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.802 total time=   3.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.802 total time=   1.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.803 total time=   3.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.803 total time=   7.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.824 total time=   1.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.825 total time=   3.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.826 total time=   1.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.827 total time=   3.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.828 total time=  10.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.828 total time=   6.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.661 total time=  25.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.669 total time=   5.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.656 total time=  11.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.686 total time=   6.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.687 total time=  11.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.687 total time=   5.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.694 total time=  12.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.714 total time=  34.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.734 total time=  21.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.734 total time=   5.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.734 total time=  10.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.734 total time=  31.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.786 total time=   2.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.791 total time=  11.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.812 total time=  16.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.812 total time=  10.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.805 total time=  14.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.805 total time=   9.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.784 total time=  19.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.786 total time=  16.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.789 total time=  10.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.800 total time=   2.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.801 total time=   5.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.799 total time=   8.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.795 total time=  16.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.816 total time=  15.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.816 total time=  22.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.802 total time=  14.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.803 total time=   3.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.804 total time=   6.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.804 total time=  20.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.816 total time=  12.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.818 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.819 total time=   1.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.805 total time=   3.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.805 total time=   1.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.805 total time=   3.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.806 total time=  11.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.808 total time=   6.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.807 total time=   1.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.808 total time=   3.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.821 total time=   9.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.817 total time=   8.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.818 total time=   7.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.819 total time=   1.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.819 total time=   3.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.809 total time=  11.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.825 total time=   6.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.805 total time=   1.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.806 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.807 total time=   9.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.664 total time=   6.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.664 total time=  12.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.662 total time=  38.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.714 total time=   5.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.714 total time=  11.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.687 total time=  34.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.721 total time=  21.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.709 total time=   5.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.721 total time=  10.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.710 total time=  31.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.811 total time=   2.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.802 total time=  11.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.790 total time=  11.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.790 total time=   2.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.790 total time=   5.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.790 total time=  15.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.819 total time=   9.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.806 total time=   3.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.806 total time=   6.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.808 total time=   3.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.809 total time=   6.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.786 total time=   2.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.787 total time=   5.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.814 total time=   2.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.790 total time=   5.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.790 total time=  11.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.790 total time=   2.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.790 total time=   5.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.796 total time=   2.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.798 total time=   4.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.811 total time=   4.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.786 total time=   4.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.803 total time=   8.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.804 total time=  24.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.807 total time=   3.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.805 total time=   7.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.798 total time=  22.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.811 total time=  13.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.794 total time=   3.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.796 total time=   6.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.797 total time=  20.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.811 total time=   1.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.821 total time=   3.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.802 total time=   1.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.798 total time=   3.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.807 total time=   1.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.808 total time=   3.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.809 total time=  10.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.827 total time=   6.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.800 total time=   1.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.816 total time=   3.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.802 total time=   9.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.800 total time=   8.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.821 total time=   7.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.822 total time=   1.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.803 total time=   3.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.804 total time=  11.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.816 total time=   6.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.815 total time=   1.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.816 total time=   3.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.802 total time=   9.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.634 total time=   6.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.647 total time=  12.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.656 total time=  37.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.704 total time=  34.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.683 total time=  22.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.726 total time=   5.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.726 total time=  10.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.726 total time=  31.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.801 total time=  19.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.813 total time=   5.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.816 total time=   2.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.817 total time=   5.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.816 total time=   2.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.817 total time=   5.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.790 total time=   2.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.790 total time=   5.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.801 total time=  15.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.805 total time=   9.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.792 total time=   3.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.795 total time=   6.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.795 total time=   3.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.792 total time=   6.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.810 total time=  18.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.817 total time=  16.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.817 total time=  10.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.819 total time=   2.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.820 total time=   4.9s\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'bootstrap': True, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "0.815034614108488\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.804\n",
      "mean_squared_errors: 1.448\n",
      "mean_absolute_errors: 0.860\n",
      "r2_score: 0.804\n",
      "acc(+-1cm): 0.680\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "SEED = 98\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.05, 0.07, 0.1], #[0.01, 0.03, 0.05],\n",
    "                'n_estimators': [50, 100, 200, 300],\n",
    "                'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                'min_samples_split': [2,3,5],\n",
    "                'min_samples_leaf': [1,2,3],\n",
    "               'bootstrap': [True, False]\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=RandomForestRegressor(random_state = SEED),\n",
    "                    n_jobs=-1,\n",
    "                    verbose=3,\n",
    "                    param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train_imputed, y_train)\n",
    "#model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = gs.predict(x_test_imputed).flatten()\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mse:.3f}')\n",
    "print(f'mean_absolute_errors: {mae:.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc3 = np.mean((y_pred >= y_test-1) & (y_pred <= y_test+1))\n",
    "print(f'acc(+-1cm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c032416c-9954-4db6-b421-a38efb486d27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T13:19:33.332648Z",
     "iopub.status.busy": "2023-02-22T13:19:33.332088Z",
     "iopub.status.idle": "2023-02-22T13:19:33.771363Z",
     "shell.execute_reply": "2023-02-22T13:19:33.770836Z",
     "shell.execute_reply.started": "2023-02-22T13:19:33.332593Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= found hyperparameter =========\n",
      "{'bootstrap': True, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "0.8139923996417548\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.800\n",
      "mean_squared_errors: 1.474\n",
      "mean_absolute_errors: 0.871\n",
      "r2_score: 0.800\n",
      "acc(+-1cm): 0.674\n"
     ]
    }
   ],
   "source": [
    "# 4 input model (age, sex, weight, height)\n",
    "y_pred = gs.predict(x_test_imputed).flatten()\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mse:.3f}')\n",
    "print(f'mean_absolute_errors: {mae:.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc3 = np.mean((y_pred >= y_test-1) & (y_pred <= y_test+1))\n",
    "print(f'acc(+-1cm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4035c275-34e3-4762-a256-624af43597df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T13:33:37.447857Z",
     "iopub.status.busy": "2023-02-22T13:33:37.447343Z",
     "iopub.status.idle": "2023-02-22T13:33:44.897572Z",
     "shell.execute_reply": "2023-02-22T13:33:44.896778Z",
     "shell.execute_reply.started": "2023-02-22T13:33:37.447802Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.804\n",
      "mean_squared_errors: 1.448\n",
      "mean_absolute_errors: 0.860\n",
      "r2_score: 0.804\n",
      "acc(+-1cm): 0.680\n"
     ]
    }
   ],
   "source": [
    "# 5 inputs model\n",
    "rf = RandomForestRegressor(bootstrap= True, max_features= 'sqrt', min_samples_leaf= 3, min_samples_split = 2, n_estimators= 300)\n",
    "rf.fit(x_train_imputed, y_train)\n",
    "\n",
    "y_pred = gs.predict(x_test_imputed).flatten()\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mse:.3f}')\n",
    "print(f'mean_absolute_errors: {mae:.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc3 = np.mean((y_pred >= y_test-1) & (y_pred <= y_test+1))\n",
    "print(f'acc(+-1cm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "07668563-eb15-4e79-9752-7f70e52a95b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T13:34:47.976036Z",
     "iopub.status.busy": "2023-02-22T13:34:47.975449Z",
     "iopub.status.idle": "2023-02-22T13:34:48.757380Z",
     "shell.execute_reply": "2023-02-22T13:34:48.756790Z",
     "shell.execute_reply.started": "2023-02-22T13:34:47.975977Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.795 total time=  16.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.798 total time=   3.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.792 total time=   7.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.814 total time=   3.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.805 total time=   7.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.808 total time=  14.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.815 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.816 total time=   7.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.816 total time=   3.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.818 total time=   6.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.818 total time=  20.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.804 total time=  13.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.801 total time=   2.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.802 total time=   4.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.802 total time=  12.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.806 total time=   7.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.800 total time=  10.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.809 total time=   9.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.808 total time=   6.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.802 total time=  12.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.807 total time=   7.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.805 total time=   1.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.805 total time=   3.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.806 total time=  11.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.808 total time=   6.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.828 total time=   9.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.676 total time=  12.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.662 total time=  25.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.699 total time=  23.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.677 total time=  22.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.703 total time=   5.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.682 total time=  11.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.716 total time=   5.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.716 total time=  10.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.716 total time=  31.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.726 total time=  21.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.795 total time=   3.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.796 total time=   6.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.797 total time=  18.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.789 total time=  10.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.797 total time=   2.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.811 total time=   5.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.791 total time=  16.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.821 total time=   9.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.800 total time=   3.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.801 total time=   6.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.802 total time=   3.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.788 total time=   6.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.784 total time=   2.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.785 total time=   5.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.787 total time=   2.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.788 total time=   5.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.800 total time=  10.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.812 total time=   2.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.811 total time=   5.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.798 total time=  15.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.801 total time=   9.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.785 total time=   4.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.815 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.815 total time=   8.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.819 total time=  24.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.818 total time=   3.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.820 total time=   7.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.820 total time=  22.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.801 total time=  13.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.799 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.801 total time=   6.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.801 total time=  20.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.795 total time=   1.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.799 total time=   3.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.823 total time=   1.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.825 total time=   3.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.823 total time=   1.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.825 total time=   3.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.807 total time=   1.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.807 total time=   3.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.800 total time=  10.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.802 total time=   6.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.790 total time=   2.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.791 total time=   4.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.807 total time=   2.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.810 total time=   4.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.822 total time=  12.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.800 total time=  11.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.801 total time=   7.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.805 total time=   1.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.804 total time=   3.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.804 total time=  10.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.660 total time=  12.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.664 total time=  24.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.656 total time=  23.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.694 total time=  23.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.704 total time=   5.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.704 total time=  11.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.734 total time=   5.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.734 total time=  10.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.710 total time=  31.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.719 total time=  21.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.802 total time=   3.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.804 total time=   6.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.805 total time=  18.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.801 total time=  16.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.802 total time=  10.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.794 total time=  14.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.821 total time=   9.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.778 total time=  19.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.798 total time=  16.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.811 total time=  10.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.815 total time=  15.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.805 total time=   9.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.817 total time=   8.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.816 total time=  16.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.818 total time=  15.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.820 total time=  22.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.820 total time=  14.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.820 total time=   3.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.823 total time=   6.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.822 total time=  20.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.794 total time=  13.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.821 total time=   7.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.809 total time=  11.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.821 total time=   7.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.821 total time=   1.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.821 total time=   3.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.821 total time=   9.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.815 total time=   8.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.817 total time=  12.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.806 total time=  11.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.807 total time=   7.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.807 total time=   1.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.808 total time=   3.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.828 total time=   6.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.677 total time=   6.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.676 total time=  37.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.688 total time=  35.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.687 total time=  22.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.695 total time=  33.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.719 total time=  21.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.778 total time=   3.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.779 total time=   6.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.806 total time=  13.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.781 total time=  12.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.814 total time=  16.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.816 total time=  10.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.819 total time=   2.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.820 total time=   4.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.819 total time=   2.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.820 total time=   4.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.820 total time=  14.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.806 total time=  13.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.796 total time=   2.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.797 total time=   5.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.797 total time=   2.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.799 total time=   5.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.805 total time=  10.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.804 total time=   2.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.790 total time=   5.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.805 total time=  15.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.816 total time=   9.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.796 total time=  25.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.820 total time=  23.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.820 total time=  15.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.821 total time=  22.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.823 total time=  13.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.812 total time=   2.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.816 total time=   4.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.816 total time=   8.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.818 total time=   8.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.824 total time=  11.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.823 total time=   7.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.805 total time=   1.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.806 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.807 total time=  10.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.825 total time=   6.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.817 total time=  13.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.821 total time=  11.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.824 total time=   7.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.815 total time=   1.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.811 total time=   3.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.811 total time=  10.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.806 total time=   6.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.658 total time=  26.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.699 total time=   5.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.699 total time=  11.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.677 total time=   5.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.677 total time=  11.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.686 total time=  34.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.704 total time=  22.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.734 total time=   5.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.734 total time=  10.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.719 total time=   5.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.718 total time=  10.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.719 total time=  31.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.792 total time=   2.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.793 total time=   5.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.797 total time=   2.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.796 total time=   5.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.797 total time=   2.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.796 total time=   5.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.790 total time=  10.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.794 total time=   2.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.794 total time=   4.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.794 total time=   2.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.794 total time=   4.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.795 total time=  14.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.779 total time=  12.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.799 total time=   2.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.801 total time=   5.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.804 total time=   2.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.804 total time=   5.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.789 total time=  10.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.800 total time=   2.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.801 total time=   5.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.795 total time=  15.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.794 total time=   9.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.788 total time=   8.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.798 total time=  16.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.800 total time=  15.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.822 total time=  22.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.822 total time=  14.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.801 total time=   3.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.801 total time=   6.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.824 total time=  20.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.820 total time=  13.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.813 total time=   7.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.824 total time=   7.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.822 total time=   1.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.822 total time=   3.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.810 total time=   1.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.811 total time=   3.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.806 total time=   6.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.826 total time=   1.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.827 total time=   3.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.813 total time=   2.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.815 total time=   4.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.815 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.804 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.822 total time=  12.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.802 total time=   1.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.800 total time=   3.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.826 total time=  11.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.804 total time=   6.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.826 total time=   1.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.827 total time=   3.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.643 total time=   6.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.643 total time=  38.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.668 total time=  35.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.694 total time=  23.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.693 total time=  33.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.715 total time=  21.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.776 total time=   3.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.777 total time=   6.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.795 total time=  13.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.793 total time=  12.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.798 total time=  16.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.800 total time=  10.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.805 total time=   2.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.805 total time=   4.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.820 total time=  14.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.798 total time=   9.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.801 total time=  19.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.802 total time=  16.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.805 total time=  10.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.790 total time=  15.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.819 total time=   9.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.801 total time=  25.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.819 total time=  23.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.800 total time=  15.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.820 total time=  22.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.822 total time=  14.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.819 total time=   2.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.821 total time=   4.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.821 total time=   8.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.822 total time=   8.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.825 total time=  11.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.826 total time=   7.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.826 total time=   1.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.827 total time=   3.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.823 total time=   1.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.825 total time=   3.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.825 total time=  10.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.816 total time=   8.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.801 total time=   1.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.797 total time=   3.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.813 total time=   1.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.808 total time=   3.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.799 total time=   7.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.822 total time=   1.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.822 total time=   3.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.805 total time=   1.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.804 total time=   3.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.825 total time=  10.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.802 total time=   6.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.631 total time=  25.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.688 total time=   6.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.669 total time=  11.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.687 total time=   5.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.687 total time=  11.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.677 total time=  34.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.695 total time=  22.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.727 total time=   5.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.727 total time=  10.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.721 total time=  32.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.807 total time=  19.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.813 total time=  11.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.817 total time=  16.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.817 total time=  10.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.796 total time=   2.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.798 total time=   4.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.799 total time=  14.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.801 total time=  13.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.808 total time=   2.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.809 total time=   5.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.797 total time=   2.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.796 total time=   5.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.796 total time=  10.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.797 total time=   2.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.798 total time=   5.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.801 total time=  15.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.798 total time=   9.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.790 total time=   2.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.805 total time=   5.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.791 total time=  15.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.808 total time=   9.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.797 total time=   4.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.801 total time=   4.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.814 total time=   8.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.795 total time=  25.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.819 total time=   3.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.820 total time=   7.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.822 total time=  22.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.823 total time=  13.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.823 total time=   3.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.823 total time=   6.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.805 total time=   2.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.808 total time=   4.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.815 total time=   8.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.802 total time=   8.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.805 total time=  11.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.806 total time=   7.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.826 total time=   7.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.826 total time=   1.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.827 total time=   3.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.828 total time=  10.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.819 total time=   8.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.804 total time=   1.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.806 total time=   3.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.819 total time=   1.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.819 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.819 total time=   7.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.820 total time=   1.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.820 total time=   3.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.825 total time=  10.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.825 total time=   6.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.630 total time=   6.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.657 total time=  38.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.671 total time=  36.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.704 total time=  22.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.683 total time=  33.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.726 total time=  21.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.786 total time=   3.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.788 total time=   6.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.803 total time=  13.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.806 total time=  12.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.811 total time=  17.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.811 total time=  10.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.815 total time=   2.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.815 total time=   4.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.799 total time=  14.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.794 total time=   9.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.796 total time=  19.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.811 total time=  16.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.800 total time=  10.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.817 total time=  15.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.795 total time=   9.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.802 total time=   8.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.797 total time=  16.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.806 total time=  15.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.800 total time=  14.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.820 total time=   3.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.821 total time=   7.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.799 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.801 total time=   6.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.801 total time=  20.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.801 total time=  13.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.798 total time=   2.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.800 total time=   4.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.800 total time=  12.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.825 total time=  11.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.825 total time=   7.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.805 total time=   1.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.804 total time=   3.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.827 total time=  10.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.793 total time=   8.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.797 total time=   1.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.799 total time=   3.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.823 total time=   1.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.824 total time=   3.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.823 total time=   7.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.824 total time=   1.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.825 total time=   3.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.823 total time=   1.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.806 total time=   3.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.817 total time=  10.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.821 total time=   6.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.660 total time=  25.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.671 total time=   6.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.688 total time=  11.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.687 total time=   5.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.694 total time=  11.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.687 total time=  34.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.703 total time=  22.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.719 total time=   5.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.715 total time=  10.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.719 total time=  31.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.803 total time=  19.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.811 total time=   5.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.792 total time=   2.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.814 total time=   5.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.792 total time=   2.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.793 total time=   5.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.789 total time=  16.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.801 total time=   9.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.815 total time=   2.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.815 total time=   4.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.805 total time=  14.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.810 total time=  12.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.794 total time=  16.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.796 total time=  10.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.813 total time=  16.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.821 total time=   9.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.821 total time=  14.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.807 total time=  13.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.811 total time=   2.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.811 total time=   5.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.813 total time=   2.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.814 total time=   5.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.814 total time=  10.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.793 total time=   2.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.794 total time=   5.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.817 total time=  15.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.820 total time=   9.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.821 total time=  12.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.813 total time=  25.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.793 total time=  23.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.819 total time=  15.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.800 total time=  22.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.802 total time=  13.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.816 total time=   2.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.818 total time=   4.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.819 total time=   8.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.794 total time=   8.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.799 total time=  11.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.800 total time=   7.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.823 total time=   1.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.825 total time=   3.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.827 total time=  10.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.827 total time=   6.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.794 total time=  13.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.799 total time=  11.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.826 total time=   7.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.805 total time=   1.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.825 total time=   3.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.804 total time=  10.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.827 total time=   6.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.653 total time=  25.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.677 total time=   5.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.686 total time=  12.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.714 total time=   5.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.694 total time=  11.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.704 total time=   5.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.704 total time=  11.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.704 total time=  34.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.719 total time=  20.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.710 total time=  31.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.784 total time=  19.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.794 total time=  11.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.814 total time=  10.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.816 total time=   2.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.817 total time=   5.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.817 total time=  16.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.795 total time=   9.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.778 total time=   3.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.779 total time=   6.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.778 total time=   3.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.780 total time=   6.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.813 total time=   2.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.814 total time=   5.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.787 total time=   2.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.815 total time=   5.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.816 total time=  10.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.816 total time=   2.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.817 total time=   5.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.805 total time=   2.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.805 total time=   4.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.805 total time=  14.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.805 total time=   7.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.815 total time=   2.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.815 total time=   4.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.801 total time=  14.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.801 total time=   7.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.812 total time=   8.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.817 total time=  16.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.805 total time=  15.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.798 total time=  22.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.807 total time=  14.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.816 total time=   3.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.818 total time=   6.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.818 total time=  20.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.802 total time=  12.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.804 total time=   3.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.805 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.819 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.819 total time=   1.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.819 total time=   3.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.820 total time=  10.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.821 total time=   6.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.821 total time=   1.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.821 total time=   3.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.809 total time=   9.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.802 total time=   8.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.805 total time=   7.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.823 total time=  10.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.803 total time=   7.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.815 total time=   1.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.816 total time=   3.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.811 total time=   9.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.660 total time=  12.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.665 total time=  25.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.670 total time=  24.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.679 total time=  34.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.693 total time=  22.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.715 total time=   5.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.718 total time=  10.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.715 total time=  32.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.780 total time=  19.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.814 total time=  11.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.805 total time=  16.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.805 total time=  10.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.808 total time=  14.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.808 total time=   9.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.790 total time=  19.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.791 total time=  16.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.793 total time=  10.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.806 total time=   2.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.808 total time=   4.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.808 total time=  14.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.808 total time=   7.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.801 total time=  17.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.797 total time=   3.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.799 total time=   7.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.799 total time=   3.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.799 total time=   7.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.819 total time=  15.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.819 total time=   3.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.819 total time=   7.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.801 total time=   3.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.821 total time=   6.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.822 total time=  20.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.822 total time=  13.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.819 total time=   2.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.820 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.822 total time=  12.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.823 total time=  11.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.803 total time=   7.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.815 total time=   1.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.816 total time=   3.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.811 total time=  10.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.803 total time=   8.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.805 total time=  12.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.809 total time=  11.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.823 total time=   7.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.823 total time=   1.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.825 total time=   3.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.827 total time=  10.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.658 total time=  13.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.680 total time=  24.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.688 total time=  23.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.687 total time=  23.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.688 total time=   5.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.688 total time=  11.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.709 total time=   5.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.721 total time=  10.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.734 total time=  31.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.728 total time=  21.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.786 total time=   3.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.788 total time=   6.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.787 total time=  18.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.789 total time=  16.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.795 total time=  10.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.821 total time=  14.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.820 total time=   9.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.780 total time=  19.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.814 total time=  16.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.816 total time=  11.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.817 total time=   2.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.818 total time=   4.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.819 total time=  14.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.794 total time=   7.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.816 total time=  25.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.799 total time=  23.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.797 total time=  14.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.799 total time=  21.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.801 total time=  13.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.795 total time=   2.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.797 total time=   4.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.797 total time=   8.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.800 total time=   8.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.821 total time=  11.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.824 total time=   7.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.805 total time=   1.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.804 total time=   3.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.804 total time=  10.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.828 total time=   6.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.821 total time=  13.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.807 total time=   1.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.814 total time=   3.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.808 total time=   7.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.807 total time=   1.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.809 total time=   3.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.826 total time=  10.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.817 total time=   9.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.631 total time=  12.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.648 total time=  25.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.677 total time=  23.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.687 total time=   5.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.687 total time=  11.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.686 total time=  34.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.728 total time=  21.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.719 total time=   5.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.719 total time=  10.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.728 total time=  31.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.784 total time=   2.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.810 total time=  11.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.817 total time=  11.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.816 total time=   2.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.817 total time=   5.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.817 total time=  15.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.820 total time=   9.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.803 total time=   3.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.805 total time=   6.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.807 total time=   3.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.810 total time=   6.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.812 total time=   2.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.813 total time=   5.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.816 total time=   2.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.817 total time=   5.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.817 total time=  10.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.816 total time=   2.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.817 total time=   5.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.794 total time=   2.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.820 total time=   4.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.820 total time=  14.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.821 total time=   7.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.803 total time=  17.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.820 total time=   4.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.821 total time=   7.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.819 total time=   3.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.820 total time=   7.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.800 total time=  22.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.798 total time=  14.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.799 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.801 total time=   6.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.801 total time=  20.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.792 total time=  13.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.818 total time=   7.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.823 total time=   7.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.802 total time=   1.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.802 total time=   3.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.815 total time=   1.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.816 total time=   3.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.802 total time=   6.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.805 total time=   1.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.825 total time=   3.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.827 total time=  10.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.794 total time=   8.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.813 total time=  11.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.814 total time=   7.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.800 total time=  10.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.802 total time=   6.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.653 total time=   6.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.655 total time=  38.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.686 total time=  36.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.714 total time=  22.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.718 total time=  33.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.728 total time=  21.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.800 total time=   3.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.801 total time=   6.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.801 total time=  13.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.810 total time=  12.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.787 total time=  17.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.801 total time=  16.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.795 total time=   9.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.819 total time=   2.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.820 total time=   4.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.821 total time=  14.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.810 total time=  12.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.787 total time=  17.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.817 total time=  10.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.819 total time=   2.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.820 total time=   4.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.821 total time=  14.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.795 total time=   7.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.815 total time=   8.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.791 total time=  16.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.800 total time=  15.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.794 total time=  22.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.821 total time=  14.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.822 total time=   3.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.822 total time=   6.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.801 total time=  20.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.821 total time=  13.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.825 total time=   7.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.804 total time=  11.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.823 total time=   7.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.800 total time=   1.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.802 total time=   3.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.802 total time=  10.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.808 total time=   8.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.811 total time=  12.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.820 total time=  11.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.814 total time=   7.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.805 total time=   1.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.806 total time=   3.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.807 total time=   9.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.655 total time=  12.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.635 total time=  24.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.664 total time=  23.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.686 total time=  23.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.693 total time=   5.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.703 total time=  11.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.719 total time=   5.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.715 total time=  10.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.719 total time=  31.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.709 total time=  21.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.778 total time=   3.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.780 total time=   6.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.811 total time=  18.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.790 total time=  16.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.790 total time=  10.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.805 total time=   2.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.805 total time=   4.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.805 total time=  14.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.795 total time=  12.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.793 total time=  18.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.812 total time=  16.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.812 total time=  10.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.815 total time=   2.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.815 total time=   4.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.816 total time=  14.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.794 total time=   4.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.794 total time=   4.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.796 total time=   8.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.817 total time=  24.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.799 total time=   3.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.799 total time=   7.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.800 total time=  22.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.802 total time=  13.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.801 total time=   3.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.821 total time=   6.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.799 total time=   2.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.801 total time=   4.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.808 total time=   8.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.811 total time=   8.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.813 total time=  11.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.814 total time=   7.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.801 total time=  10.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.809 total time=   9.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.801 total time=   8.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.802 total time=  12.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.800 total time=   7.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.812 total time=   1.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.814 total time=   3.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.807 total time=   1.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.808 total time=   3.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.809 total time=   9.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.808 total time=   6.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.643 total time=  25.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.664 total time=   5.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.664 total time=  11.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.695 total time=   5.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.686 total time=  11.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.704 total time=  22.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.711 total time=   5.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.711 total time=  11.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.719 total time=   5.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.727 total time=  10.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.712 total time=  31.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.716 total time=  20.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.789 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.790 total time=   6.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.810 total time=  12.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.812 total time=  16.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.793 total time=  10.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.796 total time=   2.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.798 total time=   4.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.806 total time=   2.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.808 total time=   4.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.808 total time=  14.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.803 total time=  12.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.790 total time=  18.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.796 total time=  16.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.798 total time=  10.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.800 total time=   2.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.801 total time=   4.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.801 total time=  14.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.789 total time=  24.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.800 total time=  23.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.815 total time=  15.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.807 total time=  22.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.797 total time=  13.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.790 total time=   2.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.791 total time=   4.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.792 total time=   8.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.795 total time=   8.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.798 total time=  11.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.799 total time=   7.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.800 total time=   1.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.802 total time=   3.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.825 total time=   6.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.805 total time=   1.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.804 total time=   3.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.800 total time=   2.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.801 total time=   4.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.803 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.817 total time=   4.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.818 total time=  12.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.807 total time=   1.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.808 total time=   3.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.800 total time=  11.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.802 total time=   6.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.823 total time=   1.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.825 total time=   3.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.827 total time=   9.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.662 total time=   6.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.680 total time=  12.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.653 total time=  38.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.694 total time=   5.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.679 total time=  11.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.694 total time=  35.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.709 total time=  20.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.721 total time=   5.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.709 total time=  10.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.721 total time=  32.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.812 total time=   2.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.787 total time=  11.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.816 total time=  16.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.817 total time=  10.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.792 total time=   2.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.794 total time=   4.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.794 total time=  14.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.778 total time=  13.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.805 total time=  18.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.814 total time=  16.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.815 total time=  10.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.792 total time=   2.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.794 total time=   4.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.808 total time=  14.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.819 total time=  25.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.797 total time=  23.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.794 total time=  14.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.795 total time=  21.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.818 total time=  13.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.801 total time=  20.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.823 total time=   2.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.799 total time=   7.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.814 total time=  11.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.800 total time=   7.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.805 total time=   1.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.806 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.807 total time=  10.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.797 total time=   8.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.811 total time=   1.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.812 total time=   3.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.797 total time=   1.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.798 total time=   3.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.814 total time=   7.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.799 total time=   1.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.803 total time=   3.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.826 total time=   1.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.827 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.807 total time=  10.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.825 total time=   6.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.655 total time=  25.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.683 total time=   5.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.683 total time=  11.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.704 total time=   5.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.704 total time=  11.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.714 total time=  33.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.694 total time=  22.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.716 total time=   5.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.716 total time=  10.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.709 total time=  20.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.806 total time=   3.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.806 total time=   6.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.779 total time=  12.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.805 total time=  12.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.791 total time=  16.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.814 total time=  10.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.817 total time=   2.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.818 total time=   4.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.817 total time=   2.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.818 total time=   4.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.819 total time=  14.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.783 total time=  13.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.811 total time=  18.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.790 total time=  16.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.790 total time=  10.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.819 total time=   2.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.820 total time=   4.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.799 total time=  14.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.800 total time=   4.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.812 total time=   4.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.794 total time=   8.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.803 total time=  24.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.818 total time=   3.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.819 total time=   7.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.820 total time=  22.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.823 total time=  13.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.822 total time=   3.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.822 total time=   6.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.823 total time=  20.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.797 total time=   1.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.824 total time=   7.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.820 total time=  11.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.807 total time=   7.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.828 total time=  10.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.804 total time=   6.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.820 total time=  13.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.824 total time=  11.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.800 total time=   7.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.821 total time=   1.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.821 total time=   3.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.821 total time=  10.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.816 total time=   6.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.647 total time=  26.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.686 total time=   6.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.677 total time=  11.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.679 total time=   5.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.679 total time=  11.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.686 total time=   5.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.694 total time=  11.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.677 total time=  34.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.716 total time=  21.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.712 total time=  31.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.790 total time=  19.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.814 total time=   5.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.787 total time=   2.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.790 total time=   5.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.787 total time=   2.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.790 total time=   5.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.804 total time=   2.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.805 total time=   5.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.805 total time=  15.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.816 total time=   9.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.786 total time=   3.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.788 total time=   6.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.792 total time=   3.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.796 total time=   6.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.781 total time=  18.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.816 total time=  16.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.817 total time=  10.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.794 total time=   2.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.794 total time=   4.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.819 total time=  14.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.794 total time=  25.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.807 total time=  23.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.801 total time=  14.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.802 total time=  21.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.804 total time=  13.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.824 total time=  20.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.820 total time=   1.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.802 total time=   7.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.806 total time=  11.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.810 total time=   7.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.810 total time=   1.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.811 total time=   3.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.817 total time=  10.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.792 total time=   8.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.800 total time=  12.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.825 total time=  11.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.826 total time=   7.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.826 total time=   1.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.827 total time=   3.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.821 total time=   1.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.821 total time=   3.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.821 total time=  10.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.646 total time=   6.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.653 total time=  12.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.664 total time=  37.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.687 total time=  34.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.688 total time=  22.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.712 total time=   5.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.712 total time=  10.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.716 total time=  31.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.788 total time=  19.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.785 total time=   5.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.787 total time=   2.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.788 total time=   5.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.804 total time=   2.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.788 total time=   5.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.814 total time=  16.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.820 total time=   9.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.794 total time=   2.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.794 total time=   4.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.784 total time=   3.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.786 total time=   6.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.789 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.790 total time=   6.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.806 total time=  18.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.801 total time=  16.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.790 total time=  10.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.806 total time=   2.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.808 total time=   4.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.794 total time=  14.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.804 total time=  25.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.816 total time=  23.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.806 total time=  15.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.809 total time=  21.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.809 total time=  13.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.800 total time=   2.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.801 total time=   4.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.801 total time=   8.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.817 total time=   8.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.819 total time=  11.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.819 total time=   7.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.807 total time=  10.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.808 total time=   6.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.811 total time=  10.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.811 total time=   8.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.824 total time=   7.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.799 total time=   1.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.824 total time=   3.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.823 total time=  11.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.828 total time=   6.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.826 total time=   1.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.827 total time=   3.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.825 total time=   9.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.653 total time=   6.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.656 total time=  12.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.635 total time=  36.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.686 total time=  23.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.690 total time=   5.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.690 total time=  11.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.727 total time=   5.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.719 total time=  10.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.721 total time=  32.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.734 total time=  20.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.808 total time=   3.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.809 total time=   6.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.810 total time=  18.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.814 total time=  16.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.815 total time=  10.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.800 total time=   2.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.801 total time=   5.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.801 total time=  14.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.790 total time=  13.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.781 total time=  18.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.805 total time=  16.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.795 total time=  10.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.817 total time=   2.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.818 total time=   4.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.820 total time=  14.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.816 total time=   4.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.798 total time=   4.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.801 total time=   8.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.791 total time=  24.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.814 total time=   3.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.793 total time=   7.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.794 total time=  22.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.822 total time=  13.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.820 total time=   3.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.801 total time=   6.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.813 total time=   2.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.815 total time=   4.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.803 total time=   8.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.804 total time=   8.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.807 total time=  11.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.808 total time=   7.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.810 total time=  10.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.811 total time=   6.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.799 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.801 total time=   4.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.801 total time=   2.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.802 total time=   4.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.795 total time=  12.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.814 total time=  10.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.821 total time=   7.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.821 total time=   1.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.821 total time=   3.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.809 total time=   9.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.643 total time=  12.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.647 total time=  25.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.686 total time=  24.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.687 total time=   5.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.677 total time=  11.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.686 total time=  34.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.719 total time=  21.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.727 total time=   5.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.727 total time=  10.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.719 total time=  31.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.813 total time=   2.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.786 total time=  11.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.793 total time=  10.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.793 total time=   2.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.794 total time=   5.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.795 total time=  15.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.808 total time=   9.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.799 total time=   3.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.801 total time=   6.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.805 total time=   3.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.805 total time=   6.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.787 total time=  18.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.793 total time=  16.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.805 total time=  10.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.805 total time=   2.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.805 total time=   4.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.805 total time=  14.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.814 total time=   4.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.817 total time=   4.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.817 total time=   8.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.797 total time=  24.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.790 total time=   3.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.807 total time=   7.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.806 total time=  22.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.797 total time=  13.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.816 total time=   3.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.818 total time=   6.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.818 total time=  20.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.816 total time=   1.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.812 total time=   3.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.813 total time=   1.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.803 total time=   3.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.822 total time=   1.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.824 total time=   3.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.826 total time=  11.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.802 total time=   9.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.802 total time=   6.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.792 total time=  13.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.807 total time=  11.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.808 total time=   7.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.810 total time=  10.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.821 total time=   6.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.660 total time=   6.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.661 total time=  38.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.656 total time=  35.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.686 total time=  23.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.694 total time=  33.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.716 total time=  21.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.799 total time=   3.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.801 total time=   6.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.790 total time=  13.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.797 total time=  12.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.786 total time=  16.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.789 total time=  10.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.792 total time=   2.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.794 total time=   4.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.815 total time=   2.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.815 total time=   4.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.816 total time=  14.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.787 total time=  13.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.797 total time=  18.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.789 total time=  16.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.802 total time=  10.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.796 total time=   2.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.798 total time=   5.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.795 total time=  14.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.796 total time=  25.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.805 total time=  23.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.808 total time=  14.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.816 total time=  21.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.811 total time=  13.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.822 total time=  20.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.822 total time=   1.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.798 total time=   7.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.800 total time=   7.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.824 total time=   1.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.825 total time=   3.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.821 total time=   1.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.821 total time=   3.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.816 total time=   6.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.815 total time=   1.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.806 total time=   3.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.804 total time=  10.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.822 total time=   8.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.798 total time=  11.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.799 total time=   7.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.804 total time=  10.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.827 total time=   6.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.656 total time=   6.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.631 total time=  37.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.664 total time=  36.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.686 total time=  23.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.691 total time=  34.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.734 total time=  20.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.780 total time=   3.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.782 total time=   6.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.783 total time=  12.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.781 total time=  12.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.802 total time=  16.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.805 total time=  10.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.806 total time=   2.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.808 total time=   4.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.795 total time=  14.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.795 total time=  10.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.807 total time=  19.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.814 total time=  16.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.790 total time=  10.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.792 total time=   2.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.794 total time=   4.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.816 total time=  14.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.816 total time=   9.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.794 total time=  14.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.798 total time=   9.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.815 total time=  26.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.821 total time=  23.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.821 total time=  14.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.823 total time=  21.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.823 total time=  13.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.791 total time=   2.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.793 total time=   4.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.793 total time=   8.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.821 total time=   8.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.802 total time=  11.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.803 total time=   7.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.807 total time=   1.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.808 total time=   3.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.811 total time=   6.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.810 total time=   1.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.811 total time=   3.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.817 total time=   9.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.804 total time=   8.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.813 total time=   7.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.826 total time=  10.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.825 total time=   7.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.826 total time=   1.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.827 total time=   3.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.825 total time=   9.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.653 total time=  12.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.656 total time=  24.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.669 total time=  23.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.687 total time=  22.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.694 total time=   5.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.694 total time=  11.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.726 total time=   5.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.726 total time=  10.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.719 total time=  31.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.715 total time=  21.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.779 total time=   3.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.781 total time=   6.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.781 total time=  18.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.805 total time=  10.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.812 total time=   2.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.798 total time=   5.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.798 total time=  15.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.801 total time=   9.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.820 total time=  14.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.781 total time=  12.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.813 total time=  11.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.816 total time=   2.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.817 total time=   5.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.817 total time=  16.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.799 total time=  14.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.819 total time=   9.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.813 total time=  16.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.817 total time=   3.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.819 total time=   7.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.818 total time=   3.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.820 total time=   7.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.820 total time=  15.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.820 total time=   3.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.821 total time=   7.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.823 total time=   3.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.823 total time=   6.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.801 total time=  20.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.802 total time=  13.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.793 total time=   2.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.794 total time=   4.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.795 total time=  12.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.800 total time=  11.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.801 total time=   7.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.826 total time=   1.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.827 total time=   3.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.804 total time=  10.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.821 total time=   8.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.823 total time=   1.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.825 total time=   3.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.823 total time=   1.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.825 total time=   3.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.804 total time=  10.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.800 total time=   7.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.800 total time=   1.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.802 total time=   3.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.802 total time=   9.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.658 total time=  12.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.658 total time=  24.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.670 total time=  23.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.679 total time=  22.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.682 total time=   5.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.695 total time=  11.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.712 total time=   5.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.718 total time=  10.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.715 total time=  32.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.721 total time=  21.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.807 total time=   3.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.810 total time=   6.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.781 total time=  18.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.793 total time=  16.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.790 total time=  10.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.819 total time=  14.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.819 total time=   9.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.807 total time=  19.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.812 total time=  16.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.814 total time=  11.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.819 total time=   2.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.794 total time=   5.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.795 total time=  15.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.820 total time=   9.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.789 total time=  16.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.814 total time=   3.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.805 total time=   7.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.804 total time=   3.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.807 total time=   7.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.794 total time=  14.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.792 total time=   3.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.798 total time=   7.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.822 total time=   3.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.801 total time=   7.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.824 total time=  20.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.823 total time=  13.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.820 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.821 total time=   4.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.822 total time=  12.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.826 total time=  11.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.821 total time=  10.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.821 total time=   6.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.825 total time=   9.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.818 total time=   8.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.799 total time=   7.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.823 total time=   1.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.822 total time=   3.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.825 total time=  10.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.811 total time=   6.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.810 total time=   1.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.811 total time=   3.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.811 total time=   9.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.665 total time=   6.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.665 total time=  12.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.658 total time=  37.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.686 total time=  34.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.711 total time=  22.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.719 total time=   5.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.721 total time=  10.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.734 total time=  31.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.796 total time=  19.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.801 total time=   5.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.804 total time=   2.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.804 total time=   5.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.787 total time=   2.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.804 total time=   5.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.796 total time=  16.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.821 total time=   9.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.819 total time=   2.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.820 total time=   4.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.795 total time=  14.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.797 total time=  12.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.802 total time=  11.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.804 total time=   2.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.804 total time=   5.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.805 total time=  16.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.816 total time=   9.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.800 total time=   2.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.815 total time=   4.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.816 total time=  10.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.818 total time=  16.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.804 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.799 total time=   7.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.799 total time=   3.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.800 total time=   7.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.801 total time=  14.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.801 total time=   3.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.802 total time=   7.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.803 total time=   3.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.804 total time=   6.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.804 total time=  20.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.818 total time=  13.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.807 total time=   2.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.810 total time=   4.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.805 total time=  12.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.808 total time=   7.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.807 total time=   1.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.820 total time=   3.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.815 total time=  10.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.816 total time=   6.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.805 total time=   2.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.808 total time=   4.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.794 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.794 total time=   4.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.816 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.818 total time=   3.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.805 total time=   1.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.805 total time=   3.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.806 total time=   7.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.807 total time=   1.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.807 total time=   3.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.815 total time=  10.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.811 total time=   6.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.647 total time=   6.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.647 total time=  38.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.683 total time=  35.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.677 total time=  22.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.703 total time=  33.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.719 total time=  20.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.784 total time=   3.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.786 total time=   6.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.787 total time=  12.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.790 total time=  12.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.794 total time=  16.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.796 total time=  10.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.800 total time=   2.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.801 total time=   4.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.816 total time=  14.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.816 total time=   9.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.803 total time=  19.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.811 total time=  11.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.813 total time=   2.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.793 total time=   5.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.793 total time=  16.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.819 total time=   9.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.817 total time=   2.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.818 total time=   4.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.820 total time=  10.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.791 total time=   4.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.792 total time=   4.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.789 total time=   8.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.814 total time=  24.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.804 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.815 total time=   7.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.816 total time=  22.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.818 total time=  13.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.807 total time=   3.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.808 total time=   6.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.809 total time=  20.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.801 total time=   1.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.825 total time=   3.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.799 total time=   1.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.800 total time=   3.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.823 total time=   1.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.822 total time=   3.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.800 total time=  11.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.817 total time=  10.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.816 total time=   6.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.809 total time=  13.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.819 total time=  11.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.819 total time=   7.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.821 total time=  11.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.804 total time=   6.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.659 total time=   6.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.653 total time=  38.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.677 total time=  35.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.687 total time=  23.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.711 total time=  33.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.711 total time=  20.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.792 total time=   3.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.795 total time=   6.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.778 total time=  13.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.787 total time=  12.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.814 total time=  17.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.816 total time=  16.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.819 total time=   9.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.817 total time=   2.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.818 total time=   4.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.819 total time=  14.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.787 total time=  12.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.814 total time=  11.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.814 total time=   2.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.790 total time=   5.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.790 total time=  16.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.795 total time=   9.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.794 total time=   2.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.794 total time=   4.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.795 total time=  10.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.795 total time=   8.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.804 total time=  16.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.820 total time=  15.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.820 total time=  22.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.801 total time=  14.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.823 total time=   3.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.821 total time=   6.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.823 total time=  20.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.798 total time=  12.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.823 total time=   3.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.823 total time=   1.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.822 total time=   3.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.802 total time=   1.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.803 total time=   3.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.825 total time=  10.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.828 total time=   6.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.826 total time=   1.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.827 total time=   3.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.828 total time=   9.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.821 total time=   8.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.805 total time=  11.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.806 total time=   7.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.807 total time=  10.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.808 total time=   6.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.804 total time=   9.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.655 total time=   6.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.634 total time=  12.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.647 total time=  37.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.714 total time=  22.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.694 total time=   5.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.693 total time=  11.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.715 total time=   5.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.712 total time=  10.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.726 total time=  31.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.711 total time=  20.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.805 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.805 total time=   6.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.806 total time=  18.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.811 total time=  10.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.790 total time=  16.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.798 total time=   9.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.806 total time=   2.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.808 total time=   4.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.816 total time=  14.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.806 total time=  12.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.810 total time=  11.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.810 total time=   2.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.811 total time=   5.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.812 total time=  16.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.805 total time=   9.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.792 total time=   2.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.794 total time=   4.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.794 total time=  11.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.795 total time=   8.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.814 total time=  16.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.793 total time=  15.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.801 total time=  22.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.816 total time=  14.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.807 total time=   3.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.808 total time=   6.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.811 total time=  20.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.803 total time=  13.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.797 total time=   3.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.807 total time=   1.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.808 total time=   3.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.813 total time=   1.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.814 total time=   3.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.804 total time=  11.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.821 total time=   9.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.821 total time=   6.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.816 total time=  13.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.798 total time=   7.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.797 total time=   1.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.798 total time=   3.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.820 total time=  11.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.821 total time=   6.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.807 total time=   1.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.808 total time=   3.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.809 total time=   9.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.647 total time=   6.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.647 total time=  12.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.665 total time=  37.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.694 total time=  34.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.718 total time=  21.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.721 total time=   5.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.709 total time=  10.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.728 total time=  31.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.778 total time=  19.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.790 total time=   5.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.813 total time=   2.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.793 total time=   5.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.813 total time=   2.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.814 total time=   5.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.793 total time=  16.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.808 total time=   9.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.796 total time=   2.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.798 total time=   4.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.799 total time=  14.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.805 total time=  12.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.787 total time=  11.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.787 total time=   2.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.815 total time=   5.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.816 total time=  16.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.821 total time=   9.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.819 total time=   2.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.820 total time=   4.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.819 total time=  11.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.815 total time=  16.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.794 total time=   3.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.818 total time=   7.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.818 total time=   3.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.819 total time=   7.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.821 total time=  15.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.800 total time=   3.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.800 total time=   7.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.820 total time=   3.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.822 total time=   6.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.823 total time=  20.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.823 total time=  13.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.814 total time=   2.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.816 total time=   4.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.818 total time=  12.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.800 total time=  10.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.814 total time=   7.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.807 total time=   1.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.808 total time=   3.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.828 total time=   6.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.812 total time=   2.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.797 total time=   4.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.798 total time=   2.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.800 total time=   4.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.795 total time=  12.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.800 total time=  10.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.810 total time=   7.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.810 total time=   1.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.811 total time=   3.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.821 total time=  10.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.646 total time=  12.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.653 total time=  25.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.683 total time=  23.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.694 total time=  23.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.717 total time=   5.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.717 total time=  10.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.721 total time=   5.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.709 total time=  10.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.728 total time=  31.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.719 total time=  20.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.792 total time=   3.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.792 total time=   6.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.793 total time=  18.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.800 total time=  10.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.817 total time=  16.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.794 total time=   9.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.805 total time=   2.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.805 total time=   4.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.801 total time=  14.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.790 total time=  12.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.794 total time=  11.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.797 total time=   2.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.796 total time=   5.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.796 total time=  16.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.801 total time=   9.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.815 total time=   2.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.801 total time=   4.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.801 total time=  12.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.793 total time=  12.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.798 total time=  11.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.797 total time=   2.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.799 total time=   5.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.789 total time=  16.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.794 total time=   9.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.805 total time=   2.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.805 total time=   4.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.808 total time=  12.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.816 total time=   8.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.802 total time=  16.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.797 total time=  15.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.806 total time=  22.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.809 total time=  14.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.794 total time=   3.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.796 total time=   6.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.797 total time=  20.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.809 total time=  13.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.805 total time=   7.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.799 total time=   7.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.802 total time=   1.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.800 total time=   3.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.823 total time=  10.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.806 total time=   6.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.795 total time=   2.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.816 total time=   4.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.814 total time=   2.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.816 total time=   4.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.805 total time=   1.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.804 total time=   3.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.825 total time=  11.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.823 total time=   7.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.801 total time=  10.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.816 total time=   6.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.657 total time=   6.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.660 total time=  38.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.670 total time=  35.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.694 total time=  23.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.704 total time=  34.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.719 total time=  31.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.807 total time=  19.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.811 total time=  11.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.796 total time=  16.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.798 total time=  10.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.801 total time=  14.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.801 total time=   9.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.788 total time=  19.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.786 total time=  11.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.787 total time=   2.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.788 total time=   5.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.801 total time=  16.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.808 total time=   9.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.806 total time=   2.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.808 total time=   4.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.805 total time=  12.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.794 total time=   4.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.795 total time=   4.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.796 total time=   8.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.797 total time=  24.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.808 total time=  22.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.795 total time=  14.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.810 total time=   3.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.811 total time=   6.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.809 total time=  20.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.817 total time=  13.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.807 total time=   7.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.826 total time=   7.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.824 total time=   1.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.825 total time=   3.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.826 total time=   1.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.827 total time=   3.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.825 total time=  10.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.806 total time=   6.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.798 total time=  13.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.802 total time=  11.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.803 total time=   7.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.823 total time=  10.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.806 total time=   6.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.658 total time=   6.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.658 total time=  39.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.699 total time=  35.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.679 total time=  22.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.688 total time=  33.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.721 total time=  21.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.803 total time=   3.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.805 total time=   6.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.807 total time=  12.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.790 total time=  18.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.796 total time=  10.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.805 total time=  16.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.816 total time=   9.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.800 total time=   2.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.801 total time=   5.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.794 total time=  14.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.781 total time=  12.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.791 total time=  11.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.792 total time=   2.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.814 total time=   5.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.814 total time=  16.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.798 total time=   9.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.796 total time=   2.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.798 total time=   4.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.799 total time=  12.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.820 total time=   9.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.819 total time=   2.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.820 total time=   4.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.821 total time=  12.0s\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "odir_f = f'mae-{mae:.3f}_mse-{mse:.3f}_RF-5inputs_{nfold}fold'\n",
    "odir = f'result/depth_8cm/{odir_f}'\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "#model.save_model(f'{odir}/model.model')\n",
    "pickle.dump(gs, open(f'{odir}/gridSearch','wb'))\n",
    "pickle.dump(rf, open(f'{odir}/model','wb'))\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "pickle.dump(param_dict, open(f'{odir}/param_dict', 'wb'))\n",
    "f = open(f'{odir}/result.txt', 'w')\n",
    "f.write(f'regression model')\n",
    "f.write(f'best params: {gs.best_params_}')\n",
    "f.write(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "f.write(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bacf7f7-ee34-4dd8-b99e-d6c1b60f2d42",
   "metadata": {},
   "source": [
    "# DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc05e910-4e1d-4ead-b3b5-4832226d2a22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T12:01:59.414486Z",
     "iopub.status.busy": "2023-02-22T12:01:59.413938Z",
     "iopub.status.idle": "2023-02-22T12:02:01.236629Z",
     "shell.execute_reply": "2023-02-22T12:02:01.236063Z",
     "shell.execute_reply.started": "2023-02-22T12:01:59.414428Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import losses, metrics\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization, Dropout, Activation, Input\n",
    "from keras.layers import GlobalAveragePooling1D, Flatten, SeparableConv1D, concatenate, Add\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
    "import tensorflow as tf\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats, interp\n",
    "import os, sys, pickle, shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, datetime, time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# tensorflow 사용 시 seed 고정\n",
    "def seed_everything(seed: int = 98):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "SEED = 98\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8975f8c-4f95-4c3b-8137-59aaec24de13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T13:41:55.485519Z",
     "iopub.status.busy": "2023-02-22T13:41:55.484972Z",
     "iopub.status.idle": "2023-02-22T13:41:55.493059Z",
     "shell.execute_reply": "2023-02-22T13:41:55.492140Z",
     "shell.execute_reply.started": "2023-02-22T13:41:55.485456Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24179, 4), (6044, 4))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c008279-30a6-4f9e-9df6-70e02fc169c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T13:41:57.314003Z",
     "iopub.status.busy": "2023-02-22T13:41:57.313543Z",
     "iopub.status.idle": "2023-02-22T13:41:57.474963Z",
     "shell.execute_reply": "2023-02-22T13:41:57.474223Z",
     "shell.execute_reply.started": "2023-02-22T13:41:57.313956Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# normalization\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(pd.DataFrame(x_train))\n",
    "x_test = sc.transform(pd.DataFrame(x_test))\n",
    "\n",
    "\n",
    "# fill missing value\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "imp = IterativeImputer().fit(x_train)\n",
    "x_train_imputed = imp.transform(x_train)\n",
    "x_test_imputed = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3c0ad7b-727a-48d6-89e3-475842a9926e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T13:42:02.718439Z",
     "iopub.status.busy": "2023-02-22T13:42:02.717879Z",
     "iopub.status.idle": "2023-02-22T13:42:02.743590Z",
     "shell.execute_reply": "2023-02-22T13:42:02.742878Z",
     "shell.execute_reply.started": "2023-02-22T13:42:02.718391Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start making test settings...done\n",
      "2023-02-22 22:42:02.741033\n"
     ]
    }
   ],
   "source": [
    "# folder\n",
    "nfold = 10  # 각각의 hyperparameter에 대해 k-fold 를 시행하고 평균을 구한다.\n",
    "ntest = 500\n",
    "rootdir = f\"result/depth_8cm/DNN_depth_both_4inputs\"\n",
    "\n",
    "if not os.path.exists(rootdir):\n",
    "    os.mkdir(rootdir)\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "f = open(f'{rootdir}/README.txt', 'w')\n",
    "f.write(f'model: DNN 2 layers, regression')\n",
    "f.write(f'input: age, sex, height, weight, cuffed 유무, tube size  output: depth')\n",
    "f.close()\n",
    "    \n",
    "\n",
    "# test_settings\n",
    "layer_settings, test_settings = [], []\n",
    "\n",
    "\n",
    "# hyperparamters pool\n",
    "dropout_opts  = [0, 0.1, 0.2, 0.3, 0.4, 0.5] # dropout rate\n",
    "dense_opts = [16, 32, 64, 128, 256, 512]\n",
    "BATCH_SIZE = [32, 64, 128, 256, 512]\n",
    "lr_opts = [0.001, 0.002, 0.0005]\n",
    "\n",
    "print('start making test settings...', end='', flush=True)\n",
    "# test settings\n",
    "dnodes, dropouts = [], []\n",
    "for i in range(2):\n",
    "    dnodes.append(0)\n",
    "    dropouts.append(0)\n",
    "\n",
    "\n",
    "for dnode1 in dense_opts:\n",
    "    for dropout1 in dropout_opts:\n",
    "        for dnode2 in dense_opts:\n",
    "            for dropout2 in dropout_opts:\n",
    "                for batch_size in BATCH_SIZE:\n",
    "                    for learning_rate in lr_opts:\n",
    "                        test_settings.append([dnode1, dropout1, dnode2, dropout2, batch_size, learning_rate])                                   \n",
    "\n",
    "                        \n",
    "print('done')\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d84e25c-5341-4b01-85f7-3c2b699f0968",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T13:42:06.266177Z",
     "iopub.status.busy": "2023-02-22T13:42:06.265571Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random search 0/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 22:42:06.450488: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-22 22:42:08.863436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30702 MB memory:  -> device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "2023-02-22 22:42:08.864179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 142 MB memory:  -> device: 1, name: Tesla V100-DGXS-32GB, pci bus id: 0000:08:00.0, compute capability: 7.0\n",
      "2023-02-22 22:42:08.864824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 190 MB memory:  -> device: 2, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0e:00.0, compute capability: 7.0\n",
      "2023-02-22 22:42:08.865498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30973 MB memory:  -> device: 3, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0f:00.0, compute capability: 7.0\n",
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "542/544 [============================>.] - ETA: 0s - loss: 7.8501\n",
      "Epoch 00001: val_loss improved from inf to 1.62585, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 7.8298 - val_loss: 1.6259\n",
      "Epoch 2/100\n",
      "530/544 [============================>.] - ETA: 0s - loss: 2.2930\n",
      "Epoch 00002: val_loss did not improve from 1.62585\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.2865 - val_loss: 1.7056\n",
      "Epoch 3/100\n",
      "540/544 [============================>.] - ETA: 0s - loss: 2.1789\n",
      "Epoch 00003: val_loss improved from 1.62585 to 1.48550, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 2.1768 - val_loss: 1.4855\n",
      "Epoch 4/100\n",
      "536/544 [============================>.] - ETA: 0s - loss: 2.0243\n",
      "Epoch 00004: val_loss improved from 1.48550 to 1.42521, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 2.0274 - val_loss: 1.4252\n",
      "Epoch 5/100\n",
      "538/544 [============================>.] - ETA: 0s - loss: 2.0326\n",
      "Epoch 00005: val_loss improved from 1.42521 to 1.39697, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.0332 - val_loss: 1.3970\n",
      "Epoch 6/100\n",
      "535/544 [============================>.] - ETA: 0s - loss: 1.9324\n",
      "Epoch 00006: val_loss improved from 1.39697 to 1.31992, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 1.9380 - val_loss: 1.3199\n",
      "Epoch 7/100\n",
      "544/544 [==============================] - ETA: 0s - loss: 1.9265\n",
      "Epoch 00007: val_loss did not improve from 1.31992\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 1.9265 - val_loss: 1.4578\n",
      "Epoch 8/100\n",
      "530/544 [============================>.] - ETA: 0s - loss: 1.8540\n",
      "Epoch 00008: val_loss did not improve from 1.31992\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 1.8458 - val_loss: 1.3229\n",
      " ###0 fold : val mae 0.819, mse 1.301###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540/544 [============================>.] - ETA: 0s - loss: 7.8579\n",
      "Epoch 00001: val_loss improved from inf to 1.52027, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 7.8199 - val_loss: 1.5203\n",
      "Epoch 2/100\n",
      "526/544 [============================>.] - ETA: 0s - loss: 2.2773\n",
      "Epoch 00002: val_loss did not improve from 1.52027\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 2.2721 - val_loss: 1.6854\n",
      "Epoch 3/100\n",
      "531/544 [============================>.] - ETA: 0s - loss: 2.1739\n",
      "Epoch 00003: val_loss did not improve from 1.52027\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 2.1663 - val_loss: 1.5729\n",
      " ###1 fold : val mae 0.883, mse 1.576###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531/544 [============================>.] - ETA: 0s - loss: 7.8189\n",
      "Epoch 00001: val_loss improved from inf to 1.47694, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 7.6945 - val_loss: 1.4769\n",
      "Epoch 2/100\n",
      "530/544 [============================>.] - ETA: 0s - loss: 2.2853\n",
      "Epoch 00002: val_loss did not improve from 1.47694\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.2738 - val_loss: 1.6477\n",
      "Epoch 3/100\n",
      "522/544 [===========================>..] - ETA: 0s - loss: 2.1745\n",
      "Epoch 00003: val_loss did not improve from 1.47694\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.1670 - val_loss: 1.5255\n",
      " ###2 fold : val mae 0.881, mse 1.598###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539/544 [============================>.] - ETA: 0s - loss: 7.8071\n",
      "Epoch 00001: val_loss improved from inf to 1.45262, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 7.7562 - val_loss: 1.4526\n",
      "Epoch 2/100\n",
      "533/544 [============================>.] - ETA: 0s - loss: 2.2659\n",
      "Epoch 00002: val_loss did not improve from 1.45262\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.2565 - val_loss: 1.5206\n",
      "Epoch 3/100\n",
      "534/544 [============================>.] - ETA: 0s - loss: 2.1712\n",
      "Epoch 00003: val_loss did not improve from 1.45262\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.1684 - val_loss: 1.4604\n",
      " ###3 fold : val mae 0.868, mse 1.504###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526/544 [============================>.] - ETA: 0s - loss: 7.8568\n",
      "Epoch 00001: val_loss improved from inf to 1.43470, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 7.6769 - val_loss: 1.4347\n",
      "Epoch 2/100\n",
      "525/544 [===========================>..] - ETA: 0s - loss: 2.2457\n",
      "Epoch 00002: val_loss did not improve from 1.43470\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.2390 - val_loss: 1.4735\n",
      "Epoch 3/100\n",
      "531/544 [============================>.] - ETA: 0s - loss: 2.1548\n",
      "Epoch 00003: val_loss did not improve from 1.43470\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.1498 - val_loss: 1.5377\n",
      " ###4 fold : val mae 0.871, mse 1.459###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527/544 [============================>.] - ETA: 0s - loss: 7.9448\n",
      "Epoch 00001: val_loss improved from inf to 1.47799, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 7.7670 - val_loss: 1.4780\n",
      "Epoch 2/100\n",
      "526/544 [============================>.] - ETA: 0s - loss: 2.2518\n",
      "Epoch 00002: val_loss improved from 1.47799 to 1.40694, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.2436 - val_loss: 1.4069\n",
      "Epoch 3/100\n",
      "521/544 [===========================>..] - ETA: 0s - loss: 2.1628\n",
      "Epoch 00003: val_loss did not improve from 1.40694\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.1612 - val_loss: 1.5201\n",
      "Epoch 4/100\n",
      "525/544 [===========================>..] - ETA: 0s - loss: 2.0372\n",
      "Epoch 00004: val_loss did not improve from 1.40694\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.0470 - val_loss: 1.5082\n",
      " ###5 fold : val mae 0.850, mse 1.429###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533/544 [============================>.] - ETA: 0s - loss: 7.7414\n",
      "Epoch 00001: val_loss improved from inf to 1.51494, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 7.6276 - val_loss: 1.5149\n",
      "Epoch 2/100\n",
      "529/544 [============================>.] - ETA: 0s - loss: 2.2721\n",
      "Epoch 00002: val_loss improved from 1.51494 to 1.45890, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 2.2596 - val_loss: 1.4589\n",
      "Epoch 3/100\n",
      "529/544 [============================>.] - ETA: 0s - loss: 2.1962\n",
      "Epoch 00003: val_loss did not improve from 1.45890\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 2.1889 - val_loss: 1.5729\n",
      "Epoch 4/100\n",
      "541/544 [============================>.] - ETA: 0s - loss: 2.0448\n",
      "Epoch 00004: val_loss did not improve from 1.45890\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.0462 - val_loss: 1.4928\n",
      " ###6 fold : val mae 0.884, mse 1.412###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526/544 [============================>.] - ETA: 0s - loss: 7.8007\n",
      "Epoch 00001: val_loss improved from inf to 1.48206, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 7.6098 - val_loss: 1.4821\n",
      "Epoch 2/100\n",
      "543/544 [============================>.] - ETA: 0s - loss: 2.2486\n",
      "Epoch 00002: val_loss improved from 1.48206 to 1.41077, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 2.2485 - val_loss: 1.4108\n",
      "Epoch 3/100\n",
      "529/544 [============================>.] - ETA: 0s - loss: 2.1819\n",
      "Epoch 00003: val_loss did not improve from 1.41077\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.1813 - val_loss: 1.6921\n",
      "Epoch 4/100\n",
      "541/544 [============================>.] - ETA: 0s - loss: 2.0553\n",
      "Epoch 00004: val_loss improved from 1.41077 to 1.40108, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 2.0577 - val_loss: 1.4011\n",
      "Epoch 5/100\n",
      "531/544 [============================>.] - ETA: 0s - loss: 2.0058\n",
      "Epoch 00005: val_loss improved from 1.40108 to 1.31842, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 2.0014 - val_loss: 1.3184\n",
      "Epoch 6/100\n",
      "525/544 [===========================>..] - ETA: 0s - loss: 1.9253\n",
      "Epoch 00006: val_loss did not improve from 1.31842\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 1.9213 - val_loss: 1.3219\n",
      "Epoch 7/100\n",
      "526/544 [============================>.] - ETA: 0s - loss: 1.8899\n",
      "Epoch 00007: val_loss did not improve from 1.31842\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 1.8978 - val_loss: 1.5335\n",
      " ###7 fold : val mae 0.831, mse 1.268###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537/544 [============================>.] - ETA: 0s - loss: 7.6594\n",
      "Epoch 00001: val_loss improved from inf to 1.45629, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "544/544 [==============================] - 2s 3ms/step - loss: 7.5866 - val_loss: 1.4563\n",
      "Epoch 2/100\n",
      "534/544 [============================>.] - ETA: 0s - loss: 2.2524\n",
      "Epoch 00002: val_loss improved from 1.45629 to 1.36440, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.2498 - val_loss: 1.3644\n",
      "Epoch 3/100\n",
      "531/544 [============================>.] - ETA: 0s - loss: 2.1720\n",
      "Epoch 00003: val_loss did not improve from 1.36440\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.1732 - val_loss: 1.5291\n",
      "Epoch 4/100\n",
      "533/544 [============================>.] - ETA: 0s - loss: 2.0543\n",
      "Epoch 00004: val_loss improved from 1.36440 to 1.33577, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 2.0636 - val_loss: 1.3358\n",
      "Epoch 5/100\n",
      "528/544 [============================>.] - ETA: 0s - loss: 1.9981\n",
      "Epoch 00005: val_loss improved from 1.33577 to 1.26429, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 1.9933 - val_loss: 1.2643\n",
      "Epoch 6/100\n",
      "542/544 [============================>.] - ETA: 0s - loss: 1.9359\n",
      "Epoch 00006: val_loss did not improve from 1.26429\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 1.9336 - val_loss: 1.2746\n",
      "Epoch 7/100\n",
      "538/544 [============================>.] - ETA: 0s - loss: 1.8901\n",
      "Epoch 00007: val_loss did not improve from 1.26429\n",
      "544/544 [==============================] - 1s 3ms/step - loss: 1.8940 - val_loss: 1.4706\n",
      " ###8 fold : val mae 0.843, mse 1.365###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538/545 [============================>.] - ETA: 0s - loss: 7.6007\n",
      "Epoch 00001: val_loss improved from inf to 1.68736, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "545/545 [==============================] - 2s 4ms/step - loss: 7.5477 - val_loss: 1.6874\n",
      "Epoch 2/100\n",
      "535/545 [============================>.] - ETA: 0s - loss: 2.3061\n",
      "Epoch 00002: val_loss improved from 1.68736 to 1.59013, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "545/545 [==============================] - 2s 3ms/step - loss: 2.3041 - val_loss: 1.5901\n",
      "Epoch 3/100\n",
      "531/545 [============================>.] - ETA: 0s - loss: 2.1447\n",
      "Epoch 00003: val_loss did not improve from 1.59013\n",
      "545/545 [==============================] - 2s 3ms/step - loss: 2.1458 - val_loss: 1.6230\n",
      "Epoch 4/100\n",
      "529/545 [============================>.] - ETA: 0s - loss: 2.0624\n",
      "Epoch 00004: val_loss improved from 1.59013 to 1.41168, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "545/545 [==============================] - 2s 3ms/step - loss: 2.0703 - val_loss: 1.4117\n",
      "Epoch 5/100\n",
      "538/545 [============================>.] - ETA: 0s - loss: 2.0006\n",
      "Epoch 00005: val_loss improved from 1.41168 to 1.33180, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "545/545 [==============================] - 2s 3ms/step - loss: 1.9951 - val_loss: 1.3318\n",
      "Epoch 6/100\n",
      "530/545 [============================>.] - ETA: 0s - loss: 1.9157\n",
      "Epoch 00006: val_loss did not improve from 1.33180\n",
      "545/545 [==============================] - 2s 3ms/step - loss: 1.9199 - val_loss: 1.7039\n",
      "Epoch 7/100\n",
      "538/545 [============================>.] - ETA: 0s - loss: 1.8974\n",
      "Epoch 00007: val_loss did not improve from 1.33180\n",
      "545/545 [==============================] - 2s 3ms/step - loss: 1.9052 - val_loss: 1.4757\n",
      " ###9 fold : val mae 0.817, mse 1.262###\n",
      "mae1.417_mse0.680\n",
      "random search 1/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/68 [======================>.......] - ETA: 0s - loss: 184.4041\n",
      "Epoch 00001: val_loss improved from inf to 168.37849, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 1s 4ms/step - loss: 181.6305 - val_loss: 168.3785\n",
      "Epoch 2/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 154.2273\n",
      "Epoch 00002: val_loss improved from 168.37849 to 135.60788, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 151.3230 - val_loss: 135.6079\n",
      "Epoch 3/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 120.9844\n",
      "Epoch 00003: val_loss improved from 135.60788 to 99.39448, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 117.3518 - val_loss: 99.3945\n",
      "Epoch 4/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 85.7740\n",
      "Epoch 00004: val_loss improved from 99.39448 to 66.12433, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 82.4820 - val_loss: 66.1243\n",
      "Epoch 5/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 56.1065\n",
      "Epoch 00005: val_loss improved from 66.12433 to 42.75335, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 54.1045 - val_loss: 42.7533\n",
      "Epoch 6/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 37.4260\n",
      "Epoch 00006: val_loss improved from 42.75335 to 29.89350, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 36.4594 - val_loss: 29.8935\n",
      "Epoch 7/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 28.0367\n",
      "Epoch 00007: val_loss improved from 29.89350 to 22.91650, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 27.2576 - val_loss: 22.9165\n",
      "Epoch 8/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 22.2096\n",
      "Epoch 00008: val_loss improved from 22.91650 to 18.06523, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 21.7984 - val_loss: 18.0652\n",
      "Epoch 9/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 17.8969\n",
      "Epoch 00009: val_loss improved from 18.06523 to 14.05286, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 17.6194 - val_loss: 14.0529\n",
      "Epoch 10/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 14.2623\n",
      "Epoch 00010: val_loss improved from 14.05286 to 10.68763, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 13.9237 - val_loss: 10.6876\n",
      "Epoch 11/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 11.4707\n",
      "Epoch 00011: val_loss improved from 10.68763 to 8.02939, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 11.1790 - val_loss: 8.0294\n",
      "Epoch 12/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 8.9919\n",
      "Epoch 00012: val_loss improved from 8.02939 to 6.01865, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 8.8149 - val_loss: 6.0186\n",
      "Epoch 13/100\n",
      "54/68 [======================>.......] - ETA: 0s - loss: 7.2910\n",
      "Epoch 00013: val_loss improved from 6.01865 to 4.57946, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 7.1232 - val_loss: 4.5795\n",
      "Epoch 14/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 6.1178\n",
      "Epoch 00014: val_loss improved from 4.57946 to 3.60140, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 6.0967 - val_loss: 3.6014\n",
      "Epoch 15/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 5.3793\n",
      "Epoch 00015: val_loss improved from 3.60140 to 2.94255, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 5.3154 - val_loss: 2.9426\n",
      "Epoch 16/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 4.9592\n",
      "Epoch 00016: val_loss improved from 2.94255 to 2.52088, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.8999 - val_loss: 2.5209\n",
      "Epoch 17/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 4.5366\n",
      "Epoch 00017: val_loss improved from 2.52088 to 2.23787, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.5152 - val_loss: 2.2379\n",
      "Epoch 18/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 4.3199\n",
      "Epoch 00018: val_loss improved from 2.23787 to 2.05177, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.2974 - val_loss: 2.0518\n",
      "Epoch 19/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 4.0900\n",
      "Epoch 00019: val_loss improved from 2.05177 to 1.92224, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.1294 - val_loss: 1.9222\n",
      "Epoch 20/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 4.0110\n",
      "Epoch 00020: val_loss improved from 1.92224 to 1.83660, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.0430 - val_loss: 1.8366\n",
      "Epoch 21/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 4.0122\n",
      "Epoch 00021: val_loss improved from 1.83660 to 1.77218, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9939 - val_loss: 1.7722\n",
      "Epoch 22/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.9400\n",
      "Epoch 00022: val_loss improved from 1.77218 to 1.73485, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9323 - val_loss: 1.7349\n",
      "Epoch 23/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.8824\n",
      "Epoch 00023: val_loss improved from 1.73485 to 1.70772, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8802 - val_loss: 1.7077\n",
      "Epoch 24/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.9023\n",
      "Epoch 00024: val_loss improved from 1.70772 to 1.66158, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8323 - val_loss: 1.6616\n",
      "Epoch 25/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.7175\n",
      "Epoch 00025: val_loss improved from 1.66158 to 1.63980, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7365 - val_loss: 1.6398\n",
      "Epoch 26/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.6649\n",
      "Epoch 00026: val_loss improved from 1.63980 to 1.62006, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.6729 - val_loss: 1.6201\n",
      "Epoch 27/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.5656\n",
      "Epoch 00027: val_loss improved from 1.62006 to 1.59725, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5429 - val_loss: 1.5972\n",
      "Epoch 28/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.6000\n",
      "Epoch 00028: val_loss improved from 1.59725 to 1.58451, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5604 - val_loss: 1.5845\n",
      "Epoch 29/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.5371\n",
      "Epoch 00029: val_loss improved from 1.58451 to 1.56415, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5568 - val_loss: 1.5642\n",
      "Epoch 30/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.6435\n",
      "Epoch 00030: val_loss improved from 1.56415 to 1.56025, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.6058 - val_loss: 1.5602\n",
      "Epoch 31/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4698\n",
      "Epoch 00031: val_loss improved from 1.56025 to 1.53598, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4569 - val_loss: 1.5360\n",
      "Epoch 32/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4308\n",
      "Epoch 00032: val_loss improved from 1.53598 to 1.53339, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4879 - val_loss: 1.5334\n",
      "Epoch 33/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4799\n",
      "Epoch 00033: val_loss improved from 1.53339 to 1.51309, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4721 - val_loss: 1.5131\n",
      "Epoch 34/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.4392\n",
      "Epoch 00034: val_loss improved from 1.51309 to 1.50824, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4467 - val_loss: 1.5082\n",
      "Epoch 35/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.3429\n",
      "Epoch 00035: val_loss improved from 1.50824 to 1.49389, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3164 - val_loss: 1.4939\n",
      "Epoch 36/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.4054\n",
      "Epoch 00036: val_loss improved from 1.49389 to 1.47953, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3730 - val_loss: 1.4795\n",
      "Epoch 37/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.3734\n",
      "Epoch 00037: val_loss improved from 1.47953 to 1.47478, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3618 - val_loss: 1.4748\n",
      "Epoch 38/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.3129\n",
      "Epoch 00038: val_loss improved from 1.47478 to 1.46304, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3221 - val_loss: 1.4630\n",
      "Epoch 39/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.2912\n",
      "Epoch 00039: val_loss improved from 1.46304 to 1.45457, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2758 - val_loss: 1.4546\n",
      "Epoch 40/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.2007\n",
      "Epoch 00040: val_loss improved from 1.45457 to 1.44521, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2132 - val_loss: 1.4452\n",
      "Epoch 41/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.2359\n",
      "Epoch 00041: val_loss improved from 1.44521 to 1.43391, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1958 - val_loss: 1.4339\n",
      "Epoch 42/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.2122\n",
      "Epoch 00042: val_loss did not improve from 1.43391\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1849 - val_loss: 1.4427\n",
      "Epoch 43/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.1757\n",
      "Epoch 00043: val_loss improved from 1.43391 to 1.41744, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1445 - val_loss: 1.4174\n",
      "Epoch 44/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.0705\n",
      "Epoch 00044: val_loss did not improve from 1.41744\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0997 - val_loss: 1.4251\n",
      "Epoch 45/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.0997\n",
      "Epoch 00045: val_loss improved from 1.41744 to 1.40500, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0970 - val_loss: 1.4050\n",
      "Epoch 46/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.1018\n",
      "Epoch 00046: val_loss did not improve from 1.40500\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1181 - val_loss: 1.4090\n",
      "Epoch 47/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.0244\n",
      "Epoch 00047: val_loss improved from 1.40500 to 1.40470, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0353 - val_loss: 1.4047\n",
      "Epoch 48/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.0655\n",
      "Epoch 00048: val_loss improved from 1.40470 to 1.38835, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0809 - val_loss: 1.3884\n",
      "Epoch 49/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.0027\n",
      "Epoch 00049: val_loss did not improve from 1.38835\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.9919 - val_loss: 1.3901\n",
      "Epoch 50/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.0010\n",
      "Epoch 00050: val_loss did not improve from 1.38835\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0336 - val_loss: 1.3929\n",
      " ###0 fold : val mae 0.832, mse 1.352###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/68 [======================>.......] - ETA: 0s - loss: 183.7610\n",
      "Epoch 00001: val_loss improved from inf to 168.41312, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 1s 5ms/step - loss: 181.0143 - val_loss: 168.4131\n",
      "Epoch 2/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 154.1823\n",
      "Epoch 00002: val_loss improved from 168.41312 to 135.69891, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 150.9586 - val_loss: 135.6989\n",
      "Epoch 3/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 120.4918\n",
      "Epoch 00003: val_loss improved from 135.69891 to 99.46870, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 117.1365 - val_loss: 99.4687\n",
      "Epoch 4/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 85.7993\n",
      "Epoch 00004: val_loss improved from 99.46870 to 66.17896, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 82.5002 - val_loss: 66.1790\n",
      "Epoch 5/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 56.3430\n",
      "Epoch 00005: val_loss improved from 66.17896 to 42.76734, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 54.2041 - val_loss: 42.7673\n",
      "Epoch 6/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 37.5126\n",
      "Epoch 00006: val_loss improved from 42.76734 to 29.87720, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 36.4902 - val_loss: 29.8772\n",
      "Epoch 7/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 27.9606\n",
      "Epoch 00007: val_loss improved from 29.87720 to 22.86167, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 27.2921 - val_loss: 22.8617\n",
      "Epoch 8/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 22.0948\n",
      "Epoch 00008: val_loss improved from 22.86167 to 17.98545, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 21.7063 - val_loss: 17.9855\n",
      "Epoch 9/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 17.8049\n",
      "Epoch 00009: val_loss improved from 17.98545 to 13.96756, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 17.5142 - val_loss: 13.9676\n",
      "Epoch 10/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 14.1350\n",
      "Epoch 00010: val_loss improved from 13.96756 to 10.62067, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 13.8650 - val_loss: 10.6207\n",
      "Epoch 11/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 11.3810\n",
      "Epoch 00011: val_loss improved from 10.62067 to 7.96252, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 11.1246 - val_loss: 7.9625\n",
      "Epoch 12/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 8.8936\n",
      "Epoch 00012: val_loss improved from 7.96252 to 5.96367, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 8.7105 - val_loss: 5.9637\n",
      "Epoch 13/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 7.2219\n",
      "Epoch 00013: val_loss improved from 5.96367 to 4.53877, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 7.0642 - val_loss: 4.5388\n",
      "Epoch 14/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 6.0352\n",
      "Epoch 00014: val_loss improved from 4.53877 to 3.57229, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 6.0269 - val_loss: 3.5723\n",
      "Epoch 15/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 5.3377\n",
      "Epoch 00015: val_loss improved from 3.57229 to 2.92660, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 5.2598 - val_loss: 2.9266\n",
      "Epoch 16/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 4.8738\n",
      "Epoch 00016: val_loss improved from 2.92660 to 2.51238, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.8317 - val_loss: 2.5124\n",
      "Epoch 17/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 4.4541\n",
      "Epoch 00017: val_loss improved from 2.51238 to 2.23275, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.4349 - val_loss: 2.2327\n",
      "Epoch 18/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 4.2828\n",
      "Epoch 00018: val_loss improved from 2.23275 to 2.05673, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.3051 - val_loss: 2.0567\n",
      "Epoch 19/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 4.0906\n",
      "Epoch 00019: val_loss improved from 2.05673 to 1.92683, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.1115 - val_loss: 1.9268\n",
      "Epoch 20/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.9868\n",
      "Epoch 00020: val_loss improved from 1.92683 to 1.84423, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.0129 - val_loss: 1.8442\n",
      "Epoch 21/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.9662\n",
      "Epoch 00021: val_loss improved from 1.84423 to 1.78153, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9508 - val_loss: 1.7815\n",
      "Epoch 22/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.9209\n",
      "Epoch 00022: val_loss improved from 1.78153 to 1.75015, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9055 - val_loss: 1.7501\n",
      "Epoch 23/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.8697\n",
      "Epoch 00023: val_loss improved from 1.75015 to 1.72534, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8456 - val_loss: 1.7253\n",
      "Epoch 24/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.8497\n",
      "Epoch 00024: val_loss improved from 1.72534 to 1.68058, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7954 - val_loss: 1.6806\n",
      "Epoch 25/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.6944\n",
      "Epoch 00025: val_loss improved from 1.68058 to 1.65794, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7212 - val_loss: 1.6579\n",
      "Epoch 26/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.6384\n",
      "Epoch 00026: val_loss improved from 1.65794 to 1.63644, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.6627 - val_loss: 1.6364\n",
      "Epoch 27/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.5377\n",
      "Epoch 00027: val_loss improved from 1.63644 to 1.61294, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5236 - val_loss: 1.6129\n",
      "Epoch 28/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.5795\n",
      "Epoch 00028: val_loss improved from 1.61294 to 1.59808, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5629 - val_loss: 1.5981\n",
      "Epoch 29/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.4946\n",
      "Epoch 00029: val_loss improved from 1.59808 to 1.58091, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5510 - val_loss: 1.5809\n",
      "Epoch 30/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.6214\n",
      "Epoch 00030: val_loss improved from 1.58091 to 1.57877, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5738 - val_loss: 1.5788\n",
      "Epoch 31/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4475\n",
      "Epoch 00031: val_loss improved from 1.57877 to 1.55097, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4357 - val_loss: 1.5510\n",
      "Epoch 32/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.4415\n",
      "Epoch 00032: val_loss improved from 1.55097 to 1.54336, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5034 - val_loss: 1.5434\n",
      "Epoch 33/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4450\n",
      "Epoch 00033: val_loss improved from 1.54336 to 1.52814, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4327 - val_loss: 1.5281\n",
      "Epoch 34/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4175\n",
      "Epoch 00034: val_loss improved from 1.52814 to 1.52213, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4153 - val_loss: 1.5221\n",
      "Epoch 35/100\n",
      "54/68 [======================>.......] - ETA: 0s - loss: 3.3475\n",
      "Epoch 00035: val_loss improved from 1.52213 to 1.50999, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3099 - val_loss: 1.5100\n",
      "Epoch 36/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.3849\n",
      "Epoch 00036: val_loss improved from 1.50999 to 1.49118, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3581 - val_loss: 1.4912\n",
      "Epoch 37/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.3436\n",
      "Epoch 00037: val_loss improved from 1.49118 to 1.48762, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3279 - val_loss: 1.4876\n",
      "Epoch 38/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.3270\n",
      "Epoch 00038: val_loss improved from 1.48762 to 1.47506, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3307 - val_loss: 1.4751\n",
      "Epoch 39/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.2792\n",
      "Epoch 00039: val_loss improved from 1.47506 to 1.46957, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2698 - val_loss: 1.4696\n",
      "Epoch 40/100\n",
      "54/68 [======================>.......] - ETA: 0s - loss: 3.2257\n",
      "Epoch 00040: val_loss improved from 1.46957 to 1.45580, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2239 - val_loss: 1.4558\n",
      "Epoch 41/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.1924\n",
      "Epoch 00041: val_loss improved from 1.45580 to 1.44421, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1683 - val_loss: 1.4442\n",
      "Epoch 42/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.1929\n",
      "Epoch 00042: val_loss did not improve from 1.44421\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1506 - val_loss: 1.4503\n",
      "Epoch 43/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.1570\n",
      "Epoch 00043: val_loss improved from 1.44421 to 1.42659, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1347 - val_loss: 1.4266\n",
      "Epoch 44/100\n",
      "53/68 [======================>.......] - ETA: 0s - loss: 3.0818\n",
      "Epoch 00044: val_loss did not improve from 1.42659\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1135 - val_loss: 1.4303\n",
      "Epoch 45/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.1085\n",
      "Epoch 00045: val_loss improved from 1.42659 to 1.41598, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1098 - val_loss: 1.4160\n",
      "Epoch 46/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.0881\n",
      "Epoch 00046: val_loss did not improve from 1.41598\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1079 - val_loss: 1.4193\n",
      "Epoch 47/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.0246\n",
      "Epoch 00047: val_loss did not improve from 1.41598\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0334 - val_loss: 1.4193\n",
      " ###1 fold : val mae 0.859, mse 1.523###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/68 [=======================>......] - ETA: 0s - loss: 183.8349\n",
      "Epoch 00001: val_loss improved from inf to 168.43472, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 1s 4ms/step - loss: 181.4359 - val_loss: 168.4347\n",
      "Epoch 2/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 154.3600\n",
      "Epoch 00002: val_loss improved from 168.43472 to 135.74472, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 151.2549 - val_loss: 135.7447\n",
      "Epoch 3/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 120.2628\n",
      "Epoch 00003: val_loss improved from 135.74472 to 99.57201, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 117.2947 - val_loss: 99.5720\n",
      "Epoch 4/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 85.5445\n",
      "Epoch 00004: val_loss improved from 99.57201 to 66.27541, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 82.4895 - val_loss: 66.2754\n",
      "Epoch 5/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 55.7637\n",
      "Epoch 00005: val_loss improved from 66.27541 to 42.85372, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 54.0753 - val_loss: 42.8537\n",
      "Epoch 6/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 37.4284\n",
      "Epoch 00006: val_loss improved from 42.85372 to 29.94432, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 36.3572 - val_loss: 29.9443\n",
      "Epoch 7/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 27.8598\n",
      "Epoch 00007: val_loss improved from 29.94432 to 22.97506, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 27.2554 - val_loss: 22.9751\n",
      "Epoch 8/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 22.0679\n",
      "Epoch 00008: val_loss improved from 22.97506 to 18.11677, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 21.6757 - val_loss: 18.1168\n",
      "Epoch 9/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 17.9064\n",
      "Epoch 00009: val_loss improved from 18.11677 to 14.09757, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 17.5207 - val_loss: 14.0976\n",
      "Epoch 10/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 14.0406\n",
      "Epoch 00010: val_loss improved from 14.09757 to 10.77042, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 13.8594 - val_loss: 10.7704\n",
      "Epoch 11/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 11.4020\n",
      "Epoch 00011: val_loss improved from 10.77042 to 8.09633, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 11.1808 - val_loss: 8.0963\n",
      "Epoch 12/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 8.9837\n",
      "Epoch 00012: val_loss improved from 8.09633 to 6.07917, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 8.8058 - val_loss: 6.0792\n",
      "Epoch 13/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 7.2868\n",
      "Epoch 00013: val_loss improved from 6.07917 to 4.63340, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 7.1233 - val_loss: 4.6334\n",
      "Epoch 14/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 6.0868\n",
      "Epoch 00014: val_loss improved from 4.63340 to 3.64365, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 6.1098 - val_loss: 3.6437\n",
      "Epoch 15/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 5.3495\n",
      "Epoch 00015: val_loss improved from 3.64365 to 2.97888, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 5.2869 - val_loss: 2.9789\n",
      "Epoch 16/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 4.9526\n",
      "Epoch 00016: val_loss improved from 2.97888 to 2.54601, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.8922 - val_loss: 2.5460\n",
      "Epoch 17/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 4.4901\n",
      "Epoch 00017: val_loss improved from 2.54601 to 2.26157, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.4904 - val_loss: 2.2616\n",
      "Epoch 18/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 4.3287\n",
      "Epoch 00018: val_loss improved from 2.26157 to 2.07299, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.3446 - val_loss: 2.0730\n",
      "Epoch 19/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 4.1592\n",
      "Epoch 00019: val_loss improved from 2.07299 to 1.93737, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.1692 - val_loss: 1.9374\n",
      "Epoch 20/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 4.0170\n",
      "Epoch 00020: val_loss improved from 1.93737 to 1.84818, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.0779 - val_loss: 1.8482\n",
      "Epoch 21/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.9988\n",
      "Epoch 00021: val_loss improved from 1.84818 to 1.78383, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9854 - val_loss: 1.7838\n",
      "Epoch 22/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.9528\n",
      "Epoch 00022: val_loss improved from 1.78383 to 1.75120, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9420 - val_loss: 1.7512\n",
      "Epoch 23/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.8275\n",
      "Epoch 00023: val_loss improved from 1.75120 to 1.71299, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8304 - val_loss: 1.7130\n",
      "Epoch 24/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.8968\n",
      "Epoch 00024: val_loss improved from 1.71299 to 1.67271, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8376 - val_loss: 1.6727\n",
      "Epoch 25/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.6821\n",
      "Epoch 00025: val_loss improved from 1.67271 to 1.65123, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7094 - val_loss: 1.6512\n",
      "Epoch 26/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.6436\n",
      "Epoch 00026: val_loss improved from 1.65123 to 1.62686, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.6820 - val_loss: 1.6269\n",
      "Epoch 27/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.5359\n",
      "Epoch 00027: val_loss improved from 1.62686 to 1.60269, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5491 - val_loss: 1.6027\n",
      "Epoch 28/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.5863\n",
      "Epoch 00028: val_loss improved from 1.60269 to 1.58782, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5764 - val_loss: 1.5878\n",
      "Epoch 29/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.5400\n",
      "Epoch 00029: val_loss improved from 1.58782 to 1.57260, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5891 - val_loss: 1.5726\n",
      "Epoch 30/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.6289\n",
      "Epoch 00030: val_loss improved from 1.57260 to 1.56383, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5951 - val_loss: 1.5638\n",
      "Epoch 31/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.4557\n",
      "Epoch 00031: val_loss improved from 1.56383 to 1.53819, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4330 - val_loss: 1.5382\n",
      "Epoch 32/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.4197\n",
      "Epoch 00032: val_loss improved from 1.53819 to 1.53345, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4762 - val_loss: 1.5334\n",
      "Epoch 33/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.4336\n",
      "Epoch 00033: val_loss improved from 1.53345 to 1.52566, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4472 - val_loss: 1.5257\n",
      "Epoch 34/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.3929\n",
      "Epoch 00034: val_loss improved from 1.52566 to 1.50473, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4019 - val_loss: 1.5047\n",
      "Epoch 35/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.3781\n",
      "Epoch 00035: val_loss improved from 1.50473 to 1.49605, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3373 - val_loss: 1.4961\n",
      "Epoch 36/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.3945\n",
      "Epoch 00036: val_loss improved from 1.49605 to 1.47655, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3527 - val_loss: 1.4766\n",
      "Epoch 37/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.3495\n",
      "Epoch 00037: val_loss did not improve from 1.47655\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3266 - val_loss: 1.4781\n",
      "Epoch 38/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.3434\n",
      "Epoch 00038: val_loss improved from 1.47655 to 1.46620, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3391 - val_loss: 1.4662\n",
      "Epoch 39/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.2495\n",
      "Epoch 00039: val_loss improved from 1.46620 to 1.46031, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2577 - val_loss: 1.4603\n",
      "Epoch 40/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.2059\n",
      "Epoch 00040: val_loss improved from 1.46031 to 1.44571, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2171 - val_loss: 1.4457\n",
      "Epoch 41/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.1762\n",
      "Epoch 00041: val_loss improved from 1.44571 to 1.43317, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1639 - val_loss: 1.4332\n",
      "Epoch 42/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.1728\n",
      "Epoch 00042: val_loss did not improve from 1.43317\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1455 - val_loss: 1.4351\n",
      "Epoch 43/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 3.1599\n",
      "Epoch 00043: val_loss improved from 1.43317 to 1.41818, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1437 - val_loss: 1.4182\n",
      "Epoch 44/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 3.1120\n",
      "Epoch 00044: val_loss did not improve from 1.41818\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1285 - val_loss: 1.4258\n",
      "Epoch 45/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.0955\n",
      "Epoch 00045: val_loss improved from 1.41818 to 1.40778, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1175 - val_loss: 1.4078\n",
      "Epoch 46/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 3.0930\n",
      "Epoch 00046: val_loss did not improve from 1.40778\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1121 - val_loss: 1.4097\n",
      "Epoch 47/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.0355\n",
      "Epoch 00047: val_loss did not improve from 1.40778\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0407 - val_loss: 1.4078\n",
      " ###2 fold : val mae 0.857, mse 1.400###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/68 [==========================>...] - ETA: 0s - loss: 182.8780\n",
      "Epoch 00001: val_loss improved from inf to 168.42101, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 1s 4ms/step - loss: 181.6766 - val_loss: 168.4210\n",
      "Epoch 2/100\n",
      "64/68 [===========================>..] - ETA: 0s - loss: 152.5502\n",
      "Epoch 00002: val_loss improved from 168.42101 to 135.69339, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 151.4928 - val_loss: 135.6934\n",
      "Epoch 3/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 119.4457\n",
      "Epoch 00003: val_loss improved from 135.69339 to 99.45230, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 117.4538 - val_loss: 99.4523\n",
      "Epoch 4/100\n",
      "63/68 [==========================>...] - ETA: 0s - loss: 83.8737\n",
      "Epoch 00004: val_loss improved from 99.45230 to 66.19446, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 82.6571 - val_loss: 66.1945\n",
      "Epoch 5/100\n",
      "64/68 [===========================>..] - ETA: 0s - loss: 54.9604\n",
      "Epoch 00005: val_loss improved from 66.19446 to 42.80651, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 54.2680 - val_loss: 42.8065\n",
      "Epoch 6/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 37.1818\n",
      "Epoch 00006: val_loss improved from 42.80651 to 29.93791, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 36.5842 - val_loss: 29.9379\n",
      "Epoch 7/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 27.8890\n",
      "Epoch 00007: val_loss improved from 29.93791 to 22.97859, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 27.4736 - val_loss: 22.9786\n",
      "Epoch 8/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 21.9209\n",
      "Epoch 00008: val_loss improved from 22.97859 to 18.12885, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 21.6992 - val_loss: 18.1289\n",
      "Epoch 9/100\n",
      "63/68 [==========================>...] - ETA: 0s - loss: 17.7640\n",
      "Epoch 00009: val_loss improved from 18.12885 to 14.10771, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 17.5957 - val_loss: 14.1077\n",
      "Epoch 10/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 14.0075\n",
      "Epoch 00010: val_loss improved from 14.10771 to 10.77113, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 13.8842 - val_loss: 10.7711\n",
      "Epoch 11/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 11.4412\n",
      "Epoch 00011: val_loss improved from 10.77113 to 8.09100, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 11.2132 - val_loss: 8.0910\n",
      "Epoch 12/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 8.8637\n",
      "Epoch 00012: val_loss improved from 8.09100 to 6.07874, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 8.8159 - val_loss: 6.0787\n",
      "Epoch 13/100\n",
      "63/68 [==========================>...] - ETA: 0s - loss: 7.2099\n",
      "Epoch 00013: val_loss improved from 6.07874 to 4.61700, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 7.1319 - val_loss: 4.6170\n",
      "Epoch 14/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 6.1200\n",
      "Epoch 00014: val_loss improved from 4.61700 to 3.62876, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 6.1076 - val_loss: 3.6288\n",
      "Epoch 15/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 5.3314\n",
      "Epoch 00015: val_loss improved from 3.62876 to 2.96606, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 5.2615 - val_loss: 2.9661\n",
      "Epoch 16/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 4.9847\n",
      "Epoch 00016: val_loss improved from 2.96606 to 2.53284, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.9158 - val_loss: 2.5328\n",
      "Epoch 17/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 4.5094\n",
      "Epoch 00017: val_loss improved from 2.53284 to 2.25471, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.5208 - val_loss: 2.2547\n",
      "Epoch 18/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 4.3127\n",
      "Epoch 00018: val_loss improved from 2.25471 to 2.06838, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.3439 - val_loss: 2.0684\n",
      "Epoch 19/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 4.1544\n",
      "Epoch 00019: val_loss improved from 2.06838 to 1.93448, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.1461 - val_loss: 1.9345\n",
      "Epoch 20/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 4.0393\n",
      "Epoch 00020: val_loss improved from 1.93448 to 1.84806, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.0706 - val_loss: 1.8481\n",
      "Epoch 21/100\n",
      "64/68 [===========================>..] - ETA: 0s - loss: 3.9499\n",
      "Epoch 00021: val_loss improved from 1.84806 to 1.78301, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9598 - val_loss: 1.7830\n",
      "Epoch 22/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 3.9541\n",
      "Epoch 00022: val_loss improved from 1.78301 to 1.74825, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9477 - val_loss: 1.7483\n",
      "Epoch 23/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 3.8276\n",
      "Epoch 00023: val_loss improved from 1.74825 to 1.71269, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8359 - val_loss: 1.7127\n",
      "Epoch 24/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 3.8721\n",
      "Epoch 00024: val_loss improved from 1.71269 to 1.67293, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8623 - val_loss: 1.6729\n",
      "Epoch 25/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 3.7147\n",
      "Epoch 00025: val_loss improved from 1.67293 to 1.65336, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7249 - val_loss: 1.6534\n",
      "Epoch 26/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 3.6445\n",
      "Epoch 00026: val_loss improved from 1.65336 to 1.62692, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7031 - val_loss: 1.6269\n",
      "Epoch 27/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 3.5741\n",
      "Epoch 00027: val_loss improved from 1.62692 to 1.60240, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5630 - val_loss: 1.6024\n",
      "Epoch 28/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 3.5999\n",
      "Epoch 00028: val_loss improved from 1.60240 to 1.58958, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5910 - val_loss: 1.5896\n",
      "Epoch 29/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 3.5843\n",
      "Epoch 00029: val_loss improved from 1.58958 to 1.57272, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.6013 - val_loss: 1.5727\n",
      "Epoch 30/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 3.6289\n",
      "Epoch 00030: val_loss improved from 1.57272 to 1.56553, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5825 - val_loss: 1.5655\n",
      "Epoch 31/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 3.4720\n",
      "Epoch 00031: val_loss improved from 1.56553 to 1.54346, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4531 - val_loss: 1.5435\n",
      "Epoch 32/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 3.4193\n",
      "Epoch 00032: val_loss improved from 1.54346 to 1.53599, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4528 - val_loss: 1.5360\n",
      "Epoch 33/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 3.4374\n",
      "Epoch 00033: val_loss improved from 1.53599 to 1.52655, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4505 - val_loss: 1.5265\n",
      "Epoch 34/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 3.4236\n",
      "Epoch 00034: val_loss improved from 1.52655 to 1.50687, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4043 - val_loss: 1.5069\n",
      "Epoch 35/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 3.3897\n",
      "Epoch 00035: val_loss improved from 1.50687 to 1.49720, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3677 - val_loss: 1.4972\n",
      "Epoch 36/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 3.3510\n",
      "Epoch 00036: val_loss improved from 1.49720 to 1.47876, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3438 - val_loss: 1.4788\n",
      "Epoch 37/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 3.3544\n",
      "Epoch 00037: val_loss did not improve from 1.47876\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 3.3526 - val_loss: 1.4809\n",
      "Epoch 38/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 3.3060\n",
      "Epoch 00038: val_loss improved from 1.47876 to 1.46816, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3117 - val_loss: 1.4682\n",
      "Epoch 39/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 3.2641\n",
      "Epoch 00039: val_loss improved from 1.46816 to 1.46523, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2634 - val_loss: 1.4652\n",
      "Epoch 40/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 3.2211\n",
      "Epoch 00040: val_loss improved from 1.46523 to 1.44713, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2206 - val_loss: 1.4471\n",
      "Epoch 41/100\n",
      "63/68 [==========================>...] - ETA: 0s - loss: 3.1979\n",
      "Epoch 00041: val_loss improved from 1.44713 to 1.43540, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1838 - val_loss: 1.4354\n",
      "Epoch 42/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 3.1268\n",
      "Epoch 00042: val_loss did not improve from 1.43540\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 3.1200 - val_loss: 1.4396\n",
      "Epoch 43/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 3.1758\n",
      "Epoch 00043: val_loss improved from 1.43540 to 1.42077, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1714 - val_loss: 1.4208\n",
      "Epoch 44/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 3.1130\n",
      "Epoch 00044: val_loss did not improve from 1.42077\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1290 - val_loss: 1.4325\n",
      "Epoch 45/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 3.1094\n",
      "Epoch 00045: val_loss improved from 1.42077 to 1.40806, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1201 - val_loss: 1.4081\n",
      "Epoch 46/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 3.0994\n",
      "Epoch 00046: val_loss improved from 1.40806 to 1.40566, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1329 - val_loss: 1.4057\n",
      "Epoch 47/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 3.0402\n",
      "Epoch 00047: val_loss did not improve from 1.40566\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0453 - val_loss: 1.4083\n",
      "Epoch 48/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.0599\n",
      "Epoch 00048: val_loss improved from 1.40566 to 1.39324, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0745 - val_loss: 1.3932\n",
      "Epoch 49/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 3.0054\n",
      "Epoch 00049: val_loss did not improve from 1.39324\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 2.9846 - val_loss: 1.3945\n",
      "Epoch 50/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 3.0139\n",
      "Epoch 00050: val_loss improved from 1.39324 to 1.38652, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0278 - val_loss: 1.3865\n",
      "Epoch 51/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 3.0104\n",
      "Epoch 00051: val_loss improved from 1.38652 to 1.38157, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0306 - val_loss: 1.3816\n",
      "Epoch 52/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 2.9086\n",
      "Epoch 00052: val_loss did not improve from 1.38157\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.9217 - val_loss: 1.3907\n",
      "Epoch 53/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 2.9374\n",
      "Epoch 00053: val_loss improved from 1.38157 to 1.37323, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.9386 - val_loss: 1.3732\n",
      "Epoch 54/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 2.8703\n",
      "Epoch 00054: val_loss did not improve from 1.37323\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.9072 - val_loss: 1.3741\n",
      "Epoch 55/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 2.8437\n",
      "Epoch 00055: val_loss improved from 1.37323 to 1.37213, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.8345 - val_loss: 1.3721\n",
      "Epoch 56/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 2.9540\n",
      "Epoch 00056: val_loss improved from 1.37213 to 1.36036, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.9332 - val_loss: 1.3604\n",
      "Epoch 57/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 2.8844\n",
      "Epoch 00057: val_loss did not improve from 1.36036\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.8887 - val_loss: 1.3650\n",
      "Epoch 58/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 2.8540\n",
      "Epoch 00058: val_loss improved from 1.36036 to 1.35630, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.8685 - val_loss: 1.3563\n",
      "Epoch 59/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 2.7986\n",
      "Epoch 00059: val_loss improved from 1.35630 to 1.35218, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.7948 - val_loss: 1.3522\n",
      "Epoch 60/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 2.7825\n",
      "Epoch 00060: val_loss did not improve from 1.35218\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.7894 - val_loss: 1.3545\n",
      "Epoch 61/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 2.7988\n",
      "Epoch 00061: val_loss did not improve from 1.35218\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 2.8165 - val_loss: 1.3547\n",
      " ###3 fold : val mae 0.828, mse 1.305###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/68 [=======================>......] - ETA: 0s - loss: 184.3633\n",
      "Epoch 00001: val_loss improved from inf to 168.34554, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 1s 4ms/step - loss: 181.6474 - val_loss: 168.3455\n",
      "Epoch 2/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 154.2596\n",
      "Epoch 00002: val_loss improved from 168.34554 to 135.58475, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 151.3913 - val_loss: 135.5847\n",
      "Epoch 3/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 120.1359\n",
      "Epoch 00003: val_loss improved from 135.58475 to 99.37938, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 117.3560 - val_loss: 99.3794\n",
      "Epoch 4/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 85.3598\n",
      "Epoch 00004: val_loss improved from 99.37938 to 66.13973, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 82.4358 - val_loss: 66.1397\n",
      "Epoch 5/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 55.9958\n",
      "Epoch 00005: val_loss improved from 66.13973 to 42.79590, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 54.0581 - val_loss: 42.7959\n",
      "Epoch 6/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 37.4848\n",
      "Epoch 00006: val_loss improved from 42.79590 to 29.96144, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 36.4980 - val_loss: 29.9614\n",
      "Epoch 7/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 28.0459\n",
      "Epoch 00007: val_loss improved from 29.96144 to 22.99612, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 27.4555 - val_loss: 22.9961\n",
      "Epoch 8/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 22.0552\n",
      "Epoch 00008: val_loss improved from 22.99612 to 18.12779, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 21.7032 - val_loss: 18.1278\n",
      "Epoch 9/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 18.0316\n",
      "Epoch 00009: val_loss improved from 18.12779 to 14.09114, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 17.6105 - val_loss: 14.0911\n",
      "Epoch 10/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 14.0388\n",
      "Epoch 00010: val_loss improved from 14.09114 to 10.75176, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 13.8495 - val_loss: 10.7518\n",
      "Epoch 11/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 11.3754\n",
      "Epoch 00011: val_loss improved from 10.75176 to 8.07782, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 11.1365 - val_loss: 8.0778\n",
      "Epoch 12/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 8.8525\n",
      "Epoch 00012: val_loss improved from 8.07782 to 6.06669, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 8.7743 - val_loss: 6.0667\n",
      "Epoch 13/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 7.2642\n",
      "Epoch 00013: val_loss improved from 6.06669 to 4.60636, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 7.1287 - val_loss: 4.6064\n",
      "Epoch 14/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 6.0836\n",
      "Epoch 00014: val_loss improved from 4.60636 to 3.61850, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 6.0592 - val_loss: 3.6185\n",
      "Epoch 15/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 5.3257\n",
      "Epoch 00015: val_loss improved from 3.61850 to 2.96858, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 5.2529 - val_loss: 2.9686\n",
      "Epoch 16/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 4.9746\n",
      "Epoch 00016: val_loss improved from 2.96858 to 2.52738, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.9129 - val_loss: 2.5274\n",
      "Epoch 17/100\n",
      "53/68 [======================>.......] - ETA: 0s - loss: 4.5428\n",
      "Epoch 00017: val_loss improved from 2.52738 to 2.24981, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.5346 - val_loss: 2.2498\n",
      "Epoch 18/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 4.2930\n",
      "Epoch 00018: val_loss improved from 2.24981 to 2.06413, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.3167 - val_loss: 2.0641\n",
      "Epoch 19/100\n",
      "54/68 [======================>.......] - ETA: 0s - loss: 4.1257\n",
      "Epoch 00019: val_loss improved from 2.06413 to 1.93556, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.1102 - val_loss: 1.9356\n",
      "Epoch 20/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 4.0552\n",
      "Epoch 00020: val_loss improved from 1.93556 to 1.84776, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.0895 - val_loss: 1.8478\n",
      "Epoch 21/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.9597\n",
      "Epoch 00021: val_loss improved from 1.84776 to 1.78166, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9547 - val_loss: 1.7817\n",
      "Epoch 22/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.9661\n",
      "Epoch 00022: val_loss improved from 1.78166 to 1.74759, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9251 - val_loss: 1.7476\n",
      "Epoch 23/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.8412\n",
      "Epoch 00023: val_loss improved from 1.74759 to 1.71059, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8328 - val_loss: 1.7106\n",
      "Epoch 24/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.9061\n",
      "Epoch 00024: val_loss improved from 1.71059 to 1.67372, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8491 - val_loss: 1.6737\n",
      "Epoch 25/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.6689\n",
      "Epoch 00025: val_loss improved from 1.67372 to 1.65220, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7054 - val_loss: 1.6522\n",
      "Epoch 26/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.6637\n",
      "Epoch 00026: val_loss improved from 1.65220 to 1.62567, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7117 - val_loss: 1.6257\n",
      "Epoch 27/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.5499\n",
      "Epoch 00027: val_loss improved from 1.62567 to 1.60130, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5694 - val_loss: 1.6013\n",
      "Epoch 28/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.5958\n",
      "Epoch 00028: val_loss improved from 1.60130 to 1.58848, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5966 - val_loss: 1.5885\n",
      "Epoch 29/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.5748\n",
      "Epoch 00029: val_loss improved from 1.58848 to 1.57667, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5931 - val_loss: 1.5767\n",
      "Epoch 30/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.6268\n",
      "Epoch 00030: val_loss improved from 1.57667 to 1.57190, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5891 - val_loss: 1.5719\n",
      "Epoch 31/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4579\n",
      "Epoch 00031: val_loss improved from 1.57190 to 1.54707, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4333 - val_loss: 1.5471\n",
      "Epoch 32/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.3920\n",
      "Epoch 00032: val_loss improved from 1.54707 to 1.53679, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4448 - val_loss: 1.5368\n",
      "Epoch 33/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.4143\n",
      "Epoch 00033: val_loss improved from 1.53679 to 1.53162, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4398 - val_loss: 1.5316\n",
      "Epoch 34/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.3824\n",
      "Epoch 00034: val_loss improved from 1.53162 to 1.50456, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3694 - val_loss: 1.5046\n",
      "Epoch 35/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.3782\n",
      "Epoch 00035: val_loss improved from 1.50456 to 1.49838, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3246 - val_loss: 1.4984\n",
      "Epoch 36/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.3025\n",
      "Epoch 00036: val_loss improved from 1.49838 to 1.48006, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3145 - val_loss: 1.4801\n",
      "Epoch 37/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.3411\n",
      "Epoch 00037: val_loss did not improve from 1.48006\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3391 - val_loss: 1.4811\n",
      "Epoch 38/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.3153\n",
      "Epoch 00038: val_loss improved from 1.48006 to 1.46662, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3210 - val_loss: 1.4666\n",
      "Epoch 39/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.2458\n",
      "Epoch 00039: val_loss improved from 1.46662 to 1.46194, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2573 - val_loss: 1.4619\n",
      "Epoch 40/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.1985\n",
      "Epoch 00040: val_loss improved from 1.46194 to 1.44803, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1945 - val_loss: 1.4480\n",
      "Epoch 41/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.1599\n",
      "Epoch 00041: val_loss improved from 1.44803 to 1.44265, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1689 - val_loss: 1.4427\n",
      "Epoch 42/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.1283\n",
      "Epoch 00042: val_loss improved from 1.44265 to 1.44099, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1044 - val_loss: 1.4410\n",
      "Epoch 43/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.1512\n",
      "Epoch 00043: val_loss improved from 1.44099 to 1.42524, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1482 - val_loss: 1.4252\n",
      "Epoch 44/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.0807\n",
      "Epoch 00044: val_loss did not improve from 1.42524\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1186 - val_loss: 1.4291\n",
      "Epoch 45/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.0943\n",
      "Epoch 00045: val_loss improved from 1.42524 to 1.40865, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1137 - val_loss: 1.4087\n",
      "Epoch 46/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.1097\n",
      "Epoch 00046: val_loss did not improve from 1.40865\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1238 - val_loss: 1.4122\n",
      "Epoch 47/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.0011\n",
      "Epoch 00047: val_loss did not improve from 1.40865\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0158 - val_loss: 1.4099\n",
      " ###4 fold : val mae 0.868, mse 1.444###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/68 [========================>.....] - ETA: 0s - loss: 184.0202\n",
      "Epoch 00001: val_loss improved from inf to 168.35954, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 1s 4ms/step - loss: 181.7013 - val_loss: 168.3595\n",
      "Epoch 2/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 153.9577\n",
      "Epoch 00002: val_loss improved from 168.35954 to 135.63333, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 151.4285 - val_loss: 135.6333\n",
      "Epoch 3/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 120.0954\n",
      "Epoch 00003: val_loss improved from 135.63333 to 99.48610, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 117.3735 - val_loss: 99.4861\n",
      "Epoch 4/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 85.2305\n",
      "Epoch 00004: val_loss improved from 99.48610 to 66.20994, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 82.4994 - val_loss: 66.2099\n",
      "Epoch 5/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 55.8642\n",
      "Epoch 00005: val_loss improved from 66.20994 to 42.83167, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 54.0930 - val_loss: 42.8317\n",
      "Epoch 6/100\n",
      "54/68 [======================>.......] - ETA: 0s - loss: 37.8443\n",
      "Epoch 00006: val_loss improved from 42.83167 to 29.95119, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 36.6037 - val_loss: 29.9512\n",
      "Epoch 7/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 27.7991\n",
      "Epoch 00007: val_loss improved from 29.95119 to 22.97268, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 27.4800 - val_loss: 22.9727\n",
      "Epoch 8/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 21.9354\n",
      "Epoch 00008: val_loss improved from 22.97268 to 18.10208, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 21.6954 - val_loss: 18.1021\n",
      "Epoch 9/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 17.9108\n",
      "Epoch 00009: val_loss improved from 18.10208 to 14.06811, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 17.5634 - val_loss: 14.0681\n",
      "Epoch 10/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 14.0423\n",
      "Epoch 00010: val_loss improved from 14.06811 to 10.73628, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 13.8828 - val_loss: 10.7363\n",
      "Epoch 11/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 11.3388\n",
      "Epoch 00011: val_loss improved from 10.73628 to 8.05561, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 11.1150 - val_loss: 8.0556\n",
      "Epoch 12/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 8.7969\n",
      "Epoch 00012: val_loss improved from 8.05561 to 6.05166, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 8.7394 - val_loss: 6.0517\n",
      "Epoch 13/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 7.2633\n",
      "Epoch 00013: val_loss improved from 6.05166 to 4.59295, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 7.1348 - val_loss: 4.5929\n",
      "Epoch 14/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 6.0350\n",
      "Epoch 00014: val_loss improved from 4.59295 to 3.60668, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 6.0331 - val_loss: 3.6067\n",
      "Epoch 15/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 5.2899\n",
      "Epoch 00015: val_loss improved from 3.60668 to 2.95470, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 5.2278 - val_loss: 2.9547\n",
      "Epoch 16/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 4.9494\n",
      "Epoch 00016: val_loss improved from 2.95470 to 2.51840, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.8929 - val_loss: 2.5184\n",
      "Epoch 17/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 4.5166\n",
      "Epoch 00017: val_loss improved from 2.51840 to 2.24537, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.5206 - val_loss: 2.2454\n",
      "Epoch 18/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 4.2987\n",
      "Epoch 00018: val_loss improved from 2.24537 to 2.06329, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.2911 - val_loss: 2.0633\n",
      "Epoch 19/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 4.1229\n",
      "Epoch 00019: val_loss improved from 2.06329 to 1.93541, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.0955 - val_loss: 1.9354\n",
      "Epoch 20/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 4.0325\n",
      "Epoch 00020: val_loss improved from 1.93541 to 1.85059, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.0349 - val_loss: 1.8506\n",
      "Epoch 21/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.9590\n",
      "Epoch 00021: val_loss improved from 1.85059 to 1.78465, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9697 - val_loss: 1.7847\n",
      "Epoch 22/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.9822\n",
      "Epoch 00022: val_loss improved from 1.78465 to 1.75169, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9376 - val_loss: 1.7517\n",
      "Epoch 23/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.8566\n",
      "Epoch 00023: val_loss improved from 1.75169 to 1.71407, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8491 - val_loss: 1.7141\n",
      "Epoch 24/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.8228\n",
      "Epoch 00024: val_loss improved from 1.71407 to 1.68347, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8039 - val_loss: 1.6835\n",
      "Epoch 25/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.6972\n",
      "Epoch 00025: val_loss improved from 1.68347 to 1.65877, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7196 - val_loss: 1.6588\n",
      "Epoch 26/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.7005\n",
      "Epoch 00026: val_loss improved from 1.65877 to 1.63113, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7173 - val_loss: 1.6311\n",
      "Epoch 27/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.5622\n",
      "Epoch 00027: val_loss improved from 1.63113 to 1.60815, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5832 - val_loss: 1.6082\n",
      "Epoch 28/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.6125\n",
      "Epoch 00028: val_loss improved from 1.60815 to 1.59364, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.6077 - val_loss: 1.5936\n",
      "Epoch 29/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.5670\n",
      "Epoch 00029: val_loss improved from 1.59364 to 1.58437, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5680 - val_loss: 1.5844\n",
      "Epoch 30/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.6398\n",
      "Epoch 00030: val_loss improved from 1.58437 to 1.58162, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5978 - val_loss: 1.5816\n",
      "Epoch 31/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4544\n",
      "Epoch 00031: val_loss improved from 1.58162 to 1.55222, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4479 - val_loss: 1.5522\n",
      "Epoch 32/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4064\n",
      "Epoch 00032: val_loss improved from 1.55222 to 1.54319, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4462 - val_loss: 1.5432\n",
      "Epoch 33/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.4408\n",
      "Epoch 00033: val_loss improved from 1.54319 to 1.53892, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4578 - val_loss: 1.5389\n",
      "Epoch 34/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.3944\n",
      "Epoch 00034: val_loss improved from 1.53892 to 1.51150, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3615 - val_loss: 1.5115\n",
      "Epoch 35/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.3977\n",
      "Epoch 00035: val_loss improved from 1.51150 to 1.50455, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3491 - val_loss: 1.5046\n",
      "Epoch 36/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.3359\n",
      "Epoch 00036: val_loss improved from 1.50455 to 1.48877, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3197 - val_loss: 1.4888\n",
      "Epoch 37/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.3062\n",
      "Epoch 00037: val_loss improved from 1.48877 to 1.48580, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3313 - val_loss: 1.4858\n",
      "Epoch 38/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.3243\n",
      "Epoch 00038: val_loss improved from 1.48580 to 1.47184, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3153 - val_loss: 1.4718\n",
      "Epoch 39/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.2189\n",
      "Epoch 00039: val_loss improved from 1.47184 to 1.47066, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2546 - val_loss: 1.4707\n",
      "Epoch 40/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.1934\n",
      "Epoch 00040: val_loss improved from 1.47066 to 1.45515, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1982 - val_loss: 1.4551\n",
      "Epoch 41/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.1652\n",
      "Epoch 00041: val_loss improved from 1.45515 to 1.44671, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1747 - val_loss: 1.4467\n",
      "Epoch 42/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.1403\n",
      "Epoch 00042: val_loss improved from 1.44671 to 1.44427, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1249 - val_loss: 1.4443\n",
      "Epoch 43/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.1674\n",
      "Epoch 00043: val_loss improved from 1.44427 to 1.43024, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1622 - val_loss: 1.4302\n",
      "Epoch 44/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.1057\n",
      "Epoch 00044: val_loss did not improve from 1.43024\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1210 - val_loss: 1.4332\n",
      "Epoch 45/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.1382\n",
      "Epoch 00045: val_loss improved from 1.43024 to 1.41430, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1514 - val_loss: 1.4143\n",
      "Epoch 46/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.0812\n",
      "Epoch 00046: val_loss did not improve from 1.41430\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1189 - val_loss: 1.4200\n",
      "Epoch 47/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.0185\n",
      "Epoch 00047: val_loss did not improve from 1.41430\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0388 - val_loss: 1.4181\n",
      " ###5 fold : val mae 0.856, mse 1.450###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/68 [========================>.....] - ETA: 0s - loss: 183.6942\n",
      "Epoch 00001: val_loss improved from inf to 168.31863, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 1s 4ms/step - loss: 181.2873 - val_loss: 168.3186\n",
      "Epoch 2/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 153.6384\n",
      "Epoch 00002: val_loss improved from 168.31863 to 135.62625, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 151.1282 - val_loss: 135.6263\n",
      "Epoch 3/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 119.9319\n",
      "Epoch 00003: val_loss improved from 135.62625 to 99.51835, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 117.1847 - val_loss: 99.5183\n",
      "Epoch 4/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 85.0813\n",
      "Epoch 00004: val_loss improved from 99.51835 to 66.10497, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 82.3844 - val_loss: 66.1050\n",
      "Epoch 5/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 55.7173\n",
      "Epoch 00005: val_loss improved from 66.10497 to 42.68665, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 53.9686 - val_loss: 42.6866\n",
      "Epoch 6/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 37.5700\n",
      "Epoch 00006: val_loss improved from 42.68665 to 29.90930, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 36.6052 - val_loss: 29.9093\n",
      "Epoch 7/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 27.9991\n",
      "Epoch 00007: val_loss improved from 29.90930 to 23.02245, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 27.4769 - val_loss: 23.0224\n",
      "Epoch 8/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 22.1117\n",
      "Epoch 00008: val_loss improved from 23.02245 to 18.17022, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 21.7669 - val_loss: 18.1702\n",
      "Epoch 9/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 17.9610\n",
      "Epoch 00009: val_loss improved from 18.17022 to 14.13317, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 17.5710 - val_loss: 14.1332\n",
      "Epoch 10/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 14.1781\n",
      "Epoch 00010: val_loss improved from 14.13317 to 10.78018, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 14.0008 - val_loss: 10.7802\n",
      "Epoch 11/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 11.3604\n",
      "Epoch 00011: val_loss improved from 10.78018 to 8.10084, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 11.1202 - val_loss: 8.1008\n",
      "Epoch 12/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 8.8218\n",
      "Epoch 00012: val_loss improved from 8.10084 to 6.09787, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 8.7803 - val_loss: 6.0979\n",
      "Epoch 13/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 7.2898\n",
      "Epoch 00013: val_loss improved from 6.09787 to 4.64351, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 7.1586 - val_loss: 4.6435\n",
      "Epoch 14/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 6.1375\n",
      "Epoch 00014: val_loss improved from 4.64351 to 3.64840, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 6.0990 - val_loss: 3.6484\n",
      "Epoch 15/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 5.2933\n",
      "Epoch 00015: val_loss improved from 3.64840 to 2.99096, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 5.2517 - val_loss: 2.9910\n",
      "Epoch 16/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 4.9472\n",
      "Epoch 00016: val_loss improved from 2.99096 to 2.53788, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.9152 - val_loss: 2.5379\n",
      "Epoch 17/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 4.5198\n",
      "Epoch 00017: val_loss improved from 2.53788 to 2.25048, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.5427 - val_loss: 2.2505\n",
      "Epoch 18/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 4.3518\n",
      "Epoch 00018: val_loss improved from 2.25048 to 2.06118, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.3222 - val_loss: 2.0612\n",
      "Epoch 19/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 4.1867\n",
      "Epoch 00019: val_loss improved from 2.06118 to 1.92865, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.1612 - val_loss: 1.9286\n",
      "Epoch 20/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 4.0653\n",
      "Epoch 00020: val_loss improved from 1.92865 to 1.84212, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.0614 - val_loss: 1.8421\n",
      "Epoch 21/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.9816\n",
      "Epoch 00021: val_loss improved from 1.84212 to 1.77573, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.0103 - val_loss: 1.7757\n",
      "Epoch 22/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 4.0004\n",
      "Epoch 00022: val_loss improved from 1.77573 to 1.74342, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9498 - val_loss: 1.7434\n",
      "Epoch 23/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.8637\n",
      "Epoch 00023: val_loss improved from 1.74342 to 1.70298, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8570 - val_loss: 1.7030\n",
      "Epoch 24/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.8713\n",
      "Epoch 00024: val_loss improved from 1.70298 to 1.67344, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8306 - val_loss: 1.6734\n",
      "Epoch 25/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.7191\n",
      "Epoch 00025: val_loss improved from 1.67344 to 1.64872, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7436 - val_loss: 1.6487\n",
      "Epoch 26/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 3.6994\n",
      "Epoch 00026: val_loss improved from 1.64872 to 1.61980, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7307 - val_loss: 1.6198\n",
      "Epoch 27/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.5983\n",
      "Epoch 00027: val_loss improved from 1.61980 to 1.59979, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.6292 - val_loss: 1.5998\n",
      "Epoch 28/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.6414\n",
      "Epoch 00028: val_loss improved from 1.59979 to 1.58503, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.6533 - val_loss: 1.5850\n",
      "Epoch 29/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.6260\n",
      "Epoch 00029: val_loss improved from 1.58503 to 1.58022, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.6081 - val_loss: 1.5802\n",
      "Epoch 30/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.6524\n",
      "Epoch 00030: val_loss improved from 1.58022 to 1.57080, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.6070 - val_loss: 1.5708\n",
      "Epoch 31/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4856\n",
      "Epoch 00031: val_loss improved from 1.57080 to 1.54683, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4830 - val_loss: 1.5468\n",
      "Epoch 32/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4516\n",
      "Epoch 00032: val_loss improved from 1.54683 to 1.53559, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4700 - val_loss: 1.5356\n",
      "Epoch 33/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4748\n",
      "Epoch 00033: val_loss improved from 1.53559 to 1.52908, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4884 - val_loss: 1.5291\n",
      "Epoch 34/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.3741\n",
      "Epoch 00034: val_loss improved from 1.52908 to 1.50496, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3468 - val_loss: 1.5050\n",
      "Epoch 35/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4284\n",
      "Epoch 00035: val_loss improved from 1.50496 to 1.49964, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3867 - val_loss: 1.4996\n",
      "Epoch 36/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.3118\n",
      "Epoch 00036: val_loss improved from 1.49964 to 1.48138, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3252 - val_loss: 1.4814\n",
      "Epoch 37/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.3386\n",
      "Epoch 00037: val_loss improved from 1.48138 to 1.47894, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3545 - val_loss: 1.4789\n",
      "Epoch 38/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.2987\n",
      "Epoch 00038: val_loss improved from 1.47894 to 1.46678, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2959 - val_loss: 1.4668\n",
      "Epoch 39/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.2332\n",
      "Epoch 00039: val_loss improved from 1.46678 to 1.46238, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2785 - val_loss: 1.4624\n",
      "Epoch 40/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.2236\n",
      "Epoch 00040: val_loss improved from 1.46238 to 1.44871, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2143 - val_loss: 1.4487\n",
      "Epoch 41/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.1752\n",
      "Epoch 00041: val_loss improved from 1.44871 to 1.44424, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1855 - val_loss: 1.4442\n",
      "Epoch 42/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.1600\n",
      "Epoch 00042: val_loss improved from 1.44424 to 1.43697, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1379 - val_loss: 1.4370\n",
      "Epoch 43/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.1587\n",
      "Epoch 00043: val_loss improved from 1.43697 to 1.42634, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1565 - val_loss: 1.4263\n",
      "Epoch 44/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 3.1412\n",
      "Epoch 00044: val_loss did not improve from 1.42634\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1397 - val_loss: 1.4264\n",
      "Epoch 45/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.1297\n",
      "Epoch 00045: val_loss improved from 1.42634 to 1.40592, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1474 - val_loss: 1.4059\n",
      "Epoch 46/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.0901\n",
      "Epoch 00046: val_loss did not improve from 1.40592\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1179 - val_loss: 1.4154\n",
      "Epoch 47/100\n",
      "54/68 [======================>.......] - ETA: 0s - loss: 3.0106\n",
      "Epoch 00047: val_loss did not improve from 1.40592\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0256 - val_loss: 1.4093\n",
      " ###6 fold : val mae 0.849, mse 1.319###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/68 [========================>.....] - ETA: 0s - loss: 183.4845\n",
      "Epoch 00001: val_loss improved from inf to 168.37740, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 1s 4ms/step - loss: 181.2397 - val_loss: 168.3774\n",
      "Epoch 2/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 153.7697\n",
      "Epoch 00002: val_loss improved from 168.37740 to 135.67883, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 151.1649 - val_loss: 135.6788\n",
      "Epoch 3/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 120.3409\n",
      "Epoch 00003: val_loss improved from 135.67883 to 99.62764, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 117.2708 - val_loss: 99.6276\n",
      "Epoch 4/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 85.4801\n",
      "Epoch 00004: val_loss improved from 99.62764 to 66.20351, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 82.5841 - val_loss: 66.2035\n",
      "Epoch 5/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 56.3038\n",
      "Epoch 00005: val_loss improved from 66.20351 to 42.72480, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 54.1776 - val_loss: 42.7248\n",
      "Epoch 6/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 37.7354\n",
      "Epoch 00006: val_loss improved from 42.72480 to 29.89772, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 36.5839 - val_loss: 29.8977\n",
      "Epoch 7/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 28.0396\n",
      "Epoch 00007: val_loss improved from 29.89772 to 22.98081, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 27.4722 - val_loss: 22.9808\n",
      "Epoch 8/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 22.1408\n",
      "Epoch 00008: val_loss improved from 22.98081 to 18.12283, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 21.7816 - val_loss: 18.1228\n",
      "Epoch 9/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 17.9348\n",
      "Epoch 00009: val_loss improved from 18.12283 to 14.08116, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 17.5576 - val_loss: 14.0812\n",
      "Epoch 10/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 14.0873\n",
      "Epoch 00010: val_loss improved from 14.08116 to 10.72345, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 13.9232 - val_loss: 10.7235\n",
      "Epoch 11/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 11.2906\n",
      "Epoch 00011: val_loss improved from 10.72345 to 8.04665, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 11.0184 - val_loss: 8.0466\n",
      "Epoch 12/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 8.8029\n",
      "Epoch 00012: val_loss improved from 8.04665 to 6.05587, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 8.7216 - val_loss: 6.0559\n",
      "Epoch 13/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 7.2611\n",
      "Epoch 00013: val_loss improved from 6.05587 to 4.61147, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 7.1187 - val_loss: 4.6115\n",
      "Epoch 14/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 6.1291\n",
      "Epoch 00014: val_loss improved from 4.61147 to 3.62683, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 6.0708 - val_loss: 3.6268\n",
      "Epoch 15/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 5.2540\n",
      "Epoch 00015: val_loss improved from 3.62683 to 2.97760, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 5.1925 - val_loss: 2.9776\n",
      "Epoch 16/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 4.8953\n",
      "Epoch 00016: val_loss improved from 2.97760 to 2.52879, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.8735 - val_loss: 2.5288\n",
      "Epoch 17/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 4.5272\n",
      "Epoch 00017: val_loss improved from 2.52879 to 2.24372, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.5207 - val_loss: 2.2437\n",
      "Epoch 18/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 4.3317\n",
      "Epoch 00018: val_loss improved from 2.24372 to 2.05829, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.3236 - val_loss: 2.0583\n",
      "Epoch 19/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 4.1690\n",
      "Epoch 00019: val_loss improved from 2.05829 to 1.92894, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.1448 - val_loss: 1.9289\n",
      "Epoch 20/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 4.0190\n",
      "Epoch 00020: val_loss improved from 1.92894 to 1.84344, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.0294 - val_loss: 1.8434\n",
      "Epoch 21/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.9251\n",
      "Epoch 00021: val_loss improved from 1.84344 to 1.77904, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9504 - val_loss: 1.7790\n",
      "Epoch 22/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.9842\n",
      "Epoch 00022: val_loss improved from 1.77904 to 1.74782, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9205 - val_loss: 1.7478\n",
      "Epoch 23/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.8222\n",
      "Epoch 00023: val_loss improved from 1.74782 to 1.70223, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7976 - val_loss: 1.7022\n",
      "Epoch 24/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.8371\n",
      "Epoch 00024: val_loss improved from 1.70223 to 1.67733, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8053 - val_loss: 1.6773\n",
      "Epoch 25/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.7251\n",
      "Epoch 00025: val_loss improved from 1.67733 to 1.65346, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7473 - val_loss: 1.6535\n",
      "Epoch 26/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.6213\n",
      "Epoch 00026: val_loss improved from 1.65346 to 1.63043, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7022 - val_loss: 1.6304\n",
      "Epoch 27/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.5481\n",
      "Epoch 00027: val_loss improved from 1.63043 to 1.60494, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5994 - val_loss: 1.6049\n",
      "Epoch 28/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.6488\n",
      "Epoch 00028: val_loss improved from 1.60494 to 1.59051, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.6628 - val_loss: 1.5905\n",
      "Epoch 29/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.6040\n",
      "Epoch 00029: val_loss improved from 1.59051 to 1.58535, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5965 - val_loss: 1.5853\n",
      "Epoch 30/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.5792\n",
      "Epoch 00030: val_loss improved from 1.58535 to 1.57826, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5482 - val_loss: 1.5783\n",
      "Epoch 31/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4602\n",
      "Epoch 00031: val_loss improved from 1.57826 to 1.55129, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4601 - val_loss: 1.5513\n",
      "Epoch 32/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4634\n",
      "Epoch 00032: val_loss improved from 1.55129 to 1.54188, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4681 - val_loss: 1.5419\n",
      "Epoch 33/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4414\n",
      "Epoch 00033: val_loss improved from 1.54188 to 1.53735, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4675 - val_loss: 1.5374\n",
      "Epoch 34/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.3406\n",
      "Epoch 00034: val_loss improved from 1.53735 to 1.51178, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3216 - val_loss: 1.5118\n",
      "Epoch 35/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4020\n",
      "Epoch 00035: val_loss improved from 1.51178 to 1.50997, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3735 - val_loss: 1.5100\n",
      "Epoch 36/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.3438\n",
      "Epoch 00036: val_loss improved from 1.50997 to 1.48504, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3189 - val_loss: 1.4850\n",
      "Epoch 37/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.3213\n",
      "Epoch 00037: val_loss improved from 1.48504 to 1.48132, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3368 - val_loss: 1.4813\n",
      "Epoch 38/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.2545\n",
      "Epoch 00038: val_loss improved from 1.48132 to 1.47679, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2611 - val_loss: 1.4768\n",
      "Epoch 39/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.2604\n",
      "Epoch 00039: val_loss improved from 1.47679 to 1.47342, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3006 - val_loss: 1.4734\n",
      "Epoch 40/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.2212\n",
      "Epoch 00040: val_loss improved from 1.47342 to 1.45710, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2050 - val_loss: 1.4571\n",
      "Epoch 41/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.1591\n",
      "Epoch 00041: val_loss improved from 1.45710 to 1.45273, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1654 - val_loss: 1.4527\n",
      "Epoch 42/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.1439\n",
      "Epoch 00042: val_loss improved from 1.45273 to 1.44042, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1263 - val_loss: 1.4404\n",
      "Epoch 43/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.1283\n",
      "Epoch 00043: val_loss improved from 1.44042 to 1.43531, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1212 - val_loss: 1.4353\n",
      "Epoch 44/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.1594\n",
      "Epoch 00044: val_loss improved from 1.43531 to 1.43103, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1706 - val_loss: 1.4310\n",
      "Epoch 45/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.1154\n",
      "Epoch 00045: val_loss improved from 1.43103 to 1.41703, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1360 - val_loss: 1.4170\n",
      "Epoch 46/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.0973\n",
      "Epoch 00046: val_loss did not improve from 1.41703\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1186 - val_loss: 1.4200\n",
      "Epoch 47/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.0077\n",
      "Epoch 00047: val_loss improved from 1.41703 to 1.41438, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0183 - val_loss: 1.4144\n",
      "Epoch 48/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 2.9973\n",
      "Epoch 00048: val_loss improved from 1.41438 to 1.40492, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0238 - val_loss: 1.4049\n",
      "Epoch 49/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 2.9792\n",
      "Epoch 00049: val_loss improved from 1.40492 to 1.39813, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.9519 - val_loss: 1.3981\n",
      "Epoch 50/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.0024\n",
      "Epoch 00050: val_loss improved from 1.39813 to 1.39035, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0070 - val_loss: 1.3904\n",
      "Epoch 51/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.0066\n",
      "Epoch 00051: val_loss did not improve from 1.39035\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.9947 - val_loss: 1.3982\n",
      "Epoch 52/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 2.8830\n",
      "Epoch 00052: val_loss did not improve from 1.39035\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.9045 - val_loss: 1.3947\n",
      " ###7 fold : val mae 0.857, mse 1.387###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/68 [========================>.....] - ETA: 0s - loss: 183.5105\n",
      "Epoch 00001: val_loss improved from inf to 167.91754, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 1s 4ms/step - loss: 181.4567 - val_loss: 167.9175\n",
      "Epoch 2/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 153.6494\n",
      "Epoch 00002: val_loss improved from 167.91754 to 135.12091, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 151.3549 - val_loss: 135.1209\n",
      "Epoch 3/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 119.9833\n",
      "Epoch 00003: val_loss improved from 135.12091 to 98.91255, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 117.4164 - val_loss: 98.9126\n",
      "Epoch 4/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 84.7341\n",
      "Epoch 00004: val_loss improved from 98.91255 to 65.39926, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 82.6473 - val_loss: 65.3993\n",
      "Epoch 5/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 56.0971\n",
      "Epoch 00005: val_loss improved from 65.39926 to 41.94980, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 54.1747 - val_loss: 41.9498\n",
      "Epoch 6/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 37.4320\n",
      "Epoch 00006: val_loss improved from 41.94980 to 29.24704, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 36.6016 - val_loss: 29.2470\n",
      "Epoch 7/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 27.9031\n",
      "Epoch 00007: val_loss improved from 29.24704 to 22.46677, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 27.4443 - val_loss: 22.4668\n",
      "Epoch 8/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 22.1250\n",
      "Epoch 00008: val_loss improved from 22.46677 to 17.71884, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 21.7989 - val_loss: 17.7188\n",
      "Epoch 9/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 17.9194\n",
      "Epoch 00009: val_loss improved from 17.71884 to 13.76266, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 17.5774 - val_loss: 13.7627\n",
      "Epoch 10/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 14.1332\n",
      "Epoch 00010: val_loss improved from 13.76266 to 10.47480, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 13.9508 - val_loss: 10.4748\n",
      "Epoch 11/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 11.2875\n",
      "Epoch 00011: val_loss improved from 10.47480 to 7.86046, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 11.0516 - val_loss: 7.8605\n",
      "Epoch 12/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 8.8147\n",
      "Epoch 00012: val_loss improved from 7.86046 to 5.92747, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 8.7446 - val_loss: 5.9275\n",
      "Epoch 13/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 7.2198\n",
      "Epoch 00013: val_loss improved from 5.92747 to 4.52890, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 7.1432 - val_loss: 4.5289\n",
      "Epoch 14/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 6.1132\n",
      "Epoch 00014: val_loss improved from 4.52890 to 3.57755, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 6.0994 - val_loss: 3.5776\n",
      "Epoch 15/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 5.2761\n",
      "Epoch 00015: val_loss improved from 3.57755 to 2.95254, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 5.2186 - val_loss: 2.9525\n",
      "Epoch 16/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 4.8800\n",
      "Epoch 00016: val_loss improved from 2.95254 to 2.51944, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.8783 - val_loss: 2.5194\n",
      "Epoch 17/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 4.5032\n",
      "Epoch 00017: val_loss improved from 2.51944 to 2.24615, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.5194 - val_loss: 2.2461\n",
      "Epoch 18/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 4.3389\n",
      "Epoch 00018: val_loss improved from 2.24615 to 2.06948, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.3337 - val_loss: 2.0695\n",
      "Epoch 19/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 4.1729\n",
      "Epoch 00019: val_loss improved from 2.06948 to 1.94707, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.1444 - val_loss: 1.9471\n",
      "Epoch 20/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 4.0272\n",
      "Epoch 00020: val_loss improved from 1.94707 to 1.86385, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.0348 - val_loss: 1.8639\n",
      "Epoch 21/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.9483\n",
      "Epoch 00021: val_loss improved from 1.86385 to 1.80121, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9624 - val_loss: 1.8012\n",
      "Epoch 22/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.9435\n",
      "Epoch 00022: val_loss improved from 1.80121 to 1.76798, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.9163 - val_loss: 1.7680\n",
      "Epoch 23/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.8306\n",
      "Epoch 00023: val_loss improved from 1.76798 to 1.72348, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8061 - val_loss: 1.7235\n",
      "Epoch 24/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.8186\n",
      "Epoch 00024: val_loss improved from 1.72348 to 1.69512, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.8126 - val_loss: 1.6951\n",
      "Epoch 25/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 3.7318\n",
      "Epoch 00025: val_loss improved from 1.69512 to 1.66954, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7403 - val_loss: 1.6695\n",
      "Epoch 26/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.6271\n",
      "Epoch 00026: val_loss improved from 1.66954 to 1.64353, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.7025 - val_loss: 1.6435\n",
      "Epoch 27/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.5546\n",
      "Epoch 00027: val_loss improved from 1.64353 to 1.61583, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.6050 - val_loss: 1.6158\n",
      "Epoch 28/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.6716\n",
      "Epoch 00028: val_loss improved from 1.61583 to 1.59603, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.6812 - val_loss: 1.5960\n",
      "Epoch 29/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.5967\n",
      "Epoch 00029: val_loss improved from 1.59603 to 1.58743, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5888 - val_loss: 1.5874\n",
      "Epoch 30/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.5737\n",
      "Epoch 00030: val_loss improved from 1.58743 to 1.57337, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.5470 - val_loss: 1.5734\n",
      "Epoch 31/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.4628\n",
      "Epoch 00031: val_loss improved from 1.57337 to 1.54607, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4610 - val_loss: 1.5461\n",
      "Epoch 32/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.4710\n",
      "Epoch 00032: val_loss improved from 1.54607 to 1.53115, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4795 - val_loss: 1.5312\n",
      "Epoch 33/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.4387\n",
      "Epoch 00033: val_loss improved from 1.53115 to 1.52709, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.4588 - val_loss: 1.5271\n",
      "Epoch 34/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.3416\n",
      "Epoch 00034: val_loss improved from 1.52709 to 1.49585, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3176 - val_loss: 1.4958\n",
      "Epoch 35/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.4174\n",
      "Epoch 00035: val_loss improved from 1.49585 to 1.48909, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3840 - val_loss: 1.4891\n",
      "Epoch 36/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 3.3412\n",
      "Epoch 00036: val_loss improved from 1.48909 to 1.46443, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3156 - val_loss: 1.4644\n",
      "Epoch 37/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.3323\n",
      "Epoch 00037: val_loss improved from 1.46443 to 1.45587, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3284 - val_loss: 1.4559\n",
      "Epoch 38/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.2550\n",
      "Epoch 00038: val_loss improved from 1.45587 to 1.44601, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2551 - val_loss: 1.4460\n",
      "Epoch 39/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.2530\n",
      "Epoch 00039: val_loss improved from 1.44601 to 1.43946, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.3062 - val_loss: 1.4395\n",
      "Epoch 40/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.2270\n",
      "Epoch 00040: val_loss improved from 1.43946 to 1.42029, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.2157 - val_loss: 1.4203\n",
      "Epoch 41/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.1411\n",
      "Epoch 00041: val_loss improved from 1.42029 to 1.41061, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1645 - val_loss: 1.4106\n",
      "Epoch 42/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.1290\n",
      "Epoch 00042: val_loss improved from 1.41061 to 1.39728, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1220 - val_loss: 1.3973\n",
      "Epoch 43/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 3.1277\n",
      "Epoch 00043: val_loss improved from 1.39728 to 1.39380, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1168 - val_loss: 1.3938\n",
      "Epoch 44/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 3.1670\n",
      "Epoch 00044: val_loss improved from 1.39380 to 1.38499, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1724 - val_loss: 1.3850\n",
      "Epoch 45/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 3.1131\n",
      "Epoch 00045: val_loss improved from 1.38499 to 1.37176, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1382 - val_loss: 1.3718\n",
      "Epoch 46/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.0952\n",
      "Epoch 00046: val_loss improved from 1.37176 to 1.37006, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.1101 - val_loss: 1.3701\n",
      "Epoch 47/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 2.9834\n",
      "Epoch 00047: val_loss improved from 1.37006 to 1.36518, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0108 - val_loss: 1.3652\n",
      "Epoch 48/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.0034\n",
      "Epoch 00048: val_loss improved from 1.36518 to 1.35337, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0310 - val_loss: 1.3534\n",
      "Epoch 49/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 2.9932\n",
      "Epoch 00049: val_loss improved from 1.35337 to 1.34446, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.9626 - val_loss: 1.3445\n",
      "Epoch 50/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 3.0091\n",
      "Epoch 00050: val_loss improved from 1.34446 to 1.33779, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 3.0186 - val_loss: 1.3378\n",
      "Epoch 51/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 2.9977\n",
      "Epoch 00051: val_loss did not improve from 1.33779\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.9927 - val_loss: 1.3445\n",
      "Epoch 52/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 2.9019\n",
      "Epoch 00052: val_loss improved from 1.33779 to 1.33690, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.9178 - val_loss: 1.3369\n",
      "Epoch 53/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 2.9128\n",
      "Epoch 00053: val_loss improved from 1.33690 to 1.32377, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.9071 - val_loss: 1.3238\n",
      "Epoch 54/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 2.9025\n",
      "Epoch 00054: val_loss did not improve from 1.32377\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.9088 - val_loss: 1.3266\n",
      "Epoch 55/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 2.8682\n",
      "Epoch 00055: val_loss improved from 1.32377 to 1.31683, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.8757 - val_loss: 1.3168\n",
      "Epoch 56/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 2.8883\n",
      "Epoch 00056: val_loss improved from 1.31683 to 1.31085, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.8819 - val_loss: 1.3109\n",
      "Epoch 57/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 2.8670\n",
      "Epoch 00057: val_loss improved from 1.31085 to 1.30310, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.8814 - val_loss: 1.3031\n",
      "Epoch 58/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 2.9101\n",
      "Epoch 00058: val_loss did not improve from 1.30310\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.8965 - val_loss: 1.3125\n",
      "Epoch 59/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 2.8217\n",
      "Epoch 00059: val_loss improved from 1.30310 to 1.29121, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.8111 - val_loss: 1.2912\n",
      "Epoch 60/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 2.7427\n",
      "Epoch 00060: val_loss improved from 1.29121 to 1.28837, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.7544 - val_loss: 1.2884\n",
      "Epoch 61/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 2.8138\n",
      "Epoch 00061: val_loss did not improve from 1.28837\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.8358 - val_loss: 1.2922\n",
      "Epoch 62/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 2.7657\n",
      "Epoch 00062: val_loss improved from 1.28837 to 1.27880, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.7971 - val_loss: 1.2788\n",
      "Epoch 63/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 2.7792\n",
      "Epoch 00063: val_loss did not improve from 1.27880\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.7785 - val_loss: 1.2871\n",
      "Epoch 64/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 2.7698\n",
      "Epoch 00064: val_loss did not improve from 1.27880\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.7785 - val_loss: 1.2905\n",
      " ###8 fold : val mae 0.848, mse 1.395###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/69 [====================>.........] - ETA: 0s - loss: 185.7145\n",
      "Epoch 00001: val_loss improved from inf to 167.46495, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 1s 4ms/step - loss: 181.5042 - val_loss: 167.4650\n",
      "Epoch 2/100\n",
      "48/69 [===================>..........] - ETA: 0s - loss: 155.9813\n",
      "Epoch 00002: val_loss improved from 167.46495 to 135.23334, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 151.4823 - val_loss: 135.2333\n",
      "Epoch 3/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 122.5304\n",
      "Epoch 00003: val_loss improved from 135.23334 to 100.86568, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 118.0954 - val_loss: 100.8657\n",
      "Epoch 4/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 89.1874\n",
      "Epoch 00004: val_loss improved from 100.86568 to 68.50945, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 84.6974 - val_loss: 68.5095\n",
      "Epoch 5/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 59.6414\n",
      "Epoch 00005: val_loss improved from 68.50945 to 44.22516, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 56.4020 - val_loss: 44.2252\n",
      "Epoch 6/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 39.1630\n",
      "Epoch 00006: val_loss improved from 44.22516 to 30.28721, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 37.5794 - val_loss: 30.2872\n",
      "Epoch 7/100\n",
      "52/69 [=====================>........] - ETA: 0s - loss: 28.5714\n",
      "Epoch 00007: val_loss improved from 30.28721 to 23.09720, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 27.6497 - val_loss: 23.0972\n",
      "Epoch 8/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 22.5155\n",
      "Epoch 00008: val_loss improved from 23.09720 to 18.40495, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 21.9636 - val_loss: 18.4049\n",
      "Epoch 9/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 18.3756\n",
      "Epoch 00009: val_loss improved from 18.40495 to 14.61086, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 17.9366 - val_loss: 14.6109\n",
      "Epoch 10/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 14.7086\n",
      "Epoch 00010: val_loss improved from 14.61086 to 11.43405, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 14.3977 - val_loss: 11.4341\n",
      "Epoch 11/100\n",
      "48/69 [===================>..........] - ETA: 0s - loss: 12.1542\n",
      "Epoch 00011: val_loss improved from 11.43405 to 8.79580, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 11.7408 - val_loss: 8.7958\n",
      "Epoch 12/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 9.5561\n",
      "Epoch 00012: val_loss improved from 8.79580 to 6.76014, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 9.4587 - val_loss: 6.7601\n",
      "Epoch 13/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 8.0311\n",
      "Epoch 00013: val_loss improved from 6.76014 to 5.23152, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 7.8191 - val_loss: 5.2315\n",
      "Epoch 14/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 6.6830\n",
      "Epoch 00014: val_loss improved from 5.23152 to 4.12397, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 6.5145 - val_loss: 4.1240\n",
      "Epoch 15/100\n",
      "48/69 [===================>..........] - ETA: 0s - loss: 5.6099\n",
      "Epoch 00015: val_loss improved from 4.12397 to 3.34974, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 5.4993 - val_loss: 3.3497\n",
      "Epoch 16/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 5.0538\n",
      "Epoch 00016: val_loss improved from 3.34974 to 2.81188, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 4.9748 - val_loss: 2.8119\n",
      "Epoch 17/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 4.6702\n",
      "Epoch 00017: val_loss improved from 2.81188 to 2.46469, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 4.6267 - val_loss: 2.4647\n",
      "Epoch 18/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 4.3880\n",
      "Epoch 00018: val_loss improved from 2.46469 to 2.24732, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 4.3960 - val_loss: 2.2473\n",
      "Epoch 19/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 4.1564\n",
      "Epoch 00019: val_loss improved from 2.24732 to 2.10478, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 4.1922 - val_loss: 2.1048\n",
      "Epoch 20/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 4.1014\n",
      "Epoch 00020: val_loss improved from 2.10478 to 2.01739, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 4.0770 - val_loss: 2.0174\n",
      "Epoch 21/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 3.9639\n",
      "Epoch 00021: val_loss improved from 2.01739 to 1.93263, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.9637 - val_loss: 1.9326\n",
      "Epoch 22/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 3.8783\n",
      "Epoch 00022: val_loss improved from 1.93263 to 1.87919, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.8401 - val_loss: 1.8792\n",
      "Epoch 23/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 3.7361\n",
      "Epoch 00023: val_loss improved from 1.87919 to 1.83821, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.7985 - val_loss: 1.8382\n",
      "Epoch 24/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 3.7951\n",
      "Epoch 00024: val_loss improved from 1.83821 to 1.80493, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.7479 - val_loss: 1.8049\n",
      "Epoch 25/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 3.6601\n",
      "Epoch 00025: val_loss improved from 1.80493 to 1.77193, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.6470 - val_loss: 1.7719\n",
      "Epoch 26/100\n",
      "47/69 [===================>..........] - ETA: 0s - loss: 3.8029\n",
      "Epoch 00026: val_loss improved from 1.77193 to 1.74980, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.7339 - val_loss: 1.7498\n",
      "Epoch 27/100\n",
      "47/69 [===================>..........] - ETA: 0s - loss: 3.5749\n",
      "Epoch 00027: val_loss improved from 1.74980 to 1.72985, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.5950 - val_loss: 1.7299\n",
      "Epoch 28/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 3.5917\n",
      "Epoch 00028: val_loss improved from 1.72985 to 1.70880, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.6058 - val_loss: 1.7088\n",
      "Epoch 29/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 3.5880\n",
      "Epoch 00029: val_loss improved from 1.70880 to 1.69058, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.5665 - val_loss: 1.6906\n",
      "Epoch 30/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 3.5934\n",
      "Epoch 00030: val_loss improved from 1.69058 to 1.67051, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.5426 - val_loss: 1.6705\n",
      "Epoch 31/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 3.5557\n",
      "Epoch 00031: val_loss improved from 1.67051 to 1.65799, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.5293 - val_loss: 1.6580\n",
      "Epoch 32/100\n",
      "47/69 [===================>..........] - ETA: 0s - loss: 3.4921\n",
      "Epoch 00032: val_loss improved from 1.65799 to 1.64055, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.4955 - val_loss: 1.6405\n",
      "Epoch 33/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 3.4698\n",
      "Epoch 00033: val_loss improved from 1.64055 to 1.61766, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.4167 - val_loss: 1.6177\n",
      "Epoch 34/100\n",
      "47/69 [===================>..........] - ETA: 0s - loss: 3.4393\n",
      "Epoch 00034: val_loss improved from 1.61766 to 1.60650, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.4130 - val_loss: 1.6065\n",
      "Epoch 35/100\n",
      "48/69 [===================>..........] - ETA: 0s - loss: 3.3100\n",
      "Epoch 00035: val_loss improved from 1.60650 to 1.60476, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.3518 - val_loss: 1.6048\n",
      "Epoch 36/100\n",
      "47/69 [===================>..........] - ETA: 0s - loss: 3.3869\n",
      "Epoch 00036: val_loss improved from 1.60476 to 1.59035, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.3586 - val_loss: 1.5903\n",
      "Epoch 37/100\n",
      "48/69 [===================>..........] - ETA: 0s - loss: 3.2347\n",
      "Epoch 00037: val_loss improved from 1.59035 to 1.56891, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.2522 - val_loss: 1.5689\n",
      "Epoch 38/100\n",
      "47/69 [===================>..........] - ETA: 0s - loss: 3.2831\n",
      "Epoch 00038: val_loss improved from 1.56891 to 1.55842, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.2940 - val_loss: 1.5584\n",
      "Epoch 39/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 3.2636\n",
      "Epoch 00039: val_loss improved from 1.55842 to 1.54273, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.2630 - val_loss: 1.5427\n",
      "Epoch 40/100\n",
      "48/69 [===================>..........] - ETA: 0s - loss: 3.3189\n",
      "Epoch 00040: val_loss improved from 1.54273 to 1.53639, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.2458 - val_loss: 1.5364\n",
      "Epoch 41/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 3.2175\n",
      "Epoch 00041: val_loss improved from 1.53639 to 1.51906, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.2203 - val_loss: 1.5191\n",
      "Epoch 42/100\n",
      "47/69 [===================>..........] - ETA: 0s - loss: 3.1830\n",
      "Epoch 00042: val_loss improved from 1.51906 to 1.51465, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.1632 - val_loss: 1.5146\n",
      "Epoch 43/100\n",
      "47/69 [===================>..........] - ETA: 0s - loss: 3.1169\n",
      "Epoch 00043: val_loss improved from 1.51465 to 1.50901, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.1466 - val_loss: 1.5090\n",
      "Epoch 44/100\n",
      "48/69 [===================>..........] - ETA: 0s - loss: 3.1099\n",
      "Epoch 00044: val_loss improved from 1.50901 to 1.50199, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.1066 - val_loss: 1.5020\n",
      "Epoch 45/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 3.1521\n",
      "Epoch 00045: val_loss improved from 1.50199 to 1.48753, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.1222 - val_loss: 1.4875\n",
      "Epoch 46/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 3.0114\n",
      "Epoch 00046: val_loss improved from 1.48753 to 1.48108, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.0644 - val_loss: 1.4811\n",
      "Epoch 47/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 3.0963\n",
      "Epoch 00047: val_loss improved from 1.48108 to 1.47038, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.0625 - val_loss: 1.4704\n",
      "Epoch 48/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 3.0509\n",
      "Epoch 00048: val_loss improved from 1.47038 to 1.45281, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.1094 - val_loss: 1.4528\n",
      "Epoch 49/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 3.0492\n",
      "Epoch 00049: val_loss did not improve from 1.45281\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.0276 - val_loss: 1.4557\n",
      "Epoch 50/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 2.9705\n",
      "Epoch 00050: val_loss improved from 1.45281 to 1.44247, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.0078 - val_loss: 1.4425\n",
      "Epoch 51/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 2.9200\n",
      "Epoch 00051: val_loss did not improve from 1.44247\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.9330 - val_loss: 1.4438\n",
      "Epoch 52/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 2.9531\n",
      "Epoch 00052: val_loss improved from 1.44247 to 1.42925, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.9652 - val_loss: 1.4292\n",
      "Epoch 53/100\n",
      "48/69 [===================>..........] - ETA: 0s - loss: 2.9228\n",
      "Epoch 00053: val_loss improved from 1.42925 to 1.42732, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.9378 - val_loss: 1.4273\n",
      "Epoch 54/100\n",
      "47/69 [===================>..........] - ETA: 0s - loss: 2.8876\n",
      "Epoch 00054: val_loss improved from 1.42732 to 1.42166, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.9035 - val_loss: 1.4217\n",
      "Epoch 55/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 2.8897\n",
      "Epoch 00055: val_loss did not improve from 1.42166\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.8995 - val_loss: 1.4266\n",
      "Epoch 56/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 2.7838\n",
      "Epoch 00056: val_loss improved from 1.42166 to 1.40762, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.8362 - val_loss: 1.4076\n",
      "Epoch 57/100\n",
      "48/69 [===================>..........] - ETA: 0s - loss: 2.8460\n",
      "Epoch 00057: val_loss improved from 1.40762 to 1.40563, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.8225 - val_loss: 1.4056\n",
      "Epoch 58/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 2.7955\n",
      "Epoch 00058: val_loss improved from 1.40563 to 1.39097, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.8194 - val_loss: 1.3910\n",
      "Epoch 59/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 2.8465\n",
      "Epoch 00059: val_loss did not improve from 1.39097\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.8219 - val_loss: 1.4012\n",
      "Epoch 60/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 2.8336\n",
      "Epoch 00060: val_loss improved from 1.39097 to 1.38225, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.8379 - val_loss: 1.3822\n",
      "Epoch 61/100\n",
      "48/69 [===================>..........] - ETA: 0s - loss: 2.7907\n",
      "Epoch 00061: val_loss improved from 1.38225 to 1.37999, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.8241 - val_loss: 1.3800\n",
      "Epoch 62/100\n",
      "46/69 [===================>..........] - ETA: 0s - loss: 2.8132\n",
      "Epoch 00062: val_loss did not improve from 1.37999\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.7907 - val_loss: 1.3872\n",
      "Epoch 63/100\n",
      "48/69 [===================>..........] - ETA: 0s - loss: 2.7400\n",
      "Epoch 00063: val_loss improved from 1.37999 to 1.37302, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.7558 - val_loss: 1.3730\n",
      "Epoch 64/100\n",
      "48/69 [===================>..........] - ETA: 0s - loss: 2.7504\n",
      "Epoch 00064: val_loss did not improve from 1.37302\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.7538 - val_loss: 1.3828\n",
      "Epoch 65/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 2.8396\n",
      "Epoch 00065: val_loss improved from 1.37302 to 1.36348, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.7890 - val_loss: 1.3635\n",
      "Epoch 66/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 2.7357\n",
      "Epoch 00066: val_loss did not improve from 1.36348\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.7540 - val_loss: 1.3657\n",
      "Epoch 67/100\n",
      "48/69 [===================>..........] - ETA: 0s - loss: 2.6835\n",
      "Epoch 00067: val_loss improved from 1.36348 to 1.35563, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.6926 - val_loss: 1.3556\n",
      "Epoch 68/100\n",
      "49/69 [====================>.........] - ETA: 0s - loss: 2.7838\n",
      "Epoch 00068: val_loss improved from 1.35563 to 1.35403, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.7566 - val_loss: 1.3540\n",
      "Epoch 69/100\n",
      "47/69 [===================>..........] - ETA: 0s - loss: 2.6782\n",
      "Epoch 00069: val_loss did not improve from 1.35403\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.7157 - val_loss: 1.3569\n",
      "Epoch 70/100\n",
      "50/69 [====================>.........] - ETA: 0s - loss: 2.7330\n",
      "Epoch 00070: val_loss did not improve from 1.35403\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.6872 - val_loss: 1.3613\n",
      " ###9 fold : val mae 0.813, mse 1.252###\n",
      "mae1.383_mse0.680\n",
      "random search 2/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/272 [============================>.] - ETA: 0s - loss: 31.2689\n",
      "Epoch 00001: val_loss improved from inf to 2.77390, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_0.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 30.7439 - val_loss: 2.7739\n",
      "Epoch 2/100\n",
      "256/272 [===========================>..] - ETA: 0s - loss: 4.5268\n",
      "Epoch 00002: val_loss improved from 2.77390 to 1.97635, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_0.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 4.4463 - val_loss: 1.9764\n",
      "Epoch 3/100\n",
      "254/272 [===========================>..] - ETA: 0s - loss: 2.9905\n",
      "Epoch 00003: val_loss did not improve from 1.97635\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.9696 - val_loss: 2.0764\n",
      "Epoch 4/100\n",
      "255/272 [===========================>..] - ETA: 0s - loss: 2.7435\n",
      "Epoch 00004: val_loss did not improve from 1.97635\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.7488 - val_loss: 2.4787\n",
      " ###0 fold : val mae 1.027, mse 1.845###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/272 [==========================>...] - ETA: 0s - loss: 32.7226\n",
      "Epoch 00001: val_loss improved from inf to 2.71131, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_1.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 30.5835 - val_loss: 2.7113\n",
      "Epoch 2/100\n",
      "268/272 [============================>.] - ETA: 0s - loss: 4.4615\n",
      "Epoch 00002: val_loss improved from 2.71131 to 1.90592, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_1.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 4.4404 - val_loss: 1.9059\n",
      "Epoch 3/100\n",
      "259/272 [===========================>..] - ETA: 0s - loss: 2.9532\n",
      "Epoch 00003: val_loss did not improve from 1.90592\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.9455 - val_loss: 2.0793\n",
      "Epoch 4/100\n",
      "260/272 [===========================>..] - ETA: 0s - loss: 2.7085\n",
      "Epoch 00004: val_loss did not improve from 1.90592\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.7148 - val_loss: 2.4348\n",
      " ###1 fold : val mae 1.039, mse 1.966###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255/272 [===========================>..] - ETA: 0s - loss: 32.0536\n",
      "Epoch 00001: val_loss improved from inf to 2.65765, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_2.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 30.4861 - val_loss: 2.6577\n",
      "Epoch 2/100\n",
      "252/272 [==========================>...] - ETA: 0s - loss: 4.5093\n",
      "Epoch 00002: val_loss improved from 2.65765 to 1.90872, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_2.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 4.4134 - val_loss: 1.9087\n",
      "Epoch 3/100\n",
      "254/272 [===========================>..] - ETA: 0s - loss: 2.9410\n",
      "Epoch 00003: val_loss did not improve from 1.90872\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.9353 - val_loss: 2.1894\n",
      "Epoch 4/100\n",
      "253/272 [==========================>...] - ETA: 0s - loss: 2.7275\n",
      "Epoch 00004: val_loss did not improve from 1.90872\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.7300 - val_loss: 2.1256\n",
      " ###2 fold : val mae 1.050, mse 1.919###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/272 [==========================>...] - ETA: 0s - loss: 32.6316\n",
      "Epoch 00001: val_loss improved from inf to 2.57803, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_3.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 30.5812 - val_loss: 2.5780\n",
      "Epoch 2/100\n",
      "267/272 [============================>.] - ETA: 0s - loss: 4.4529\n",
      "Epoch 00002: val_loss improved from 2.57803 to 1.91360, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_3.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 4.4193 - val_loss: 1.9136\n",
      "Epoch 3/100\n",
      "263/272 [============================>.] - ETA: 0s - loss: 2.9427\n",
      "Epoch 00003: val_loss did not improve from 1.91360\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.9321 - val_loss: 2.1317\n",
      "Epoch 4/100\n",
      "264/272 [============================>.] - ETA: 0s - loss: 2.7314\n",
      "Epoch 00004: val_loss did not improve from 1.91360\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.7369 - val_loss: 2.1167\n",
      " ###3 fold : val mae 1.034, mse 1.882###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/272 [============================>.] - ETA: 0s - loss: 30.9696\n",
      "Epoch 00001: val_loss improved from inf to 2.71779, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_4.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 30.6243 - val_loss: 2.7178\n",
      "Epoch 2/100\n",
      "264/272 [============================>.] - ETA: 0s - loss: 4.5007\n",
      "Epoch 00002: val_loss improved from 2.71779 to 1.87695, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_4.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 4.4580 - val_loss: 1.8770\n",
      "Epoch 3/100\n",
      "258/272 [===========================>..] - ETA: 0s - loss: 2.9530\n",
      "Epoch 00003: val_loss did not improve from 1.87695\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.9483 - val_loss: 1.9551\n",
      "Epoch 4/100\n",
      "254/272 [===========================>..] - ETA: 0s - loss: 2.7243\n",
      "Epoch 00004: val_loss did not improve from 1.87695\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.7376 - val_loss: 1.9708\n",
      " ###4 fold : val mae 1.030, mse 1.868###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255/272 [===========================>..] - ETA: 0s - loss: 32.2854\n",
      "Epoch 00001: val_loss improved from inf to 2.79925, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_5.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 30.7273 - val_loss: 2.7993\n",
      "Epoch 2/100\n",
      "254/272 [===========================>..] - ETA: 0s - loss: 4.6474\n",
      "Epoch 00002: val_loss improved from 2.79925 to 1.78652, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_5.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 4.5485 - val_loss: 1.7865\n",
      "Epoch 3/100\n",
      "254/272 [===========================>..] - ETA: 0s - loss: 2.9508\n",
      "Epoch 00003: val_loss did not improve from 1.78652\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.9430 - val_loss: 1.9960\n",
      "Epoch 4/100\n",
      "254/272 [===========================>..] - ETA: 0s - loss: 2.7347\n",
      "Epoch 00004: val_loss did not improve from 1.78652\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.7416 - val_loss: 1.9358\n",
      " ###5 fold : val mae 0.988, mse 1.699###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255/272 [===========================>..] - ETA: 0s - loss: 32.1478\n",
      "Epoch 00001: val_loss improved from inf to 2.71950, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_6.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 30.6022 - val_loss: 2.7195\n",
      "Epoch 2/100\n",
      "254/272 [===========================>..] - ETA: 0s - loss: 4.6329\n",
      "Epoch 00002: val_loss improved from 2.71950 to 1.82536, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_6.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 4.5389 - val_loss: 1.8254\n",
      "Epoch 3/100\n",
      "254/272 [===========================>..] - ETA: 0s - loss: 2.9717\n",
      "Epoch 00003: val_loss did not improve from 1.82536\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.9597 - val_loss: 1.9354\n",
      "Epoch 4/100\n",
      "254/272 [===========================>..] - ETA: 0s - loss: 2.7728\n",
      "Epoch 00004: val_loss did not improve from 1.82536\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.7701 - val_loss: 1.9547\n",
      " ###6 fold : val mae 0.997, mse 1.721###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/272 [===========================>..] - ETA: 0s - loss: 31.8359\n",
      "Epoch 00001: val_loss improved from inf to 2.76628, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_7.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 30.4853 - val_loss: 2.7663\n",
      "Epoch 2/100\n",
      "266/272 [============================>.] - ETA: 0s - loss: 4.6561\n",
      "Epoch 00002: val_loss improved from 2.76628 to 1.78369, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_7.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 4.6156 - val_loss: 1.7837\n",
      "Epoch 3/100\n",
      "256/272 [===========================>..] - ETA: 0s - loss: 2.9671\n",
      "Epoch 00003: val_loss did not improve from 1.78369\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.9612 - val_loss: 1.9227\n",
      "Epoch 4/100\n",
      "254/272 [===========================>..] - ETA: 0s - loss: 2.7643\n",
      "Epoch 00004: val_loss did not improve from 1.78369\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.7664 - val_loss: 1.9468\n",
      " ###7 fold : val mae 1.003, mse 1.752###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/272 [===========================>..] - ETA: 0s - loss: 32.1633\n",
      "Epoch 00001: val_loss improved from inf to 2.54980, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_8.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 30.5249 - val_loss: 2.5498\n",
      "Epoch 2/100\n",
      "265/272 [============================>.] - ETA: 0s - loss: 4.6772\n",
      "Epoch 00002: val_loss improved from 2.54980 to 1.69956, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_8.hdf5\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 4.6292 - val_loss: 1.6996\n",
      "Epoch 3/100\n",
      "254/272 [===========================>..] - ETA: 0s - loss: 2.9750\n",
      "Epoch 00003: val_loss did not improve from 1.69956\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.9593 - val_loss: 1.8135\n",
      "Epoch 4/100\n",
      "254/272 [===========================>..] - ETA: 0s - loss: 2.7552\n",
      "Epoch 00004: val_loss did not improve from 1.69956\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 2.7586 - val_loss: 1.9520\n",
      " ###8 fold : val mae 1.010, mse 1.853###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/273 [============================>.] - ETA: 0s - loss: 30.2926\n",
      "Epoch 00001: val_loss improved from inf to 2.54811, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_9.hdf5\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 30.2910 - val_loss: 2.5481\n",
      "Epoch 2/100\n",
      "272/273 [============================>.] - ETA: 0s - loss: 4.8445\n",
      "Epoch 00002: val_loss improved from 2.54811 to 2.02648, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_9.hdf5\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 4.8443 - val_loss: 2.0265\n",
      "Epoch 3/100\n",
      "271/273 [============================>.] - ETA: 0s - loss: 2.9444\n",
      "Epoch 00003: val_loss did not improve from 2.02648\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 2.9424 - val_loss: 2.1179\n",
      "Epoch 4/100\n",
      "272/273 [============================>.] - ETA: 0s - loss: 2.7279\n",
      "Epoch 00004: val_loss improved from 2.02648 to 1.93390, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_9.hdf5\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 2.7281 - val_loss: 1.9339\n",
      "Epoch 5/100\n",
      "257/273 [===========================>..] - ETA: 0s - loss: 2.6104\n",
      "Epoch 00005: val_loss did not improve from 1.93390\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 2.6007 - val_loss: 2.3610\n",
      "Epoch 6/100\n",
      "272/273 [============================>.] - ETA: 0s - loss: 2.5258\n",
      "Epoch 00006: val_loss improved from 1.93390 to 1.78328, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_9.hdf5\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 2.5258 - val_loss: 1.7833\n",
      "Epoch 7/100\n",
      "273/273 [==============================] - ETA: 0s - loss: 2.3787\n",
      "Epoch 00007: val_loss did not improve from 1.78328\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 2.3787 - val_loss: 2.3676\n",
      "Epoch 8/100\n",
      "258/273 [===========================>..] - ETA: 0s - loss: 2.2757\n",
      "Epoch 00008: val_loss did not improve from 1.78328\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 2.2788 - val_loss: 2.5003\n",
      " ###9 fold : val mae 0.979, mse 1.677###\n",
      "mae1.818_mse0.680\n",
      "random search 3/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/34 [=====================>........] - ETA: 0s - loss: 174.2106 \n",
      "Epoch 00001: val_loss improved from inf to 149.17841, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 1s 7ms/step - loss: 169.0621 - val_loss: 149.1784\n",
      "Epoch 2/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 130.0371\n",
      "Epoch 00002: val_loss improved from 149.17841 to 89.68479, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 121.9366 - val_loss: 89.6848\n",
      "Epoch 3/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 73.6380\n",
      "Epoch 00003: val_loss improved from 89.68479 to 39.51245, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 67.5925 - val_loss: 39.5125\n",
      "Epoch 4/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 41.7189\n",
      "Epoch 00004: val_loss improved from 39.51245 to 22.86900, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 40.4805 - val_loss: 22.8690\n",
      "Epoch 5/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 32.3154\n",
      "Epoch 00005: val_loss improved from 22.86900 to 15.76367, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 31.5898 - val_loss: 15.7637\n",
      "Epoch 6/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 26.9794\n",
      "Epoch 00006: val_loss improved from 15.76367 to 10.51737, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 26.0479 - val_loss: 10.5174\n",
      "Epoch 7/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 23.2469\n",
      "Epoch 00007: val_loss improved from 10.51737 to 7.14611, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 22.5728 - val_loss: 7.1461\n",
      "Epoch 8/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 19.3273\n",
      "Epoch 00008: val_loss improved from 7.14611 to 5.23074, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 19.1193 - val_loss: 5.2307\n",
      "Epoch 9/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 17.6518\n",
      "Epoch 00009: val_loss improved from 5.23074 to 4.04467, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 17.6616 - val_loss: 4.0447\n",
      "Epoch 10/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 16.5096\n",
      "Epoch 00010: val_loss improved from 4.04467 to 3.37354, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 16.5776 - val_loss: 3.3735\n",
      "Epoch 11/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 16.2784\n",
      "Epoch 00011: val_loss improved from 3.37354 to 3.18227, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 16.0835 - val_loss: 3.1823\n",
      "Epoch 12/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 15.5669\n",
      "Epoch 00012: val_loss improved from 3.18227 to 2.90393, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.5864 - val_loss: 2.9039\n",
      "Epoch 13/100\n",
      "23/34 [===================>..........] - ETA: 0s - loss: 15.4050\n",
      "Epoch 00013: val_loss improved from 2.90393 to 2.83849, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.4818 - val_loss: 2.8385\n",
      "Epoch 14/100\n",
      "26/34 [=====================>........] - ETA: 0s - loss: 15.1031\n",
      "Epoch 00014: val_loss did not improve from 2.83849\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 15.1114 - val_loss: 2.8856\n",
      "Epoch 15/100\n",
      "23/34 [===================>..........] - ETA: 0s - loss: 14.9395\n",
      "Epoch 00015: val_loss improved from 2.83849 to 2.67412, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.9184 - val_loss: 2.6741\n",
      "Epoch 16/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.8387\n",
      "Epoch 00016: val_loss improved from 2.67412 to 2.61364, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.5889 - val_loss: 2.6136\n",
      "Epoch 17/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.1238\n",
      "Epoch 00017: val_loss improved from 2.61364 to 2.57029, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.2217 - val_loss: 2.5703\n",
      "Epoch 18/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.2074\n",
      "Epoch 00018: val_loss did not improve from 2.57029\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.3082 - val_loss: 2.5813\n",
      "Epoch 19/100\n",
      "26/34 [=====================>........] - ETA: 0s - loss: 14.2266\n",
      "Epoch 00019: val_loss improved from 2.57029 to 2.47931, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.2178 - val_loss: 2.4793\n",
      "Epoch 20/100\n",
      "26/34 [=====================>........] - ETA: 0s - loss: 14.0591\n",
      "Epoch 00020: val_loss improved from 2.47931 to 2.29816, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.0964 - val_loss: 2.2982\n",
      "Epoch 21/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.0536\n",
      "Epoch 00021: val_loss did not improve from 2.29816\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 13.8625 - val_loss: 2.4258\n",
      "Epoch 22/100\n",
      "23/34 [===================>..........] - ETA: 0s - loss: 13.9321\n",
      "Epoch 00022: val_loss did not improve from 2.29816\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 13.8752 - val_loss: 2.4127\n",
      " ###0 fold : val mae 1.131, mse 2.209###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/34 [=====================>........] - ETA: 0s - loss: 173.5971 \n",
      "Epoch 00001: val_loss improved from inf to 149.15747, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 1s 7ms/step - loss: 168.4942 - val_loss: 149.1575\n",
      "Epoch 2/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 129.0501\n",
      "Epoch 00002: val_loss improved from 149.15747 to 89.67062, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 121.5751 - val_loss: 89.6706\n",
      "Epoch 3/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 73.3227\n",
      "Epoch 00003: val_loss improved from 89.67062 to 39.42171, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 67.3282 - val_loss: 39.4217\n",
      "Epoch 4/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 41.7401\n",
      "Epoch 00004: val_loss improved from 39.42171 to 22.85277, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 40.4143 - val_loss: 22.8528\n",
      "Epoch 5/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 32.2312\n",
      "Epoch 00005: val_loss improved from 22.85277 to 15.68616, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 31.5921 - val_loss: 15.6862\n",
      "Epoch 6/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 26.8822\n",
      "Epoch 00006: val_loss improved from 15.68616 to 10.44386, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 25.9279 - val_loss: 10.4439\n",
      "Epoch 7/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 23.0062\n",
      "Epoch 00007: val_loss improved from 10.44386 to 7.16028, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 22.6596 - val_loss: 7.1603\n",
      "Epoch 8/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 19.4490\n",
      "Epoch 00008: val_loss improved from 7.16028 to 5.22590, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 19.1302 - val_loss: 5.2259\n",
      "Epoch 9/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 17.6429\n",
      "Epoch 00009: val_loss improved from 5.22590 to 4.12715, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 17.7492 - val_loss: 4.1271\n",
      "Epoch 10/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 16.3620\n",
      "Epoch 00010: val_loss improved from 4.12715 to 3.38756, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 16.4796 - val_loss: 3.3876\n",
      "Epoch 11/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 16.1747\n",
      "Epoch 00011: val_loss improved from 3.38756 to 3.16789, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 16.0215 - val_loss: 3.1679\n",
      "Epoch 12/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 15.4468\n",
      "Epoch 00012: val_loss improved from 3.16789 to 2.94378, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.4367 - val_loss: 2.9438\n",
      "Epoch 13/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 15.3501\n",
      "Epoch 00013: val_loss improved from 2.94378 to 2.84692, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.4324 - val_loss: 2.8469\n",
      "Epoch 14/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.9246\n",
      "Epoch 00014: val_loss did not improve from 2.84692\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.9303 - val_loss: 2.8735\n",
      "Epoch 15/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.9541\n",
      "Epoch 00015: val_loss improved from 2.84692 to 2.70384, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.9301 - val_loss: 2.7038\n",
      "Epoch 16/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.6546\n",
      "Epoch 00016: val_loss improved from 2.70384 to 2.60151, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.4314 - val_loss: 2.6015\n",
      "Epoch 17/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.0091\n",
      "Epoch 00017: val_loss improved from 2.60151 to 2.54888, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.1436 - val_loss: 2.5489\n",
      "Epoch 18/100\n",
      "28/34 [=======================>......] - ETA: 0s - loss: 14.1232\n",
      "Epoch 00018: val_loss did not improve from 2.54888\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 14.2403 - val_loss: 2.5840\n",
      "Epoch 19/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.3169\n",
      "Epoch 00019: val_loss improved from 2.54888 to 2.49590, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.2069 - val_loss: 2.4959\n",
      "Epoch 20/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.0128\n",
      "Epoch 00020: val_loss improved from 2.49590 to 2.32400, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.0117 - val_loss: 2.3240\n",
      "Epoch 21/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.0070\n",
      "Epoch 00021: val_loss did not improve from 2.32400\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 13.8308 - val_loss: 2.4193\n",
      "Epoch 22/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 13.8411\n",
      "Epoch 00022: val_loss did not improve from 2.32400\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 13.7834 - val_loss: 2.4066\n",
      " ###1 fold : val mae 1.174, mse 2.508###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/34 [====================>.........] - ETA: 0s - loss: 174.6173 \n",
      "Epoch 00001: val_loss improved from inf to 149.22466, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 1s 7ms/step - loss: 168.8926 - val_loss: 149.2247\n",
      "Epoch 2/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 129.4604\n",
      "Epoch 00002: val_loss improved from 149.22466 to 89.83922, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 121.9355 - val_loss: 89.8392\n",
      "Epoch 3/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 73.4479\n",
      "Epoch 00003: val_loss improved from 89.83922 to 39.31335, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 67.3233 - val_loss: 39.3134\n",
      "Epoch 4/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 41.6252\n",
      "Epoch 00004: val_loss improved from 39.31335 to 22.81185, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 40.1126 - val_loss: 22.8119\n",
      "Epoch 5/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 32.1911\n",
      "Epoch 00005: val_loss improved from 22.81185 to 15.73726, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 31.4776 - val_loss: 15.7373\n",
      "Epoch 6/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 26.8031\n",
      "Epoch 00006: val_loss improved from 15.73726 to 10.62071, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 26.0078 - val_loss: 10.6207\n",
      "Epoch 7/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 22.7341\n",
      "Epoch 00007: val_loss improved from 10.62071 to 7.23580, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 22.5576 - val_loss: 7.2358\n",
      "Epoch 8/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 19.4211\n",
      "Epoch 00008: val_loss improved from 7.23580 to 5.34503, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 19.1969 - val_loss: 5.3450\n",
      "Epoch 9/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 18.0349\n",
      "Epoch 00009: val_loss improved from 5.34503 to 4.19221, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 17.8645 - val_loss: 4.1922\n",
      "Epoch 10/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 16.4272\n",
      "Epoch 00010: val_loss improved from 4.19221 to 3.43356, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 16.5032 - val_loss: 3.4336\n",
      "Epoch 11/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 16.1012\n",
      "Epoch 00011: val_loss improved from 3.43356 to 3.14823, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.9657 - val_loss: 3.1482\n",
      "Epoch 12/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 15.5656\n",
      "Epoch 00012: val_loss improved from 3.14823 to 2.98036, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.5757 - val_loss: 2.9804\n",
      "Epoch 13/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 15.4248\n",
      "Epoch 00013: val_loss improved from 2.98036 to 2.82725, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.4885 - val_loss: 2.8272\n",
      "Epoch 14/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.8915\n",
      "Epoch 00014: val_loss improved from 2.82725 to 2.81869, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.8602 - val_loss: 2.8187\n",
      "Epoch 15/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 15.0556\n",
      "Epoch 00015: val_loss improved from 2.81869 to 2.74096, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.0473 - val_loss: 2.7410\n",
      "Epoch 16/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.7658\n",
      "Epoch 00016: val_loss improved from 2.74096 to 2.54583, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.5161 - val_loss: 2.5458\n",
      "Epoch 17/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.0114\n",
      "Epoch 00017: val_loss did not improve from 2.54583\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 14.1551 - val_loss: 2.5883\n",
      "Epoch 18/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.2485\n",
      "Epoch 00018: val_loss did not improve from 2.54583\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 14.3532 - val_loss: 2.5636\n",
      " ###2 fold : val mae 1.226, mse 2.515###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/34 [====================>.........] - ETA: 0s - loss: 174.9548 \n",
      "Epoch 00001: val_loss improved from inf to 149.25322, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 1s 7ms/step - loss: 169.1993 - val_loss: 149.2532\n",
      "Epoch 2/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 129.7301\n",
      "Epoch 00002: val_loss improved from 149.25322 to 89.78982, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 122.2125 - val_loss: 89.7898\n",
      "Epoch 3/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 73.7969\n",
      "Epoch 00003: val_loss improved from 89.78982 to 39.42245, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 67.5383 - val_loss: 39.4225\n",
      "Epoch 4/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 41.7908\n",
      "Epoch 00004: val_loss improved from 39.42245 to 22.88555, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 40.6084 - val_loss: 22.8855\n",
      "Epoch 5/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 32.1844\n",
      "Epoch 00005: val_loss improved from 22.88555 to 15.72077, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 31.4265 - val_loss: 15.7208\n",
      "Epoch 6/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 26.7946\n",
      "Epoch 00006: val_loss improved from 15.72077 to 10.60659, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 25.8994 - val_loss: 10.6066\n",
      "Epoch 7/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 23.1223\n",
      "Epoch 00007: val_loss improved from 10.60659 to 7.25521, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 22.5455 - val_loss: 7.2552\n",
      "Epoch 8/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 19.4058\n",
      "Epoch 00008: val_loss improved from 7.25521 to 5.33139, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 19.2046 - val_loss: 5.3314\n",
      "Epoch 9/100\n",
      "23/34 [===================>..........] - ETA: 0s - loss: 18.2381\n",
      "Epoch 00009: val_loss improved from 5.33139 to 4.17372, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 17.9376 - val_loss: 4.1737\n",
      "Epoch 10/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 16.4659\n",
      "Epoch 00010: val_loss improved from 4.17372 to 3.43452, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 16.4979 - val_loss: 3.4345\n",
      "Epoch 11/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 16.0056\n",
      "Epoch 00011: val_loss improved from 3.43452 to 3.15175, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.8964 - val_loss: 3.1518\n",
      "Epoch 12/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 15.7499\n",
      "Epoch 00012: val_loss improved from 3.15175 to 2.95713, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.6735 - val_loss: 2.9571\n",
      "Epoch 13/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 15.4182\n",
      "Epoch 00013: val_loss improved from 2.95713 to 2.82075, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.4582 - val_loss: 2.8208\n",
      "Epoch 14/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.8373\n",
      "Epoch 00014: val_loss improved from 2.82075 to 2.75729, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.8236 - val_loss: 2.7573\n",
      "Epoch 15/100\n",
      "23/34 [===================>..........] - ETA: 0s - loss: 15.0966\n",
      "Epoch 00015: val_loss improved from 2.75729 to 2.75524, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.1012 - val_loss: 2.7552\n",
      "Epoch 16/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.7274\n",
      "Epoch 00016: val_loss improved from 2.75524 to 2.53267, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.4806 - val_loss: 2.5327\n",
      "Epoch 17/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 13.9481\n",
      "Epoch 00017: val_loss did not improve from 2.53267\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.1490 - val_loss: 2.5932\n",
      "Epoch 18/100\n",
      "23/34 [===================>..........] - ETA: 0s - loss: 14.3598\n",
      "Epoch 00018: val_loss did not improve from 2.53267\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.4462 - val_loss: 2.5521\n",
      " ###3 fold : val mae 1.207, mse 2.485###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/34 [=====================>........] - ETA: 0s - loss: 174.3388 \n",
      "Epoch 00001: val_loss improved from inf to 149.17052, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "34/34 [==============================] - 1s 7ms/step - loss: 169.1732 - val_loss: 149.1705\n",
      "Epoch 2/100\n",
      "26/34 [=====================>........] - ETA: 0s - loss: 128.9118\n",
      "Epoch 00002: val_loss improved from 149.17052 to 89.72990, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 122.1460 - val_loss: 89.7299\n",
      "Epoch 3/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 73.6081\n",
      "Epoch 00003: val_loss improved from 89.72990 to 39.14625, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 67.2923 - val_loss: 39.1462\n",
      "Epoch 4/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 41.4868\n",
      "Epoch 00004: val_loss improved from 39.14625 to 22.68973, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 40.2138 - val_loss: 22.6897\n",
      "Epoch 5/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 31.8099\n",
      "Epoch 00005: val_loss improved from 22.68973 to 15.69057, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 31.3687 - val_loss: 15.6906\n",
      "Epoch 6/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 26.3800\n",
      "Epoch 00006: val_loss improved from 15.69057 to 10.62998, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 25.8109 - val_loss: 10.6300\n",
      "Epoch 7/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 22.7756\n",
      "Epoch 00007: val_loss improved from 10.62998 to 7.22103, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 22.3639 - val_loss: 7.2210\n",
      "Epoch 8/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 19.3588\n",
      "Epoch 00008: val_loss improved from 7.22103 to 5.33392, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 19.0831 - val_loss: 5.3339\n",
      "Epoch 9/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 18.1795\n",
      "Epoch 00009: val_loss improved from 5.33392 to 4.18950, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 17.8766 - val_loss: 4.1895\n",
      "Epoch 10/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 16.4696\n",
      "Epoch 00010: val_loss improved from 4.18950 to 3.41526, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 16.4853 - val_loss: 3.4153\n",
      "Epoch 11/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 15.8708\n",
      "Epoch 00011: val_loss improved from 3.41526 to 3.16269, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.8488 - val_loss: 3.1627\n",
      "Epoch 12/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 15.7035\n",
      "Epoch 00012: val_loss improved from 3.16269 to 2.94369, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.6631 - val_loss: 2.9437\n",
      "Epoch 13/100\n",
      "26/34 [=====================>........] - ETA: 0s - loss: 15.3507\n",
      "Epoch 00013: val_loss improved from 2.94369 to 2.84194, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.3750 - val_loss: 2.8419\n",
      "Epoch 14/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.8619\n",
      "Epoch 00014: val_loss improved from 2.84194 to 2.74340, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.8121 - val_loss: 2.7434\n",
      "Epoch 15/100\n",
      "26/34 [=====================>........] - ETA: 0s - loss: 15.1343\n",
      "Epoch 00015: val_loss did not improve from 2.74340\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 15.1621 - val_loss: 2.8064\n",
      "Epoch 16/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.7306\n",
      "Epoch 00016: val_loss improved from 2.74340 to 2.48775, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.5070 - val_loss: 2.4878\n",
      "Epoch 17/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.0389\n",
      "Epoch 00017: val_loss did not improve from 2.48775\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 14.2340 - val_loss: 2.6554\n",
      "Epoch 18/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.3615\n",
      "Epoch 00018: val_loss did not improve from 2.48775\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 14.4308 - val_loss: 2.5922\n",
      " ###4 fold : val mae 1.206, mse 2.516###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/34 [=====================>........] - ETA: 0s - loss: 174.3244 \n",
      "Epoch 00001: val_loss improved from inf to 149.19998, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "34/34 [==============================] - 1s 7ms/step - loss: 169.2151 - val_loss: 149.2000\n",
      "Epoch 2/100\n",
      "27/34 [======================>.......] - ETA: 0s - loss: 127.9186\n",
      "Epoch 00002: val_loss improved from 149.19998 to 89.73583, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 122.1047 - val_loss: 89.7358\n",
      "Epoch 3/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 73.2442\n",
      "Epoch 00003: val_loss improved from 89.73583 to 39.22998, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 67.4195 - val_loss: 39.2300\n",
      "Epoch 4/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 41.3362\n",
      "Epoch 00004: val_loss improved from 39.22998 to 22.69511, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 40.2564 - val_loss: 22.6951\n",
      "Epoch 5/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 32.0355\n",
      "Epoch 00005: val_loss improved from 22.69511 to 15.69287, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 31.5187 - val_loss: 15.6929\n",
      "Epoch 6/100\n",
      "22/34 [==================>...........] - ETA: 0s - loss: 26.6738\n",
      "Epoch 00006: val_loss improved from 15.69287 to 10.59023, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 25.7608 - val_loss: 10.5902\n",
      "Epoch 7/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 22.8228\n",
      "Epoch 00007: val_loss improved from 10.59023 to 7.24726, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 22.4194 - val_loss: 7.2473\n",
      "Epoch 8/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 19.3217\n",
      "Epoch 00008: val_loss improved from 7.24726 to 5.28037, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 19.0648 - val_loss: 5.2804\n",
      "Epoch 9/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 18.2968\n",
      "Epoch 00009: val_loss improved from 5.28037 to 4.20647, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 17.9588 - val_loss: 4.2065\n",
      "Epoch 10/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 16.4935\n",
      "Epoch 00010: val_loss improved from 4.20647 to 3.36399, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 16.4339 - val_loss: 3.3640\n",
      "Epoch 11/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 15.8723\n",
      "Epoch 00011: val_loss improved from 3.36399 to 3.16205, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.8646 - val_loss: 3.1621\n",
      "Epoch 12/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 15.7409\n",
      "Epoch 00012: val_loss improved from 3.16205 to 2.92260, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.6826 - val_loss: 2.9226\n",
      "Epoch 13/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 15.2717\n",
      "Epoch 00013: val_loss improved from 2.92260 to 2.83918, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.3919 - val_loss: 2.8392\n",
      "Epoch 14/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.7950\n",
      "Epoch 00014: val_loss improved from 2.83918 to 2.71964, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.7804 - val_loss: 2.7196\n",
      "Epoch 15/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 15.0344\n",
      "Epoch 00015: val_loss did not improve from 2.71964\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 15.0441 - val_loss: 2.7692\n",
      "Epoch 16/100\n",
      "23/34 [===================>..........] - ETA: 0s - loss: 14.7761\n",
      "Epoch 00016: val_loss improved from 2.71964 to 2.47278, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.5291 - val_loss: 2.4728\n",
      "Epoch 17/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.0827\n",
      "Epoch 00017: val_loss did not improve from 2.47278\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 14.2352 - val_loss: 2.6230\n",
      "Epoch 18/100\n",
      "23/34 [===================>..........] - ETA: 0s - loss: 14.3916\n",
      "Epoch 00018: val_loss did not improve from 2.47278\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.3973 - val_loss: 2.5638\n",
      " ###5 fold : val mae 1.201, mse 2.477###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/34 [===================>..........] - ETA: 0s - loss: 175.0381 \n",
      "Epoch 00001: val_loss improved from inf to 149.14636, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 1s 8ms/step - loss: 168.8349 - val_loss: 149.1464\n",
      "Epoch 2/100\n",
      "23/34 [===================>..........] - ETA: 0s - loss: 130.7152\n",
      "Epoch 00002: val_loss improved from 149.14636 to 89.69541, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 121.7352 - val_loss: 89.6954\n",
      "Epoch 3/100\n",
      "23/34 [===================>..........] - ETA: 0s - loss: 74.4767\n",
      "Epoch 00003: val_loss improved from 89.69541 to 39.51619, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 67.3748 - val_loss: 39.5162\n",
      "Epoch 4/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 41.8170\n",
      "Epoch 00004: val_loss improved from 39.51619 to 22.83571, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 40.6062 - val_loss: 22.8357\n",
      "Epoch 5/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 32.4102\n",
      "Epoch 00005: val_loss improved from 22.83571 to 15.71323, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 31.6784 - val_loss: 15.7132\n",
      "Epoch 6/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 26.3050\n",
      "Epoch 00006: val_loss improved from 15.71323 to 10.54801, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 25.6089 - val_loss: 10.5480\n",
      "Epoch 7/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 22.8726\n",
      "Epoch 00007: val_loss improved from 10.54801 to 7.29726, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 22.4610 - val_loss: 7.2973\n",
      "Epoch 8/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 19.1766\n",
      "Epoch 00008: val_loss improved from 7.29726 to 5.27795, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 19.0006 - val_loss: 5.2779\n",
      "Epoch 9/100\n",
      "23/34 [===================>..........] - ETA: 0s - loss: 18.3858\n",
      "Epoch 00009: val_loss improved from 5.27795 to 4.28158, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 18.0148 - val_loss: 4.2816\n",
      "Epoch 10/100\n",
      "23/34 [===================>..........] - ETA: 0s - loss: 16.6814\n",
      "Epoch 00010: val_loss improved from 4.28158 to 3.41834, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 16.4739 - val_loss: 3.4183\n",
      "Epoch 11/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 16.0022\n",
      "Epoch 00011: val_loss improved from 3.41834 to 3.23775, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.9639 - val_loss: 3.2378\n",
      "Epoch 12/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 15.7374\n",
      "Epoch 00012: val_loss improved from 3.23775 to 2.93430, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.7145 - val_loss: 2.9343\n",
      "Epoch 13/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 15.2470\n",
      "Epoch 00013: val_loss improved from 2.93430 to 2.86892, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.4110 - val_loss: 2.8689\n",
      "Epoch 14/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.7062\n",
      "Epoch 00014: val_loss improved from 2.86892 to 2.72408, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.6872 - val_loss: 2.7241\n",
      "Epoch 15/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 15.0432\n",
      "Epoch 00015: val_loss improved from 2.72408 to 2.72262, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.0522 - val_loss: 2.7226\n",
      "Epoch 16/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.8088\n",
      "Epoch 00016: val_loss improved from 2.72262 to 2.47320, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.6344 - val_loss: 2.4732\n",
      "Epoch 17/100\n",
      "23/34 [===================>..........] - ETA: 0s - loss: 14.0390\n",
      "Epoch 00017: val_loss did not improve from 2.47320\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.3053 - val_loss: 2.5913\n",
      "Epoch 18/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.4472\n",
      "Epoch 00018: val_loss did not improve from 2.47320\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 14.4124 - val_loss: 2.6104\n",
      " ###6 fold : val mae 1.190, mse 2.342###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/34 [=====================>........] - ETA: 0s - loss: 173.3940 \n",
      "Epoch 00001: val_loss improved from inf to 149.18616, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 1s 7ms/step - loss: 168.8494 - val_loss: 149.1862\n",
      "Epoch 2/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 129.1657\n",
      "Epoch 00002: val_loss improved from 149.18616 to 89.72356, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 121.7670 - val_loss: 89.7236\n",
      "Epoch 3/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 74.0550\n",
      "Epoch 00003: val_loss improved from 89.72356 to 39.30354, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 67.5196 - val_loss: 39.3035\n",
      "Epoch 4/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 41.7735\n",
      "Epoch 00004: val_loss improved from 39.30354 to 22.74619, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 40.4207 - val_loss: 22.7462\n",
      "Epoch 5/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 32.1509\n",
      "Epoch 00005: val_loss improved from 22.74619 to 15.70076, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 31.4282 - val_loss: 15.7008\n",
      "Epoch 6/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 26.3587\n",
      "Epoch 00006: val_loss improved from 15.70076 to 10.59203, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 25.7097 - val_loss: 10.5920\n",
      "Epoch 7/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 22.8124\n",
      "Epoch 00007: val_loss improved from 10.59203 to 7.33857, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 22.3754 - val_loss: 7.3386\n",
      "Epoch 8/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 19.3804\n",
      "Epoch 00008: val_loss improved from 7.33857 to 5.19286, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 19.1285 - val_loss: 5.1929\n",
      "Epoch 9/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 18.3329\n",
      "Epoch 00009: val_loss improved from 5.19286 to 4.26010, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 18.0571 - val_loss: 4.2601\n",
      "Epoch 10/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 16.5064\n",
      "Epoch 00010: val_loss improved from 4.26010 to 3.37459, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 16.3760 - val_loss: 3.3746\n",
      "Epoch 11/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 15.9802\n",
      "Epoch 00011: val_loss improved from 3.37459 to 3.15908, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.9389 - val_loss: 3.1591\n",
      "Epoch 12/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 15.7099\n",
      "Epoch 00012: val_loss improved from 3.15908 to 2.97524, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.7111 - val_loss: 2.9752\n",
      "Epoch 13/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 15.1095\n",
      "Epoch 00013: val_loss improved from 2.97524 to 2.86308, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.2889 - val_loss: 2.8631\n",
      "Epoch 14/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.5993\n",
      "Epoch 00014: val_loss improved from 2.86308 to 2.73162, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.5982 - val_loss: 2.7316\n",
      "Epoch 15/100\n",
      "23/34 [===================>..........] - ETA: 0s - loss: 15.1056\n",
      "Epoch 00015: val_loss improved from 2.73162 to 2.68215, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.0662 - val_loss: 2.6821\n",
      "Epoch 16/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.8495\n",
      "Epoch 00016: val_loss improved from 2.68215 to 2.51503, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.6660 - val_loss: 2.5150\n",
      "Epoch 17/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.1034\n",
      "Epoch 00017: val_loss did not improve from 2.51503\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.3346 - val_loss: 2.5817\n",
      "Epoch 18/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.4290\n",
      "Epoch 00018: val_loss did not improve from 2.51503\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.4325 - val_loss: 2.6262\n",
      " ###7 fold : val mae 1.239, mse 2.593###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/34 [======================>.......] - ETA: 0s - loss: 173.2143 \n",
      "Epoch 00001: val_loss improved from inf to 148.56100, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 1s 7ms/step - loss: 169.0874 - val_loss: 148.5610\n",
      "Epoch 2/100\n",
      "27/34 [======================>.......] - ETA: 0s - loss: 127.8300\n",
      "Epoch 00002: val_loss improved from 148.56100 to 88.99036, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 121.9423 - val_loss: 88.9904\n",
      "Epoch 3/100\n",
      "26/34 [=====================>........] - ETA: 0s - loss: 72.9567\n",
      "Epoch 00003: val_loss improved from 88.99036 to 38.46870, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 67.6086 - val_loss: 38.4687\n",
      "Epoch 4/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 41.4976\n",
      "Epoch 00004: val_loss improved from 38.46870 to 22.02989, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 40.4526 - val_loss: 22.0299\n",
      "Epoch 5/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 32.0814\n",
      "Epoch 00005: val_loss improved from 22.02989 to 15.12442, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 31.3508 - val_loss: 15.1244\n",
      "Epoch 6/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 26.1934\n",
      "Epoch 00006: val_loss improved from 15.12442 to 10.19100, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 25.7178 - val_loss: 10.1910\n",
      "Epoch 7/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 22.5082\n",
      "Epoch 00007: val_loss improved from 10.19100 to 7.02096, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 22.1947 - val_loss: 7.0210\n",
      "Epoch 8/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 19.2936\n",
      "Epoch 00008: val_loss improved from 7.02096 to 4.98672, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 19.0861 - val_loss: 4.9867\n",
      "Epoch 9/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 18.3751\n",
      "Epoch 00009: val_loss improved from 4.98672 to 4.11405, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 18.0941 - val_loss: 4.1141\n",
      "Epoch 10/100\n",
      "26/34 [=====================>........] - ETA: 0s - loss: 16.4913\n",
      "Epoch 00010: val_loss improved from 4.11405 to 3.26756, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 16.3157 - val_loss: 3.2676\n",
      "Epoch 11/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 15.9629\n",
      "Epoch 00011: val_loss improved from 3.26756 to 3.08564, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.9396 - val_loss: 3.0856\n",
      "Epoch 12/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 15.6943\n",
      "Epoch 00012: val_loss improved from 3.08564 to 2.90724, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.6816 - val_loss: 2.9072\n",
      "Epoch 13/100\n",
      "26/34 [=====================>........] - ETA: 0s - loss: 15.1905\n",
      "Epoch 00013: val_loss improved from 2.90724 to 2.81488, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.2791 - val_loss: 2.8149\n",
      "Epoch 14/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.6137\n",
      "Epoch 00014: val_loss improved from 2.81488 to 2.69137, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.6077 - val_loss: 2.6914\n",
      "Epoch 15/100\n",
      "26/34 [=====================>........] - ETA: 0s - loss: 15.1490\n",
      "Epoch 00015: val_loss improved from 2.69137 to 2.67385, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 15.1100 - val_loss: 2.6738\n",
      "Epoch 16/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.7736\n",
      "Epoch 00016: val_loss improved from 2.67385 to 2.51022, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 14.6621 - val_loss: 2.5102\n",
      "Epoch 17/100\n",
      "25/34 [=====================>........] - ETA: 0s - loss: 14.1337\n",
      "Epoch 00017: val_loss did not improve from 2.51022\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 14.3571 - val_loss: 2.5801\n",
      "Epoch 18/100\n",
      "24/34 [====================>.........] - ETA: 0s - loss: 14.4673\n",
      "Epoch 00018: val_loss did not improve from 2.51022\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 14.4658 - val_loss: 2.5957\n",
      " ###8 fold : val mae 1.237, mse 2.598###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/35 [================>.............] - ETA: 0s - loss: 177.2683 \n",
      "Epoch 00001: val_loss improved from inf to 147.34944, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 1s 8ms/step - loss: 169.2138 - val_loss: 147.3494\n",
      "Epoch 2/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 132.3695\n",
      "Epoch 00002: val_loss improved from 147.34944 to 87.82903, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 121.3745 - val_loss: 87.8290\n",
      "Epoch 3/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 76.5994\n",
      "Epoch 00003: val_loss improved from 87.82903 to 40.12585, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 68.1386 - val_loss: 40.1259\n",
      "Epoch 4/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 43.3284\n",
      "Epoch 00004: val_loss improved from 40.12585 to 23.27951, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 40.7771 - val_loss: 23.2795\n",
      "Epoch 5/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 34.9577\n",
      "Epoch 00005: val_loss improved from 23.27951 to 19.42264, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 34.1798 - val_loss: 19.4226\n",
      "Epoch 6/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 29.9044\n",
      "Epoch 00006: val_loss improved from 19.42264 to 15.02396, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 29.0717 - val_loss: 15.0240\n",
      "Epoch 7/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 27.8241\n",
      "Epoch 00007: val_loss improved from 15.02396 to 11.96874, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 26.6659 - val_loss: 11.9687\n",
      "Epoch 8/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 24.1395\n",
      "Epoch 00008: val_loss improved from 11.96874 to 9.12653, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 23.8547 - val_loss: 9.1265\n",
      "Epoch 9/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 22.3818\n",
      "Epoch 00009: val_loss improved from 9.12653 to 7.17538, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 21.5712 - val_loss: 7.1754\n",
      "Epoch 10/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 20.1339\n",
      "Epoch 00010: val_loss improved from 7.17538 to 5.57243, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 19.6775 - val_loss: 5.5724\n",
      "Epoch 11/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 18.2987\n",
      "Epoch 00011: val_loss improved from 5.57243 to 4.49865, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 18.3503 - val_loss: 4.4987\n",
      "Epoch 12/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 17.1283\n",
      "Epoch 00012: val_loss improved from 4.49865 to 4.08555, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 17.2545 - val_loss: 4.0856\n",
      "Epoch 13/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 17.0121\n",
      "Epoch 00013: val_loss improved from 4.08555 to 3.83809, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 16.7958 - val_loss: 3.8381\n",
      "Epoch 14/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 17.0103\n",
      "Epoch 00014: val_loss improved from 3.83809 to 3.40736, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 16.4872 - val_loss: 3.4074\n",
      "Epoch 15/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 16.3090\n",
      "Epoch 00015: val_loss improved from 3.40736 to 3.36728, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 16.2176 - val_loss: 3.3673\n",
      "Epoch 16/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 15.7354\n",
      "Epoch 00016: val_loss improved from 3.36728 to 2.91050, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 15.5622 - val_loss: 2.9105\n",
      "Epoch 17/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 15.8078\n",
      "Epoch 00017: val_loss did not improve from 2.91050\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 15.5731 - val_loss: 3.0027\n",
      "Epoch 18/100\n",
      "19/35 [===============>..............] - ETA: 0s - loss: 14.8854\n",
      "Epoch 00018: val_loss improved from 2.91050 to 2.77292, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 14.8975 - val_loss: 2.7729\n",
      "Epoch 19/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 14.8975\n",
      "Epoch 00019: val_loss did not improve from 2.77292\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 14.9979 - val_loss: 3.1451\n",
      "Epoch 20/100\n",
      "20/35 [================>.............] - ETA: 0s - loss: 15.5361\n",
      "Epoch 00020: val_loss did not improve from 2.77292\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 15.4390 - val_loss: 3.1435\n",
      " ###9 fold : val mae 1.262, mse 2.602###\n",
      "mae2.484_mse0.680\n",
      "random search 4/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/68 [==========================>...] - ETA: 0s - loss: 168.9060\n",
      "Epoch 00001: val_loss improved from inf to 134.02843, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 1s 4ms/step - loss: 166.1119 - val_loss: 134.0284\n",
      "Epoch 2/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 100.1870\n",
      "Epoch 00002: val_loss improved from 134.02843 to 62.37402, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 96.9534 - val_loss: 62.3740\n",
      "Epoch 3/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 43.4659\n",
      "Epoch 00003: val_loss improved from 62.37402 to 27.84564, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 41.4348 - val_loss: 27.8456\n",
      "Epoch 4/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 22.2868\n",
      "Epoch 00004: val_loss improved from 27.84564 to 16.97917, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 21.6120 - val_loss: 16.9792\n",
      "Epoch 5/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 13.4253\n",
      "Epoch 00005: val_loss improved from 16.97917 to 9.90675, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 12.9805 - val_loss: 9.9067\n",
      "Epoch 6/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 7.6788\n",
      "Epoch 00006: val_loss improved from 9.90675 to 5.54439, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 7.3689 - val_loss: 5.5444\n",
      "Epoch 7/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 4.4090\n",
      "Epoch 00007: val_loss improved from 5.54439 to 3.27846, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.2313 - val_loss: 3.2785\n",
      "Epoch 8/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 2.8120\n",
      "Epoch 00008: val_loss improved from 3.27846 to 2.26753, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.7108 - val_loss: 2.2675\n",
      "Epoch 9/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 2.0976\n",
      "Epoch 00009: val_loss improved from 2.26753 to 1.84030, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.0594 - val_loss: 1.8403\n",
      "Epoch 10/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 1.7788\n",
      "Epoch 00010: val_loss improved from 1.84030 to 1.65869, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.7888 - val_loss: 1.6587\n",
      "Epoch 11/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 1.6673\n",
      "Epoch 00011: val_loss improved from 1.65869 to 1.57063, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.6652 - val_loss: 1.5706\n",
      "Epoch 12/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 1.5689\n",
      "Epoch 00012: val_loss improved from 1.57063 to 1.52055, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5964 - val_loss: 1.5205\n",
      "Epoch 13/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.5657\n",
      "Epoch 00013: val_loss improved from 1.52055 to 1.48757, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5484 - val_loss: 1.4876\n",
      "Epoch 14/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.5162\n",
      "Epoch 00014: val_loss improved from 1.48757 to 1.46091, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5115 - val_loss: 1.4609\n",
      "Epoch 15/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 1.4696\n",
      "Epoch 00015: val_loss improved from 1.46091 to 1.43491, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4807 - val_loss: 1.4349\n",
      "Epoch 16/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.4675\n",
      "Epoch 00016: val_loss improved from 1.43491 to 1.41976, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4544 - val_loss: 1.4198\n",
      "Epoch 17/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.4383\n",
      "Epoch 00017: val_loss improved from 1.41976 to 1.40369, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4317 - val_loss: 1.4037\n",
      "Epoch 18/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.4191\n",
      "Epoch 00018: val_loss improved from 1.40369 to 1.39067, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4120 - val_loss: 1.3907\n",
      "Epoch 19/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3826\n",
      "Epoch 00019: val_loss improved from 1.39067 to 1.37593, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3940 - val_loss: 1.3759\n",
      "Epoch 20/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.3731\n",
      "Epoch 00020: val_loss improved from 1.37593 to 1.36581, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3778 - val_loss: 1.3658\n",
      "Epoch 21/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3693\n",
      "Epoch 00021: val_loss improved from 1.36581 to 1.36518, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3662 - val_loss: 1.3652\n",
      "Epoch 22/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3327\n",
      "Epoch 00022: val_loss improved from 1.36518 to 1.35391, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3531 - val_loss: 1.3539\n",
      "Epoch 23/100\n",
      "55/68 [=======================>......] - ETA: 0s - loss: 1.3378\n",
      "Epoch 00023: val_loss improved from 1.35391 to 1.34675, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3431 - val_loss: 1.3467\n",
      "Epoch 24/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.3326\n",
      "Epoch 00024: val_loss improved from 1.34675 to 1.33974, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3340 - val_loss: 1.3397\n",
      "Epoch 25/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.3276\n",
      "Epoch 00025: val_loss improved from 1.33974 to 1.33683, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3261 - val_loss: 1.3368\n",
      "Epoch 26/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.3285\n",
      "Epoch 00026: val_loss improved from 1.33683 to 1.33172, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3176 - val_loss: 1.3317\n",
      "Epoch 27/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.3153\n",
      "Epoch 00027: val_loss did not improve from 1.33172\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3122 - val_loss: 1.3342\n",
      "Epoch 28/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 1.3174\n",
      "Epoch 00028: val_loss improved from 1.33172 to 1.33116, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3085 - val_loss: 1.3312\n",
      "Epoch 29/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.2965\n",
      "Epoch 00029: val_loss improved from 1.33116 to 1.32286, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3018 - val_loss: 1.3229\n",
      "Epoch 30/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3013\n",
      "Epoch 00030: val_loss improved from 1.32286 to 1.32047, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2997 - val_loss: 1.3205\n",
      "Epoch 31/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 1.2855\n",
      "Epoch 00031: val_loss improved from 1.32047 to 1.31904, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2951 - val_loss: 1.3190\n",
      "Epoch 32/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.2636\n",
      "Epoch 00032: val_loss did not improve from 1.31904\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2902 - val_loss: 1.3215\n",
      "Epoch 33/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.2839\n",
      "Epoch 00033: val_loss improved from 1.31904 to 1.31521, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2866 - val_loss: 1.3152\n",
      "Epoch 34/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.2865\n",
      "Epoch 00034: val_loss did not improve from 1.31521\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.2834 - val_loss: 1.3197\n",
      "Epoch 35/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.2754\n",
      "Epoch 00035: val_loss improved from 1.31521 to 1.31096, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2805 - val_loss: 1.3110\n",
      "Epoch 36/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.3049\n",
      "Epoch 00036: val_loss improved from 1.31096 to 1.30832, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2797 - val_loss: 1.3083\n",
      "Epoch 37/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.2823\n",
      "Epoch 00037: val_loss did not improve from 1.30832\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.2759 - val_loss: 1.3099\n",
      "Epoch 38/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.2924\n",
      "Epoch 00038: val_loss improved from 1.30832 to 1.30546, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2745 - val_loss: 1.3055\n",
      "Epoch 39/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.2857\n",
      "Epoch 00039: val_loss improved from 1.30546 to 1.30432, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2709 - val_loss: 1.3043\n",
      "Epoch 40/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.2581\n",
      "Epoch 00040: val_loss improved from 1.30432 to 1.30081, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2707 - val_loss: 1.3008\n",
      "Epoch 41/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.2844\n",
      "Epoch 00041: val_loss improved from 1.30081 to 1.30054, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2685 - val_loss: 1.3005\n",
      "Epoch 42/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.2782\n",
      "Epoch 00042: val_loss improved from 1.30054 to 1.29966, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2679 - val_loss: 1.2997\n",
      "Epoch 43/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.2874\n",
      "Epoch 00043: val_loss improved from 1.29966 to 1.29786, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2659 - val_loss: 1.2979\n",
      "Epoch 44/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.2454\n",
      "Epoch 00044: val_loss improved from 1.29786 to 1.29642, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2639 - val_loss: 1.2964\n",
      "Epoch 45/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.2405\n",
      "Epoch 00045: val_loss did not improve from 1.29642\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2614 - val_loss: 1.3005\n",
      "Epoch 46/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.2594\n",
      "Epoch 00046: val_loss did not improve from 1.29642\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2594 - val_loss: 1.2971\n",
      " ###0 fold : val mae 0.807, mse 1.259###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/68 [============================>.] - ETA: 0s - loss: 166.4934\n",
      "Epoch 00001: val_loss improved from inf to 134.12706, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 1s 4ms/step - loss: 165.6168 - val_loss: 134.1271\n",
      "Epoch 2/100\n",
      "67/68 [============================>.] - ETA: 0s - loss: 97.3907 \n",
      "Epoch 00002: val_loss improved from 134.12706 to 62.41774, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 96.8953 - val_loss: 62.4177\n",
      "Epoch 3/100\n",
      "65/68 [===========================>..] - ETA: 0s - loss: 42.1400\n",
      "Epoch 00003: val_loss improved from 62.41774 to 27.78060, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 41.4636 - val_loss: 27.7806\n",
      "Epoch 4/100\n",
      "34/68 [==============>...............] - ETA: 0s - loss: 24.1348\n",
      "Epoch 00004: val_loss improved from 27.78060 to 16.87188, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 21.5341 - val_loss: 16.8719\n",
      "Epoch 5/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 13.2209\n",
      "Epoch 00005: val_loss improved from 16.87188 to 9.84636, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 12.8893 - val_loss: 9.8464\n",
      "Epoch 6/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 7.5411\n",
      "Epoch 00006: val_loss improved from 9.84636 to 5.51647, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 7.3062 - val_loss: 5.5165\n",
      "Epoch 7/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 4.3192\n",
      "Epoch 00007: val_loss improved from 5.51647 to 3.27711, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.1893 - val_loss: 3.2771\n",
      "Epoch 8/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 2.7387\n",
      "Epoch 00008: val_loss improved from 3.27711 to 2.26825, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.6791 - val_loss: 2.2683\n",
      "Epoch 9/100\n",
      "63/68 [==========================>...] - ETA: 0s - loss: 2.0548\n",
      "Epoch 00009: val_loss improved from 2.26825 to 1.84327, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.0293 - val_loss: 1.8433\n",
      "Epoch 10/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 1.7412\n",
      "Epoch 00010: val_loss improved from 1.84327 to 1.66484, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.7629 - val_loss: 1.6648\n",
      "Epoch 11/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.6488\n",
      "Epoch 00011: val_loss improved from 1.66484 to 1.57840, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.6449 - val_loss: 1.5784\n",
      "Epoch 12/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.5880\n",
      "Epoch 00012: val_loss improved from 1.57840 to 1.52904, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5798 - val_loss: 1.5290\n",
      "Epoch 13/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.5459\n",
      "Epoch 00013: val_loss improved from 1.52904 to 1.49656, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5349 - val_loss: 1.4966\n",
      "Epoch 14/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 1.4928\n",
      "Epoch 00014: val_loss improved from 1.49656 to 1.47119, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5014 - val_loss: 1.4712\n",
      "Epoch 15/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.4653\n",
      "Epoch 00015: val_loss improved from 1.47119 to 1.44454, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4725 - val_loss: 1.4445\n",
      "Epoch 16/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 1.4492\n",
      "Epoch 00016: val_loss improved from 1.44454 to 1.42964, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4480 - val_loss: 1.4296\n",
      "Epoch 17/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 1.4308\n",
      "Epoch 00017: val_loss improved from 1.42964 to 1.41026, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4266 - val_loss: 1.4103\n",
      "Epoch 18/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 1.4061\n",
      "Epoch 00018: val_loss improved from 1.41026 to 1.39809, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4072 - val_loss: 1.3981\n",
      "Epoch 19/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 1.3887\n",
      "Epoch 00019: val_loss improved from 1.39809 to 1.38337, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3898 - val_loss: 1.3834\n",
      "Epoch 20/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.3725\n",
      "Epoch 00020: val_loss improved from 1.38337 to 1.37136, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3745 - val_loss: 1.3714\n",
      "Epoch 21/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.3526\n",
      "Epoch 00021: val_loss improved from 1.37136 to 1.37097, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3618 - val_loss: 1.3710\n",
      "Epoch 22/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 1.3411\n",
      "Epoch 00022: val_loss improved from 1.37097 to 1.35863, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3489 - val_loss: 1.3586\n",
      "Epoch 23/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 1.3433\n",
      "Epoch 00023: val_loss improved from 1.35863 to 1.35093, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3395 - val_loss: 1.3509\n",
      "Epoch 24/100\n",
      "66/68 [============================>.] - ETA: 0s - loss: 1.3341\n",
      "Epoch 00024: val_loss improved from 1.35093 to 1.34422, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3291 - val_loss: 1.3442\n",
      "Epoch 25/100\n",
      "63/68 [==========================>...] - ETA: 0s - loss: 1.3241\n",
      "Epoch 00025: val_loss improved from 1.34422 to 1.34005, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3216 - val_loss: 1.3401\n",
      "Epoch 26/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 1.3204\n",
      "Epoch 00026: val_loss improved from 1.34005 to 1.33401, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3134 - val_loss: 1.3340\n",
      "Epoch 27/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 1.3003\n",
      "Epoch 00027: val_loss improved from 1.33401 to 1.33390, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3065 - val_loss: 1.3339\n",
      "Epoch 28/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.2873\n",
      "Epoch 00028: val_loss did not improve from 1.33390\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.3006 - val_loss: 1.3357\n",
      "Epoch 29/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 1.3028\n",
      "Epoch 00029: val_loss improved from 1.33390 to 1.32395, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2952 - val_loss: 1.3239\n",
      "Epoch 30/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 1.3086\n",
      "Epoch 00030: val_loss improved from 1.32395 to 1.32259, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2931 - val_loss: 1.3226\n",
      "Epoch 31/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 1.2800\n",
      "Epoch 00031: val_loss did not improve from 1.32259\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2879 - val_loss: 1.3229\n",
      "Epoch 32/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.2796\n",
      "Epoch 00032: val_loss did not improve from 1.32259\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.2829 - val_loss: 1.3234\n",
      " ###1 fold : val mae 0.822, mse 1.362###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/68 [===========================>..] - ETA: 0s - loss: 167.9181\n",
      "Epoch 00001: val_loss improved from inf to 134.22983, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 1s 4ms/step - loss: 166.0036 - val_loss: 134.2298\n",
      "Epoch 2/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 101.8584\n",
      "Epoch 00002: val_loss improved from 134.22983 to 62.51373, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 97.0093 - val_loss: 62.5137\n",
      "Epoch 3/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 43.2084\n",
      "Epoch 00003: val_loss improved from 62.51373 to 27.88456, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 41.3416 - val_loss: 27.8846\n",
      "Epoch 4/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 22.1837\n",
      "Epoch 00004: val_loss improved from 27.88456 to 17.05664, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 21.5052 - val_loss: 17.0566\n",
      "Epoch 5/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 13.3186\n",
      "Epoch 00005: val_loss improved from 17.05664 to 10.03869, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 12.9895 - val_loss: 10.0387\n",
      "Epoch 6/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 7.7172\n",
      "Epoch 00006: val_loss improved from 10.03869 to 5.67964, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 7.4449 - val_loss: 5.6796\n",
      "Epoch 7/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 4.4438\n",
      "Epoch 00007: val_loss improved from 5.67964 to 3.37726, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.3050 - val_loss: 3.3773\n",
      "Epoch 8/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 2.8204\n",
      "Epoch 00008: val_loss improved from 3.37726 to 2.31668, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.7611 - val_loss: 2.3167\n",
      "Epoch 9/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 2.1149\n",
      "Epoch 00009: val_loss improved from 2.31668 to 1.86236, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.0837 - val_loss: 1.8624\n",
      "Epoch 10/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.7701\n",
      "Epoch 00010: val_loss improved from 1.86236 to 1.67066, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.8001 - val_loss: 1.6707\n",
      "Epoch 11/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.6576\n",
      "Epoch 00011: val_loss improved from 1.67066 to 1.57643, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.6720 - val_loss: 1.5764\n",
      "Epoch 12/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.6177\n",
      "Epoch 00012: val_loss improved from 1.57643 to 1.52395, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.6007 - val_loss: 1.5239\n",
      "Epoch 13/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.5672\n",
      "Epoch 00013: val_loss improved from 1.52395 to 1.48908, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5517 - val_loss: 1.4891\n",
      "Epoch 14/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.4966\n",
      "Epoch 00014: val_loss improved from 1.48908 to 1.46118, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5141 - val_loss: 1.4612\n",
      "Epoch 15/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 1.4769\n",
      "Epoch 00015: val_loss improved from 1.46118 to 1.43714, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4825 - val_loss: 1.4371\n",
      "Epoch 16/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.4670\n",
      "Epoch 00016: val_loss improved from 1.43714 to 1.42230, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4554 - val_loss: 1.4223\n",
      "Epoch 17/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.4313\n",
      "Epoch 00017: val_loss improved from 1.42230 to 1.40411, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4324 - val_loss: 1.4041\n",
      "Epoch 18/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.4106\n",
      "Epoch 00018: val_loss improved from 1.40411 to 1.39021, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4118 - val_loss: 1.3902\n",
      "Epoch 19/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.4053\n",
      "Epoch 00019: val_loss improved from 1.39021 to 1.37737, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3930 - val_loss: 1.3774\n",
      "Epoch 20/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.3687\n",
      "Epoch 00020: val_loss improved from 1.37737 to 1.36518, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3777 - val_loss: 1.3652\n",
      "Epoch 21/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3636\n",
      "Epoch 00021: val_loss improved from 1.36518 to 1.36380, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3631 - val_loss: 1.3638\n",
      "Epoch 22/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3442\n",
      "Epoch 00022: val_loss improved from 1.36380 to 1.35377, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3504 - val_loss: 1.3538\n",
      "Epoch 23/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.3316\n",
      "Epoch 00023: val_loss improved from 1.35377 to 1.34709, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3403 - val_loss: 1.3471\n",
      "Epoch 24/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3514\n",
      "Epoch 00024: val_loss improved from 1.34709 to 1.33889, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3304 - val_loss: 1.3389\n",
      "Epoch 25/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3260\n",
      "Epoch 00025: val_loss improved from 1.33889 to 1.33623, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3229 - val_loss: 1.3362\n",
      "Epoch 26/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.3160\n",
      "Epoch 00026: val_loss improved from 1.33623 to 1.33009, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3158 - val_loss: 1.3301\n",
      "Epoch 27/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.2990\n",
      "Epoch 00027: val_loss improved from 1.33009 to 1.32954, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3086 - val_loss: 1.3295\n",
      "Epoch 28/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.2938\n",
      "Epoch 00028: val_loss improved from 1.32954 to 1.32716, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3028 - val_loss: 1.3272\n",
      "Epoch 29/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.3110\n",
      "Epoch 00029: val_loss improved from 1.32716 to 1.32230, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2983 - val_loss: 1.3223\n",
      "Epoch 30/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 1.2998\n",
      "Epoch 00030: val_loss improved from 1.32230 to 1.32131, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2951 - val_loss: 1.3213\n",
      "Epoch 31/100\n",
      "64/68 [===========================>..] - ETA: 0s - loss: 1.2924\n",
      "Epoch 00031: val_loss improved from 1.32131 to 1.32080, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2911 - val_loss: 1.3208\n",
      "Epoch 32/100\n",
      "63/68 [==========================>...] - ETA: 0s - loss: 1.2795\n",
      "Epoch 00032: val_loss improved from 1.32080 to 1.31874, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2864 - val_loss: 1.3187\n",
      "Epoch 33/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.2817\n",
      "Epoch 00033: val_loss improved from 1.31874 to 1.31596, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2831 - val_loss: 1.3160\n",
      "Epoch 34/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.2799\n",
      "Epoch 00034: val_loss improved from 1.31596 to 1.31406, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2809 - val_loss: 1.3141\n",
      "Epoch 35/100\n",
      "63/68 [==========================>...] - ETA: 0s - loss: 1.2742\n",
      "Epoch 00035: val_loss improved from 1.31406 to 1.30769, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2774 - val_loss: 1.3077\n",
      "Epoch 36/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.2971\n",
      "Epoch 00036: val_loss improved from 1.30769 to 1.30679, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2759 - val_loss: 1.3068\n",
      "Epoch 37/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.2721\n",
      "Epoch 00037: val_loss improved from 1.30679 to 1.30560, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2728 - val_loss: 1.3056\n",
      "Epoch 38/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 1.2873\n",
      "Epoch 00038: val_loss improved from 1.30560 to 1.30156, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2700 - val_loss: 1.3016\n",
      "Epoch 39/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.2755\n",
      "Epoch 00039: val_loss did not improve from 1.30156\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.2678 - val_loss: 1.3059\n",
      "Epoch 40/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.2683\n",
      "Epoch 00040: val_loss improved from 1.30156 to 1.29900, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2672 - val_loss: 1.2990\n",
      "Epoch 41/100\n",
      "63/68 [==========================>...] - ETA: 0s - loss: 1.2672\n",
      "Epoch 00041: val_loss did not improve from 1.29900\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.2657 - val_loss: 1.2993\n",
      "Epoch 42/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.2801\n",
      "Epoch 00042: val_loss improved from 1.29900 to 1.29637, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2637 - val_loss: 1.2964\n",
      "Epoch 43/100\n",
      "64/68 [===========================>..] - ETA: 0s - loss: 1.2631\n",
      "Epoch 00043: val_loss improved from 1.29637 to 1.29463, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2619 - val_loss: 1.2946\n",
      "Epoch 44/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.2601\n",
      "Epoch 00044: val_loss did not improve from 1.29463\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.2612 - val_loss: 1.2958\n",
      "Epoch 45/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.2411\n",
      "Epoch 00045: val_loss did not improve from 1.29463\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.2577 - val_loss: 1.3060\n",
      " ###2 fold : val mae 0.816, mse 1.286###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/68 [========================>.....] - ETA: 0s - loss: 171.3131\n",
      "Epoch 00001: val_loss improved from inf to 134.23267, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 1s 4ms/step - loss: 166.2404 - val_loss: 134.2327\n",
      "Epoch 2/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 102.0549\n",
      "Epoch 00002: val_loss improved from 134.23267 to 62.48118, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 97.2469 - val_loss: 62.4812\n",
      "Epoch 3/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 44.0452\n",
      "Epoch 00003: val_loss improved from 62.48118 to 27.85103, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 41.4719 - val_loss: 27.8510\n",
      "Epoch 4/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 22.3247\n",
      "Epoch 00004: val_loss improved from 27.85103 to 17.03204, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 21.5742 - val_loss: 17.0320\n",
      "Epoch 5/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 13.5545\n",
      "Epoch 00005: val_loss improved from 17.03204 to 10.01515, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 13.0343 - val_loss: 10.0151\n",
      "Epoch 6/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 7.7487\n",
      "Epoch 00006: val_loss improved from 10.01515 to 5.66086, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 7.4665 - val_loss: 5.6609\n",
      "Epoch 7/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 4.4751\n",
      "Epoch 00007: val_loss improved from 5.66086 to 3.36897, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.3123 - val_loss: 3.3690\n",
      "Epoch 8/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 2.8497\n",
      "Epoch 00008: val_loss improved from 3.36897 to 2.31399, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.7661 - val_loss: 2.3140\n",
      "Epoch 9/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 2.1555\n",
      "Epoch 00009: val_loss improved from 2.31399 to 1.86124, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.0908 - val_loss: 1.8612\n",
      "Epoch 10/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.7713\n",
      "Epoch 00010: val_loss improved from 1.86124 to 1.67063, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.8068 - val_loss: 1.6706\n",
      "Epoch 11/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.6639\n",
      "Epoch 00011: val_loss improved from 1.67063 to 1.57699, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.6791 - val_loss: 1.5770\n",
      "Epoch 12/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.6275\n",
      "Epoch 00012: val_loss improved from 1.57699 to 1.52536, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.6085 - val_loss: 1.5254\n",
      "Epoch 13/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.5832\n",
      "Epoch 00013: val_loss improved from 1.52536 to 1.48950, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5601 - val_loss: 1.4895\n",
      "Epoch 14/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.5111\n",
      "Epoch 00014: val_loss improved from 1.48950 to 1.46204, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5226 - val_loss: 1.4620\n",
      "Epoch 15/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.4786\n",
      "Epoch 00015: val_loss improved from 1.46204 to 1.43896, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4907 - val_loss: 1.4390\n",
      "Epoch 16/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.4742\n",
      "Epoch 00016: val_loss improved from 1.43896 to 1.42188, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4639 - val_loss: 1.4219\n",
      "Epoch 17/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.4420\n",
      "Epoch 00017: val_loss improved from 1.42188 to 1.40660, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4404 - val_loss: 1.4066\n",
      "Epoch 18/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 1.4203\n",
      "Epoch 00018: val_loss improved from 1.40660 to 1.39200, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4207 - val_loss: 1.3920\n",
      "Epoch 19/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.4074\n",
      "Epoch 00019: val_loss improved from 1.39200 to 1.37850, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4016 - val_loss: 1.3785\n",
      "Epoch 20/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.3787\n",
      "Epoch 00020: val_loss improved from 1.37850 to 1.36715, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3849 - val_loss: 1.3671\n",
      "Epoch 21/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.3644\n",
      "Epoch 00021: val_loss improved from 1.36715 to 1.36293, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3709 - val_loss: 1.3629\n",
      "Epoch 22/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.3669\n",
      "Epoch 00022: val_loss improved from 1.36293 to 1.35573, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3579 - val_loss: 1.3557\n",
      "Epoch 23/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.3342\n",
      "Epoch 00023: val_loss improved from 1.35573 to 1.34675, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3481 - val_loss: 1.3468\n",
      "Epoch 24/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.3393\n",
      "Epoch 00024: val_loss improved from 1.34675 to 1.34204, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3375 - val_loss: 1.3420\n",
      "Epoch 25/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3329\n",
      "Epoch 00025: val_loss improved from 1.34204 to 1.33818, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3298 - val_loss: 1.3382\n",
      "Epoch 26/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3022\n",
      "Epoch 00026: val_loss improved from 1.33818 to 1.33142, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3229 - val_loss: 1.3314\n",
      "Epoch 27/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3206\n",
      "Epoch 00027: val_loss did not improve from 1.33142\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3146 - val_loss: 1.3315\n",
      "Epoch 28/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 1.2962\n",
      "Epoch 00028: val_loss improved from 1.33142 to 1.32726, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3098 - val_loss: 1.3273\n",
      "Epoch 29/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3162\n",
      "Epoch 00029: val_loss improved from 1.32726 to 1.32320, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3059 - val_loss: 1.3232\n",
      "Epoch 30/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3144\n",
      "Epoch 00030: val_loss improved from 1.32320 to 1.32087, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3008 - val_loss: 1.3209\n",
      "Epoch 31/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.2954\n",
      "Epoch 00031: val_loss did not improve from 1.32087\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2966 - val_loss: 1.3266\n",
      "Epoch 32/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.2753\n",
      "Epoch 00032: val_loss did not improve from 1.32087\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2928 - val_loss: 1.3216\n",
      " ###3 fold : val mae 0.811, mse 1.276###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/68 [========================>.....] - ETA: 0s - loss: 170.8041\n",
      "Epoch 00001: val_loss improved from inf to 134.07509, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 1s 4ms/step - loss: 166.1466 - val_loss: 134.0751\n",
      "Epoch 2/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 102.1695\n",
      "Epoch 00002: val_loss improved from 134.07509 to 62.41833, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 97.0085 - val_loss: 62.4183\n",
      "Epoch 3/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 43.7339\n",
      "Epoch 00003: val_loss improved from 62.41833 to 27.84838, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 41.3397 - val_loss: 27.8484\n",
      "Epoch 4/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 22.3610\n",
      "Epoch 00004: val_loss improved from 27.84838 to 17.02390, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 21.5284 - val_loss: 17.0239\n",
      "Epoch 5/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 13.4521\n",
      "Epoch 00005: val_loss improved from 17.02390 to 9.97360, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 12.9856 - val_loss: 9.9736\n",
      "Epoch 6/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 7.7292\n",
      "Epoch 00006: val_loss improved from 9.97360 to 5.61010, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 7.3936 - val_loss: 5.6101\n",
      "Epoch 7/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 4.4070\n",
      "Epoch 00007: val_loss improved from 5.61010 to 3.33361, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.2538 - val_loss: 3.3336\n",
      "Epoch 8/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 2.8449\n",
      "Epoch 00008: val_loss improved from 3.33361 to 2.29454, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.7298 - val_loss: 2.2945\n",
      "Epoch 9/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 2.1271\n",
      "Epoch 00009: val_loss improved from 2.29454 to 1.84991, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.0658 - val_loss: 1.8499\n",
      "Epoch 10/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.7673\n",
      "Epoch 00010: val_loss improved from 1.84991 to 1.66131, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.7866 - val_loss: 1.6613\n",
      "Epoch 11/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.6506\n",
      "Epoch 00011: val_loss improved from 1.66131 to 1.56761, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.6600 - val_loss: 1.5676\n",
      "Epoch 12/100\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 1.5879\n",
      "Epoch 00012: val_loss improved from 1.56761 to 1.51667, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5897 - val_loss: 1.5167\n",
      "Epoch 13/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.5693\n",
      "Epoch 00013: val_loss improved from 1.51667 to 1.48214, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5427 - val_loss: 1.4821\n",
      "Epoch 14/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.4865\n",
      "Epoch 00014: val_loss improved from 1.48214 to 1.45749, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5048 - val_loss: 1.4575\n",
      "Epoch 15/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.4672\n",
      "Epoch 00015: val_loss improved from 1.45749 to 1.43507, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4736 - val_loss: 1.4351\n",
      "Epoch 16/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.4555\n",
      "Epoch 00016: val_loss improved from 1.43507 to 1.41695, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4469 - val_loss: 1.4169\n",
      "Epoch 17/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.4221\n",
      "Epoch 00017: val_loss improved from 1.41695 to 1.40292, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4245 - val_loss: 1.4029\n",
      "Epoch 18/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3997\n",
      "Epoch 00018: val_loss improved from 1.40292 to 1.39005, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4040 - val_loss: 1.3900\n",
      "Epoch 19/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3919\n",
      "Epoch 00019: val_loss improved from 1.39005 to 1.37652, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3865 - val_loss: 1.3765\n",
      "Epoch 20/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3666\n",
      "Epoch 00020: val_loss improved from 1.37652 to 1.36657, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3695 - val_loss: 1.3666\n",
      "Epoch 21/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3547\n",
      "Epoch 00021: val_loss improved from 1.36657 to 1.36022, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3566 - val_loss: 1.3602\n",
      "Epoch 22/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3558\n",
      "Epoch 00022: val_loss improved from 1.36022 to 1.35249, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3433 - val_loss: 1.3525\n",
      "Epoch 23/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.3173\n",
      "Epoch 00023: val_loss improved from 1.35249 to 1.34452, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3336 - val_loss: 1.3445\n",
      "Epoch 24/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.3244\n",
      "Epoch 00024: val_loss improved from 1.34452 to 1.34200, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3232 - val_loss: 1.3420\n",
      "Epoch 25/100\n",
      "60/68 [=========================>....] - ETA: 0s - loss: 1.3264\n",
      "Epoch 00025: val_loss improved from 1.34200 to 1.33983, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3147 - val_loss: 1.3398\n",
      "Epoch 26/100\n",
      "61/68 [=========================>....] - ETA: 0s - loss: 1.3053\n",
      "Epoch 00026: val_loss improved from 1.33983 to 1.33026, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3083 - val_loss: 1.3303\n",
      "Epoch 27/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.2922\n",
      "Epoch 00027: val_loss did not improve from 1.33026\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.3003 - val_loss: 1.3359\n",
      "Epoch 28/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.2787\n",
      "Epoch 00028: val_loss improved from 1.33026 to 1.32665, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2961 - val_loss: 1.3267\n",
      "Epoch 29/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.3020\n",
      "Epoch 00029: val_loss improved from 1.32665 to 1.32244, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2909 - val_loss: 1.3224\n",
      "Epoch 30/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.2946\n",
      "Epoch 00030: val_loss improved from 1.32244 to 1.31975, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2863 - val_loss: 1.3197\n",
      "Epoch 31/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.2979\n",
      "Epoch 00031: val_loss did not improve from 1.31975\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2811 - val_loss: 1.3265\n",
      "Epoch 32/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.2664\n",
      "Epoch 00032: val_loss did not improve from 1.31975\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.2787 - val_loss: 1.3198\n",
      " ###4 fold : val mae 0.839, mse 1.362###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/68 [=========================>....] - ETA: 0s - loss: 170.3318\n",
      "Epoch 00001: val_loss improved from inf to 134.08670, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 1s 4ms/step - loss: 166.1812 - val_loss: 134.0867\n",
      "Epoch 2/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 102.1031\n",
      "Epoch 00002: val_loss improved from 134.08670 to 62.41707, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 96.9875 - val_loss: 62.4171\n",
      "Epoch 3/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 43.7634\n",
      "Epoch 00003: val_loss improved from 62.41707 to 27.85546, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 41.3786 - val_loss: 27.8555\n",
      "Epoch 4/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 22.2721\n",
      "Epoch 00004: val_loss improved from 27.85546 to 17.01027, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 21.5877 - val_loss: 17.0103\n",
      "Epoch 5/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 13.4906\n",
      "Epoch 00005: val_loss improved from 17.01027 to 9.94410, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 13.0063 - val_loss: 9.9441\n",
      "Epoch 6/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 7.6479\n",
      "Epoch 00006: val_loss improved from 9.94410 to 5.59390, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 7.3940 - val_loss: 5.5939\n",
      "Epoch 7/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 4.4195\n",
      "Epoch 00007: val_loss improved from 5.59390 to 3.32162, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 4.2480 - val_loss: 3.3216\n",
      "Epoch 8/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 2.8274\n",
      "Epoch 00008: val_loss improved from 3.32162 to 2.29038, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.7153 - val_loss: 2.2904\n",
      "Epoch 9/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 2.1001\n",
      "Epoch 00009: val_loss improved from 2.29038 to 1.84813, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 2.0496 - val_loss: 1.8481\n",
      "Epoch 10/100\n",
      "58/68 [========================>.....] - ETA: 0s - loss: 1.7495\n",
      "Epoch 00010: val_loss improved from 1.84813 to 1.66462, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.7720 - val_loss: 1.6646\n",
      "Epoch 11/100\n",
      "59/68 [=========================>....] - ETA: 0s - loss: 1.6280\n",
      "Epoch 00011: val_loss improved from 1.66462 to 1.57378, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.6493 - val_loss: 1.5738\n",
      "Epoch 12/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 1.5940\n",
      "Epoch 00012: val_loss improved from 1.57378 to 1.52482, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5828 - val_loss: 1.5248\n",
      "Epoch 13/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 1.5598\n",
      "Epoch 00013: val_loss improved from 1.52482 to 1.49026, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5412 - val_loss: 1.4903\n",
      "Epoch 14/100\n",
      "62/68 [==========================>...] - ETA: 0s - loss: 1.4972\n",
      "Epoch 00014: val_loss improved from 1.49026 to 1.46564, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.5057 - val_loss: 1.4656\n",
      "Epoch 15/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.4745\n",
      "Epoch 00015: val_loss improved from 1.46564 to 1.44159, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4777 - val_loss: 1.4416\n",
      "Epoch 16/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.4599\n",
      "Epoch 00016: val_loss improved from 1.44159 to 1.42401, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4532 - val_loss: 1.4240\n",
      "Epoch 17/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.4150\n",
      "Epoch 00017: val_loss improved from 1.42401 to 1.41040, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4327 - val_loss: 1.4104\n",
      "Epoch 18/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.4075\n",
      "Epoch 00018: val_loss improved from 1.41040 to 1.39640, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.4137 - val_loss: 1.3964\n",
      "Epoch 19/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.4105\n",
      "Epoch 00019: val_loss improved from 1.39640 to 1.38172, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3972 - val_loss: 1.3817\n",
      "Epoch 20/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3888\n",
      "Epoch 00020: val_loss improved from 1.38172 to 1.36999, saving model to result/depth_8cm/DNN_depth_both_4inputs/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 1.3808 - val_loss: 1.3700\n",
      "Epoch 21/100\n",
      "57/68 [========================>.....] - ETA: 0s - loss: 1.3649"
     ]
    }
   ],
   "source": [
    "from keras import metrics\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "# random search for hyperparameter\n",
    "ntrial = ntest\n",
    "train_errs, val_errs = [] ,[]\n",
    "test_acc, test_roc, test_prc = [], [], []\n",
    "#test_rmse, test_mae, test_auc = [], [], []\n",
    "random_settings = []\n",
    "\n",
    "\n",
    "for itrial in range(ntrial):\n",
    "    # grid search\n",
    "    # test_setting = test_settings[itrial]\n",
    "\n",
    "    # random search\n",
    "    print('random search {}/{}'.format(itrial, ntrial))\n",
    "    \n",
    "    # total conv layers of the model\n",
    "    nlayer = random.choice([1,2]) \n",
    "    # test settings\n",
    "    dnodes[0], dropouts[0], dnodes[1], dropouts[1], batch_size, learning_rate = random.choice(test_settings)\n",
    "    \n",
    "\n",
    "    # 이번 옵션에 대한 결과 디렉토리\n",
    "    odir_f = f'batch{batch_size},'\n",
    "    for i in range(nlayer):\n",
    "        odir_f += f'dnodes{dnodes[i]}_dropout{dropouts[i]},'\n",
    "    odir_f += f'lr{learning_rate}'\n",
    "    random_settings.append(odir_f)\n",
    "    \n",
    "    odir = rootdir + '/' + odir_f\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "\n",
    "    # build a model\n",
    "    inp = Input(shape=(x_train.shape[1],))\n",
    "    out = inp\n",
    "\n",
    "    \n",
    "    for i in range(nlayer):      \n",
    "        out = Dense(dnodes[i], activation='relu')(out)\n",
    "        out = Dropout(dropouts[i])(out)\n",
    "    \n",
    "    out = Dense(1)(out)\n",
    "\n",
    "\n",
    "    model = Model(inputs=[inp], outputs=[out])\n",
    "    model.save_weights(f'{odir}/initial_weights.hdf5')\n",
    "        \n",
    "\n",
    "    # 4-fold cv\n",
    "    kfold = KFold(nfold)\n",
    "    acc1s, acc3s, maes, mses = [], [], [], []\n",
    "\n",
    "    switch = 0\n",
    "    for fold, (train_mask, test_mask) in enumerate(kfold.split(y_train)):\n",
    "        X_train = x_train_imputed[train_mask]\n",
    "        X_test = x_train_imputed[test_mask] \n",
    "        \n",
    "        Y_train = y_train[train_mask]\n",
    "        Y_test = y_train[test_mask]\n",
    "\n",
    "\n",
    "        # model 학습\n",
    "        try:\n",
    "            weightcache = f\"{odir}/weights_{fold}.hdf5\"\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=learning_rate), metrics=[])\n",
    "            hist = model.fit(X_train, Y_train, validation_split=0.2, epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=2, verbose=0, mode='auto')])\n",
    "\n",
    "            model.load_weights(weightcache)\n",
    "            y_pred = model.predict(X_test).flatten()\n",
    "            \n",
    "            mae = mean_absolute_error(Y_test, y_pred)    \n",
    "            mse = mean_squared_error(Y_test, y_pred)\n",
    "            \n",
    "            maes.append(mae)\n",
    "            mses.append(mse)\n",
    "\n",
    "            print(f' ###{fold} fold : val mae {mae:.3f}, mse {mse:.3f}###')\n",
    "            tf.keras.backend.clear_session()\n",
    "            model.load_weights(f'{odir}/initial_weights.hdf5')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            switch = 1\n",
    "            shutil.rmtree(odir)\n",
    "            itrial -= 1\n",
    "            break\n",
    "\n",
    "    if switch:\n",
    "        switch = 0\n",
    "        continue\n",
    "    \n",
    "\n",
    "    print(f'mae{np.mean(mses):.3f}_mse{np.mean(acc3):.3f}')\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    os.rename(odir, rootdir+f'/mae{np.mean(maes):.3f}_mse{np.mean(mses):.3f}_{odir_f}')\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17a830-20a0-4248-92e5-a6f22db873ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecc5bf0d-fec8-494b-b912-2dcd1b4ea01a",
   "metadata": {},
   "source": [
    "# Corelation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f553cb5e-0f9f-470e-ae51-7869ecd1f89b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T07:13:05.164137Z",
     "iopub.status.busy": "2023-02-22T07:13:05.163576Z",
     "iopub.status.idle": "2023-02-22T07:13:05.180315Z",
     "shell.execute_reply": "2023-02-22T07:13:05.179410Z",
     "shell.execute_reply.started": "2023-02-22T07:13:05.164083Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.03861307987455406, pvalue=0.002678583454782267)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "# sex\n",
    "stats.spearmanr(x_test[:,1], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a35c0ef-3f5d-45c2-8e79-84bef2762fdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T07:15:24.755302Z",
     "iopub.status.busy": "2023-02-22T07:15:24.754745Z",
     "iopub.status.idle": "2023-02-22T07:15:24.767460Z",
     "shell.execute_reply": "2023-02-22T07:15:24.766658Z",
     "shell.execute_reply.started": "2023-02-22T07:15:24.755247Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.8975062136826104, pvalue=0.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.spearmanr(x_test_imputed[:,2], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3914687-6910-43ef-96b1-43b12529295e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T07:29:49.963596Z",
     "iopub.status.busy": "2023-02-22T07:29:49.963099Z",
     "iopub.status.idle": "2023-02-22T07:29:49.975949Z",
     "shell.execute_reply": "2023-02-22T07:29:49.974949Z",
     "shell.execute_reply.started": "2023-02-22T07:29:49.963542Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.1583183985229819, pvalue=3.198205039865417e-35)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.spearmanr(x_test[:,4], y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "painstudy_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
