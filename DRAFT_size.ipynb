{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31fd2ee2-ec28-478f-86f8-af4c4bf3619e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T06:53:03.208158Z",
     "iopub.status.busy": "2023-02-17T06:53:03.207645Z",
     "iopub.status.idle": "2023-02-17T06:53:10.351340Z",
     "shell.execute_reply": "2023-02-17T06:53:10.350792Z",
     "shell.execute_reply.started": "2023-02-17T06:53:03.208034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복되는 hid는 첫번째 수술 외 제외: 34042\n",
      "소아 10세 미만에서 cuffed ETT 사용 비율: 0.287\n",
      "소아 10세 미만 최종 opid수: 34042\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import random, os, datetime, pickle\n",
    "import scipy, csv, math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# tube.csv에 수술 시점 weight, height 추가\n",
    "df0 = pd.read_csv('tube.csv')\n",
    "df0.drop(columns=['weight', 'height'], inplace=True)\n",
    "df = pd.read_csv('demography_revised.csv')\n",
    "df = df0.merge(df[['opid', 'weight', 'height']], how='left', on='opid', suffixes=('_o',''))\n",
    "\n",
    "df.loc[df['weight'] <= 0, 'weight'] = None\n",
    "df.loc[df['height'] <= 0, 'height'] = None\n",
    "df['age'] = df['age'].astype(int)\n",
    "df = df.loc[df['airway_tube_type'] == 'plain']\n",
    "# [nan 'plain' 'RAE(oral)' 'reinforced' 'LMA' 'T-tube' 'CobraPLA', 'double lumen tube' 'RAE(nasal)' 'laser' 'combitube' 'univent']\n",
    "\n",
    "# age, sex, airway tube size 값이 없는 경우는 제외\n",
    "df.dropna(subset=['age', 'airway_tube_size'], inplace=True)\n",
    "df['sex'] = (df['sex'] == 'M')\n",
    "\n",
    "# 나이 계산 -> age_cal 열에 추가\n",
    "df_b = pd.read_csv('birth_sex.csv')\n",
    "df_b.rename(columns={'생년월일':'birth_date'}, inplace=True)\n",
    "df_b['birth_date'] = df_b['birth_date'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "\n",
    "df_o = pd.read_csv('opdates.csv')\n",
    "df_o['opdate'] = df_o['opdate'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "\n",
    "df1 = pd.merge(df_o, df_b, how='inner', on='hid')\n",
    "df1['age_cal'] = (df1['opdate'] - df1['birth_date'])/pd.Timedelta(days=365.2425)\n",
    "\n",
    "df = pd.merge(df, df1[['opid', 'age_cal', 'opdate', 'birth_date']], how='inner', on='opid')\n",
    "\n",
    "# inclusion criteria : 소아 10세 미만\n",
    "df = df.loc[df['age_cal'] < 10-0.01]\n",
    "df = df.loc[df['age_cal'] > 0]\n",
    "df['age'] = df['age_cal'].apply(lambda x: math.floor(x))\n",
    "\n",
    "\n",
    "# cuffed 여부와 fixed depth 추가\n",
    "df_t = pd.read_csv('tube_type.csv')\n",
    "df_t['cuffed'] = (df_t['cuffed'] == 1)\n",
    "\n",
    "df_f = pd.read_csv('tube_fixed.csv')\n",
    "\n",
    "# merge 하면서 cuffed 데이터가 없는 경우는 제외\n",
    "df = df.merge(df_f, how='left', on='opid')\n",
    "df = df.merge(df_t[['opid', 'cuffed']], how='inner', on='opid')\n",
    "\n",
    "# 중복되는 hid 경우 제외 (첫번째 수술기록만 가져오기)\n",
    "df = df.merge(df_o[['opid','hid']], how='inner', on='opid')\n",
    "df = df.loc[df[['hid', 'opid']].groupby('hid')['opid'].idxmin()]\n",
    "print(f'중복되는 hid는 첫번째 수술 외 제외: {len(df)}')\n",
    "\n",
    "\n",
    "perc = np.mean(df['cuffed'].values)\n",
    "print(f'소아 10세 미만에서 cuffed ETT 사용 비율: {perc:.3f}')\n",
    "print(f'소아 10세 미만 최종 opid수: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f82e62e-3ee3-4050-ab21-9466dcbd434b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T06:53:10.354937Z",
     "iopub.status.busy": "2023-02-17T06:53:10.354707Z",
     "iopub.status.idle": "2023-02-17T06:53:11.148175Z",
     "shell.execute_reply": "2023-02-17T06:53:11.147500Z",
     "shell.execute_reply.started": "2023-02-17T06:53:10.354913Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# age-based formula에 따른 ETT size\n",
    "OLD_VAR = 'old_tube_size'\n",
    "# df[OLD_VAR] = np.round((df['age'] / 4 + 4) * 2) / 2\n",
    "df[OLD_VAR] = df['age'].apply(lambda x: np.round((x / 4 + 4) * 2) / 2 if x >= 2 else (3.5 if x < 1 else 4)) \n",
    "df[OLD_VAR] = df.apply(lambda x: x[OLD_VAR] - 0.5 if x['cuffed'] else x[OLD_VAR], axis=1)\n",
    "\n",
    "#y_old = df[[OLD_VAR]].values.flatten().astype(float)\n",
    "#y_test_old = y[-ntest:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f23ca35-a543-4697-9ad9-d14bc15897d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:19:33.414352Z",
     "iopub.status.busy": "2023-02-17T05:19:33.413850Z",
     "iopub.status.idle": "2023-02-17T05:19:33.451649Z",
     "shell.execute_reply": "2023-02-17T05:19:33.450950Z",
     "shell.execute_reply.started": "2023-02-17T05:19:33.414296Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (27234, 5), x_test: (6808, 5)\n"
     ]
    }
   ],
   "source": [
    "SEED = 98\n",
    "INPUT_VARS = ['age_cal','sex','weight','height', 'cuffed']\n",
    "TARGET_VAR = 'airway_tube_size'\n",
    "\n",
    "random.seed(SEED)\n",
    "df = shuffle(df)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "y = df[[TARGET_VAR]].values.flatten().astype(float)\n",
    "c = df['opid'].values.flatten().astype(int)\n",
    "y_old = df[[OLD_VAR]].values.flatten().astype(float)\n",
    "x = df.loc[:, INPUT_VARS].values.astype(float)\n",
    "\n",
    "# 저장하기\n",
    "np.savez(f'dataset/ETT_size.npz', x=x, y=y, y_old=y_old, c=c)\n",
    "\n",
    "\n",
    "# training set의 뒤쪽 20%를 test set 으로 사용\n",
    "nsamp = len(y)\n",
    "ntest = int(nsamp * 0.2)\n",
    "ntrain = nsamp - ntest\n",
    "x_test = x[-ntest:, :]\n",
    "y_test = y[-ntest:]\n",
    "y_test_old = y_old[-ntest:]\n",
    "x_train = x[:ntrain, :]\n",
    "y_train = y[:ntrain]\n",
    "\n",
    "print(f'x_train: {(x_train).shape}, x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d209c7a3-ddf4-41c1-a132-3822ee72da6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T15:07:59.373429Z",
     "iopub.status.busy": "2023-02-17T15:07:59.372880Z",
     "iopub.status.idle": "2023-02-17T15:07:59.391969Z",
     "shell.execute_reply": "2023-02-17T15:07:59.391092Z",
     "shell.execute_reply.started": "2023-02-17T15:07:59.373363Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (27234, 5), x_test: (6808, 5)\n"
     ]
    }
   ],
   "source": [
    "dat = np.load(f'dataset/ETT_size.npz')\n",
    "x, y = dat['x'], dat['y']\n",
    "y_old  = dat['y_old']\n",
    "\n",
    "# training set의 뒤쪽 20%를 test set 으로 사용\n",
    "nsamp = len(y)\n",
    "ntest = int(nsamp * 0.2)\n",
    "ntrain = nsamp - ntest\n",
    "x_test = x[-ntest:, :]\n",
    "y_test = y[-ntest:]\n",
    "y_test_old = y_old[-ntest:]\n",
    "x_train = x[:ntrain, :]\n",
    "y_train = y[:ntrain]\n",
    "\n",
    "print(f'x_train: {(x_train).shape}, x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc467c2-cea5-47b9-99b7-b4a79170e409",
   "metadata": {},
   "source": [
    "# Cuffed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a4f0a97-0e8f-463f-8290-ff5771a1e8dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:29:12.793930Z",
     "iopub.status.busy": "2023-02-17T05:29:12.793434Z",
     "iopub.status.idle": "2023-02-17T05:29:12.802087Z",
     "shell.execute_reply": "2023-02-17T05:29:12.801178Z",
     "shell.execute_reply.started": "2023-02-17T05:29:12.793881Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_mask = (x_train[:,4] == 1)\n",
    "test_mask = (x_test[:,4] == 1)\n",
    "\n",
    "x_train_c = x_train[train_mask][:,0:4]\n",
    "y_train_c = y_train[train_mask]\n",
    "x_test_c = x_test[test_mask][:,0:4]\n",
    "y_test_c = y_test[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40119064-8c2e-4356-ab4b-39531734256b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:29:17.620440Z",
     "iopub.status.busy": "2023-02-17T05:29:17.619956Z",
     "iopub.status.idle": "2023-02-17T05:29:36.222744Z",
     "shell.execute_reply": "2023-02-17T05:29:36.220679Z",
     "shell.execute_reply.started": "2023-02-17T05:29:17.620391Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 80 candidates, totalling 800 fits\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.881 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.880 total time=   2.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.883 total time=   2.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.880 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.874 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.855 total time=   5.5s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.875 total time=   5.2s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.878 total time=   1.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.875 total time=   2.1s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.879 total time=   3.1s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.877 total time=   4.6s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.877 total time=   2.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.875 total time=   2.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.878 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   1.1s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.870 total time=   3.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.852 total time=   7.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.884 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.872 total time=   4.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.874 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.877 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.881 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.882 total time=   4.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.860 total time=   6.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.873 total time=   4.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.879 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.872 total time=   7.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.875 total time=   3.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.872 total time=   6.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.880 total time=   2.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.873 total time=   4.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.874 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.877 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.880 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.874 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.883 total time=   6.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.879 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.888 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.887 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.875 total time=   2.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.876 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.879 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.875 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.868 total time=   2.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.882 total time=   2.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   2.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.873 total time=   4.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.878 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.881 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.878 total time=   3.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.884 total time=   1.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.872 total time=   2.3s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.886 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.879 total time=   5.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.882 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.882 total time=   2.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.864 total time=   8.8s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.0s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.887 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   1.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.4s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.872 total time=   2.6s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.876 total time=   3.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.886 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.879 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.880 total time=   3.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.886 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.887 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.875 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.866 total time=   5.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.883 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.882 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.887 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.887 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.887 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.886 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.884 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.875 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.882 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.885 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.875 total time=   6.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.882 total time=   2.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.870 total time=   7.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.877 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.865 total time=   3.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.863 total time=  11.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.877 total time=   5.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.882 total time=   2.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   2.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.880 total time=   3.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.876 total time=   9.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.872 total time=   3.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.871 total time=   4.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   1.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.882 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.866 total time=   4.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.886 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.884 total time=   3.3s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.877 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.2s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.870 total time=   3.1s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.878 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.6s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.8s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.871 total time=   6.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.865 total time=   2.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.869 total time=   3.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.874 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.885 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.887 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.883 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.886 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.876 total time=   3.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.885 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.876 total time=   4.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.881 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.870 total time=   6.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.875 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.880 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.887 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.887 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.885 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   2.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.883 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.871 total time=   2.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.870 total time=   7.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.872 total time=   2.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.878 total time=   3.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.859 total time=   9.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.875 total time=   4.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.878 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.877 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.878 total time=   6.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.883 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.879 total time=   7.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.876 total time=   3.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.879 total time=   3.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.863 total time=   9.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.881 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.886 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.879 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.876 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   3.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.877 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.877 total time=   4.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.877 total time=   2.4s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.873 total time=   1.4s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.847 total time=   8.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.876 total time=   1.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.886 total time=   1.1s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   2.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.873 total time=   6.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.864 total time=   5.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.877 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.879 total time=   3.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.872 total time=   3.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.883 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.880 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.876 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.875 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.877 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.871 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.858 total time=   9.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.875 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.880 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.867 total time=   5.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.887 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.874 total time=   2.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.864 total time=   8.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.874 total time=   2.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.875 total time=   3.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.881 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.881 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.880 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.883 total time=   5.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.877 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.878 total time=   6.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.873 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   2.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.870 total time=   9.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.876 total time=   3.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.874 total time=   4.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.911 total time=   0.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.886 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.878 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.877 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.878 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.885 total time=   1.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.873 total time=   2.0s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.868 total time=   5.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.874 total time=   4.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.883 total time=   1.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.874 total time=   2.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.882 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.882 total time=   4.7s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.878 total time=   2.1s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.0s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.884 total time=   2.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.870 total time=   2.5s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.874 total time=   3.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.873 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.876 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.884 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   3.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.882 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.885 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.879 total time=   3.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.879 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.879 total time=   5.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.863 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.858 total time=   7.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.880 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.880 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.875 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.887 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.870 total time=   5.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.879 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.877 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.873 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.871 total time=   2.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.881 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.850 total time=  10.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.881 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.886 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.877 total time=   4.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   3.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.884 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.879 total time=   2.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.883 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   2.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.868 total time=   9.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.882 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.875 total time=   2.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.849 total time=  11.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.906 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.909 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.876 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.886 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.882 total time=   4.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.873 total time=   1.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.853 total time=   6.7s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.870 total time=   4.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.871 total time=   5.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.886 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.875 total time=   5.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.870 total time=   8.5s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.873 total time=   2.4s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.855 total time=   6.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.877 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.888 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.874 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.874 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.874 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.887 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.884 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.880 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.885 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.868 total time=   5.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.872 total time=   2.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.869 total time=   6.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.886 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.872 total time=   4.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.873 total time=   2.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.877 total time=   4.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.880 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.878 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.885 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.883 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.872 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.868 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.875 total time=   4.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.886 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.875 total time=   5.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.885 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.882 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.883 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.878 total time=   2.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.872 total time=   8.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.873 total time=   2.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.874 total time=   3.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.861 total time=   9.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.877 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.886 total time=   1.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.881 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.870 total time=   3.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.862 total time=   6.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   2.1s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.885 total time=   2.3s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.875 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.871 total time=   3.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.886 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.877 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.1s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.877 total time=   4.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.886 total time=   1.3s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.877 total time=   6.9s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.882 total time=   1.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.863 total time=   4.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.884 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.882 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.883 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   4.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.886 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.864 total time=   4.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.874 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.875 total time=   2.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.886 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.886 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.875 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.884 total time=   5.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.869 total time=   6.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.881 total time=   2.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.864 total time=   8.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.872 total time=   3.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.861 total time=  11.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.875 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.883 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.883 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.886 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.874 total time=   6.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.879 total time=   2.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.878 total time=   3.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.876 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.886 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.880 total time=   2.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.877 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.872 total time=   5.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.864 total time=   6.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.909 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.909 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.912 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.891 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.913 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.886 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.883 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.886 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.879 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.875 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.872 total time=   2.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.864 total time=   6.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.885 total time=   1.7s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.881 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.869 total time=   6.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.861 total time=   8.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.881 total time=   1.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.873 total time=   2.4s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.878 total time=   7.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.876 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.885 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.884 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.874 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.877 total time=   3.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.884 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.881 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.870 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.877 total time=   1.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.860 total time=   6.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.887 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.882 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.880 total time=   6.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.885 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.878 total time=   3.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.874 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.881 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.882 total time=   1.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.876 total time=   3.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.851 total time=  10.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.879 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.874 total time=   5.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.874 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   7.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.879 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.885 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.878 total time=   2.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.880 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.885 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.883 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.882 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.882 total time=   2.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.845 total time=   8.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.913 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.886 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   1.0s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.881 total time=   1.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.886 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.885 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.880 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.872 total time=   2.5s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.881 total time=   2.0s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.887 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.6s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.3s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.883 total time=   2.1s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.878 total time=   2.4s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.871 total time=   3.3s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.885 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.886 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.0s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.882 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.869 total time=   7.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.874 total time=   1.8s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.869 total time=   3.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.881 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.876 total time=   2.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.886 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.879 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.876 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.884 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.882 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.878 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.876 total time=   3.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.881 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.869 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.862 total time=   6.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.881 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.887 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.885 total time=   5.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   5.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.883 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.876 total time=   1.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.879 total time=   2.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.870 total time=   7.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.867 total time=   3.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.862 total time=   9.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.883 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.885 total time=   5.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.886 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   6.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.881 total time=   1.9s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.880 total time=   2.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.872 total time=   7.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   1.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.866 total time=   4.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.857 total time=  10.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.912 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.874 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.876 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.878 total time=   3.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.871 total time=   4.9s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.884 total time=   4.3s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.0s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.886 total time=   2.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.867 total time=   5.8s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.880 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.882 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.886 total time=   1.6s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.9s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.876 total time=   3.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.870 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.871 total time=   3.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.861 total time=   7.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.878 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.873 total time=   2.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.883 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.885 total time=   3.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.881 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.886 total time=   1.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.873 total time=   3.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.878 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.876 total time=   1.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.875 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.886 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.878 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.884 total time=   4.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.885 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.875 total time=   1.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.882 total time=   6.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.868 total time=   8.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.877 total time=   3.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.865 total time=  10.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.871 total time=   4.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.886 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.877 total time=   8.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   3.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.869 total time=   8.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.864 total time=   3.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.870 total time=   4.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.866 total time=   9.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.912 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.8;, score=0.912 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.876 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.900 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.908 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.890 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.907 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.886 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.880 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.878 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.876 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.875 total time=   1.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.875 total time=   1.9s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.6s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   5.0s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.885 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.875 total time=   3.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.869 total time=   3.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.846 total time=   9.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.871 total time=   6.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.877 total time=   1.0s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.868 total time=   2.6s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.873 total time=   3.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.874 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.886 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.885 total time=   2.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.882 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.877 total time=   4.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.873 total time=   4.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.882 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.879 total time=   2.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.875 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.875 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.885 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.886 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.875 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.886 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.879 total time=   6.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.871 total time=   9.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.875 total time=   2.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.845 total time=  11.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.879 total time=   5.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.877 total time=   3.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.881 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.884 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.873 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.874 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.880 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   2.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.863 total time=   7.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.873 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.879 total time=   2.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.873 total time=   3.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.862 total time=  10.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.906 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.908 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.911 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.868 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.879 total time=   3.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.875 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.876 total time=   1.8s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.877 total time=   4.0s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.877 total time=   1.9s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.880 total time=   1.9s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.877 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.876 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.865 total time=   3.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.877 total time=   4.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.884 total time=   1.9s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.877 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.866 total time=   7.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.886 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.887 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.881 total time=   3.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.870 total time=   3.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.883 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.872 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.884 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.869 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.878 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.858 total time=   6.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.883 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.876 total time=   4.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.880 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.879 total time=   5.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.874 total time=   5.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.879 total time=   2.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.854 total time=  11.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.887 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.880 total time=   2.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.871 total time=   7.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.875 total time=   3.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   8.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.868 total time=   3.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.873 total time=   4.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.909 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.910 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.906 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.900 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.878 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.900 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.910 total time=   0.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.885 total time=   1.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.875 total time=   2.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.883 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.880 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.870 total time=   4.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.866 total time=   5.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.881 total time=   1.5s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.883 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.867 total time=   7.0s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.886 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.879 total time=   3.8s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.887 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.876 total time=   5.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.872 total time=   2.2s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.872 total time=   3.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.882 total time=   2.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.881 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.879 total time=   4.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.879 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.871 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.875 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.874 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.878 total time=   2.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.849 total time=   9.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.882 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.881 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.875 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.882 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.881 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.886 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.882 total time=   2.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.868 total time=   7.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.881 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.870 total time=   3.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.862 total time=  10.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.875 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.880 total time=   4.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.880 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.886 total time=   2.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.879 total time=   6.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.885 total time=   2.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.863 total time=   9.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   2.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.867 total time=   3.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.850 total time=  10.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.894 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.905 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.886 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.911 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.907 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.883 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.886 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.910 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.911 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.906 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.876 total time=   2.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.886 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.881 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.873 total time=   3.6s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.877 total time=   2.1s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.878 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.886 total time=   1.4s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   2.1s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.875 total time=   6.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.887 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.881 total time=   2.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.879 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.876 total time=   5.4s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.873 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.880 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.873 total time=   2.5s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.879 total time=   3.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.874 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.878 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.887 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.879 total time=   2.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.880 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.887 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.887 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.874 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.873 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.886 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.882 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.872 total time=   5.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.866 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.870 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.873 total time=   5.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.887 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.878 total time=   5.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.878 total time=   2.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.880 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.885 total time=   2.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.864 total time=   7.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.872 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.871 total time=   4.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.867 total time=   8.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.880 total time=   6.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.879 total time=   2.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.873 total time=   6.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.872 total time=   2.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.857 total time=   8.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.871 total time=   2.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.870 total time=   4.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.913 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.910 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.911 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.910 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.899 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.884 total time=   3.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.885 total time=   1.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.877 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.874 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.874 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.854 total time=   5.3s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.881 total time=   1.1s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.883 total time=   4.3s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.880 total time=   5.4s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.865 total time=   7.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.881 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.876 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.869 total time=   7.0s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.855 total time=   6.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.883 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.885 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.875 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.879 total time=   3.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.874 total time=   4.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.887 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.873 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.885 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.874 total time=   5.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.876 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.867 total time=   6.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.881 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.878 total time=   3.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.884 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.886 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.875 total time=   5.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.881 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.886 total time=   2.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.881 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.875 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.876 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.879 total time=   2.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.870 total time=   9.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.885 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.884 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.881 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.879 total time=   2.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.877 total time=   6.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.884 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   1.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.879 total time=   2.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.873 total time=   8.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.875 total time=   3.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.845 total time=  10.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.916 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.892 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.907 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.906 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.912 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.909 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.914 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.909 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.904 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.879 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.886 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.877 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.877 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.877 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.869 total time=   3.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.869 total time=   5.3s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.885 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.879 total time=   4.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.876 total time=   2.1s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.870 total time=   4.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.885 total time=   3.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.875 total time=   1.9s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.877 total time=   1.6s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.866 total time=   7.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.878 total time=   3.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.882 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.875 total time=   4.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.872 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   4.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.875 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.872 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.869 total time=   5.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.876 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.886 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   5.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.882 total time=   2.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.883 total time=   4.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.883 total time=   2.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.879 total time=   2.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.882 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.870 total time=   2.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.865 total time=   4.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.887 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.886 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.885 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.886 total time=   4.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.884 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.883 total time=   6.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   2.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.865 total time=   9.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.870 total time=   3.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.881 total time=   4.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.910 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.904 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.916 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.909 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.913 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.875 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.902 total time=   1.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.875 total time=   3.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.877 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.873 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.872 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.885 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.0s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.0s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.887 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.887 total time=   1.4s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.887 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   2.4s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.869 total time=   8.9s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.876 total time=   4.5s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.876 total time=   1.9s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.882 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.882 total time=   3.2s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.868 total time=   8.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.883 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.877 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.877 total time=   3.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.882 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   3.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.874 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.882 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.869 total time=   5.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.871 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.880 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.868 total time=   6.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.875 total time=   4.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.875 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.877 total time=   2.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.882 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.886 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.881 total time=   2.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.879 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.881 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.882 total time=   1.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.868 total time=   2.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.868 total time=   3.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.868 total time=   9.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.881 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.875 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.874 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.874 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.873 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.872 total time=   7.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.868 total time=   3.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.875 total time=   8.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.871 total time=   2.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.864 total time=   3.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.855 total time=   9.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.909 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.904 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.5;, score=0.877 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.5;, score=0.899 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=200, subsample=0.8;, score=0.898 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=200, subsample=0.8;, score=0.896 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   2.1s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.881 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.878 total time=   1.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.880 total time=   3.5s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.877 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.885 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.876 total time=   3.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   5.1s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.881 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.875 total time=   2.1s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.871 total time=   2.6s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.881 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.877 total time=   1.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.887 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.878 total time=   1.1s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.875 total time=   2.3s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.871 total time=   6.3s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.866 total time=   5.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.876 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.887 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.873 total time=   3.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.878 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.882 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.876 total time=   4.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.871 total time=   1.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.877 total time=   4.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.875 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.857 total time=   6.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.880 total time=   5.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.875 total time=   1.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.878 total time=   6.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   2.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.877 total time=   2.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.871 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.873 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.878 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.876 total time=   1.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.875 total time=   1.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.875 total time=   2.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.849 total time=  10.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.886 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.877 total time=   5.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.884 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.872 total time=   2.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.873 total time=   7.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.877 total time=   2.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.867 total time=  10.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.873 total time=   2.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.852 total time=  11.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.897 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.908 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.903 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.901 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.883 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.909 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.889 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.862 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.886 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.886 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.877 total time=   2.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.885 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.875 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.861 total time=   6.8s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.872 total time=   5.5s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.873 total time=   2.1s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.873 total time=   2.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.880 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.876 total time=   4.3s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.7s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.880 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.862 total time=   8.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.876 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.883 total time=   3.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.884 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.883 total time=   4.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.882 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.874 total time=   5.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.874 total time=   2.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.871 total time=   7.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.873 total time=   3.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.886 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.872 total time=   8.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.883 total time=   2.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.885 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.886 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.873 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.870 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.873 total time=   3.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.875 total time=   3.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.880 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.877 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.887 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.884 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.888 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.873 total time=   4.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.887 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.883 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.881 total time=   3.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.863 total time=   9.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.876 total time=   3.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.851 total time=  12.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.891 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.901 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.5;, score=0.902 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.5;, score=0.899 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.881 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.906 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.909 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.899 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.875 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.878 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.882 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.878 total time=   2.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.883 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.858 total time=   5.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.886 total time=   1.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.884 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.876 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.872 total time=   1.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.873 total time=   2.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.847 total time=   8.5s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.882 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.873 total time=   6.1s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.877 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.868 total time=   3.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.843 total time=   9.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.875 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.884 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.878 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.880 total time=   3.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.874 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.878 total time=   4.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.868 total time=   5.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.872 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.872 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.874 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.881 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.885 total time=   3.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.882 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.873 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.876 total time=   5.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.873 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.884 total time=   2.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.869 total time=   8.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.863 total time=   3.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.864 total time=  10.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.878 total time=   5.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.877 total time=   6.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.876 total time=   2.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.866 total time=   9.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.875 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.872 total time=   5.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.858 total time=   9.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.912 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.904 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.910 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.887 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.897 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.894 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.910 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.893 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.870 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.911 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.906 total time=   0.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.886 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.879 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.875 total time=   4.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.870 total time=   1.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.886 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.881 total time=   3.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.874 total time=   5.4s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.866 total time=   2.8s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.863 total time=   9.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.881 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.880 total time=   1.0s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.868 total time=   8.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.883 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.885 total time=   3.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.886 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.884 total time=   3.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.878 total time=   4.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.861 total time=   6.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.877 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.876 total time=   3.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.878 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.877 total time=   5.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   3.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.878 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.876 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.878 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   3.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.853 total time=  10.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.887 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.879 total time=   4.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.888 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.879 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.866 total time=   7.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.886 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   2.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   7.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.868 total time=   2.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.878 total time=   3.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.870 total time=   8.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.881 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.915 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.910 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.884 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.911 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.898 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.907 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.887 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.868 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.891 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.908 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.910 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.892 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.910 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.911 total time=   0.2s[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.886 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.879 total time=   4.3s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.882 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.874 total time=   3.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   3.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.877 total time=   2.3s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.873 total time=   2.7s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.881 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.880 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.876 total time=   3.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.881 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.874 total time=   6.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.872 total time=   2.3s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.843 total time=   7.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.881 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.876 total time=   4.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.881 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.881 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.873 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.868 total time=   4.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.877 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.877 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.867 total time=   5.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.886 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.882 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.883 total time=   6.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.879 total time=   2.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.875 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.886 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.873 total time=   2.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.884 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.868 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.876 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.874 total time=   3.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.862 total time=  10.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.876 total time=   4.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.887 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.875 total time=   6.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.875 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.876 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.875 total time=   7.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.870 total time=   2.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.868 total time=   4.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.860 total time=  11.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.909 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.904 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.8;, score=0.903 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.8;, score=0.902 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.892 total time=   1.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.878 total time=   2.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.877 total time=   4.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.877 total time=   1.6s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.881 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.876 total time=   3.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.880 total time=   1.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.886 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.877 total time=   1.4s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.864 total time=   6.8s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.884 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.879 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.883 total time=   5.5s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.880 total time=   6.5s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.880 total time=   2.0s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.865 total time=   3.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.875 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.880 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.878 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.878 total time=   3.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.879 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.877 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.871 total time=   4.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.871 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.872 total time=   2.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.862 total time=   6.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.884 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.877 total time=   4.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.880 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.881 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.882 total time=   6.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   2.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.869 total time=   7.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.877 total time=   2.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.870 total time=   5.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.887 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.881 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.881 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.886 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.884 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.887 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.884 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   4.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.881 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.884 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.883 total time=   2.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.875 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.887 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.873 total time=   2.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.866 total time=  10.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   3.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.855 total time=  12.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.908 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.885 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.8;, score=0.872 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.8;, score=0.894 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.894 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.871 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.904 total time=   0.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.877 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.871 total time=   3.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.885 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.877 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.869 total time=   1.9s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.882 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.874 total time=   4.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   2.1s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.872 total time=   6.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.886 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.876 total time=   5.6s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.872 total time=   7.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.872 total time=   2.6s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.840 total time=   7.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.876 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.887 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.875 total time=   3.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.871 total time=   3.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.884 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.875 total time=   4.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.876 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.862 total time=   5.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.881 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.881 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.880 total time=   4.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.878 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.885 total time=   2.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.875 total time=   4.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.873 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.880 total time=   2.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.879 total time=   7.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.878 total time=   3.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.852 total time=  10.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.879 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.884 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.888 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.888 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.886 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.886 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.876 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.878 total time=   6.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.884 total time=   3.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.876 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.872 total time=   2.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.875 total time=   1.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.879 total time=   3.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.875 total time=   3.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.867 total time=   9.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.910 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.8;, score=0.908 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.8;, score=0.906 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.8;, score=0.901 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.889 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.887 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.883 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.881 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.876 total time=   2.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.869 total time=   5.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.882 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.876 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.873 total time=   5.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.886 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.875 total time=   6.2s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   1.1s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.876 total time=   3.0s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.873 total time=   7.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   3.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.876 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.874 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.879 total time=   3.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.880 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   5.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.877 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.849 total time=   6.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.884 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.878 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.874 total time=   4.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.888 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.879 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.879 total time=   2.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.888 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.882 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.878 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.877 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.877 total time=   7.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.874 total time=   2.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.873 total time=   3.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.861 total time=   9.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.882 total time=   5.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.880 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.875 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.884 total time=   6.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.883 total time=   2.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.872 total time=   7.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.875 total time=   3.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.872 total time=   4.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.912 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.915 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.911 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.893 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.903 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.877 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.896 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.903 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=200, subsample=0.5;, score=0.882 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=200, subsample=0.5;, score=0.874 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.879 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.877 total time=   1.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.875 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.882 total time=   1.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.886 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.886 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.882 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.878 total time=   1.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   2.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.885 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.870 total time=   3.2s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.857 total time=   9.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.874 total time=   5.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.875 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.875 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.866 total time=   3.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.884 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.886 total time=   2.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.874 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.882 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.873 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.880 total time=   4.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.883 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.881 total time=   4.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.875 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.862 total time=   6.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.884 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.888 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.882 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   5.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.875 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.878 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.875 total time=   7.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.870 total time=   3.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.856 total time=  11.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.883 total time=   4.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.883 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.879 total time=   6.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.886 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   5.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.881 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.881 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.876 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.878 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.866 total time=   3.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.870 total time=   3.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.868 total time=   6.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.911 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.908 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.904 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=200, subsample=0.5;, score=0.871 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=200, subsample=0.5;, score=0.890 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.885 total time=   2.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.884 total time=   2.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.876 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.885 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.874 total time=   4.0s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.875 total time=   4.9s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.881 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   1.1s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.874 total time=   4.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.873 total time=   1.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.865 total time=   3.8s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.879 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.884 total time=   1.9s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.887 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.881 total time=   2.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.886 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.885 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.870 total time=   7.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.875 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.886 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.881 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.887 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.884 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.885 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.885 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.881 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.876 total time=   3.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.880 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.870 total time=   6.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.869 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.869 total time=   5.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.886 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.882 total time=   4.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.875 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.875 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.885 total time=   2.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.863 total time=   8.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.877 total time=   2.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.843 total time=  11.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.874 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   5.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.886 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   7.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.879 total time=   2.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   3.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.871 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.873 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.877 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.872 total time=   2.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.879 total time=   1.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   2.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.848 total time=  11.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.912 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.883 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.907 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.892 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.890 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.895 total time=   1.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.886 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.877 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.877 total time=   2.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.875 total time=   2.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.873 total time=   4.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.886 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.876 total time=   4.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.882 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.876 total time=   2.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.880 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.872 total time=   2.1s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.875 total time=   3.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.878 total time=   1.6s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.878 total time=   4.7s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.871 total time=   7.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.874 total time=   3.5s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.861 total time=   7.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.877 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.887 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.881 total time=   3.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.886 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.877 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.880 total time=   6.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.874 total time=   5.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.882 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.859 total time=   8.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.880 total time=   6.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.874 total time=   5.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.885 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.874 total time=   2.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.881 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.874 total time=   6.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.866 total time=   2.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.874 total time=   3.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.868 total time=   9.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.879 total time=   4.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.886 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.887 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.876 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.874 total time=   5.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.882 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.875 total time=   3.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.880 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.874 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.885 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.871 total time=   2.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.883 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.873 total time=   4.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.855 total time=  10.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.916 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.8;, score=0.905 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.875 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.901 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.906 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.888 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.905 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.904 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.901 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.903 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=200, subsample=0.5;, score=0.882 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=200, subsample=0.5;, score=0.875 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=200, subsample=0.8;, score=0.900 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.906 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.884 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.877 total time=   1.1s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   2.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   5.0s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.7s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.877 total time=   4.8s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.865 total time=   2.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.869 total time=   9.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.882 total time=   3.0s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.884 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.878 total time=   2.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.871 total time=   3.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.872 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.874 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.885 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.878 total time=   2.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.877 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.880 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.880 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.879 total time=   2.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.872 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.876 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.875 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.874 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.877 total time=   5.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.887 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.876 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.878 total time=   4.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.884 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.874 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.870 total time=   6.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.878 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.876 total time=   7.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.873 total time=   2.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.873 total time=   3.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.874 total time=   9.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.871 total time=   6.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.888 total time=   1.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.875 total time=   7.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.877 total time=   2.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.871 total time=   8.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.880 total time=   2.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.872 total time=   4.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.915 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.907 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.891 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.867 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.891 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.845 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.874 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.876 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.891 total time=   1.9s\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'colsample_bytree': 0.8, 'max_depth': 3, 'n_estimators': 25, 'subsample': 0.8}\n",
      "0.9040845258641736\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.894\n",
      "mean_squared_errors: 0.117\n",
      "r2_score: 0.894\n",
      "acc: 0.677\n",
      "acc(+-0.5mm): 0.960\n"
     ]
    }
   ],
   "source": [
    "# age (일단위)\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.05, 0.07, 0.1], #[0.01, 0.03, 0.05],\n",
    "                'max_depth': [3, 4, 5, 7],#[3,4,5],\n",
    "                'n_estimators': [25, 50, 100, 200, 300],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                'subsample': [0.5, 0.8], #[0.5, 0.8, 1],\n",
    "                'colsample_bytree': [0.5, 0.8], #[0.8, 1],\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=xgb.sklearn.XGBRegressor(),\n",
    "                    n_jobs=-1,\n",
    "                    verbose=3,\n",
    "                    param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train_c, y_train_c)\n",
    "model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = gs.predict(x_test_c).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test_c, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test_c, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test_c, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test_c)\n",
    "acc3 = np.mean((y_pred >= y_test_c-0.5) & (y_pred <= y_test_c+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e18b60e-2b2c-4457-a169-dc3a784e4f69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:30:20.138452Z",
     "iopub.status.busy": "2023-02-17T05:30:20.137951Z",
     "iopub.status.idle": "2023-02-17T05:30:20.149537Z",
     "shell.execute_reply": "2023-02-17T05:30:20.148807Z",
     "shell.execute_reply.started": "2023-02-17T05:30:20.138401Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "odir_f = f'acc1-{acc1:.3f}_acc3-{acc3:.3f}_XGBR-cuffed_{nfold}fold'\n",
    "odir = f'result/{odir_f}'\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "model.save_model(f'{odir}/model.model')\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "pickle.dump(param_dict, open(f'{odir}/param_dict', 'wb'))\n",
    "f = open(f'{odir}/result.txt', 'w')\n",
    "f.write(f'classification model')\n",
    "f.write(f'explained_variance_score: {explained_variance_score(y_test_c, y_pred):.3f}')\n",
    "f.write(f'mean_squared_errors: {mean_squared_error(y_test_c, y_pred):.3f}')\n",
    "f.write(f'mean_absolute_errors: {mean_absolute_error(y_test_c, y_pred):.3f}')\n",
    "f.write(f'r2_score: {r2_score(y_test_c, y_pred):.3f}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864a18f-df29-45b3-94e7-eed9558dc149",
   "metadata": {},
   "source": [
    "# Uncuffed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07e91b57-c0b6-458e-8a79-c65429d60086",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:30:28.162814Z",
     "iopub.status.busy": "2023-02-17T05:30:28.162324Z",
     "iopub.status.idle": "2023-02-17T05:30:28.171147Z",
     "shell.execute_reply": "2023-02-17T05:30:28.170242Z",
     "shell.execute_reply.started": "2023-02-17T05:30:28.162763Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_mask = (x_train[:,4] == 0)\n",
    "test_mask = (x_test[:,4] == 0)\n",
    "\n",
    "x_train_c = x_train[train_mask][:,0:4]\n",
    "y_train_c = y_train[train_mask]\n",
    "x_test_c = x_test[test_mask][:,0:4]\n",
    "y_test_c = y_test[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37ac67de-4920-4e99-8881-e1e0220b86a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:30:34.615613Z",
     "iopub.status.busy": "2023-02-17T05:30:34.615111Z",
     "iopub.status.idle": "2023-02-17T05:30:53.447084Z",
     "shell.execute_reply": "2023-02-17T05:30:53.446180Z",
     "shell.execute_reply.started": "2023-02-17T05:30:34.615563Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.877 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.875 total time=   2.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.873 total time=   1.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.866 total time=   4.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.885 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.871 total time=   6.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.873 total time=   3.0s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.858 total time=  11.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.862 total time=   7.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.859 total time=   5.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.885 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.882 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.880 total time=   2.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.883 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.885 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.883 total time=   4.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.874 total time=   4.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.875 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.868 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.863 total time=   6.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.875 total time=   4.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   6.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.882 total time=   2.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.864 total time=   7.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.883 total time=   1.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.872 total time=   4.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.859 total time=   9.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.881 total time=   2.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.880 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.883 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.881 total time=   2.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   6.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.882 total time=   2.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.878 total time=   2.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.869 total time=   7.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.872 total time=   2.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.877 total time=   4.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.868 total time=   9.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.906 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.8;, score=0.905 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.880 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.906 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.908 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=200, subsample=0.8;, score=0.871 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=200, subsample=0.8;, score=0.900 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.853 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.873 total time=   2.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.896 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=200, subsample=0.8;, score=0.901 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.906 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=200, subsample=0.5;, score=0.883 total time=   2.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.869 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.869 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.886 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.875 total time=   3.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.874 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.878 total time=   1.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.860 total time=   7.4s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.877 total time=   1.4s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.866 total time=   6.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.865 total time=  10.0s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.883 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.881 total time=   2.6s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.869 total time=   8.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.873 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.880 total time=   3.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.879 total time=   3.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.883 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.883 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.871 total time=   5.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.870 total time=   2.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.874 total time=   6.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.880 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.887 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.882 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.885 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   6.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.878 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.861 total time=   7.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.880 total time=   2.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.871 total time=   3.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.866 total time=   9.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.884 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.879 total time=   2.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.869 total time=   6.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.872 total time=   2.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.868 total time=   7.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.875 total time=   2.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.865 total time=   3.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.856 total time=   7.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.911 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.902 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.883 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.862 total time=   1.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.880 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.908 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=200, subsample=0.5;, score=0.902 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.907 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.904 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=200, subsample=0.8;, score=0.880 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.890 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=200, subsample=0.5;, score=0.888 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.893 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.881 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.863 total time=   2.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.8;, score=0.868 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.868 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   2.1s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.880 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   3.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.871 total time=   3.6s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.876 total time=   3.3s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.871 total time=   5.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.876 total time=   1.4s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.874 total time=   1.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.851 total time=   9.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.883 total time=   5.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.878 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.875 total time=   2.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.867 total time=   4.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.875 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.884 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.874 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.886 total time=   3.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.874 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.883 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   3.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.872 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.881 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.881 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.883 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.876 total time=   2.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.864 total time=   5.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.887 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.884 total time=   4.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   6.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   2.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.869 total time=   6.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.879 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.868 total time=   4.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.857 total time=  10.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.877 total time=   6.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   6.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.885 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.884 total time=   2.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.884 total time=   2.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.864 total time=   8.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.869 total time=   3.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.836 total time=  12.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.914 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.8;, score=0.882 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.5;, score=0.874 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.5;, score=0.894 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.890 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.892 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.906 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.912 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.914 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=200, subsample=0.5;, score=0.887 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.897 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=200, subsample=0.5;, score=0.897 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.870 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.862 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=200, subsample=0.5;, score=0.877 total time=   1.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.866 total time=   2.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.867 total time=   1.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.866 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.859 total time=   0.7s\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'colsample_bytree': 0.8, 'max_depth': 4, 'n_estimators': 25, 'subsample': 0.8}\n",
      "0.8699761780592447\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.849\n",
      "mean_squared_errors: 0.142\n",
      "r2_score: 0.849\n",
      "acc: 0.572\n",
      "acc(+-0.5mm): 0.968\n"
     ]
    }
   ],
   "source": [
    "# age (일단위)\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.05, 0.07, 0.1], #[0.01, 0.03, 0.05],\n",
    "                'max_depth': [3, 4, 5, 7],#[3,4,5],\n",
    "                'n_estimators': [25, 50, 100, 200, 300],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                'subsample': [0.5, 0.8], #[0.5, 0.8, 1],\n",
    "                'colsample_bytree': [0.5, 0.8], #[0.8, 1],\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 5\n",
    "gs = GridSearchCV(estimator=xgb.sklearn.XGBRegressor(),\n",
    "                    n_jobs=-1,\n",
    "                    verbose=3,\n",
    "                    param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train_c, y_train_c)\n",
    "model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = gs.predict(x_test_c).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test_c, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test_c, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test_c, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test_c)\n",
    "acc3 = np.mean((y_pred >= y_test_c-0.5) & (y_pred <= y_test_c+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1a8ecb7-1e6c-4ad3-ba1c-61125b87f9fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:31:00.670552Z",
     "iopub.status.busy": "2023-02-17T05:31:00.670178Z",
     "iopub.status.idle": "2023-02-17T05:31:00.680754Z",
     "shell.execute_reply": "2023-02-17T05:31:00.679982Z",
     "shell.execute_reply.started": "2023-02-17T05:31:00.670516Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "odir_f = f'acc1-{acc1:.3f}_acc3-{acc3:.3f}_XGBR-uncuffed_{nfold}fold'\n",
    "odir = f'result/{odir_f}'\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "model.save_model(f'{odir}/model.model')\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "pickle.dump(param_dict, open(f'{odir}/param_dict', 'wb'))\n",
    "f = open(f'{odir}/result.txt', 'w')\n",
    "f.write(f'classification model')\n",
    "f.write(f'explained_variance_score: {explained_variance_score(y_test_c, y_pred):.3f}')\n",
    "f.write(f'mean_squared_errors: {mean_squared_error(y_test_c, y_pred):.3f}')\n",
    "f.write(f'mean_absolute_errors: {mean_absolute_error(y_test_c, y_pred):.3f}')\n",
    "f.write(f'r2_score: {r2_score(y_test_c, y_pred):.3f}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80084585-3ed7-4b97-9b18-0e7cb53b9d74",
   "metadata": {},
   "source": [
    "# Cuffed + Uncuffed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae122fa-efde-4758-86b9-db8e8195973b",
   "metadata": {},
   "source": [
    "## Age-based (Cole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36e07937-5aa6-4936-a6d6-75dbb8e69172",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:23:40.608237Z",
     "iopub.status.busy": "2023-02-17T05:23:40.607731Z",
     "iopub.status.idle": "2023-02-17T05:23:40.621993Z",
     "shell.execute_reply": "2023-02-17T05:23:40.621252Z",
     "shell.execute_reply.started": "2023-02-17T05:23:40.608185Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model = age/4+4\n",
      "--------------\n",
      "explained_variance_score: 0.819\n",
      "mean_squared_errors: 0.272\n",
      "mean_absolute_errors: 0.395\n",
      "r2_score: 0.725\n",
      "acc: 0.344\n",
      "acc(+-0.5mm): 0.878\n"
     ]
    }
   ],
   "source": [
    "print('--------------')\n",
    "print('old model = age/4+4')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_test_old):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_test_old):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_test_old):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_test_old):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_test_old==y_test)\n",
    "acc3 = np.mean((y_test_old >= y_test-0.5) & (y_test_old <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c472067-3191-445f-878a-32d9d0415341",
   "metadata": {},
   "source": [
    "## XGBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "422e3b07-64c9-41aa-95de-35f9e941c852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:25:34.779719Z",
     "iopub.status.busy": "2023-02-17T05:25:34.779215Z",
     "iopub.status.idle": "2023-02-17T05:27:30.093272Z",
     "shell.execute_reply": "2023-02-17T05:27:30.092544Z",
     "shell.execute_reply.started": "2023-02-17T05:25:34.779668Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 180 candidates, totalling 1800 fits\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.878 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.877 total time=   2.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.875 total time=   2.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.873 total time=   2.1s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.882 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.878 total time=   4.8s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.874 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.881 total time=   2.0s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.851 total time=   8.6s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.883 total time=   1.9s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.876 total time=   2.2s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.876 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.880 total time=   2.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.850 total time=   6.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.875 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.882 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.880 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.881 total time=   2.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.882 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.879 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.878 total time=   3.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.875 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.886 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.874 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.876 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.877 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.874 total time=   2.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.885 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.877 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.886 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.884 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.884 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.882 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.882 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.879 total time=   3.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.885 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.887 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.867 total time=   6.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.876 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.880 total time=   7.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.874 total time=   2.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.876 total time=   3.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.875 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.874 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.875 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.875 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.886 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.879 total time=   2.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.876 total time=   1.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.875 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.873 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.881 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.882 total time=   1.3s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.873 total time=   5.4s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.875 total time=   2.4s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.859 total time=   8.8s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.878 total time=   1.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.879 total time=   3.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.872 total time=   2.9s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.870 total time=   3.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.883 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.879 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.875 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.876 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.877 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.877 total time=   3.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.880 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.876 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.878 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.876 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.873 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.882 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.870 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.876 total time=   2.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.881 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.875 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.880 total time=   4.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.887 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.884 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.872 total time=   2.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.873 total time=   7.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.882 total time=   3.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.866 total time=   9.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.877 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.880 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.875 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.877 total time=   1.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.880 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.878 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.875 total time=   3.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.868 total time=   4.8s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.882 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.880 total time=   1.7s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.8s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.879 total time=   1.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.869 total time=   2.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.873 total time=   2.7s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.5s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   4.9s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.878 total time=   2.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.874 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.880 total time=   2.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   1.6s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.861 total time=   3.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.875 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.886 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.884 total time=   4.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.876 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.881 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.879 total time=   5.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.882 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.882 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.868 total time=   6.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.875 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.880 total time=   4.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.880 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.881 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.875 total time=   6.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.884 total time=   2.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.867 total time=   7.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.875 total time=   2.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.872 total time=   3.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.874 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.881 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.875 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.885 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.886 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.882 total time=   2.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.875 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.876 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.877 total time=   2.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.875 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.884 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   3.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.869 total time=   1.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.881 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.877 total time=   5.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.875 total time=   2.2s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.869 total time=   7.4s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   1.9s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.883 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.882 total time=   1.3s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.1s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.875 total time=   5.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.874 total time=   1.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   2.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.857 total time=   6.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.874 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.882 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.877 total time=   3.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.876 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.885 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.877 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.885 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.878 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.878 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.875 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.883 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.874 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.874 total time=   2.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.885 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.880 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.875 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.875 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.886 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.881 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.877 total time=   5.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.881 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.883 total time=   1.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.883 total time=   5.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   7.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.875 total time=   2.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.879 total time=   3.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.885 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.885 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.885 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.887 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.886 total time=   2.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.875 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.876 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   6.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.878 total time=   2.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.879 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.881 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.877 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.878 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.881 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.872 total time=   3.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.875 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.871 total time=   1.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.885 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.882 total time=   2.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.3s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.878 total time=   2.4s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.862 total time=   8.6s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.886 total time=   1.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.881 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   1.9s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.878 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.879 total time=   3.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.874 total time=   3.0s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.869 total time=   3.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.875 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.884 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.881 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.885 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.884 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.877 total time=   3.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.883 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.878 total time=   3.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.884 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.877 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.872 total time=   6.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.879 total time=   5.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.886 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.886 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.874 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.874 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.879 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.878 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.871 total time=   7.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.883 total time=   2.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.857 total time=  11.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.887 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.888 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.886 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.874 total time=   2.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.887 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.886 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.881 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.880 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.876 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.876 total time=   2.3s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.878 total time=   2.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   6.6s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.874 total time=   2.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.857 total time=   8.0s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.7s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.879 total time=   1.0s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.884 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.876 total time=   2.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   1.5s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.845 total time=   7.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.887 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.880 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.883 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.876 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.884 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.886 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.878 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.884 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.883 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.871 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   4.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.873 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.869 total time=   1.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.870 total time=   6.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.888 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.885 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.874 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.881 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.871 total time=   7.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.875 total time=   3.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.858 total time=  12.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.878 total time=   4.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.886 total time=   2.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.880 total time=   7.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.871 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.881 total time=   2.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.871 total time=   1.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.881 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.885 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.885 total time=   2.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.876 total time=   2.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.875 total time=   1.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.7s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.886 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.882 total time=   5.2s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.878 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.876 total time=   2.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.870 total time=   5.5s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.882 total time=   1.0s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.879 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.887 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.886 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.883 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.882 total time=   1.1s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   1.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.862 total time=   7.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.884 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.875 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.879 total time=   3.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.877 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.875 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   4.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   1.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.871 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.875 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.875 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.880 total time=   2.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.883 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.881 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.875 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.887 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.877 total time=   5.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.880 total time=   1.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   5.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.871 total time=   2.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.871 total time=   7.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.884 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.861 total time=   3.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.848 total time=   9.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.885 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.880 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.880 total time=   5.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   3.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.887 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.886 total time=   1.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   3.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.883 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.886 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.884 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.886 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.876 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.886 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.879 total time=   1.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.876 total time=   2.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.860 total time=   5.0s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.884 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.878 total time=   1.0s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.879 total time=   3.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.868 total time=   3.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.856 total time=  11.8s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.873 total time=   2.4s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.871 total time=   6.4s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.861 total time=   5.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.878 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.879 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.877 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.876 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.877 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.874 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.869 total time=   4.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.882 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.874 total time=   4.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.876 total time=   2.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.874 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.877 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.878 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   4.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.886 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.881 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.877 total time=   4.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.887 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.873 total time=   7.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.865 total time=   2.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.875 total time=   4.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.883 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.881 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.885 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.882 total time=   4.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.880 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.872 total time=   7.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.877 total time=   2.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.885 total time=   2.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.883 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   1.7s\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'colsample_bytree': 1, 'max_depth': 5, 'n_estimators': 25, 'subsample': 1}\n",
      "0.8818542105115561\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.864\n",
      "mean_squared_errors: 0.134\n",
      "mean_absolute_errors: 0.219\n",
      "r2_score: 0.864\n",
      "acc: 0.601\n",
      "acc(+-0.5mm): 0.966\n"
     ]
    }
   ],
   "source": [
    "# age (일단위)\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.05, 0.07, 0.1], #[0.01, 0.03, 0.05],\n",
    "                'max_depth': [3, 4, 5, 7],#[3,4,5],\n",
    "                'n_estimators': [25, 50, 75, 100, 300],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                'subsample': [0.5, 0.8, 1], #[0.5, 0.8, 1],\n",
    "                'colsample_bytree': [0.5, 0.8, 1], #[0.8, 1],\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=xgb.sklearn.XGBRegressor(),\n",
    "                n_jobs=-1,\n",
    "                verbose=3,\n",
    "                param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train, y_train)\n",
    "model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = gs.predict(x_test).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test)\n",
    "acc3 = np.mean((y_pred >= y_test-0.5) & (y_pred <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25839b01-4685-45e4-bed2-ac5e16ba8fc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:28:12.934052Z",
     "iopub.status.busy": "2023-02-17T05:28:12.933554Z",
     "iopub.status.idle": "2023-02-17T05:28:12.947264Z",
     "shell.execute_reply": "2023-02-17T05:28:12.946591Z",
     "shell.execute_reply.started": "2023-02-17T05:28:12.934003Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "odir_f = f'acc1-{acc1:.3f}_acc3-{acc3:.3f}_XGBR_{nfold}fold'\n",
    "odir = f'result/{odir_f}'\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "model.save_model(f'{odir}/model.model')\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "pickle.dump(param_dict, open(f'{odir}/param_dict', 'wb'))\n",
    "f = open(f'{odir}/result.txt', 'w')\n",
    "f.write(f'classification model')\n",
    "f.write(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "f.write(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48966d1a-2ea9-437e-adb6-bb9847a34295",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27cf56d6-429a-426b-8fa4-91934f6b6bea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:32:36.721507Z",
     "iopub.status.busy": "2023-02-17T05:32:36.721129Z",
     "iopub.status.idle": "2023-02-17T05:32:36.908119Z",
     "shell.execute_reply": "2023-02-17T05:32:36.907414Z",
     "shell.execute_reply.started": "2023-02-17T05:32:36.721470Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imp = IterativeImputer().fit(x_train)\n",
    "x_train_imputed = imp.transform(x_train)\n",
    "x_test_imputed = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a245e14c-23ee-489c-968e-acb576844941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:34:44.830061Z",
     "iopub.status.busy": "2023-02-17T05:34:44.829565Z",
     "iopub.status.idle": "2023-02-17T05:37:31.767454Z",
     "shell.execute_reply": "2023-02-17T05:37:31.766883Z",
     "shell.execute_reply.started": "2023-02-17T05:34:44.830006Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 216 candidates, totalling 2160 fits\n",
      "[CV 4/10] END bootstrap=True, max_depth=3, max_features=auto, max_leaf_nodes=3, n_estimators=300;, score=0.734 total time=   5.0s\n",
      "[CV 6/10] END bootstrap=True, max_depth=3, max_features=auto, max_leaf_nodes=7, n_estimators=100;, score=0.837 total time=   2.0s\n",
      "[CV 3/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.735 total time=   0.5s\n",
      "[CV 1/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.744 total time=   0.9s\n",
      "[CV 10/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.752 total time=   1.3s\n",
      "[CV 4/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=3, n_estimators=300;, score=0.754 total time=   4.2s\n",
      "[CV 1/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=7, n_estimators=300;, score=0.839 total time=   3.4s\n",
      "[CV 8/10] END bootstrap=True, max_depth=3, max_features=log2, max_leaf_nodes=5, n_estimators=100;, score=0.820 total time=   1.2s\n",
      "[CV 4/10] END bootstrap=True, max_depth=3, max_features=log2, max_leaf_nodes=7, n_estimators=50;, score=0.851 total time=   0.5s\n",
      "[CV 6/10] END bootstrap=True, max_depth=3, max_features=log2, max_leaf_nodes=7, n_estimators=100;, score=0.848 total time=   1.3s\n",
      "[CV 5/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=3, n_estimators=50;, score=0.740 total time=   0.9s\n",
      "[CV 3/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=3, n_estimators=100;, score=0.719 total time=   1.7s\n",
      "[CV 1/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=5, n_estimators=50;, score=0.814 total time=   1.1s\n",
      "[CV 7/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=5, n_estimators=100;, score=0.819 total time=   2.3s\n",
      "[CV 6/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=7, n_estimators=50;, score=0.835 total time=   1.5s\n",
      "[CV 10/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=7, n_estimators=100;, score=0.838 total time=   2.6s\n",
      "[CV 10/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.742 total time=   0.5s\n",
      "[CV 9/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.760 total time=   1.8s\n",
      "[CV 6/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=300;, score=0.745 total time=   4.0s\n",
      "[CV 6/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=7, n_estimators=100;, score=0.848 total time=   1.3s\n",
      "[CV 10/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=7, n_estimators=300;, score=0.844 total time=   4.5s\n",
      "[CV 7/10] END bootstrap=True, max_depth=5, max_features=log2, max_leaf_nodes=7, n_estimators=50;, score=0.841 total time=   0.6s\n",
      "[CV 1/10] END bootstrap=True, max_depth=5, max_features=log2, max_leaf_nodes=7, n_estimators=200;, score=0.838 total time=   2.6s\n",
      "[CV 9/10] END bootstrap=True, max_depth=7, max_features=auto, max_leaf_nodes=3, n_estimators=200;, score=0.739 total time=   3.9s\n",
      "[CV 7/10] END bootstrap=True, max_depth=7, max_features=auto, max_leaf_nodes=5, n_estimators=300;, score=0.820 total time=   7.4s\n",
      "[CV 5/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=300;, score=0.757 total time=   3.3s\n",
      "[CV 4/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=7, n_estimators=50;, score=0.850 total time=   0.6s\n",
      "[CV 8/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=7, n_estimators=100;, score=0.840 total time=   1.4s\n",
      "[CV 5/10] END bootstrap=True, max_depth=7, max_features=log2, max_leaf_nodes=3, n_estimators=50;, score=0.749 total time=   0.5s\n",
      "[CV 3/10] END bootstrap=True, max_depth=7, max_features=log2, max_leaf_nodes=3, n_estimators=100;, score=0.744 total time=   1.0s\n",
      "[CV 6/10] END bootstrap=True, max_depth=7, max_features=log2, max_leaf_nodes=3, n_estimators=300;, score=0.745 total time=   3.1s\n",
      "[CV 4/10] END bootstrap=True, max_depth=7, max_features=log2, max_leaf_nodes=7, n_estimators=100;, score=0.851 total time=   1.4s\n",
      "[CV 3/10] END bootstrap=False, max_depth=3, max_features=auto, max_leaf_nodes=3, n_estimators=50;, score=0.709 total time=   1.8s\n",
      "[CV 1/10] END bootstrap=False, max_depth=3, max_features=auto, max_leaf_nodes=3, n_estimators=300;, score=0.705 total time=   8.5s\n",
      "[CV 8/10] END bootstrap=False, max_depth=3, max_features=auto, max_leaf_nodes=7, n_estimators=200;, score=0.815 total time=   8.0s\n",
      "[CV 10/10] END bootstrap=False, max_depth=3, max_features=sqrt, max_leaf_nodes=5, n_estimators=300;, score=0.825 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=False, max_depth=3, max_features=log2, max_leaf_nodes=3, n_estimators=200;, score=0.740 total time=   2.5s\n",
      "[CV 2/10] END bootstrap=False, max_depth=3, max_features=log2, max_leaf_nodes=5, n_estimators=200;, score=0.827 total time=   3.3s\n",
      "[CV 5/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=3, n_estimators=50;, score=0.730 total time=   1.1s\n",
      "[CV 7/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=3, n_estimators=100;, score=0.717 total time=   2.8s\n",
      "[CV 5/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=5, n_estimators=100;, score=0.813 total time=   3.6s\n",
      "[CV 6/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=7, n_estimators=50;, score=0.823 total time=   1.6s\n",
      "[CV 3/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=7, n_estimators=100;, score=0.819 total time=   4.8s\n",
      "[CV 3/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.741 total time=   1.2s\n",
      "[CV 4/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=200;, score=0.749 total time=   2.2s\n",
      "[CV 9/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=5, n_estimators=50;, score=0.829 total time=   0.7s\n",
      "[CV 4/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=5, n_estimators=200;, score=0.833 total time=   3.0s\n",
      "[CV 9/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=7, n_estimators=100;, score=0.848 total time=   1.7s\n",
      "[CV 3/10] END bootstrap=False, max_depth=5, max_features=log2, max_leaf_nodes=3, n_estimators=50;, score=0.732 total time=   0.6s\n",
      "[CV 3/10] END bootstrap=False, max_depth=5, max_features=log2, max_leaf_nodes=3, n_estimators=100;, score=0.741 total time=   1.2s\n",
      "[CV 1/10] END bootstrap=False, max_depth=5, max_features=log2, max_leaf_nodes=3, n_estimators=300;, score=0.741 total time=   3.4s\n",
      "[CV 8/10] END bootstrap=False, max_depth=5, max_features=log2, max_leaf_nodes=7, n_estimators=50;, score=0.840 total time=   0.8s\n",
      "[CV 3/10] END bootstrap=False, max_depth=5, max_features=log2, max_leaf_nodes=7, n_estimators=200;, score=0.840 total time=   3.2s\n",
      "[CV 10/10] END bootstrap=False, max_depth=7, max_features=auto, max_leaf_nodes=3, n_estimators=100;, score=0.721 total time=   2.6s\n",
      "[CV 4/10] END bootstrap=False, max_depth=7, max_features=auto, max_leaf_nodes=5, n_estimators=100;, score=0.798 total time=   3.3s\n",
      "[CV 2/10] END bootstrap=False, max_depth=7, max_features=auto, max_leaf_nodes=7, n_estimators=50;, score=0.818 total time=   1.6s\n",
      "[CV 4/10] END bootstrap=False, max_depth=7, max_features=auto, max_leaf_nodes=7, n_estimators=100;, score=0.819 total time=   3.2s\n",
      "[CV 2/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.742 total time=   0.5s\n",
      "[CV 9/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.747 total time=   0.6s\n",
      "[CV 9/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.757 total time=   1.1s\n",
      "[CV 7/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=200;, score=0.744 total time=   3.1s\n",
      "[CV 7/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=5, n_estimators=100;, score=0.823 total time=   1.8s\n",
      "[CV 1/10] END bootstrap=True, max_depth=3, max_features=auto, max_leaf_nodes=3, n_estimators=300;, score=0.719 total time=   5.2s\n",
      "[CV 9/10] END bootstrap=True, max_depth=3, max_features=auto, max_leaf_nodes=7, n_estimators=100;, score=0.840 total time=   2.4s\n",
      "[CV 10/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.742 total time=   0.5s\n",
      "[CV 7/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.749 total time=   1.2s\n",
      "[CV 6/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=3, n_estimators=200;, score=0.744 total time=   2.0s\n",
      "[CV 3/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=5, n_estimators=200;, score=0.823 total time=   2.3s\n",
      "[CV 1/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=7, n_estimators=200;, score=0.839 total time=   2.2s\n",
      "[CV 7/10] END bootstrap=True, max_depth=3, max_features=log2, max_leaf_nodes=3, n_estimators=200;, score=0.745 total time=   2.2s\n",
      "[CV 9/10] END bootstrap=True, max_depth=3, max_features=log2, max_leaf_nodes=5, n_estimators=200;, score=0.832 total time=   2.2s\n",
      "[CV 3/10] END bootstrap=True, max_depth=3, max_features=log2, max_leaf_nodes=7, n_estimators=300;, score=0.843 total time=   4.1s\n",
      "[CV 10/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=5, n_estimators=100;, score=0.822 total time=   2.7s\n",
      "[CV 8/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=7, n_estimators=50;, score=0.835 total time=   1.2s\n",
      "[CV 2/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=7, n_estimators=200;, score=0.836 total time=   5.3s\n",
      "[CV 1/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=5, n_estimators=50;, score=0.818 total time=   0.7s\n",
      "[CV 4/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=5, n_estimators=100;, score=0.835 total time=   1.7s\n",
      "[CV 4/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=7, n_estimators=50;, score=0.850 total time=   0.9s\n",
      "[CV 4/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=7, n_estimators=100;, score=0.851 total time=   1.8s\n",
      "[CV 8/10] END bootstrap=True, max_depth=5, max_features=log2, max_leaf_nodes=3, n_estimators=50;, score=0.731 total time=   0.5s\n",
      "[CV 10/10] END bootstrap=True, max_depth=5, max_features=log2, max_leaf_nodes=3, n_estimators=100;, score=0.752 total time=   1.4s\n",
      "[CV 9/10] END bootstrap=True, max_depth=5, max_features=log2, max_leaf_nodes=5, n_estimators=50;, score=0.831 total time=   0.7s\n",
      "[CV 10/10] END bootstrap=True, max_depth=5, max_features=log2, max_leaf_nodes=5, n_estimators=100;, score=0.826 total time=   1.2s\n",
      "[CV 6/10] END bootstrap=True, max_depth=5, max_features=log2, max_leaf_nodes=7, n_estimators=50;, score=0.848 total time=   0.6s\n",
      "[CV 7/10] END bootstrap=True, max_depth=5, max_features=log2, max_leaf_nodes=7, n_estimators=100;, score=0.841 total time=   1.3s\n",
      "[CV 2/10] END bootstrap=True, max_depth=7, max_features=auto, max_leaf_nodes=3, n_estimators=50;, score=0.727 total time=   0.8s\n",
      "[CV 1/10] END bootstrap=True, max_depth=7, max_features=auto, max_leaf_nodes=3, n_estimators=100;, score=0.719 total time=   2.0s\n",
      "[CV 6/10] END bootstrap=True, max_depth=7, max_features=auto, max_leaf_nodes=5, n_estimators=50;, score=0.817 total time=   1.1s\n",
      "[CV 9/10] END bootstrap=True, max_depth=7, max_features=auto, max_leaf_nodes=5, n_estimators=100;, score=0.824 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_depth=7, max_features=auto, max_leaf_nodes=7, n_estimators=50;, score=0.836 total time=   1.3s\n",
      "[CV 5/10] END bootstrap=True, max_depth=7, max_features=auto, max_leaf_nodes=7, n_estimators=100;, score=0.849 total time=   3.0s\n",
      "[CV 7/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.740 total time=   0.7s\n",
      "[CV 10/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.752 total time=   1.0s\n",
      "[CV 9/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=200;, score=0.756 total time=   1.9s\n",
      "[CV 6/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=5, n_estimators=100;, score=0.829 total time=   1.2s\n",
      "[CV 1/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=5, n_estimators=300;, score=0.819 total time=   4.3s\n",
      "[CV 7/10] END bootstrap=True, max_depth=7, max_features=log2, max_leaf_nodes=3, n_estimators=200;, score=0.745 total time=   2.3s\n",
      "[CV 9/10] END bootstrap=True, max_depth=7, max_features=log2, max_leaf_nodes=5, n_estimators=200;, score=0.832 total time=   2.3s\n",
      "[CV 8/10] END bootstrap=True, max_depth=7, max_features=log2, max_leaf_nodes=7, n_estimators=300;, score=0.841 total time=   4.4s\n",
      "[CV 6/10] END bootstrap=False, max_depth=3, max_features=auto, max_leaf_nodes=5, n_estimators=100;, score=0.800 total time=   3.8s\n",
      "[CV 8/10] END bootstrap=False, max_depth=3, max_features=auto, max_leaf_nodes=7, n_estimators=50;, score=0.815 total time=   1.7s\n",
      "[CV 1/10] END bootstrap=False, max_depth=3, max_features=auto, max_leaf_nodes=7, n_estimators=200;, score=0.817 total time=   6.2s\n",
      "[CV 5/10] END bootstrap=False, max_depth=3, max_features=sqrt, max_leaf_nodes=5, n_estimators=50;, score=0.832 total time=   0.6s\n",
      "[CV 8/10] END bootstrap=False, max_depth=3, max_features=sqrt, max_leaf_nodes=5, n_estimators=100;, score=0.819 total time=   1.6s\n",
      "[CV 8/10] END bootstrap=False, max_depth=3, max_features=sqrt, max_leaf_nodes=5, n_estimators=300;, score=0.820 total time=   4.2s\n",
      "[CV 2/10] END bootstrap=False, max_depth=3, max_features=log2, max_leaf_nodes=3, n_estimators=200;, score=0.749 total time=   2.6s\n",
      "[CV 3/10] END bootstrap=False, max_depth=3, max_features=log2, max_leaf_nodes=5, n_estimators=300;, score=0.822 total time=   4.0s\n",
      "[CV 2/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=3, n_estimators=100;, score=0.717 total time=   2.5s\n",
      "[CV 5/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=5, n_estimators=50;, score=0.813 total time=   1.5s\n",
      "[CV 8/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=5, n_estimators=200;, score=0.791 total time=   5.7s\n",
      "[CV 3/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=7, n_estimators=300;, score=0.819 total time=  10.5s\n",
      "[CV 3/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=7, n_estimators=200;, score=0.840 total time=   3.1s\n",
      "[CV 8/10] END bootstrap=False, max_depth=5, max_features=log2, max_leaf_nodes=3, n_estimators=200;, score=0.736 total time=   2.2s\n",
      "[CV 5/10] END bootstrap=False, max_depth=5, max_features=log2, max_leaf_nodes=5, n_estimators=200;, score=0.834 total time=   2.8s\n",
      "[CV 1/10] END bootstrap=False, max_depth=5, max_features=log2, max_leaf_nodes=7, n_estimators=300;, score=0.836 total time=   4.8s\n",
      "[CV 4/10] END bootstrap=False, max_depth=7, max_features=auto, max_leaf_nodes=5, n_estimators=50;, score=0.798 total time=   2.0s\n",
      "[CV 9/10] END bootstrap=False, max_depth=7, max_features=auto, max_leaf_nodes=5, n_estimators=200;, score=0.803 total time=   7.0s\n",
      "[CV 6/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.735 total time=   0.6s\n",
      "[CV 4/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.752 total time=   1.1s\n",
      "[CV 4/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=200;, score=0.749 total time=   2.2s\n",
      "[CV 9/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=300;, score=0.755 total time=   3.7s\n",
      "[CV 7/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=7, n_estimators=50;, score=0.840 total time=   0.8s\n",
      "[CV 8/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=7, n_estimators=100;, score=0.839 total time=   1.6s\n",
      "[CV 10/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=7, n_estimators=300;, score=0.843 total time=   4.7s\n",
      "[CV 3/10] END bootstrap=True, max_depth=3, max_features=auto, max_leaf_nodes=3, n_estimators=100;, score=0.719 total time=   1.8s\n",
      "[CV 8/10] END bootstrap=True, max_depth=3, max_features=auto, max_leaf_nodes=5, n_estimators=100;, score=0.815 total time=   2.3s\n",
      "[CV 6/10] END bootstrap=True, max_depth=3, max_features=auto, max_leaf_nodes=7, n_estimators=50;, score=0.835 total time=   1.0s\n",
      "[CV 5/10] END bootstrap=True, max_depth=3, max_features=auto, max_leaf_nodes=7, n_estimators=100;, score=0.848 total time=   2.1s\n",
      "[CV 1/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.736 total time=   0.5s\n",
      "[CV 9/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.751 total time=   0.6s\n",
      "[CV 8/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.741 total time=   1.2s\n",
      "[CV 8/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=3, n_estimators=200;, score=0.739 total time=   3.1s\n",
      "[CV 8/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=5, n_estimators=300;, score=0.821 total time=   3.7s\n",
      "[CV 3/10] END bootstrap=True, max_depth=3, max_features=log2, max_leaf_nodes=3, n_estimators=300;, score=0.742 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=True, max_depth=3, max_features=log2, max_leaf_nodes=7, n_estimators=100;, score=0.846 total time=   1.2s\n",
      "[CV 7/10] END bootstrap=True, max_depth=3, max_features=log2, max_leaf_nodes=7, n_estimators=300;, score=0.842 total time=   3.3s\n",
      "[CV 2/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=5, n_estimators=100;, score=0.820 total time=   2.6s\n",
      "[CV 4/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=7, n_estimators=50;, score=0.837 total time=   1.5s\n",
      "[CV 8/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=7, n_estimators=100;, score=0.835 total time=   2.4s\n",
      "[CV 2/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.744 total time=   0.6s\n",
      "[CV 2/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.754 total time=   1.4s\n",
      "[CV 8/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=200;, score=0.739 total time=   2.5s\n",
      "[CV 8/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=5, n_estimators=200;, score=0.820 total time=   2.4s\n",
      "[CV 3/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=7, n_estimators=200;, score=0.842 total time=   2.8s\n",
      "[CV 5/10] END bootstrap=True, max_depth=5, max_features=log2, max_leaf_nodes=3, n_estimators=300;, score=0.757 total time=   2.9s\n",
      "[CV 2/10] END bootstrap=True, max_depth=5, max_features=log2, max_leaf_nodes=7, n_estimators=100;, score=0.846 total time=   1.3s\n",
      "[CV 5/10] END bootstrap=True, max_depth=5, max_features=log2, max_leaf_nodes=7, n_estimators=300;, score=0.852 total time=   4.1s\n",
      "[CV 5/10] END bootstrap=True, max_depth=7, max_features=auto, max_leaf_nodes=5, n_estimators=100;, score=0.833 total time=   2.2s\n",
      "[CV 5/10] END bootstrap=True, max_depth=7, max_features=auto, max_leaf_nodes=7, n_estimators=50;, score=0.849 total time=   1.4s\n",
      "[CV 6/10] END bootstrap=True, max_depth=7, max_features=auto, max_leaf_nodes=7, n_estimators=100;, score=0.837 total time=   2.7s\n",
      "[CV 6/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.736 total time=   0.6s\n",
      "[CV 3/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.744 total time=   1.0s\n",
      "[CV 5/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=200;, score=0.755 total time=   2.4s\n",
      "[CV 2/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=5, n_estimators=200;, score=0.829 total time=   2.3s\n",
      "[CV 3/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=7, n_estimators=100;, score=0.842 total time=   1.3s\n",
      "[CV 9/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=7, n_estimators=300;, score=0.849 total time=   4.4s\n",
      "[CV 5/10] END bootstrap=True, max_depth=7, max_features=log2, max_leaf_nodes=7, n_estimators=50;, score=0.852 total time=   0.6s\n",
      "[CV 7/10] END bootstrap=True, max_depth=7, max_features=log2, max_leaf_nodes=7, n_estimators=100;, score=0.841 total time=   1.4s\n",
      "[CV 6/10] END bootstrap=False, max_depth=3, max_features=auto, max_leaf_nodes=3, n_estimators=50;, score=0.711 total time=   1.1s\n",
      "[CV 6/10] END bootstrap=False, max_depth=3, max_features=auto, max_leaf_nodes=3, n_estimators=100;, score=0.711 total time=   2.3s\n",
      "[CV 9/10] END bootstrap=False, max_depth=3, max_features=auto, max_leaf_nodes=5, n_estimators=50;, score=0.803 total time=   1.6s\n",
      "[CV 8/10] END bootstrap=False, max_depth=3, max_features=auto, max_leaf_nodes=5, n_estimators=200;, score=0.791 total time=   5.7s\n",
      "[CV 4/10] END bootstrap=False, max_depth=3, max_features=auto, max_leaf_nodes=7, n_estimators=300;, score=0.819 total time=   9.4s\n",
      "[CV 2/10] END bootstrap=False, max_depth=3, max_features=sqrt, max_leaf_nodes=7, n_estimators=300;, score=0.846 total time=   4.5s\n",
      "[CV 9/10] END bootstrap=False, max_depth=3, max_features=log2, max_leaf_nodes=5, n_estimators=200;, score=0.830 total time=   3.2s\n",
      "[CV 6/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=3, n_estimators=50;, score=0.711 total time=   1.1s\n",
      "[CV 8/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=3, n_estimators=100;, score=0.704 total time=   2.2s\n",
      "[CV 6/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=5, n_estimators=50;, score=0.800 total time=   1.4s\n",
      "[CV 4/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=5, n_estimators=200;, score=0.798 total time=   5.6s\n",
      "[CV 9/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=7, n_estimators=200;, score=0.823 total time=   6.3s\n",
      "[CV 10/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=300;, score=0.747 total time=   3.3s\n",
      "[CV 4/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=7, n_estimators=50;, score=0.851 total time=   0.8s\n",
      "[CV 3/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=7, n_estimators=100;, score=0.840 total time=   1.6s\n",
      "[CV 5/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=7, n_estimators=300;, score=0.852 total time=   4.7s\n",
      "[CV 10/10] END bootstrap=False, max_depth=5, max_features=log2, max_leaf_nodes=5, n_estimators=200;, score=0.824 total time=   2.7s\n",
      "[CV 5/10] END bootstrap=False, max_depth=5, max_features=log2, max_leaf_nodes=7, n_estimators=300;, score=0.852 total time=   4.7s\n",
      "[CV 2/10] END bootstrap=False, max_depth=7, max_features=auto, max_leaf_nodes=5, n_estimators=100;, score=0.798 total time=   3.4s\n",
      "[CV 1/10] END bootstrap=False, max_depth=7, max_features=auto, max_leaf_nodes=7, n_estimators=50;, score=0.817 total time=   1.6s\n",
      "[CV 5/10] END bootstrap=False, max_depth=7, max_features=auto, max_leaf_nodes=7, n_estimators=100;, score=0.831 total time=   3.6s\n",
      "[CV 5/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.746 total time=   0.6s\n",
      "[CV 3/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.741 total time=   1.1s\n",
      "[CV 3/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=200;, score=0.738 total time=   2.3s\n",
      "[CV 10/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=300;, score=0.747 total time=   3.6s\n",
      "[CV 6/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=7, n_estimators=50;, score=0.847 total time=   1.5s\n",
      "[CV 10/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=7, n_estimators=200;, score=0.842 total time=   4.1s\n",
      "[CV 1/10] END bootstrap=False, max_depth=7, max_features=log2, max_leaf_nodes=5, n_estimators=100;, score=0.815 total time=   1.4s\n",
      "[CV 5/10] END bootstrap=True, max_depth=3, max_features=auto, max_leaf_nodes=3, n_estimators=50;, score=0.740 total time=   1.0s\n",
      "[CV 7/10] END bootstrap=True, max_depth=3, max_features=auto, max_leaf_nodes=5, n_estimators=50;, score=0.818 total time=   1.4s\n",
      "[CV 6/10] END bootstrap=True, max_depth=3, max_features=auto, max_leaf_nodes=5, n_estimators=200;, score=0.820 total time=   5.4s\n",
      "[CV 6/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.736 total time=   0.5s\n",
      "[CV 4/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.756 total time=   1.2s\n",
      "[CV 4/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=3, n_estimators=200;, score=0.752 total time=   2.5s\n",
      "[CV 5/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=5, n_estimators=200;, score=0.834 total time=   2.5s\n",
      "[CV 9/10] END bootstrap=True, max_depth=3, max_features=sqrt, max_leaf_nodes=7, n_estimators=200;, score=0.850 total time=   2.3s\n",
      "[CV 5/10] END bootstrap=True, max_depth=3, max_features=log2, max_leaf_nodes=3, n_estimators=300;, score=0.757 total time=   2.9s\n",
      "[CV 4/10] END bootstrap=True, max_depth=3, max_features=log2, max_leaf_nodes=7, n_estimators=100;, score=0.851 total time=   1.2s\n",
      "[CV 1/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=3, n_estimators=50;, score=0.720 total time=   0.9s\n",
      "[CV 1/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=3, n_estimators=100;, score=0.719 total time=   2.2s\n",
      "[CV 9/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=5, n_estimators=50;, score=0.824 total time=   1.5s\n",
      "[CV 9/10] END bootstrap=True, max_depth=5, max_features=auto, max_leaf_nodes=5, n_estimators=200;, score=0.825 total time=   5.1s\n",
      "[CV 1/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.736 total time=   0.5s\n",
      "[CV 7/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.740 total time=   0.5s\n",
      "[CV 6/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.746 total time=   1.0s\n",
      "[CV 6/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=200;, score=0.744 total time=   2.4s\n",
      "[CV 10/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=5, n_estimators=100;, score=0.826 total time=   1.5s\n",
      "[CV 3/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=7, n_estimators=50;, score=0.842 total time=   1.0s\n",
      "[CV 7/10] END bootstrap=True, max_depth=5, max_features=sqrt, max_leaf_nodes=7, n_estimators=100;, score=0.841 total time=   1.4s\n",
      "[CV 4/10] END bootstrap=True, max_depth=5, max_features=log2, max_leaf_nodes=3, n_estimators=50;, score=0.745 total time=   0.5s\n",
      "[CV 2/10] END bootstrap=True, max_depth=5, max_features=log2, max_leaf_nodes=3, n_estimators=100;, score=0.754 total time=   1.0s\n",
      "[CV 6/10] END bootstrap=True, max_depth=5, max_features=log2, max_leaf_nodes=3, n_estimators=300;, score=0.745 total time=   4.4s\n",
      "[CV 9/10] END bootstrap=True, max_depth=5, max_features=log2, max_leaf_nodes=7, n_estimators=300;, score=0.849 total time=   4.1s\n",
      "[CV 10/10] END bootstrap=True, max_depth=7, max_features=auto, max_leaf_nodes=5, n_estimators=100;, score=0.822 total time=   2.6s\n",
      "[CV 8/10] END bootstrap=True, max_depth=7, max_features=auto, max_leaf_nodes=7, n_estimators=50;, score=0.835 total time=   1.3s\n",
      "[CV 4/10] END bootstrap=True, max_depth=7, max_features=auto, max_leaf_nodes=7, n_estimators=200;, score=0.839 total time=   5.2s\n",
      "[CV 8/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=300;, score=0.741 total time=   3.4s\n",
      "[CV 9/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=7, n_estimators=50;, score=0.849 total time=   0.8s\n",
      "[CV 4/10] END bootstrap=True, max_depth=7, max_features=sqrt, max_leaf_nodes=7, n_estimators=300;, score=0.852 total time=   4.7s\n",
      "[CV 5/10] END bootstrap=True, max_depth=7, max_features=log2, max_leaf_nodes=5, n_estimators=300;, score=0.835 total time=   3.7s\n",
      "[CV 4/10] END bootstrap=False, max_depth=3, max_features=auto, max_leaf_nodes=3, n_estimators=200;, score=0.720 total time=   5.1s\n",
      "[CV 7/10] END bootstrap=False, max_depth=3, max_features=auto, max_leaf_nodes=5, n_estimators=300;, score=0.795 total time=   9.1s\n",
      "[CV 9/10] END bootstrap=False, max_depth=3, max_features=sqrt, max_leaf_nodes=3, n_estimators=300;, score=0.755 total time=   4.2s\n",
      "[CV 3/10] END bootstrap=False, max_depth=3, max_features=sqrt, max_leaf_nodes=7, n_estimators=200;, score=0.842 total time=   3.9s\n",
      "[CV 8/10] END bootstrap=False, max_depth=3, max_features=log2, max_leaf_nodes=5, n_estimators=50;, score=0.817 total time=   0.6s\n",
      "[CV 5/10] END bootstrap=False, max_depth=3, max_features=log2, max_leaf_nodes=5, n_estimators=100;, score=0.834 total time=   1.3s\n",
      "[CV 3/10] END bootstrap=False, max_depth=3, max_features=log2, max_leaf_nodes=7, n_estimators=50;, score=0.841 total time=   0.7s\n",
      "[CV 5/10] END bootstrap=False, max_depth=3, max_features=log2, max_leaf_nodes=7, n_estimators=100;, score=0.851 total time=   1.3s\n",
      "[CV 6/10] END bootstrap=False, max_depth=3, max_features=log2, max_leaf_nodes=7, n_estimators=300;, score=0.849 total time=   4.2s\n",
      "[CV 3/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=5, n_estimators=100;, score=0.794 total time=   3.6s\n",
      "[CV 7/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=7, n_estimators=50;, score=0.818 total time=   1.7s\n",
      "[CV 6/10] END bootstrap=False, max_depth=5, max_features=auto, max_leaf_nodes=7, n_estimators=100;, score=0.823 total time=   4.1s\n",
      "[CV 7/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.736 total time=   0.9s\n",
      "[CV 10/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.748 total time=   1.7s\n",
      "[CV 5/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=3, n_estimators=300;, score=0.755 total time=   3.7s\n",
      "[CV 3/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=7, n_estimators=50;, score=0.840 total time=   0.8s\n",
      "[CV 1/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=7, n_estimators=100;, score=0.836 total time=   1.9s\n",
      "[CV 6/10] END bootstrap=False, max_depth=5, max_features=sqrt, max_leaf_nodes=7, n_estimators=300;, score=0.848 total time=   5.7s\n",
      "[CV 3/10] END bootstrap=False, max_depth=5, max_features=log2, max_leaf_nodes=7, n_estimators=100;, score=0.840 total time=   1.6s\n",
      "[CV 9/10] END bootstrap=False, max_depth=5, max_features=log2, max_leaf_nodes=7, n_estimators=300;, score=0.848 total time=   5.5s\n",
      "[CV 8/10] END bootstrap=False, max_depth=7, max_features=auto, max_leaf_nodes=5, n_estimators=100;, score=0.791 total time=   2.8s\n",
      "[CV 3/10] END bootstrap=False, max_depth=7, max_features=auto, max_leaf_nodes=7, n_estimators=50;, score=0.819 total time=   1.6s\n",
      "[CV 6/10] END bootstrap=False, max_depth=7, max_features=auto, max_leaf_nodes=7, n_estimators=100;, score=0.823 total time=   3.3s\n",
      "[CV 4/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=50;, score=0.743 total time=   0.9s\n",
      "[CV 7/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=100;, score=0.746 total time=   1.5s\n",
      "[CV 9/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=3, n_estimators=200;, score=0.754 total time=   2.7s\n",
      "[CV 1/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=5, n_estimators=100;, score=0.815 total time=   1.5s\n",
      "[CV 4/10] END bootstrap=False, max_depth=7, max_features=sqrt, max_leaf_nodes=5, n_estimators=300;, score=0.833 total time=   5.8s\n",
      "[CV 6/10] END bootstrap=False, max_depth=7, max_features=log2, max_leaf_nodes=3, n_estimators=300;, score=0.744 total time=   3.7s\n",
      "[CV 8/10] END bootstrap=False, max_depth=7, max_features=log2, max_leaf_nodes=7, n_estimators=100;, score=0.839 total time=   1.6s\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'bootstrap': True, 'max_depth': 3, 'max_features': 'sqrt', 'max_leaf_nodes': 7, 'n_estimators': 300}\n",
      "0.8461397460605973\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "SEED = 98\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.05, 0.07, 0.1], #[0.01, 0.03, 0.05],\n",
    "                'max_depth': [3, 5, 7],#[3,4,5],\n",
    "                'max_leaf_nodes': [3, 5, 7],\n",
    "                'n_estimators': [50, 100, 200, 300],\n",
    "                'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                #'min_samples_split': [2,3,5],\n",
    "                #'min_samples_leaf': [1,2,3],\n",
    "               'bootstrap': [True, False]\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=RandomForestRegressor(random_state = SEED),\n",
    "                    n_jobs=-1,\n",
    "                    verbose=5,\n",
    "                    param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train_imputed, y_train)\n",
    "#model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc997700-db8f-4591-92b9-3856c1aa7b8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:58:19.714124Z",
     "iopub.status.busy": "2023-02-17T05:58:19.713622Z",
     "iopub.status.idle": "2023-02-17T06:10:09.690231Z",
     "shell.execute_reply": "2023-02-17T06:10:09.689550Z",
     "shell.execute_reply.started": "2023-02-17T05:58:19.714074Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 216 candidates, totalling 2160 fits\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   6.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.870 total time=   5.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.873 total time=  19.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.874 total time=  30.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.873 total time=  20.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.875 total time=   5.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   9.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.878 total time=   3.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   8.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.876 total time=  29.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.875 total time=   2.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.872 total time=   2.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.878 total time=   4.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.878 total time=  16.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.881 total time=   2.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.877 total time=   4.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.867 total time=   2.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.868 total time=   4.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.879 total time=  14.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=   8.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.874 total time=   2.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   5.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.873 total time=   2.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.871 total time=   5.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.875 total time=   5.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.876 total time=   2.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.875 total time=   4.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.878 total time=   2.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.880 total time=   4.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.876 total time=  14.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.877 total time=  13.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.871 total time=  13.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.764 total time=   7.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.780 total time=  28.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.794 total time=  14.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.809 total time=   9.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.800 total time=  14.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.800 total time=   7.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.798 total time=  15.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.808 total time=  43.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.825 total time=  27.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.820 total time=   6.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.824 total time=  15.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.818 total time=  39.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.869 total time=   3.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.866 total time=  15.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.869 total time=   4.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.871 total time=   7.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.871 total time=   8.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.870 total time=   3.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.873 total time=   6.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.874 total time=   3.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   7.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.867 total time=   4.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.873 total time=   4.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.874 total time=   4.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.875 total time=  10.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.870 total time=   4.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.872 total time=  10.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.872 total time=   4.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.872 total time=  11.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.876 total time=   4.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.877 total time=   9.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.872 total time=  31.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   4.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   9.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.879 total time=   3.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   7.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.876 total time=  26.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.876 total time=   3.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.877 total time=   9.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.880 total time=   8.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.879 total time=  16.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.876 total time=   2.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   4.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.876 total time=   1.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.882 total time=   3.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.880 total time=  11.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.878 total time=   4.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.880 total time=   5.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.867 total time=   2.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.879 total time=   4.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.868 total time=   4.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.879 total time=  14.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.882 total time=  13.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=   7.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.786 total time=  15.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.777 total time=  32.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.778 total time=  33.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.798 total time=   7.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.800 total time=  15.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.814 total time=   6.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.797 total time=  13.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.813 total time=  45.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.825 total time=  27.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.860 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.868 total time=   7.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.866 total time=  14.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.869 total time=   7.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.870 total time=  25.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.872 total time=  20.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.872 total time=  15.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.870 total time=   2.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   6.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.875 total time=   5.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.865 total time=  19.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.869 total time=  15.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.869 total time=  14.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.869 total time=   4.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.870 total time=   9.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.862 total time=  11.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.872 total time=   4.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.873 total time=   8.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   4.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.875 total time=   8.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.875 total time=  21.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.872 total time=   6.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.875 total time=   9.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.875 total time=   4.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.875 total time=   9.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   9.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.875 total time=  25.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.864 total time=  14.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.880 total time=   5.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   2.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.881 total time=   4.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.882 total time=  14.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.877 total time=  13.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.883 total time=   8.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  13.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.875 total time=  12.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.873 total time=   2.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.877 total time=   4.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.875 total time=   2.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.876 total time=   4.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.879 total time=   2.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.876 total time=   4.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.879 total time=  13.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.882 total time=   7.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.877 total time=   7.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.790 total time=  32.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.777 total time=   8.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.783 total time=  14.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.802 total time=   7.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.802 total time=  14.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.792 total time=   6.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.792 total time=  13.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.798 total time=  29.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.814 total time=   7.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.814 total time=  13.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.818 total time=  44.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.819 total time=  27.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.869 total time=   7.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.869 total time=  22.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.872 total time=  15.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.870 total time=  21.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  13.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   2.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   7.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.864 total time=   3.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.865 total time=   6.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.867 total time=  23.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.869 total time=  22.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.872 total time=  14.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   4.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.874 total time=   4.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=  10.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.874 total time=   4.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.875 total time=   9.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   4.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.875 total time=  10.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.875 total time=  26.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.875 total time=  19.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.875 total time=  27.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.875 total time=  17.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.871 total time=   2.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.871 total time=   5.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.872 total time=  15.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.880 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.880 total time=  15.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.879 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.880 total time=   4.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.871 total time=  14.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.864 total time=  15.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.875 total time=   9.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.878 total time=   2.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.879 total time=   4.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.877 total time=  14.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  13.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=   8.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.770 total time=  34.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.794 total time=   8.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.799 total time=  16.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.808 total time=   8.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.807 total time=  13.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.808 total time=   8.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.807 total time=  13.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.795 total time=   6.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.805 total time=  15.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.824 total time=   7.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.820 total time=  12.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.819 total time=  30.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.819 total time=   7.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.814 total time=  28.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.867 total time=  24.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.869 total time=  13.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.872 total time=  14.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.874 total time=   6.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.874 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.875 total time=   6.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.875 total time=  20.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=  15.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.861 total time=   3.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.862 total time=   6.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.868 total time=   3.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.872 total time=   7.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.872 total time=  22.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.871 total time=  14.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   7.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.873 total time=  12.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.870 total time=  19.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.877 total time=  27.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.875 total time=  18.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.876 total time=  29.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.873 total time=  16.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.876 total time=  26.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.875 total time=   6.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.878 total time=   5.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.876 total time=  14.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.882 total time=  13.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.880 total time=  10.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.871 total time=   2.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.872 total time=   5.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.872 total time=   2.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=   4.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.880 total time=   3.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.875 total time=   5.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.875 total time=   2.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.873 total time=   5.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.876 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.878 total time=   8.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.876 total time=   9.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.879 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.880 total time=   4.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  14.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.774 total time=  31.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.786 total time=   7.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.787 total time=  17.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.794 total time=   7.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.794 total time=  15.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.794 total time=   7.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.794 total time=  16.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.816 total time=   7.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.806 total time=  15.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.809 total time=  46.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.819 total time=  41.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.866 total time=  22.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.865 total time=   3.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.873 total time=  14.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.872 total time=  22.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.872 total time=  13.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   6.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.864 total time=   2.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.870 total time=   3.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.871 total time=   5.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.864 total time=   3.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.866 total time=   6.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.869 total time=   4.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.865 total time=   7.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.869 total time=   3.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.870 total time=   6.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.871 total time=  14.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.873 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.870 total time=   7.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.870 total time=   3.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.875 total time=   6.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.869 total time=   4.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.872 total time=   4.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=  10.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  33.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   4.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.874 total time=   9.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.872 total time=   4.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.877 total time=   9.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.875 total time=  25.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.876 total time=  15.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.875 total time=  17.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.876 total time=   2.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.876 total time=   4.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  17.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.880 total time=  14.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.869 total time=  13.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.871 total time=   9.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.876 total time=   2.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.872 total time=   4.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.876 total time=   2.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.876 total time=   4.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.867 total time=   2.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.867 total time=   4.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.877 total time=   9.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   2.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.876 total time=   4.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.880 total time=  13.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=   7.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.883 total time=   7.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.875 total time=   7.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.761 total time=  29.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.777 total time=  41.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.792 total time=  29.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.803 total time=  49.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.824 total time=   6.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.824 total time=  13.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.824 total time=  42.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.868 total time=  23.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.872 total time=   6.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.867 total time=  20.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.868 total time=  13.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   2.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   5.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.871 total time=  13.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   4.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   6.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.863 total time=   4.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.861 total time=   8.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.868 total time=   8.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.865 total time=   3.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.866 total time=   7.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.867 total time=   4.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.867 total time=   8.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.869 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.875 total time=   7.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.874 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   7.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.872 total time=  20.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.876 total time=   4.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.874 total time=  10.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.876 total time=   4.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.877 total time=  10.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.872 total time=  27.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.872 total time=  20.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   5.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.873 total time=   9.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   5.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.877 total time=  17.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.871 total time=  14.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.879 total time=   4.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.867 total time=   2.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.868 total time=   5.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.878 total time=   2.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.875 total time=   4.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.879 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.876 total time=   5.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.876 total time=   2.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.883 total time=   5.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.881 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.878 total time=   4.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.882 total time=  13.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.875 total time=   9.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.874 total time=  13.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.876 total time=   8.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.878 total time=  12.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   4.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.876 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.882 total time=   3.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.876 total time=  14.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.785 total time=  31.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.784 total time=   6.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.789 total time=  13.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.792 total time=   8.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.812 total time=  15.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.799 total time=  43.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.805 total time=  26.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.818 total time=  28.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.818 total time=   8.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.824 total time=  15.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.860 total time=   3.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.866 total time=   8.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.868 total time=  20.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.857 total time=   3.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.873 total time=  14.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.872 total time=  19.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.862 total time=  14.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  18.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.875 total time=  11.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.852 total time=  24.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.869 total time=  15.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   3.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.871 total time=   6.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.862 total time=   6.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.873 total time=  19.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.874 total time=   5.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.876 total time=   9.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.876 total time=   4.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.874 total time=   8.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.877 total time=  16.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.865 total time=  25.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.867 total time=  19.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.875 total time=  19.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.871 total time=   3.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.871 total time=   6.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.873 total time=  15.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.868 total time=  11.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.878 total time=   2.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.879 total time=   4.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.882 total time=  12.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.871 total time=   8.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.875 total time=   8.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   2.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.863 total time=   5.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.874 total time=   2.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.876 total time=   4.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.878 total time=  13.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.878 total time=  10.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.867 total time=   2.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.879 total time=   4.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.870 total time=   3.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.883 total time=   4.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.879 total time=   2.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.883 total time=   4.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.881 total time=   2.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.882 total time=   3.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.884 total time=  13.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.789 total time=   9.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.783 total time=  31.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.799 total time=  27.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.810 total time=  40.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.817 total time=  28.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.820 total time=  28.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.824 total time=   6.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.820 total time=  14.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.814 total time=  40.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.870 total time=   7.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.861 total time=   3.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.861 total time=   6.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.871 total time=  21.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.871 total time=  12.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.864 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.865 total time=   6.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.873 total time=  19.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.868 total time=  16.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.872 total time=   3.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.871 total time=   7.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.869 total time=  21.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.870 total time=  13.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.874 total time=   3.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   7.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.874 total time=   3.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.869 total time=  20.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.875 total time=   5.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.871 total time=  11.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   5.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.871 total time=   9.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   5.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.871 total time=  10.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.874 total time=   4.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.873 total time=   9.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.878 total time=   4.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.879 total time=   8.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.877 total time=  24.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.877 total time=  17.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.863 total time=   2.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.873 total time=   2.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=   5.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  14.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.868 total time=  15.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.881 total time=  14.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.877 total time=  11.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   5.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.874 total time=   2.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.878 total time=  10.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.876 total time=  14.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.876 total time=   9.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.878 total time=   6.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.876 total time=   2.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   4.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.871 total time=   5.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.769 total time=   8.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.762 total time=  52.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.810 total time=   7.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.810 total time=  13.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.803 total time=  44.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.806 total time=  29.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.814 total time=   6.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.814 total time=  13.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.814 total time=  42.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.862 total time=  20.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.870 total time=   3.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.868 total time=   7.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   3.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.870 total time=   6.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.861 total time=   6.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.861 total time=  19.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.875 total time=  15.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   5.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.877 total time=  20.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.870 total time=  16.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.861 total time=   3.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.871 total time=   6.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.871 total time=  12.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.868 total time=  13.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   2.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.865 total time=   6.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.861 total time=  19.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.873 total time=   4.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.874 total time=   9.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.872 total time=  25.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.875 total time=  16.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.875 total time=  18.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.873 total time=  19.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.875 total time=   4.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.875 total time=   8.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   2.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.872 total time=   6.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.874 total time=  16.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.875 total time=   5.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.876 total time=   2.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.876 total time=   4.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.876 total time=   2.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.876 total time=   5.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.879 total time=   4.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.870 total time=   1.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.874 total time=   2.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.875 total time=   4.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.883 total time=   2.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.880 total time=   3.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  12.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.878 total time=  10.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.872 total time=  14.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.877 total time=  10.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.877 total time=   9.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.875 total time=   7.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.884 total time=  12.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.769 total time=  14.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.783 total time=  15.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.779 total time=  45.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.798 total time=  30.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.800 total time=   7.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.804 total time=  15.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.824 total time=   8.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.820 total time=  13.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.825 total time=  41.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.820 total time=  28.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.868 total time=   4.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.855 total time=  14.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.871 total time=  13.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.871 total time=  12.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.872 total time=  19.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  12.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=  12.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.873 total time=  19.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.865 total time=  14.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.873 total time=  15.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   3.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.870 total time=   7.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.869 total time=   4.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   6.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.871 total time=  22.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.871 total time=   5.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.871 total time=   8.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   4.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.873 total time=   9.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.877 total time=  26.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.877 total time=  19.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.875 total time=   3.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.876 total time=   9.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.879 total time=  29.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.878 total time=   5.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.876 total time=  14.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.878 total time=  10.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.880 total time=   4.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.881 total time=   2.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.880 total time=   3.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=   7.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.880 total time=  12.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.871 total time=   4.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=   5.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.872 total time=   2.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.876 total time=   4.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.878 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.878 total time=   4.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.875 total time=   3.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.875 total time=   5.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.875 total time=   2.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.876 total time=   4.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.881 total time=   2.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.880 total time=   4.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   1.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.871 total time=   4.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.880 total time=   6.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.780 total time=   7.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.774 total time=  45.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.797 total time=  53.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.810 total time=  45.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.824 total time=  37.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.819 total time=  12.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.867 total time=   3.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.852 total time=   7.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.851 total time=  15.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.868 total time=   8.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.863 total time=  24.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.867 total time=   4.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.861 total time=  15.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.873 total time=  22.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  22.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.864 total time=  25.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   3.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.875 total time=   6.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.875 total time=  18.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.862 total time=  11.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.865 total time=  11.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.874 total time=  27.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.864 total time=  18.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.865 total time=  19.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.866 total time=   4.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.875 total time=   8.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.876 total time=  28.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.879 total time=  18.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   2.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.869 total time=   2.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   4.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.873 total time=  10.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.875 total time=   8.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.878 total time=  12.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.876 total time=   4.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.876 total time=  14.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.877 total time=  13.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.871 total time=  13.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.875 total time=  11.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.878 total time=   2.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.868 total time=   4.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.878 total time=  12.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.877 total time=   4.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.876 total time=   2.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   4.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.881 total time=   1.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.878 total time=   4.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.877 total time=  14.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.780 total time=  34.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.798 total time=   9.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.776 total time=  27.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.812 total time=  26.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.800 total time=  30.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.818 total time=   6.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.825 total time=  15.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.824 total time=   6.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.824 total time=  13.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.820 total time=  42.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.864 total time=  22.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.870 total time=   3.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.871 total time=  21.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.872 total time=  12.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.872 total time=  11.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  11.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  13.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.865 total time=   3.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.861 total time=   8.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.867 total time=   7.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.868 total time=   4.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.869 total time=   7.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.869 total time=   4.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.870 total time=   8.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.871 total time=   6.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.874 total time=   7.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.874 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.875 total time=   7.2s\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'bootstrap': True, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "0.8782390066936319\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.860\n",
      "mean_squared_errors: 0.139\n",
      "mean_absolute_errors: 0.224\n",
      "r2_score: 0.860\n",
      "acc: 0.595\n",
      "acc(+-0.5mm): 0.963\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.873 total time=  17.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.862 total time=  28.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.874 total time=  16.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.872 total time=  18.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.876 total time=   4.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.876 total time=   9.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.867 total time=   4.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.875 total time=   8.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.875 total time=  18.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.875 total time=   2.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   4.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.874 total time=   9.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.865 total time=  10.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.873 total time=   8.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.874 total time=  10.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.874 total time=   5.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.882 total time=   4.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.880 total time=  14.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.875 total time=   8.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.873 total time=  14.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.881 total time=   9.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.880 total time=  12.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.874 total time=   4.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.879 total time=   1.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.871 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.871 total time=  13.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.883 total time=   8.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.778 total time=  29.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.776 total time=   7.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.796 total time=  15.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.799 total time=  53.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.809 total time=   8.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.797 total time=  26.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.824 total time=  27.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.814 total time=  27.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.850 total time=   4.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.867 total time=   7.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.868 total time=  16.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.870 total time=  15.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.869 total time=  23.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.870 total time=  14.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   3.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  18.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.865 total time=  13.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.868 total time=  25.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.871 total time=  21.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.861 total time=  23.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  22.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.874 total time=  31.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.873 total time=  28.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.873 total time=  21.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   5.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   8.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.876 total time=   4.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.879 total time=   8.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.868 total time=  26.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.875 total time=  10.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.865 total time=  15.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.876 total time=   9.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.877 total time=  14.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=   7.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  11.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   4.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.871 total time=  16.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.875 total time=   2.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.876 total time=   4.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.874 total time=  15.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.877 total time=  13.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=   8.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.877 total time=  12.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.778 total time=   6.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.779 total time=  17.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.796 total time=   7.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.802 total time=  15.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.812 total time=   7.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.792 total time=  13.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.803 total time=  29.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.806 total time=   6.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.816 total time=  14.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.805 total time=  43.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.820 total time=  28.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.865 total time=   4.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.865 total time=   8.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.866 total time=  16.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.868 total time=  13.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.869 total time=  22.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.872 total time=  14.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   3.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.865 total time=  20.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=  12.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.866 total time=  22.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.859 total time=  13.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.868 total time=  12.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.870 total time=  18.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.875 total time=  11.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.877 total time=  14.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.872 total time=  19.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=  13.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.873 total time=  32.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.872 total time=  31.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.875 total time=  29.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.876 total time=  17.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.879 total time=   7.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.879 total time=  24.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.867 total time=   2.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.872 total time=   2.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.875 total time=   9.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.876 total time=   8.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.874 total time=  14.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.884 total time=  11.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.871 total time=   8.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.873 total time=  10.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.877 total time=   4.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.878 total time=  13.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.868 total time=   9.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.869 total time=  13.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.880 total time=  10.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.761 total time=   7.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.776 total time=  49.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.784 total time=  45.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.803 total time=  34.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.819 total time=   6.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.824 total time=  13.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.814 total time=   7.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.814 total time=  13.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.814 total time=   7.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.824 total time=  14.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.820 total time=  42.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.859 total time=  14.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.868 total time=  14.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.868 total time=   3.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   7.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   3.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.871 total time=   5.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.874 total time=   3.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.875 total time=   6.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  20.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.864 total time=  14.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.857 total time=   3.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.859 total time=   7.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.869 total time=  23.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.872 total time=  22.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.873 total time=  13.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.870 total time=   3.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.871 total time=   6.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.860 total time=  11.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.871 total time=  22.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.866 total time=   5.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.877 total time=  10.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.866 total time=   4.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.866 total time=  10.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.873 total time=  27.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.877 total time=  17.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.876 total time=  27.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.875 total time=  15.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.877 total time=  10.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.878 total time=   9.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.878 total time=   9.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   3.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   3.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.879 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.880 total time=   3.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.883 total time=   7.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.874 total time=  14.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.873 total time=   4.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.881 total time=   2.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.881 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.876 total time=  13.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.882 total time=   8.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=   8.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  12.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.776 total time=  16.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.773 total time=  29.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.797 total time=  30.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.808 total time=  28.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.805 total time=   7.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.795 total time=  15.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.814 total time=   6.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.814 total time=  14.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.820 total time=  42.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.820 total time=  27.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.869 total time=   4.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.867 total time=   7.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.870 total time=  23.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.875 total time=  22.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.872 total time=  21.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  19.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.866 total time=  13.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.866 total time=  15.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   3.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.874 total time=   7.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.870 total time=  21.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.871 total time=  14.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.870 total time=   3.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.871 total time=   6.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   7.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.874 total time=  31.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.876 total time=  28.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.877 total time=  16.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.874 total time=  16.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.879 total time=  17.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.876 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.868 total time=   9.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.875 total time=  26.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.867 total time=   4.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.877 total time=  13.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.874 total time=   9.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.880 total time=   4.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  13.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.880 total time=   8.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.875 total time=  15.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.878 total time=   9.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.879 total time=  12.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.878 total time=   9.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.880 total time=  12.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=   9.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.761 total time=  30.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.789 total time=   7.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.786 total time=  17.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.798 total time=   7.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.809 total time=  14.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.802 total time=   6.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.812 total time=  13.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.792 total time=  45.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.814 total time=  27.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.824 total time=   6.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.814 total time=  13.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.824 total time=  43.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.870 total time=   6.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.869 total time=   4.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.871 total time=   7.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   3.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.870 total time=   6.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.861 total time=   3.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.862 total time=   6.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.871 total time=  19.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  13.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.871 total time=  19.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.870 total time=  14.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.871 total time=  21.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.870 total time=  14.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.868 total time=  22.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   7.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.869 total time=   9.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.872 total time=   9.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.871 total time=  30.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.873 total time=  31.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.874 total time=  27.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.877 total time=  27.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.874 total time=  16.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.873 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.875 total time=   2.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.877 total time=   4.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.873 total time=   2.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.881 total time=   5.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.877 total time=   4.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   2.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   3.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  12.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=   9.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.878 total time=  14.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.875 total time=   9.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.882 total time=  12.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.881 total time=   9.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.880 total time=  14.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.880 total time=  13.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.783 total time=   9.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.777 total time=  30.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.787 total time=  31.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.808 total time=  42.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.795 total time=  29.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.819 total time=  43.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.814 total time=  42.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.868 total time=   3.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.859 total time=  21.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.870 total time=  14.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.873 total time=  18.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.865 total time=  13.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  19.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.868 total time=  14.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.873 total time=  13.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.875 total time=  21.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.868 total time=  13.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.873 total time=   7.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.873 total time=   7.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.869 total time=   4.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.872 total time=   6.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.862 total time=  18.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.874 total time=  19.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.865 total time=  27.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.878 total time=  20.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.879 total time=   4.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.879 total time=   9.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.879 total time=  26.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.878 total time=  14.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.881 total time=  10.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.877 total time=  13.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.882 total time=   8.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.876 total time=  13.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.877 total time=   9.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.875 total time=  15.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.876 total time=  15.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.882 total time=   4.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.877 total time=  11.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.871 total time=   7.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.880 total time=   7.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.770 total time=  13.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.780 total time=  14.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.761 total time=  46.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.811 total time=  30.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.811 total time=   7.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.800 total time=  15.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.820 total time=   7.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.824 total time=  13.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.814 total time=  41.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.824 total time=  27.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.868 total time=   3.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.861 total time=   4.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.868 total time=  13.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.866 total time=   6.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   3.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.875 total time=   7.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   4.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.871 total time=   6.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.872 total time=   3.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.875 total time=   6.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.874 total time=   3.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   6.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.873 total time=  19.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=  13.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.866 total time=  21.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.871 total time=  14.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.861 total time=   3.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.861 total time=   7.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.871 total time=  21.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  23.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.873 total time=   6.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.864 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.865 total time=   6.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.871 total time=  13.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.873 total time=  11.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.870 total time=  19.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.874 total time=  28.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.874 total time=  19.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.878 total time=  26.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.877 total time=  18.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.875 total time=  26.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.876 total time=  14.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.880 total time=  10.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.879 total time=   1.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.878 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.877 total time=   8.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.881 total time=   2.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.880 total time=   4.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.861 total time=   3.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.876 total time=   6.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.871 total time=   2.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.879 total time=  10.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.877 total time=   3.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.880 total time=   4.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.877 total time=  13.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.877 total time=   8.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.884 total time=  11.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.871 total time=   8.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.761 total time=  16.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.764 total time=  30.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.786 total time=  28.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.792 total time=  44.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.813 total time=  29.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.824 total time=  41.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.814 total time=  28.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.864 total time=   4.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.869 total time=  16.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.873 total time=  22.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.871 total time=  12.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.862 total time=  22.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  19.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.867 total time=  15.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.870 total time=  20.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.872 total time=  14.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.875 total time=  23.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   6.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  14.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.872 total time=  20.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.871 total time=   4.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.876 total time=   9.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   4.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.866 total time=   9.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.877 total time=  19.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.875 total time=   4.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.875 total time=   9.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.874 total time=  27.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.876 total time=  20.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   4.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.864 total time=  10.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.876 total time=   9.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.880 total time=  10.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.876 total time=  13.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.877 total time=  10.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   4.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   5.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.873 total time=   2.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.877 total time=   4.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.875 total time=   2.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.865 total time=   4.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  13.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.868 total time=   9.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.877 total time=   9.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.879 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.880 total time=   3.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  11.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.760 total time=   7.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.761 total time=  53.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.803 total time=   7.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.803 total time=  16.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.803 total time=   7.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.802 total time=  13.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.812 total time=  41.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.814 total time=  29.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.814 total time=  45.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.854 total time=   3.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.865 total time=   7.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.868 total time=  22.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.870 total time=  21.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.871 total time=  12.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  20.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.871 total time=  12.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.868 total time=  23.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.859 total time=  22.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.874 total time=  12.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.872 total time=  13.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   3.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   6.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.877 total time=  16.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.871 total time=  18.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.875 total time=  11.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.875 total time=   9.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.860 total time=   4.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.861 total time=   5.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.869 total time=   9.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.864 total time=   4.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.864 total time=   9.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.874 total time=  28.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.874 total time=  19.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.867 total time=   3.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.868 total time=   9.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.868 total time=  28.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.876 total time=  17.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.874 total time=   2.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=   5.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.874 total time=  15.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.875 total time=   2.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.868 total time=   5.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.877 total time=  12.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.880 total time=   7.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.880 total time=   7.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=   8.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.877 total time=  14.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.876 total time=   8.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.874 total time=   9.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.881 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.880 total time=   4.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.874 total time=  13.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.880 total time=  13.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.780 total time=  16.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.790 total time=  28.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.789 total time=  29.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.811 total time=  47.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.797 total time=  44.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.824 total time=  27.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.820 total time=  39.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.872 total time=   3.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.871 total time=  14.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.870 total time=  13.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.875 total time=  23.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.871 total time=  18.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.877 total time=  11.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.866 total time=  14.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.867 total time=  21.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.870 total time=  14.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.872 total time=   4.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.868 total time=   7.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.870 total time=   3.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.871 total time=   6.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   2.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   5.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  17.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.869 total time=  10.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.873 total time=  19.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.877 total time=  19.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.875 total time=  26.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.866 total time=  18.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.875 total time=  17.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.876 total time=   5.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   8.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.874 total time=   2.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.873 total time=   2.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.872 total time=   4.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.873 total time=   9.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.878 total time=  11.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.877 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.873 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.873 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.875 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.878 total time=   5.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.878 total time=   4.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.879 total time=  14.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  12.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.877 total time=   9.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.876 total time=  15.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.876 total time=   8.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.882 total time=  13.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.882 total time=   7.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=   7.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.880 total time=  11.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.761 total time=   8.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.761 total time=  16.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.764 total time=  49.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.810 total time=   7.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.803 total time=  14.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.803 total time=  46.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.820 total time=  42.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.818 total time=  27.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.870 total time=   6.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.868 total time=  21.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.861 total time=  12.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.874 total time=  14.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.868 total time=  20.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  13.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.867 total time=   3.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.867 total time=   7.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.854 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.870 total time=   8.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.870 total time=   3.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.869 total time=   7.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.870 total time=   3.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.872 total time=   7.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.870 total time=  21.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.872 total time=  12.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.874 total time=   3.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.875 total time=   7.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  17.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.861 total time=  31.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.875 total time=  29.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.877 total time=  18.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.875 total time=  27.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.877 total time=  19.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.876 total time=   2.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.877 total time=   6.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.875 total time=  15.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.877 total time=   4.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.873 total time=  13.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.877 total time=   9.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.883 total time=   2.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.871 total time=  13.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.880 total time=   9.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.873 total time=  13.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.877 total time=   9.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.882 total time=   8.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.877 total time=  14.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=   8.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.880 total time=  11.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.761 total time=  15.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.777 total time=  15.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.794 total time=  48.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.800 total time=  46.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.804 total time=  31.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.824 total time=   7.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.824 total time=  13.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.825 total time=   7.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.814 total time=  13.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.819 total time=  44.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.869 total time=  20.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.872 total time=  13.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.875 total time=  14.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   4.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.871 total time=   7.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.865 total time=   8.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.864 total time=   4.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.868 total time=   7.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.868 total time=   3.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.864 total time=   7.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.866 total time=  22.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.872 total time=  13.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.872 total time=  19.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=  12.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  18.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.871 total time=  11.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.873 total time=  17.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.871 total time=   9.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.871 total time=   8.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.874 total time=  29.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.873 total time=  18.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.875 total time=   4.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.866 total time=  11.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.872 total time=  29.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.873 total time=  29.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.865 total time=   5.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.877 total time=  15.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.868 total time=   9.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.878 total time=  14.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.882 total time=   8.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.876 total time=  13.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.874 total time=  16.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.878 total time=  14.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.882 total time=   8.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.876 total time=  14.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.870 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.875 total time=   5.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.785 total time=   7.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.785 total time=  47.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.786 total time=  45.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.794 total time=  27.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.804 total time=  41.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.824 total time=  28.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.825 total time=  41.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.873 total time=   7.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.869 total time=   3.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.870 total time=   6.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.875 total time=  21.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.875 total time=  14.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   6.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   6.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  23.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.863 total time=  25.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   3.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.874 total time=   7.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.872 total time=   3.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.870 total time=   6.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.875 total time=  22.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  17.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   6.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.875 total time=  19.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.868 total time=   6.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.871 total time=   5.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.875 total time=  18.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.872 total time=  21.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.872 total time=   5.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.872 total time=   8.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.877 total time=  26.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.876 total time=  18.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.875 total time=  25.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.877 total time=  15.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.877 total time=   4.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.878 total time=   2.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.880 total time=   4.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.877 total time=  14.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.877 total time=   8.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.880 total time=  14.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=   9.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.863 total time=   2.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.875 total time=   4.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  14.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.879 total time=   8.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.876 total time=   9.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.883 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.882 total time=   4.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.870 total time=   2.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.874 total time=   1.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   4.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.876 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.880 total time=   9.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.769 total time=  31.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.787 total time=   9.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.777 total time=  16.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.800 total time=   7.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.798 total time=  15.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.809 total time=   7.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.809 total time=  14.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.799 total time=  46.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.814 total time=  43.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.824 total time=  29.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.868 total time=   8.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.867 total time=  22.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.875 total time=  12.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.870 total time=  18.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.865 total time=  11.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.875 total time=  11.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=  14.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.868 total time=   3.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.855 total time=   6.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.869 total time=  19.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.861 total time=  13.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.872 total time=  14.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.862 total time=  18.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.865 total time=  13.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=   9.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  12.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  18.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.867 total time=   7.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.868 total time=   3.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.871 total time=   6.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.872 total time=  20.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.865 total time=  19.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.870 total time=  19.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.870 total time=  30.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.873 total time=  27.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.873 total time=  18.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.879 total time=  23.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.867 total time=  15.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.878 total time=  13.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  14.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.882 total time=   9.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.875 total time=   2.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.877 total time=   3.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.876 total time=  13.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.877 total time=   8.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.884 total time=  11.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=   5.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.876 total time=   3.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.868 total time=  10.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.867 total time=   2.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.878 total time=   5.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.868 total time=   4.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.877 total time=  14.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.877 total time=   9.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.770 total time=   8.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.780 total time=  49.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.794 total time=  44.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.810 total time=  27.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.801 total time=  44.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.818 total time=  28.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.864 total time=   4.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.861 total time=   7.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.852 total time=  22.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.870 total time=   3.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.869 total time=   8.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.867 total time=   3.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.867 total time=   6.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.869 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.875 total time=   7.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.874 total time=   3.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   7.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   3.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  19.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=  13.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.868 total time=  25.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.873 total time=   4.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.871 total time=   6.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.871 total time=  19.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.872 total time=  13.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.865 total time=  18.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.865 total time=  11.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  11.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.877 total time=  10.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.871 total time=  35.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   5.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.874 total time=   8.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   5.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.874 total time=   9.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.873 total time=  30.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.876 total time=  26.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.873 total time=  15.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.874 total time=   2.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.875 total time=   4.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.872 total time=   8.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.880 total time=  12.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.868 total time=   8.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.879 total time=   8.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.880 total time=   8.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.870 total time=   1.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.871 total time=   5.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  15.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.879 total time=  15.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.868 total time=  15.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.879 total time=  13.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.880 total time=   9.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.790 total time=   8.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.770 total time=  45.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.776 total time=  45.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.812 total time=  27.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.811 total time=  30.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.818 total time=   7.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.818 total time=  14.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.820 total time=   8.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.825 total time=  13.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.824 total time=  42.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.869 total time=  14.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.861 total time=  21.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.873 total time=  14.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.874 total time=   3.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   5.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  16.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.864 total time=   8.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.864 total time=   4.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.855 total time=  15.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.871 total time=  14.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.868 total time=  22.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.871 total time=  21.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.871 total time=  18.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.874 total time=  19.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.870 total time=  30.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.874 total time=  31.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.876 total time=   5.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   8.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.876 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.868 total time=   9.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.877 total time=  25.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.871 total time=  10.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.874 total time=   9.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.876 total time=  14.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.876 total time=  10.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.876 total time=   1.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.882 total time=  15.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.876 total time=  13.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.872 total time=  11.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   2.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.876 total time=   4.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.881 total time=   2.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.881 total time=   4.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.878 total time=   4.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.881 total time=  13.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=   9.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.876 total time=  12.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.779 total time=   7.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.777 total time=  16.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.790 total time=  48.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.812 total time=   8.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.810 total time=  14.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.811 total time=  42.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.824 total time=  27.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.824 total time=  28.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.863 total time=   3.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.864 total time=   7.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.865 total time=  18.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.855 total time=  23.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.872 total time=  13.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.872 total time=  20.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  14.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.873 total time=   6.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.860 total time=   3.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.865 total time=   7.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.862 total time=  22.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.871 total time=  20.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.861 total time=  14.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.873 total time=  18.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  12.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=  10.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=  11.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.873 total time=  19.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.874 total time=  21.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.876 total time=   4.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.877 total time=   8.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.877 total time=  27.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.879 total time=  18.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.875 total time=   3.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.875 total time=  10.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.861 total time=   3.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   4.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.875 total time=  11.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.879 total time=   9.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.881 total time=  13.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.876 total time=   9.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.874 total time=   9.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.876 total time=   2.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.883 total time=   5.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.875 total time=   4.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   2.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   4.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.865 total time=  10.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.867 total time=  12.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.877 total time=   4.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.874 total time=  12.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.871 total time=  10.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.879 total time=   2.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.880 total time=   5.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.776 total time=   8.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.770 total time=  48.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.787 total time=  47.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.808 total time=  29.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.825 total time=   7.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.818 total time=  16.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.825 total time=   7.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.825 total time=  12.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.824 total time=  38.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.866 total time=  22.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.868 total time=   3.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.869 total time=   6.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   3.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.871 total time=   8.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   3.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.874 total time=   6.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.872 total time=  19.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=  11.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.873 total time=  12.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.850 total time=   4.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.852 total time=   7.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.863 total time=   4.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.869 total time=   7.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.868 total time=  20.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.872 total time=  13.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.870 total time=  22.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  18.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=  11.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.871 total time=  10.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.874 total time=  17.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.875 total time=  19.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.873 total time=  18.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.874 total time=   4.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.875 total time=   9.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.878 total time=  30.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.879 total time=   9.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.873 total time=  25.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.880 total time=   2.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.867 total time=  14.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.876 total time=   8.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.876 total time=   8.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  13.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.882 total time=   7.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.871 total time=  10.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.875 total time=   2.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.877 total time=   4.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.881 total time=  14.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.880 total time=  10.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.876 total time=   3.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.875 total time=   5.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.875 total time=   4.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   4.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.777 total time=   7.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.790 total time=  46.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.802 total time=  45.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.792 total time=  28.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.801 total time=  31.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.819 total time=   7.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.819 total time=  14.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.824 total time=   6.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.818 total time=  14.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.864 total time=   3.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.867 total time=   8.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.864 total time=  14.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.865 total time=  15.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.872 total time=  20.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.867 total time=   6.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.870 total time=   5.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.868 total time=   6.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   2.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.873 total time=   7.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   6.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.877 total time=  19.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.868 total time=  16.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.868 total time=   6.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.866 total time=  12.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.872 total time=  21.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.875 total time=  13.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.871 total time=  18.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=  11.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.871 total time=  30.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.864 total time=  27.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.865 total time=  18.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.875 total time=  18.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.875 total time=  28.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.877 total time=  27.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.875 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.876 total time=  10.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.874 total time=  13.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.877 total time=   9.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.874 total time=   2.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.875 total time=   4.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   5.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.869 total time=   2.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.864 total time=  10.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  16.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.876 total time=  15.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.882 total time=  13.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.871 total time=  12.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.778 total time=  16.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.779 total time=  29.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.802 total time=  28.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.803 total time=  27.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.794 total time=  40.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.820 total time=  25.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.820 total time=  28.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.867 total time=   3.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.865 total time=   7.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.867 total time=  16.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.867 total time=  16.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.871 total time=  24.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.868 total time=  20.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.873 total time=  12.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   6.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  20.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.867 total time=  15.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.869 total time=   3.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.870 total time=   6.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.872 total time=  24.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.875 total time=  20.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.873 total time=  10.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.871 total time=   4.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.873 total time=   5.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.872 total time=   9.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.873 total time=  31.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   5.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.875 total time=   9.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.875 total time=  28.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.875 total time=  17.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.867 total time=   4.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.876 total time=   8.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.879 total time=  26.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.878 total time=   2.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.875 total time=   8.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.879 total time=   8.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.868 total time=  14.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=   9.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.879 total time=   2.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.878 total time=   4.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.880 total time=  11.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.878 total time=   4.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.878 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.876 total time=   4.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.876 total time=  13.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.873 total time=   4.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.869 total time=   8.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.880 total time=   8.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.875 total time=   8.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  12.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.773 total time=   6.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.794 total time=  15.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.777 total time=  45.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.794 total time=  28.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.798 total time=   7.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.814 total time=  13.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.811 total time=  45.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.824 total time=  42.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.868 total time=  22.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.859 total time=   7.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.870 total time=  21.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.875 total time=  20.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.877 total time=  12.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  16.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.868 total time=   6.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.865 total time=  24.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.872 total time=  19.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.873 total time=  13.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.877 total time=  19.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=  11.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  20.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.871 total time=  10.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.873 total time=   4.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.869 total time=   4.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.873 total time=   9.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.873 total time=  27.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.875 total time=  17.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.875 total time=  26.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.877 total time=  16.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.879 total time=  15.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.868 total time=  26.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.875 total time=   2.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.876 total time=   4.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.881 total time=   2.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.879 total time=   4.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.878 total time=  12.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.869 total time=  10.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.870 total time=   3.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.882 total time=   4.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   3.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.882 total time=  13.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.875 total time=   9.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.880 total time=  14.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.868 total time=  14.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.883 total time=  10.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   3.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.774 total time=   7.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.778 total time=  48.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.789 total time=  45.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.811 total time=  27.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.809 total time=  27.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.820 total time=   6.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.820 total time=  13.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.820 total time=  39.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.861 total time=  14.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.866 total time=  14.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.871 total time=  22.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.875 total time=  13.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.868 total time=  12.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   4.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   5.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.871 total time=  20.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.861 total time=  14.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.870 total time=   4.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.870 total time=   7.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   4.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.871 total time=   7.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   3.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.871 total time=   6.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.872 total time=  20.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.877 total time=  13.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   6.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  12.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   5.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.873 total time=  11.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.875 total time=  13.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.874 total time=  19.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.873 total time=  32.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   4.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.873 total time=  10.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   5.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.873 total time=  10.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   3.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.875 total time=   9.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.873 total time=  26.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.877 total time=  28.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.876 total time=   4.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.878 total time=   2.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.875 total time=   4.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   2.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.873 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.876 total time=  14.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.879 total time=   2.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.871 total time=   3.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=   9.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.875 total time=   3.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.874 total time=   9.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.877 total time=  16.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.877 total time=  12.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.879 total time=   9.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.877 total time=   8.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.882 total time=  12.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.790 total time=  15.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.761 total time=  32.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.783 total time=  28.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.800 total time=  30.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.804 total time=   6.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.811 total time=  12.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.795 total time=  47.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.818 total time=  42.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.862 total time=  24.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.871 total time=  15.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.872 total time=   6.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.871 total time=  22.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  19.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.873 total time=  12.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.862 total time=  22.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.869 total time=  14.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.870 total time=   3.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.872 total time=   6.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.875 total time=  20.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  13.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.865 total time=   6.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.877 total time=  13.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.870 total time=  30.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.872 total time=  20.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   4.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.875 total time=  10.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.874 total time=  28.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.877 total time=  25.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.876 total time=  17.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.873 total time=  15.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.878 total time=  10.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.879 total time=  14.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.877 total time=  14.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.884 total time=  12.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.875 total time=   8.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.874 total time=  10.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.877 total time=  14.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.876 total time=  15.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.883 total time=   2.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   4.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.883 total time=   2.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.883 total time=   3.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  11.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.779 total time=   8.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.764 total time=  16.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.783 total time=  46.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.794 total time=  51.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.820 total time=   6.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.824 total time=  13.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.824 total time=  39.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.824 total time=  29.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.863 total time=   4.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.864 total time=   7.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.865 total time=  25.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.861 total time=   4.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.870 total time=   7.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.872 total time=   3.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.870 total time=   6.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.871 total time=  20.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.865 total time=  19.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.862 total time=  14.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.868 total time=  22.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.874 total time=  14.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.861 total time=   3.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   7.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.864 total time=   4.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.873 total time=  21.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  13.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.867 total time=   7.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.869 total time=   7.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.870 total time=  22.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.875 total time=  15.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   7.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   3.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   7.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   6.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   6.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.865 total time=  14.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=  10.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.870 total time=   8.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  30.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.877 total time=  28.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.866 total time=  27.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.867 total time=  16.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.879 total time=  18.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.876 total time=   4.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.875 total time=  10.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.875 total time=   2.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.876 total time=   4.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.877 total time=   8.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.882 total time=   4.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.877 total time=  13.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.882 total time=   9.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   3.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.874 total time=   9.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.865 total time=  15.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.880 total time=  10.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.877 total time=   5.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   2.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   6.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.880 total time=   4.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.879 total time=   2.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.878 total time=   3.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.882 total time=  12.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.776 total time=   8.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.793 total time=  32.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.794 total time=  27.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.803 total time=  47.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.817 total time=  46.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.819 total time=  26.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.864 total time=   4.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.861 total time=   7.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.862 total time=  16.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.870 total time=  14.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.869 total time=  12.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.874 total time=  14.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.869 total time=   3.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   6.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.864 total time=   2.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.865 total time=   5.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.877 total time=  12.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.875 total time=  20.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.865 total time=  15.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.870 total time=   4.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.873 total time=   7.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.867 total time=  20.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.875 total time=  13.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.875 total time=  22.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   7.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  13.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.873 total time=  20.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  11.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  14.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.869 total time=  28.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.875 total time=  18.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.874 total time=  21.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.872 total time=   4.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   9.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   4.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.873 total time=   9.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.875 total time=   5.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.875 total time=   7.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.878 total time=   5.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   9.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   2.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.876 total time=   4.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.877 total time=  10.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.876 total time=   9.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.878 total time=  14.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.879 total time=   8.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.881 total time=   7.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.875 total time=   7.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.880 total time=  13.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.873 total time=  10.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.875 total time=   2.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.879 total time=   5.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.873 total time=  13.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.879 total time=   8.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.874 total time=   8.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.876 total time=  13.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  12.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.794 total time=   7.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.773 total time=  14.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.780 total time=  46.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.810 total time=  28.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.800 total time=  43.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.824 total time=  27.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.814 total time=   6.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.820 total time=  13.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.824 total time=  42.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.871 total time=   7.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.870 total time=   3.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.872 total time=   7.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.870 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.871 total time=   6.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.875 total time=  22.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.877 total time=  20.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  22.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.855 total time=  23.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.870 total time=  13.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.868 total time=  19.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  13.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.864 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.875 total time=   6.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  14.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.870 total time=   5.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.869 total time=   5.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.874 total time=  18.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.876 total time=  18.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.872 total time=  18.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   4.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.878 total time=   9.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.879 total time=   4.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.879 total time=   8.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.879 total time=  26.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.879 total time=  18.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=   4.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.879 total time=  15.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.876 total time=   9.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.881 total time=   2.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.876 total time=   4.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.874 total time=  13.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=   8.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  13.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.876 total time=   9.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.873 total time=  11.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.873 total time=   2.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.876 total time=   8.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.877 total time=   8.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  12.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.882 total time=   7.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.774 total time=  14.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.790 total time=  15.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.773 total time=  46.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.812 total time=  39.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.809 total time=  15.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.824 total time=   7.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.819 total time=  13.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.820 total time=   7.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.820 total time=  12.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.825 total time=  37.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.868 total time=  14.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.862 total time=   7.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.866 total time=  22.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.870 total time=  13.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   6.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.872 total time=  20.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.871 total time=  13.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.860 total time=   4.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.851 total time=  15.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.863 total time=  13.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.873 total time=  19.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.872 total time=  12.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.875 total time=  12.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  13.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   3.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   6.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.873 total time=  14.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.872 total time=  29.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.872 total time=  18.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.875 total time=  27.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.876 total time=  18.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.876 total time=  23.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.873 total time=   9.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.863 total time=   5.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.874 total time=  10.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.875 total time=  10.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.876 total time=  13.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.882 total time=  10.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.879 total time=   1.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.871 total time=   4.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.883 total time=   9.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.883 total time=   2.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.883 total time=   4.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.877 total time=  12.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.876 total time=   9.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.880 total time=   8.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.876 total time=   8.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.874 total time=  11.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.874 total time=   2.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.880 total time=   8.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  14.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.776 total time=  30.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.802 total time=   6.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.776 total time=  14.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.778 total time=  44.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.803 total time=  27.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.806 total time=  38.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.814 total time=  28.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.825 total time=  25.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.855 total time=   7.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.863 total time=  16.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.873 total time=   4.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.871 total time=   6.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.868 total time=  21.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.875 total time=  19.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  12.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.875 total time=  22.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.868 total time=  14.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.861 total time=   7.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.861 total time=  20.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.871 total time=  14.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  21.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.871 total time=  14.0s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "SEED = 98\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.05, 0.07, 0.1], #[0.01, 0.03, 0.05],\n",
    "                'n_estimators': [50, 100, 200, 300],\n",
    "                'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                'min_samples_split': [2,3,5],\n",
    "                'min_samples_leaf': [1,2,3],\n",
    "               'bootstrap': [True, False]\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=RandomForestRegressor(random_state = SEED),\n",
    "                    n_jobs=-1,\n",
    "                    verbose=5,\n",
    "                    param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train_imputed, y_train)\n",
    "#model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = gs.predict(x_test_imputed).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test)\n",
    "acc3 = np.mean((y_pred >= y_test-0.5) & (y_pred <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da3b3e47-4210-4003-bca3-e541c27756e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T06:25:07.436597Z",
     "iopub.status.busy": "2023-02-17T06:25:07.436082Z",
     "iopub.status.idle": "2023-02-17T06:25:07.451274Z",
     "shell.execute_reply": "2023-02-17T06:25:07.450547Z",
     "shell.execute_reply.started": "2023-02-17T06:25:07.436540Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.860\n",
      "mean_squared_errors: 0.139\n",
      "mean_absolute_errors: 0.224\n",
      "r2_score: 0.860\n",
      "acc: 0.595\n",
      "acc(+-0.5mm): 0.963\n"
     ]
    }
   ],
   "source": [
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test)\n",
    "acc3 = np.mean((y_pred >= y_test-0.5) & (y_pred <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')\n",
    "\n",
    "# save model\n",
    "odir_f = f'acc1-{acc1:.3f}_acc3-{acc3:.3f}_RF_{nfold}fold'\n",
    "odir = f'result/{odir_f}'\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "#model.save_model(f'{odir}/model.model')\n",
    "pickle.dump(gs, open(f'{odir}/gridSearch','wb'))\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "pickle.dump(param_dict, open(f'{odir}/param_dict', 'wb'))\n",
    "f = open(f'{odir}/result.txt', 'w')\n",
    "f.write(f'classification model')\n",
    "f.write(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "f.write(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7dd5a1-5bb0-47f7-a345-2db0fe75a0e7",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe6dfa3-2cac-401f-a2fd-2131dd8311e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T01:58:53.028999Z",
     "iopub.status.busy": "2023-02-18T01:58:53.028449Z",
     "iopub.status.idle": "2023-02-18T01:58:56.117327Z",
     "shell.execute_reply": "2023-02-18T01:58:56.116737Z",
     "shell.execute_reply.started": "2023-02-18T01:58:53.028873Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import losses, metrics\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization, Dropout, Activation, Input\n",
    "from keras.layers import GlobalAveragePooling1D, Flatten, SeparableConv1D, concatenate, Add\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats, interp\n",
    "import os, sys, pickle, shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, datetime, time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# tensorflow 사용 시 seed 고정\n",
    "def seed_everything(seed: int = 98):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "SEED = 98\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf224896-6db3-40ad-a6df-549453d8e54d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T01:58:56.118615Z",
     "iopub.status.busy": "2023-02-18T01:58:56.118380Z",
     "iopub.status.idle": "2023-02-18T01:58:56.128196Z",
     "shell.execute_reply": "2023-02-18T01:58:56.127676Z",
     "shell.execute_reply.started": "2023-02-18T01:58:56.118591Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (27234, 5), x_test: (6808, 5)\n"
     ]
    }
   ],
   "source": [
    "dat = np.load(f'dataset/ETT_size.npz')\n",
    "x, y = dat['x'], dat['y']\n",
    "#y_old  = dat['y_old']\n",
    "\n",
    "# training set의 뒤쪽 20%를 test set 으로 사용\n",
    "nsamp = len(y)\n",
    "ntest = int(nsamp * 0.2)\n",
    "ntrain = nsamp - ntest\n",
    "x_test = x[-ntest:, :]\n",
    "y_test = y[-ntest:]\n",
    "#y_test_old = y_old[-ntest:]\n",
    "x_train = x[:ntrain, :]\n",
    "y_train = y[:ntrain]\n",
    "\n",
    "print(f'x_train: {(x_train).shape}, x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81c772d-efb6-4ed1-96a1-cd75fc9dd925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T01:58:58.508581Z",
     "iopub.status.busy": "2023-02-18T01:58:58.508165Z",
     "iopub.status.idle": "2023-02-18T01:58:58.526396Z",
     "shell.execute_reply": "2023-02-18T01:58:58.525757Z",
     "shell.execute_reply.started": "2023-02-18T01:58:58.508533Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(pd.DataFrame(x_train))\n",
    "x_test = sc.transform(pd.DataFrame(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e22a5-fffb-4909-8ca0-942c4abbcd28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T06:55:13.891179Z",
     "iopub.status.busy": "2023-02-17T06:55:13.890746Z",
     "iopub.status.idle": "2023-02-17T06:55:14.216891Z",
     "shell.execute_reply": "2023-02-17T06:55:14.215723Z",
     "shell.execute_reply.started": "2023-02-17T06:55:13.891138Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(df.loc[:ntrain-1,INPUT_VARS])\n",
    "x_test = sc.transform(df.loc[ntrain:,INPUT_VARS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3c74cd-f749-476a-a20b-6fc7ca76c452",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T01:58:59.758569Z",
     "iopub.status.busy": "2023-02-18T01:58:59.758128Z",
     "iopub.status.idle": "2023-02-18T01:58:59.943546Z",
     "shell.execute_reply": "2023-02-18T01:58:59.942822Z",
     "shell.execute_reply.started": "2023-02-18T01:58:59.758520Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "imp = IterativeImputer().fit(x_train)\n",
    "x_train_imputed = imp.transform(x_train)\n",
    "x_test_imputed = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd558086-ada5-465e-b2fd-487f61617d34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T01:59:01.402974Z",
     "iopub.status.busy": "2023-02-18T01:59:01.402627Z",
     "iopub.status.idle": "2023-02-18T01:59:01.427110Z",
     "shell.execute_reply": "2023-02-18T01:59:01.426518Z",
     "shell.execute_reply.started": "2023-02-18T01:59:01.402934Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start making test settings...done\n",
      "2023-02-18 10:59:01.424856\n"
     ]
    }
   ],
   "source": [
    "# folder\n",
    "nfold = 10  # 각각의 hyperparameter에 대해 k-fold 를 시행하고 평균을 구한다.\n",
    "ntest = 500\n",
    "rootdir = f\"result/size/DNN_size_both_y\"\n",
    "\n",
    "if not os.path.exists(rootdir):\n",
    "    os.mkdir(rootdir)\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "f = open(f'{rootdir}/README.txt', 'w')\n",
    "f.write(f'model: DNN 2 layers, regression')\n",
    "f.write(f'input: age, sex, height, weight, cuffed 유무  output: tube size')\n",
    "f.close()\n",
    "    \n",
    "\n",
    "# test_settings\n",
    "layer_settings, test_settings = [], []\n",
    "\n",
    "\n",
    "# hyperparamters pool\n",
    "dropout_opts  = [0, 0.1, 0.2, 0.3, 0.4, 0.5] # dropout rate\n",
    "dense_opts = [16, 32, 64, 128, 256, 512]\n",
    "BATCH_SIZE = [32, 64, 128, 256, 512]\n",
    "lr_opts = [0.001, 0.002, 0.0005]\n",
    "\n",
    "print('start making test settings...', end='', flush=True)\n",
    "# test settings\n",
    "dnodes, dropouts = [], []\n",
    "for i in range(2):\n",
    "    dnodes.append(0)\n",
    "    dropouts.append(0)\n",
    "\n",
    "\n",
    "for dnode1 in dense_opts:\n",
    "    for dropout1 in dropout_opts:\n",
    "        for dnode2 in dense_opts:\n",
    "            for dropout2 in dropout_opts:\n",
    "                for batch_size in BATCH_SIZE:\n",
    "                    for learning_rate in lr_opts:\n",
    "                        test_settings.append([dnode1, dropout1, dnode2, dropout2, batch_size, learning_rate])                                   \n",
    "\n",
    "                        \n",
    "print('done')\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0817b887-6325-4dfb-8f4c-269786b0d41d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T01:59:03.626753Z",
     "iopub.status.busy": "2023-02-18T01:59:03.626023Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random search 0/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 10:59:03.711163: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 10:59:04.435367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30973 MB memory:  -> device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0f:00.0, compute capability: 7.0\n",
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.9278\n",
      "Epoch 00001: val_loss improved from inf to 0.19348, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "613/613 [==============================] - 4s 4ms/step - loss: 0.9244 - val_loss: 0.1935\n",
      "Epoch 2/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.2676\n",
      "Epoch 00002: val_loss improved from 0.19348 to 0.13977, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2676 - val_loss: 0.1398\n",
      "Epoch 3/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.2599\n",
      "Epoch 00003: val_loss did not improve from 0.13977\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2595 - val_loss: 0.1916\n",
      "Epoch 4/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.2632\n",
      "Epoch 00004: val_loss improved from 0.13977 to 0.13927, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2624 - val_loss: 0.1393\n",
      "Epoch 5/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.2518\n",
      "Epoch 00005: val_loss did not improve from 0.13927\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2514 - val_loss: 0.1444\n",
      "Epoch 6/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.2522\n",
      "Epoch 00006: val_loss did not improve from 0.13927\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2515 - val_loss: 0.1553\n",
      " ###0 fold : val acc1 0.559, acc3 0.956, mae 0.249###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612/613 [============================>.] - ETA: 0s - loss: 0.8756\n",
      "Epoch 00001: val_loss improved from inf to 0.20557, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 0.8750 - val_loss: 0.2056\n",
      "Epoch 2/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.2763\n",
      "Epoch 00002: val_loss improved from 0.20557 to 0.13710, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2759 - val_loss: 0.1371\n",
      "Epoch 3/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.2791\n",
      "Epoch 00003: val_loss did not improve from 0.13710\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2789 - val_loss: 0.2821\n",
      "Epoch 4/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.2498\n",
      "Epoch 00004: val_loss did not improve from 0.13710\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2492 - val_loss: 0.1432\n",
      " ###1 fold : val acc1 0.566, acc3 0.959, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - ETA: 0s - loss: 0.8660\n",
      "Epoch 00001: val_loss improved from inf to 0.20387, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 0.8660 - val_loss: 0.2039\n",
      "Epoch 2/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.2742\n",
      "Epoch 00002: val_loss improved from 0.20387 to 0.14028, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2740 - val_loss: 0.1403\n",
      "Epoch 3/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.2838\n",
      "Epoch 00003: val_loss did not improve from 0.14028\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2842 - val_loss: 0.3150\n",
      "Epoch 4/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.2604\n",
      "Epoch 00004: val_loss did not improve from 0.14028\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2604 - val_loss: 0.1680\n",
      " ###2 fold : val acc1 0.544, acc3 0.958, mae 0.252###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599/613 [============================>.] - ETA: 0s - loss: 0.8958\n",
      "Epoch 00001: val_loss improved from inf to 0.18587, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 0.8830 - val_loss: 0.1859\n",
      "Epoch 2/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.2737\n",
      "Epoch 00002: val_loss improved from 0.18587 to 0.14256, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2729 - val_loss: 0.1426\n",
      "Epoch 3/100\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.3089\n",
      "Epoch 00003: val_loss did not improve from 0.14256\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.3089 - val_loss: 0.2841\n",
      "Epoch 4/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.2555\n",
      "Epoch 00004: val_loss did not improve from 0.14256\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2551 - val_loss: 0.1820\n",
      " ###3 fold : val acc1 0.554, acc3 0.960, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - ETA: 0s - loss: 0.9150\n",
      "Epoch 00001: val_loss improved from inf to 0.17072, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 0.9150 - val_loss: 0.1707\n",
      "Epoch 2/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.2974\n",
      "Epoch 00002: val_loss improved from 0.17072 to 0.15626, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2973 - val_loss: 0.1563\n",
      "Epoch 3/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.3098\n",
      "Epoch 00003: val_loss improved from 0.15626 to 0.15126, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3090 - val_loss: 0.1513\n",
      "Epoch 4/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.2597\n",
      "Epoch 00004: val_loss did not improve from 0.15126\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2598 - val_loss: 0.1533\n",
      "Epoch 5/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.2502\n",
      "Epoch 00005: val_loss improved from 0.15126 to 0.13472, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2499 - val_loss: 0.1347\n",
      "Epoch 6/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.2817\n",
      "Epoch 00006: val_loss improved from 0.13472 to 0.13291, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2792 - val_loss: 0.1329\n",
      "Epoch 7/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.2113\n",
      "Epoch 00007: val_loss did not improve from 0.13291\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2109 - val_loss: 0.1425\n",
      "Epoch 8/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.2169\n",
      "Epoch 00008: val_loss improved from 0.13291 to 0.13017, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2165 - val_loss: 0.1302\n",
      "Epoch 9/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.2243\n",
      "Epoch 00009: val_loss did not improve from 0.13017\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2239 - val_loss: 0.1321\n",
      "Epoch 10/100\n",
      "608/613 [============================>.] - ETA: 0s - loss: 0.2404\n",
      "Epoch 00010: val_loss did not improve from 0.13017\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2405 - val_loss: 0.1326\n",
      " ###4 fold : val acc1 0.565, acc3 0.965, mae 0.238###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611/613 [============================>.] - ETA: 0s - loss: 0.5446\n",
      "Epoch 00001: val_loss improved from inf to 0.20312, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 0.5436 - val_loss: 0.2031\n",
      "Epoch 2/100\n",
      "608/613 [============================>.] - ETA: 0s - loss: 0.2506\n",
      "Epoch 00002: val_loss improved from 0.20312 to 0.17047, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2505 - val_loss: 0.1705\n",
      "Epoch 3/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.2181\n",
      "Epoch 00003: val_loss improved from 0.17047 to 0.14858, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2179 - val_loss: 0.1486\n",
      "Epoch 4/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.2055\n",
      "Epoch 00004: val_loss did not improve from 0.14858\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2056 - val_loss: 0.1520\n",
      "Epoch 5/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.1894\n",
      "Epoch 00005: val_loss did not improve from 0.14858\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.1893 - val_loss: 0.2021\n",
      " ###5 fold : val acc1 0.547, acc3 0.958, mae 0.259###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609/613 [============================>.] - ETA: 0s - loss: 1.1688\n",
      "Epoch 00001: val_loss improved from inf to 0.14863, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 1.1633 - val_loss: 0.1486\n",
      "Epoch 2/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.2978\n",
      "Epoch 00002: val_loss did not improve from 0.14863\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2978 - val_loss: 0.1860\n",
      "Epoch 3/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.2587\n",
      "Epoch 00003: val_loss improved from 0.14863 to 0.14522, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2577 - val_loss: 0.1452\n",
      "Epoch 4/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.2847\n",
      "Epoch 00004: val_loss did not improve from 0.14522\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2846 - val_loss: 0.1459\n",
      "Epoch 5/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.3208\n",
      "Epoch 00005: val_loss did not improve from 0.14522\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3192 - val_loss: 0.1499\n",
      " ###6 fold : val acc1 0.545, acc3 0.950, mae 0.257###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601/613 [============================>.] - ETA: 0s - loss: 1.1516\n",
      "Epoch 00001: val_loss improved from inf to 0.15978, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 1.1346 - val_loss: 0.1598\n",
      "Epoch 2/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.3361\n",
      "Epoch 00002: val_loss did not improve from 0.15978\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3354 - val_loss: 0.1656\n",
      "Epoch 3/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.3062\n",
      "Epoch 00003: val_loss improved from 0.15978 to 0.14360, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3046 - val_loss: 0.1436\n",
      "Epoch 4/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.3030\n",
      "Epoch 00004: val_loss did not improve from 0.14360\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3016 - val_loss: 0.1659\n",
      "Epoch 5/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.3114\n",
      "Epoch 00005: val_loss did not improve from 0.14360\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3107 - val_loss: 0.1438\n",
      " ###7 fold : val acc1 0.546, acc3 0.954, mae 0.254###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - ETA: 0s - loss: 1.1416\n",
      "Epoch 00001: val_loss improved from inf to 0.15102, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 1.1416 - val_loss: 0.1510\n",
      "Epoch 2/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.3356\n",
      "Epoch 00002: val_loss did not improve from 0.15102\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3348 - val_loss: 0.1543\n",
      "Epoch 3/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.3109\n",
      "Epoch 00003: val_loss improved from 0.15102 to 0.14850, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3098 - val_loss: 0.1485\n",
      "Epoch 4/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.3055\n",
      "Epoch 00004: val_loss did not improve from 0.14850\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3046 - val_loss: 0.1565\n",
      "Epoch 5/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.3216\n",
      "Epoch 00005: val_loss improved from 0.14850 to 0.13994, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3203 - val_loss: 0.1399\n",
      "Epoch 6/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.2523\n",
      "Epoch 00006: val_loss did not improve from 0.13994\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2522 - val_loss: 0.1402\n",
      "Epoch 7/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.2281\n",
      "Epoch 00007: val_loss improved from 0.13994 to 0.13547, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2279 - val_loss: 0.1355\n",
      "Epoch 8/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.1940\n",
      "Epoch 00008: val_loss improved from 0.13547 to 0.12956, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.1939 - val_loss: 0.1296\n",
      "Epoch 9/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.1904\n",
      "Epoch 00009: val_loss did not improve from 0.12956\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.1905 - val_loss: 0.1388\n",
      "Epoch 10/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.2009\n",
      "Epoch 00010: val_loss did not improve from 0.12956\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2013 - val_loss: 0.1389\n",
      " ###8 fold : val acc1 0.572, acc3 0.957, mae 0.239###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612/613 [============================>.] - ETA: 0s - loss: 1.1428\n",
      "Epoch 00001: val_loss improved from inf to 0.14796, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 1.1416 - val_loss: 0.1480\n",
      "Epoch 2/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.3350\n",
      "Epoch 00002: val_loss did not improve from 0.14796\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3348 - val_loss: 0.1540\n",
      "Epoch 3/100\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.3098\n",
      "Epoch 00003: val_loss improved from 0.14796 to 0.14688, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3098 - val_loss: 0.1469\n",
      "Epoch 4/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.3061\n",
      "Epoch 00004: val_loss did not improve from 0.14688\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3046 - val_loss: 0.1575\n",
      "Epoch 5/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.3225\n",
      "Epoch 00005: val_loss improved from 0.14688 to 0.14155, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3203 - val_loss: 0.1416\n",
      "Epoch 6/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.2528\n",
      "Epoch 00006: val_loss improved from 0.14155 to 0.13911, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2522 - val_loss: 0.1391\n",
      "Epoch 7/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.2282\n",
      "Epoch 00007: val_loss improved from 0.13911 to 0.13635, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2279 - val_loss: 0.1364\n",
      "Epoch 8/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.1941\n",
      "Epoch 00008: val_loss improved from 0.13635 to 0.12956, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.1939 - val_loss: 0.1296\n",
      "Epoch 9/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.1902\n",
      "Epoch 00009: val_loss did not improve from 0.12956\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.1905 - val_loss: 0.1388\n",
      "Epoch 10/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.2014\n",
      "Epoch 00010: val_loss did not improve from 0.12956\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2013 - val_loss: 0.1369\n",
      " ###9 fold : val acc1 0.580, acc3 0.961, mae 0.234###\n",
      "acc10.558_acc30.958\n",
      "random search 1/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/77 [========================>.....] - ETA: 0s - loss: 19.2984\n",
      "Epoch 00001: val_loss improved from inf to 13.59509, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5708 - val_loss: 13.5951\n",
      "Epoch 2/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 10.5923\n",
      "Epoch 00002: val_loss improved from 13.59509 to 6.51969, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.0363 - val_loss: 6.5197\n",
      "Epoch 3/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 4.8782\n",
      "Epoch 00003: val_loss improved from 6.51969 to 2.76211, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 4.7532 - val_loss: 2.7621\n",
      "Epoch 4/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 2.3928\n",
      "Epoch 00004: val_loss improved from 2.76211 to 1.47162, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.6058 - val_loss: 1.4716\n",
      "Epoch 5/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.8734\n",
      "Epoch 00005: val_loss improved from 1.47162 to 0.92223, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7302 - val_loss: 0.9222\n",
      "Epoch 6/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.2339\n",
      "Epoch 00006: val_loss improved from 0.92223 to 0.63317, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2233 - val_loss: 0.6332\n",
      "Epoch 7/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.0257\n",
      "Epoch 00007: val_loss improved from 0.63317 to 0.46899, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0112 - val_loss: 0.4690\n",
      "Epoch 8/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.8263\n",
      "Epoch 00008: val_loss improved from 0.46899 to 0.37659, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8155 - val_loss: 0.3766\n",
      "Epoch 9/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.6934\n",
      "Epoch 00009: val_loss improved from 0.37659 to 0.31443, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6863 - val_loss: 0.3144\n",
      "Epoch 10/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6112\n",
      "Epoch 00010: val_loss improved from 0.31443 to 0.27736, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6115 - val_loss: 0.2774\n",
      "Epoch 11/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6322\n",
      "Epoch 00011: val_loss improved from 0.27736 to 0.25121, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6271 - val_loss: 0.2512\n",
      "Epoch 12/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6171\n",
      "Epoch 00012: val_loss improved from 0.25121 to 0.23599, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6143 - val_loss: 0.2360\n",
      "Epoch 13/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.5821\n",
      "Epoch 00013: val_loss improved from 0.23599 to 0.22436, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5796 - val_loss: 0.2244\n",
      "Epoch 14/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5124\n",
      "Epoch 00014: val_loss improved from 0.22436 to 0.20965, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5111 - val_loss: 0.2096\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4983\n",
      "Epoch 00015: val_loss improved from 0.20965 to 0.20056, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4983 - val_loss: 0.2006\n",
      "Epoch 16/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4822\n",
      "Epoch 00016: val_loss improved from 0.20056 to 0.19279, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4824 - val_loss: 0.1928\n",
      "Epoch 17/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4884\n",
      "Epoch 00017: val_loss improved from 0.19279 to 0.18764, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4875 - val_loss: 0.1876\n",
      "Epoch 18/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.5179\n",
      "Epoch 00018: val_loss improved from 0.18764 to 0.18198, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5165 - val_loss: 0.1820\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4666\n",
      "Epoch 00019: val_loss improved from 0.18198 to 0.17927, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4666 - val_loss: 0.1793\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4494\n",
      "Epoch 00020: val_loss improved from 0.17927 to 0.17377, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4494 - val_loss: 0.1738\n",
      "Epoch 21/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.4365\n",
      "Epoch 00021: val_loss improved from 0.17377 to 0.17204, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4356 - val_loss: 0.1720\n",
      "Epoch 22/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.4365\n",
      "Epoch 00022: val_loss improved from 0.17204 to 0.16477, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4377 - val_loss: 0.1648\n",
      "Epoch 23/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.4319\n",
      "Epoch 00023: val_loss improved from 0.16477 to 0.16327, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4312 - val_loss: 0.1633\n",
      "Epoch 24/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.4235\n",
      "Epoch 00024: val_loss improved from 0.16327 to 0.15864, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4241 - val_loss: 0.1586\n",
      "Epoch 25/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4028\n",
      "Epoch 00025: val_loss improved from 0.15864 to 0.15863, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4172 - val_loss: 0.1586\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4216\n",
      "Epoch 00026: val_loss improved from 0.15863 to 0.15695, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4216 - val_loss: 0.1570\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4062\n",
      "Epoch 00027: val_loss improved from 0.15695 to 0.15318, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4059 - val_loss: 0.1532\n",
      "Epoch 28/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4017\n",
      "Epoch 00028: val_loss improved from 0.15318 to 0.15149, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4004 - val_loss: 0.1515\n",
      "Epoch 29/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3777\n",
      "Epoch 00029: val_loss improved from 0.15149 to 0.14829, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3834 - val_loss: 0.1483\n",
      "Epoch 30/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4253\n",
      "Epoch 00030: val_loss did not improve from 0.14829\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4236 - val_loss: 0.1498\n",
      "Epoch 31/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3798\n",
      "Epoch 00031: val_loss improved from 0.14829 to 0.14567, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3806 - val_loss: 0.1457\n",
      "Epoch 32/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3721\n",
      "Epoch 00032: val_loss improved from 0.14567 to 0.14508, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3730 - val_loss: 0.1451\n",
      "Epoch 33/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3730\n",
      "Epoch 00033: val_loss improved from 0.14508 to 0.14374, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3726 - val_loss: 0.1437\n",
      "Epoch 34/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3675\n",
      "Epoch 00034: val_loss did not improve from 0.14374\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3688 - val_loss: 0.1461\n",
      "Epoch 35/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3828\n",
      "Epoch 00035: val_loss did not improve from 0.14374\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3828 - val_loss: 0.1439\n",
      " ###0 fold : val acc1 0.566, acc3 0.953, mae 0.247###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/77 [==========================>...] - ETA: 0s - loss: 18.9252\n",
      "Epoch 00001: val_loss improved from inf to 13.58276, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 18.5561 - val_loss: 13.5828\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 10.3997\n",
      "Epoch 00002: val_loss improved from 13.58276 to 6.51094, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.0374 - val_loss: 6.5109\n",
      "Epoch 3/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 4.8014\n",
      "Epoch 00003: val_loss improved from 6.51094 to 2.75632, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.7617 - val_loss: 2.7563\n",
      "Epoch 4/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 2.3692\n",
      "Epoch 00004: val_loss improved from 2.75632 to 1.46880, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.6110 - val_loss: 1.4688\n",
      "Epoch 5/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.8291\n",
      "Epoch 00005: val_loss improved from 1.46880 to 0.92119, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.7304 - val_loss: 0.9212\n",
      "Epoch 6/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.2864\n",
      "Epoch 00006: val_loss improved from 0.92119 to 0.63334, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.2234 - val_loss: 0.6333\n",
      "Epoch 7/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.0806\n",
      "Epoch 00007: val_loss improved from 0.63334 to 0.46965, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0208 - val_loss: 0.4697\n",
      "Epoch 8/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7252\n",
      "Epoch 00008: val_loss improved from 0.46965 to 0.37583, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8210 - val_loss: 0.3758\n",
      "Epoch 9/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.7160\n",
      "Epoch 00009: val_loss improved from 0.37583 to 0.31443, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6940 - val_loss: 0.3144\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5990\n",
      "Epoch 00010: val_loss improved from 0.31443 to 0.27667, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6168 - val_loss: 0.2767\n",
      "Epoch 11/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6489\n",
      "Epoch 00011: val_loss improved from 0.27667 to 0.25051, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6352 - val_loss: 0.2505\n",
      "Epoch 12/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6386\n",
      "Epoch 00012: val_loss improved from 0.25051 to 0.23407, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6223 - val_loss: 0.2341\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.5860\n",
      "Epoch 00013: val_loss improved from 0.23407 to 0.22291, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5860 - val_loss: 0.2229\n",
      "Epoch 14/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5180\n",
      "Epoch 00014: val_loss improved from 0.22291 to 0.20716, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5168 - val_loss: 0.2072\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4984\n",
      "Epoch 00015: val_loss improved from 0.20716 to 0.19744, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4984 - val_loss: 0.1974\n",
      "Epoch 16/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4962\n",
      "Epoch 00016: val_loss improved from 0.19744 to 0.18939, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4889 - val_loss: 0.1894\n",
      "Epoch 17/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4791\n",
      "Epoch 00017: val_loss improved from 0.18939 to 0.18494, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4906 - val_loss: 0.1849\n",
      "Epoch 18/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.5309\n",
      "Epoch 00018: val_loss improved from 0.18494 to 0.17807, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5214 - val_loss: 0.1781\n",
      "Epoch 19/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4778\n",
      "Epoch 00019: val_loss improved from 0.17807 to 0.17618, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4718 - val_loss: 0.1762\n",
      "Epoch 20/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4481\n",
      "Epoch 00020: val_loss improved from 0.17618 to 0.17059, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4497 - val_loss: 0.1706\n",
      "Epoch 21/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4418\n",
      "Epoch 00021: val_loss improved from 0.17059 to 0.16870, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4376 - val_loss: 0.1687\n",
      "Epoch 22/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4449\n",
      "Epoch 00022: val_loss improved from 0.16870 to 0.16221, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4418 - val_loss: 0.1622\n",
      "Epoch 23/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4385\n",
      "Epoch 00023: val_loss improved from 0.16221 to 0.16110, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4337 - val_loss: 0.1611\n",
      "Epoch 24/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4209\n",
      "Epoch 00024: val_loss improved from 0.16110 to 0.15634, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4244 - val_loss: 0.1563\n",
      "Epoch 25/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4049\n",
      "Epoch 00025: val_loss improved from 0.15634 to 0.15608, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4199 - val_loss: 0.1561\n",
      "Epoch 26/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4089\n",
      "Epoch 00026: val_loss improved from 0.15608 to 0.15455, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4228 - val_loss: 0.1545\n",
      "Epoch 27/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4053\n",
      "Epoch 00027: val_loss improved from 0.15455 to 0.15088, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4072 - val_loss: 0.1509\n",
      "Epoch 28/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4053\n",
      "Epoch 00028: val_loss improved from 0.15088 to 0.14917, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4038 - val_loss: 0.1492\n",
      "Epoch 29/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3849\n",
      "Epoch 00029: val_loss improved from 0.14917 to 0.14649, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3851 - val_loss: 0.1465\n",
      "Epoch 30/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4409\n",
      "Epoch 00030: val_loss did not improve from 0.14649\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4301 - val_loss: 0.1472\n",
      "Epoch 31/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3827\n",
      "Epoch 00031: val_loss improved from 0.14649 to 0.14382, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3832 - val_loss: 0.1438\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3749\n",
      "Epoch 00032: val_loss improved from 0.14382 to 0.14325, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3749 - val_loss: 0.1433\n",
      "Epoch 33/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3730\n",
      "Epoch 00033: val_loss improved from 0.14325 to 0.14200, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3726 - val_loss: 0.1420\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3684\n",
      "Epoch 00034: val_loss did not improve from 0.14200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3684 - val_loss: 0.1453\n",
      "Epoch 35/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3854\n",
      "Epoch 00035: val_loss did not improve from 0.14200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3841 - val_loss: 0.1427\n",
      " ###1 fold : val acc1 0.569, acc3 0.960, mae 0.239###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 19.2794\n",
      "Epoch 00001: val_loss improved from inf to 13.57704, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5766 - val_loss: 13.5770\n",
      "Epoch 2/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 10.4306\n",
      "Epoch 00002: val_loss improved from 13.57704 to 6.51213, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.0256 - val_loss: 6.5121\n",
      "Epoch 3/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 4.8058\n",
      "Epoch 00003: val_loss improved from 6.51213 to 2.75964, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.7449 - val_loss: 2.7596\n",
      "Epoch 4/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 2.7508\n",
      "Epoch 00004: val_loss improved from 2.75964 to 1.47530, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.6081 - val_loss: 1.4753\n",
      "Epoch 5/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.8332\n",
      "Epoch 00005: val_loss improved from 1.47530 to 0.92420, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.7259 - val_loss: 0.9242\n",
      "Epoch 6/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.2776\n",
      "Epoch 00006: val_loss improved from 0.92420 to 0.63452, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.2214 - val_loss: 0.6345\n",
      "Epoch 7/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.0727\n",
      "Epoch 00007: val_loss improved from 0.63452 to 0.47093, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0242 - val_loss: 0.4709\n",
      "Epoch 8/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7288\n",
      "Epoch 00008: val_loss improved from 0.47093 to 0.37501, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8194 - val_loss: 0.3750\n",
      "Epoch 9/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7146\n",
      "Epoch 00009: val_loss improved from 0.37501 to 0.31192, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6912 - val_loss: 0.3119\n",
      "Epoch 10/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5890\n",
      "Epoch 00010: val_loss improved from 0.31192 to 0.27462, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6075 - val_loss: 0.2746\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6266\n",
      "Epoch 00011: val_loss improved from 0.27462 to 0.24916, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6266 - val_loss: 0.2492\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6186\n",
      "Epoch 00012: val_loss improved from 0.24916 to 0.23238, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6186 - val_loss: 0.2324\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.5830\n",
      "Epoch 00013: val_loss improved from 0.23238 to 0.22241, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5830 - val_loss: 0.2224\n",
      "Epoch 14/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.5201\n",
      "Epoch 00014: val_loss improved from 0.22241 to 0.20735, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5139 - val_loss: 0.2074\n",
      "Epoch 15/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4932\n",
      "Epoch 00015: val_loss improved from 0.20735 to 0.19864, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4976 - val_loss: 0.1986\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4889\n",
      "Epoch 00016: val_loss improved from 0.19864 to 0.19029, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4889 - val_loss: 0.1903\n",
      "Epoch 17/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.4892\n",
      "Epoch 00017: val_loss improved from 0.19029 to 0.18614, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4870 - val_loss: 0.1861\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.5181\n",
      "Epoch 00018: val_loss improved from 0.18614 to 0.17990, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5181 - val_loss: 0.1799\n",
      "Epoch 19/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4746\n",
      "Epoch 00019: val_loss improved from 0.17990 to 0.17732, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4688 - val_loss: 0.1773\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4491\n",
      "Epoch 00020: val_loss improved from 0.17732 to 0.17245, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4491 - val_loss: 0.1724\n",
      "Epoch 21/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4418\n",
      "Epoch 00021: val_loss improved from 0.17245 to 0.16994, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4374 - val_loss: 0.1699\n",
      "Epoch 22/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4431\n",
      "Epoch 00022: val_loss improved from 0.16994 to 0.16360, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4389 - val_loss: 0.1636\n",
      "Epoch 23/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.4358\n",
      "Epoch 00023: val_loss improved from 0.16360 to 0.16256, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4316 - val_loss: 0.1626\n",
      "Epoch 24/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4194\n",
      "Epoch 00024: val_loss improved from 0.16256 to 0.15839, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4231 - val_loss: 0.1584\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4208\n",
      "Epoch 00025: val_loss improved from 0.15839 to 0.15754, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4208 - val_loss: 0.1575\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4221\n",
      "Epoch 00026: val_loss improved from 0.15754 to 0.15575, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4221 - val_loss: 0.1558\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4083\n",
      "Epoch 00027: val_loss improved from 0.15575 to 0.15202, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4093 - val_loss: 0.1520\n",
      "Epoch 28/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.4038\n",
      "Epoch 00028: val_loss improved from 0.15202 to 0.15044, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4016 - val_loss: 0.1504\n",
      "Epoch 29/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3842\n",
      "Epoch 00029: val_loss improved from 0.15044 to 0.14813, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3862 - val_loss: 0.1481\n",
      "Epoch 30/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4428\n",
      "Epoch 00030: val_loss improved from 0.14813 to 0.14719, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4295 - val_loss: 0.1472\n",
      "Epoch 31/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3796\n",
      "Epoch 00031: val_loss improved from 0.14719 to 0.14490, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3801 - val_loss: 0.1449\n",
      "Epoch 32/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3770\n",
      "Epoch 00032: val_loss improved from 0.14490 to 0.14474, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3753 - val_loss: 0.1447\n",
      "Epoch 33/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3735\n",
      "Epoch 00033: val_loss improved from 0.14474 to 0.14293, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3727 - val_loss: 0.1429\n",
      "Epoch 34/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.3687\n",
      "Epoch 00034: val_loss did not improve from 0.14293\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3682 - val_loss: 0.1455\n",
      "Epoch 35/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3863\n",
      "Epoch 00035: val_loss did not improve from 0.14293\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3854 - val_loss: 0.1434\n",
      " ###2 fold : val acc1 0.569, acc3 0.956, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 19.2071\n",
      "Epoch 00001: val_loss improved from inf to 13.59920, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5740 - val_loss: 13.5992\n",
      "Epoch 2/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 10.2361\n",
      "Epoch 00002: val_loss improved from 13.59920 to 6.52496, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.0392 - val_loss: 6.5250\n",
      "Epoch 3/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 4.8193\n",
      "Epoch 00003: val_loss improved from 6.52496 to 2.76727, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.7580 - val_loss: 2.7673\n",
      "Epoch 4/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 2.3649\n",
      "Epoch 00004: val_loss improved from 2.76727 to 1.47361, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.6066 - val_loss: 1.4736\n",
      "Epoch 5/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.8494\n",
      "Epoch 00005: val_loss improved from 1.47361 to 0.92054, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7264 - val_loss: 0.9205\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.2152\n",
      "Epoch 00006: val_loss improved from 0.92054 to 0.63269, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2152 - val_loss: 0.6327\n",
      "Epoch 7/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.0935\n",
      "Epoch 00007: val_loss improved from 0.63269 to 0.47055, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0224 - val_loss: 0.4706\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8174\n",
      "Epoch 00008: val_loss improved from 0.47055 to 0.37473, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8174 - val_loss: 0.3747\n",
      "Epoch 9/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7077\n",
      "Epoch 00009: val_loss improved from 0.37473 to 0.31248, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6881 - val_loss: 0.3125\n",
      "Epoch 10/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.5896\n",
      "Epoch 00010: val_loss improved from 0.31248 to 0.27581, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6086 - val_loss: 0.2758\n",
      "Epoch 11/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.6390\n",
      "Epoch 00011: val_loss improved from 0.27581 to 0.25035, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6227 - val_loss: 0.2504\n",
      "Epoch 12/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6406\n",
      "Epoch 00012: val_loss improved from 0.25035 to 0.23394, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6188 - val_loss: 0.2339\n",
      "Epoch 13/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5958\n",
      "Epoch 00013: val_loss improved from 0.23394 to 0.22292, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5849 - val_loss: 0.2229\n",
      "Epoch 14/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.5215\n",
      "Epoch 00014: val_loss improved from 0.22292 to 0.20867, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5145 - val_loss: 0.2087\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4943\n",
      "Epoch 00015: val_loss improved from 0.20867 to 0.19928, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4943 - val_loss: 0.1993\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4900\n",
      "Epoch 00016: val_loss improved from 0.19928 to 0.19114, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4900 - val_loss: 0.1911\n",
      "Epoch 17/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4802\n",
      "Epoch 00017: val_loss improved from 0.19114 to 0.18755, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4906 - val_loss: 0.1875\n",
      "Epoch 18/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5292\n",
      "Epoch 00018: val_loss improved from 0.18755 to 0.18045, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5181 - val_loss: 0.1804\n",
      "Epoch 19/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4673\n",
      "Epoch 00019: val_loss improved from 0.18045 to 0.17709, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4656 - val_loss: 0.1771\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4491\n",
      "Epoch 00020: val_loss improved from 0.17709 to 0.17273, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4491 - val_loss: 0.1727\n",
      "Epoch 21/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4402\n",
      "Epoch 00021: val_loss improved from 0.17273 to 0.17018, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4370 - val_loss: 0.1702\n",
      "Epoch 22/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4448\n",
      "Epoch 00022: val_loss improved from 0.17018 to 0.16443, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4407 - val_loss: 0.1644\n",
      "Epoch 23/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.4331\n",
      "Epoch 00023: val_loss improved from 0.16443 to 0.16248, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4325 - val_loss: 0.1625\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4241\n",
      "Epoch 00024: val_loss improved from 0.16248 to 0.15900, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4241 - val_loss: 0.1590\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4227\n",
      "Epoch 00025: val_loss improved from 0.15900 to 0.15699, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4227 - val_loss: 0.1570\n",
      "Epoch 26/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4099\n",
      "Epoch 00026: val_loss improved from 0.15699 to 0.15575, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4226 - val_loss: 0.1558\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4081\n",
      "Epoch 00027: val_loss improved from 0.15575 to 0.15240, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4081 - val_loss: 0.1524\n",
      "Epoch 28/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4025\n",
      "Epoch 00028: val_loss improved from 0.15240 to 0.15160, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4019 - val_loss: 0.1516\n",
      "Epoch 29/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3837\n",
      "Epoch 00029: val_loss improved from 0.15160 to 0.14831, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3849 - val_loss: 0.1483\n",
      "Epoch 30/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4393\n",
      "Epoch 00030: val_loss improved from 0.14831 to 0.14732, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4271 - val_loss: 0.1473\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3825\n",
      "Epoch 00031: val_loss improved from 0.14732 to 0.14497, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3825 - val_loss: 0.1450\n",
      "Epoch 32/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3783\n",
      "Epoch 00032: val_loss did not improve from 0.14497\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3783 - val_loss: 0.1455\n",
      "Epoch 33/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3774\n",
      "Epoch 00033: val_loss improved from 0.14497 to 0.14285, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3746 - val_loss: 0.1428\n",
      "Epoch 34/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3697\n",
      "Epoch 00034: val_loss did not improve from 0.14285\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3693 - val_loss: 0.1457\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3864\n",
      "Epoch 00035: val_loss did not improve from 0.14285\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3864 - val_loss: 0.1431\n",
      " ###3 fold : val acc1 0.582, acc3 0.955, mae 0.235###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/77 [=========================>....] - ETA: 0s - loss: 19.1370\n",
      "Epoch 00001: val_loss improved from inf to 13.62650, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5864 - val_loss: 13.6265\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 10.5958\n",
      "Epoch 00002: val_loss improved from 13.62650 to 6.56603, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.1806 - val_loss: 6.5660\n",
      "Epoch 3/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 5.3245\n",
      "Epoch 00003: val_loss improved from 6.56603 to 2.93432, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 5.0215 - val_loss: 2.9343\n",
      "Epoch 4/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 2.8258\n",
      "Epoch 00004: val_loss improved from 2.93432 to 1.52784, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.6141 - val_loss: 1.5278\n",
      "Epoch 5/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.9322\n",
      "Epoch 00005: val_loss improved from 1.52784 to 0.95404, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7930 - val_loss: 0.9540\n",
      "Epoch 6/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.7358\n",
      "Epoch 00006: val_loss improved from 0.95404 to 0.65672, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5751 - val_loss: 0.6567\n",
      "Epoch 7/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.2499\n",
      "Epoch 00007: val_loss improved from 0.65672 to 0.48737, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1508 - val_loss: 0.4874\n",
      "Epoch 8/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.0105\n",
      "Epoch 00008: val_loss improved from 0.48737 to 0.38431, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9431 - val_loss: 0.3843\n",
      "Epoch 9/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6549\n",
      "Epoch 00009: val_loss improved from 0.38431 to 0.31984, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7302 - val_loss: 0.3198\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6188\n",
      "Epoch 00010: val_loss improved from 0.31984 to 0.28005, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6114 - val_loss: 0.2801\n",
      "Epoch 11/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5803\n",
      "Epoch 00011: val_loss improved from 0.28005 to 0.25495, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5744 - val_loss: 0.2549\n",
      "Epoch 12/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5349\n",
      "Epoch 00012: val_loss improved from 0.25495 to 0.23682, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6402 - val_loss: 0.2368\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.5354\n",
      "Epoch 00013: val_loss improved from 0.23682 to 0.22430, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5354 - val_loss: 0.2243\n",
      "Epoch 14/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6187\n",
      "Epoch 00014: val_loss improved from 0.22430 to 0.21283, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5950 - val_loss: 0.2128\n",
      "Epoch 15/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4955\n",
      "Epoch 00015: val_loss improved from 0.21283 to 0.20407, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4994 - val_loss: 0.2041\n",
      "Epoch 16/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.5475\n",
      "Epoch 00016: val_loss improved from 0.20407 to 0.19576, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5357 - val_loss: 0.1958\n",
      "Epoch 17/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.5882\n",
      "Epoch 00017: val_loss improved from 0.19576 to 0.19364, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5678 - val_loss: 0.1936\n",
      "Epoch 18/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4957\n",
      "Epoch 00018: val_loss improved from 0.19364 to 0.18525, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4885 - val_loss: 0.1853\n",
      "Epoch 19/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4624\n",
      "Epoch 00019: val_loss improved from 0.18525 to 0.18076, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4664 - val_loss: 0.1808\n",
      "Epoch 20/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4786\n",
      "Epoch 00020: val_loss improved from 0.18076 to 0.17643, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4702 - val_loss: 0.1764\n",
      "Epoch 21/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4617\n",
      "Epoch 00021: val_loss improved from 0.17643 to 0.17356, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4556 - val_loss: 0.1736\n",
      "Epoch 22/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4657\n",
      "Epoch 00022: val_loss improved from 0.17356 to 0.17048, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4584 - val_loss: 0.1705\n",
      "Epoch 23/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4289\n",
      "Epoch 00023: val_loss improved from 0.17048 to 0.16636, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4381 - val_loss: 0.1664\n",
      "Epoch 24/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4277\n",
      "Epoch 00024: val_loss improved from 0.16636 to 0.16483, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4383 - val_loss: 0.1648\n",
      "Epoch 25/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4118\n",
      "Epoch 00025: val_loss improved from 0.16483 to 0.15938, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4162 - val_loss: 0.1594\n",
      "Epoch 26/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4184\n",
      "Epoch 00026: val_loss improved from 0.15938 to 0.15543, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4195 - val_loss: 0.1554\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4059\n",
      "Epoch 00027: val_loss improved from 0.15543 to 0.15406, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4072 - val_loss: 0.1541\n",
      "Epoch 28/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4040\n",
      "Epoch 00028: val_loss improved from 0.15406 to 0.15185, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3998 - val_loss: 0.1519\n",
      "Epoch 29/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4154\n",
      "Epoch 00029: val_loss improved from 0.15185 to 0.15157, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4070 - val_loss: 0.1516\n",
      "Epoch 30/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3976\n",
      "Epoch 00030: val_loss improved from 0.15157 to 0.14767, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3936 - val_loss: 0.1477\n",
      "Epoch 31/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3957\n",
      "Epoch 00031: val_loss improved from 0.14767 to 0.14715, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3933 - val_loss: 0.1472\n",
      "Epoch 32/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3988\n",
      "Epoch 00032: val_loss did not improve from 0.14715\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3947 - val_loss: 0.1475\n",
      "Epoch 33/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3812\n",
      "Epoch 00033: val_loss improved from 0.14715 to 0.14548, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3802 - val_loss: 0.1455\n",
      "Epoch 34/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3727\n",
      "Epoch 00034: val_loss improved from 0.14548 to 0.14511, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3711 - val_loss: 0.1451\n",
      "Epoch 35/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3760\n",
      "Epoch 00035: val_loss improved from 0.14511 to 0.14312, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3803 - val_loss: 0.1431\n",
      "Epoch 36/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3743\n",
      "Epoch 00036: val_loss improved from 0.14312 to 0.14015, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3729 - val_loss: 0.1401\n",
      "Epoch 37/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3660\n",
      "Epoch 00037: val_loss did not improve from 0.14015\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3634 - val_loss: 0.1407\n",
      "Epoch 38/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3645\n",
      "Epoch 00038: val_loss improved from 0.14015 to 0.13781, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3597 - val_loss: 0.1378\n",
      "Epoch 39/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.3585\n",
      "Epoch 00039: val_loss did not improve from 0.13781\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3592 - val_loss: 0.1380\n",
      "Epoch 40/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3605\n",
      "Epoch 00040: val_loss did not improve from 0.13781\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3823 - val_loss: 0.1394\n",
      " ###4 fold : val acc1 0.580, acc3 0.961, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/77 [=========================>....] - ETA: 0s - loss: 19.0090\n",
      "Epoch 00001: val_loss improved from inf to 13.47621, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5166 - val_loss: 13.4762\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 10.1652\n",
      "Epoch 00002: val_loss improved from 13.47621 to 6.26303, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 9.7668 - val_loss: 6.2630\n",
      "Epoch 3/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 4.6294\n",
      "Epoch 00003: val_loss improved from 6.26303 to 2.72390, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.4389 - val_loss: 2.7239\n",
      "Epoch 4/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 2.3670\n",
      "Epoch 00004: val_loss improved from 2.72390 to 1.45767, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.2763 - val_loss: 1.4577\n",
      "Epoch 5/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.4814\n",
      "Epoch 00005: val_loss improved from 1.45767 to 0.88988, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.4282 - val_loss: 0.8899\n",
      "Epoch 6/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.0487\n",
      "Epoch 00006: val_loss improved from 0.88988 to 0.59162, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0264 - val_loss: 0.5916\n",
      "Epoch 7/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.7986\n",
      "Epoch 00007: val_loss improved from 0.59162 to 0.42763, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7874 - val_loss: 0.4276\n",
      "Epoch 8/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6728\n",
      "Epoch 00008: val_loss improved from 0.42763 to 0.33362, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6652 - val_loss: 0.3336\n",
      "Epoch 9/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6007\n",
      "Epoch 00009: val_loss improved from 0.33362 to 0.27957, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5915 - val_loss: 0.2796\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5530\n",
      "Epoch 00010: val_loss improved from 0.27957 to 0.24978, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5515 - val_loss: 0.2498\n",
      "Epoch 11/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.5260\n",
      "Epoch 00011: val_loss improved from 0.24978 to 0.23060, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5254 - val_loss: 0.2306\n",
      "Epoch 12/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5084\n",
      "Epoch 00012: val_loss improved from 0.23060 to 0.21652, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5085 - val_loss: 0.2165\n",
      "Epoch 13/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4945\n",
      "Epoch 00013: val_loss improved from 0.21652 to 0.20830, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4922 - val_loss: 0.2083\n",
      "Epoch 14/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4839\n",
      "Epoch 00014: val_loss improved from 0.20830 to 0.19891, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4806 - val_loss: 0.1989\n",
      "Epoch 15/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4598\n",
      "Epoch 00015: val_loss improved from 0.19891 to 0.19193, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4598 - val_loss: 0.1919\n",
      "Epoch 16/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4580\n",
      "Epoch 00016: val_loss improved from 0.19193 to 0.18506, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4600 - val_loss: 0.1851\n",
      "Epoch 17/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4556\n",
      "Epoch 00017: val_loss improved from 0.18506 to 0.18167, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4513 - val_loss: 0.1817\n",
      "Epoch 18/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.4350\n",
      "Epoch 00018: val_loss improved from 0.18167 to 0.17597, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4359 - val_loss: 0.1760\n",
      "Epoch 19/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.4312\n",
      "Epoch 00019: val_loss improved from 0.17597 to 0.17227, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4366 - val_loss: 0.1723\n",
      "Epoch 20/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.4300\n",
      "Epoch 00020: val_loss improved from 0.17227 to 0.16830, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4264 - val_loss: 0.1683\n",
      "Epoch 21/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.4231\n",
      "Epoch 00021: val_loss improved from 0.16830 to 0.16649, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4218 - val_loss: 0.1665\n",
      "Epoch 22/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4200\n",
      "Epoch 00022: val_loss improved from 0.16649 to 0.16256, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4186 - val_loss: 0.1626\n",
      "Epoch 23/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4069\n",
      "Epoch 00023: val_loss improved from 0.16256 to 0.15924, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4034 - val_loss: 0.1592\n",
      "Epoch 24/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4024\n",
      "Epoch 00024: val_loss improved from 0.15924 to 0.15748, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4052 - val_loss: 0.1575\n",
      "Epoch 25/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3949\n",
      "Epoch 00025: val_loss improved from 0.15748 to 0.15423, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3997 - val_loss: 0.1542\n",
      "Epoch 26/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3918\n",
      "Epoch 00026: val_loss improved from 0.15423 to 0.15302, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3939 - val_loss: 0.1530\n",
      "Epoch 27/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.3822\n",
      "Epoch 00027: val_loss improved from 0.15302 to 0.15257, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3825 - val_loss: 0.1526\n",
      "Epoch 28/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3825\n",
      "Epoch 00028: val_loss improved from 0.15257 to 0.15061, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3810 - val_loss: 0.1506\n",
      "Epoch 29/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3764\n",
      "Epoch 00029: val_loss improved from 0.15061 to 0.14910, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3702 - val_loss: 0.1491\n",
      "Epoch 30/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3702\n",
      "Epoch 00030: val_loss improved from 0.14910 to 0.14736, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3700 - val_loss: 0.1474\n",
      "Epoch 31/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3626\n",
      "Epoch 00031: val_loss improved from 0.14736 to 0.14699, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3645 - val_loss: 0.1470\n",
      "Epoch 32/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3668\n",
      "Epoch 00032: val_loss did not improve from 0.14699\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3668 - val_loss: 0.1472\n",
      "Epoch 33/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3658\n",
      "Epoch 00033: val_loss improved from 0.14699 to 0.14632, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3635 - val_loss: 0.1463\n",
      "Epoch 34/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3579\n",
      "Epoch 00034: val_loss improved from 0.14632 to 0.14574, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3570 - val_loss: 0.1457\n",
      "Epoch 35/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3535\n",
      "Epoch 00035: val_loss improved from 0.14574 to 0.14388, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3564 - val_loss: 0.1439\n",
      "Epoch 36/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3550\n",
      "Epoch 00036: val_loss improved from 0.14388 to 0.14289, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3544 - val_loss: 0.1429\n",
      "Epoch 37/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3456\n",
      "Epoch 00037: val_loss did not improve from 0.14289\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3457 - val_loss: 0.1431\n",
      "Epoch 38/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3416\n",
      "Epoch 00038: val_loss improved from 0.14289 to 0.14040, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3403 - val_loss: 0.1404\n",
      "Epoch 39/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3443\n",
      "Epoch 00039: val_loss did not improve from 0.14040\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3439 - val_loss: 0.1412\n",
      "Epoch 40/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3475\n",
      "Epoch 00040: val_loss improved from 0.14040 to 0.13967, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3465 - val_loss: 0.1397\n",
      "Epoch 41/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3374\n",
      "Epoch 00041: val_loss did not improve from 0.13967\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3384 - val_loss: 0.1399\n",
      "Epoch 42/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3355\n",
      "Epoch 00042: val_loss did not improve from 0.13967\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3368 - val_loss: 0.1416\n",
      " ###5 fold : val acc1 0.591, acc3 0.961, mae 0.243###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/77 [======================>.......] - ETA: 0s - loss: 19.5526\n",
      "Epoch 00001: val_loss improved from inf to 13.48230, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 18.5092 - val_loss: 13.4823\n",
      "Epoch 2/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 10.4086\n",
      "Epoch 00002: val_loss improved from 13.48230 to 6.32192, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 9.9932 - val_loss: 6.3219\n",
      "Epoch 3/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 5.2128\n",
      "Epoch 00003: val_loss improved from 6.32192 to 2.90692, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 4.8909 - val_loss: 2.9069\n",
      "Epoch 4/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 2.6499\n",
      "Epoch 00004: val_loss improved from 2.90692 to 1.54957, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.6319 - val_loss: 1.5496\n",
      "Epoch 5/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.9349\n",
      "Epoch 00005: val_loss improved from 1.54957 to 0.97609, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.9036 - val_loss: 0.9761\n",
      "Epoch 6/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.3762\n",
      "Epoch 00006: val_loss improved from 0.97609 to 0.66664, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3605 - val_loss: 0.6666\n",
      "Epoch 7/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.0637\n",
      "Epoch 00007: val_loss improved from 0.66664 to 0.49519, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0536 - val_loss: 0.4952\n",
      "Epoch 8/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.0939\n",
      "Epoch 00008: val_loss improved from 0.49519 to 0.38703, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0696 - val_loss: 0.3870\n",
      "Epoch 9/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7399\n",
      "Epoch 00009: val_loss improved from 0.38703 to 0.31840, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7361 - val_loss: 0.3184\n",
      "Epoch 10/100\n",
      "58/77 [=====================>........] - ETA: 0s - loss: 0.8136\n",
      "Epoch 00010: val_loss improved from 0.31840 to 0.27836, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7589 - val_loss: 0.2784\n",
      "Epoch 11/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6761\n",
      "Epoch 00011: val_loss improved from 0.27836 to 0.25379, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6752 - val_loss: 0.2538\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.5770\n",
      "Epoch 00012: val_loss improved from 0.25379 to 0.23271, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5770 - val_loss: 0.2327\n",
      "Epoch 13/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6907\n",
      "Epoch 00013: val_loss improved from 0.23271 to 0.22231, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6890 - val_loss: 0.2223\n",
      "Epoch 14/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6066\n",
      "Epoch 00014: val_loss improved from 0.22231 to 0.21063, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6053 - val_loss: 0.2106\n",
      "Epoch 15/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5597\n",
      "Epoch 00015: val_loss improved from 0.21063 to 0.20423, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5595 - val_loss: 0.2042\n",
      "Epoch 16/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.5417\n",
      "Epoch 00016: val_loss improved from 0.20423 to 0.19576, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5369 - val_loss: 0.1958\n",
      "Epoch 17/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.4973\n",
      "Epoch 00017: val_loss improved from 0.19576 to 0.19125, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4954 - val_loss: 0.1912\n",
      "Epoch 18/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.4647\n",
      "Epoch 00018: val_loss improved from 0.19125 to 0.18430, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4693 - val_loss: 0.1843\n",
      "Epoch 19/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.4988\n",
      "Epoch 00019: val_loss improved from 0.18430 to 0.17934, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5000 - val_loss: 0.1793\n",
      "Epoch 20/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4859\n",
      "Epoch 00020: val_loss improved from 0.17934 to 0.17435, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4815 - val_loss: 0.1743\n",
      "Epoch 21/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.4411\n",
      "Epoch 00021: val_loss improved from 0.17435 to 0.17129, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4417 - val_loss: 0.1713\n",
      "Epoch 22/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4483\n",
      "Epoch 00022: val_loss improved from 0.17129 to 0.16602, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4479 - val_loss: 0.1660\n",
      "Epoch 23/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4329\n",
      "Epoch 00023: val_loss improved from 0.16602 to 0.16249, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4312 - val_loss: 0.1625\n",
      "Epoch 24/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.4521\n",
      "Epoch 00024: val_loss improved from 0.16249 to 0.16067, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4508 - val_loss: 0.1607\n",
      "Epoch 25/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.4318\n",
      "Epoch 00025: val_loss improved from 0.16067 to 0.15625, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4327 - val_loss: 0.1562\n",
      "Epoch 26/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4213\n",
      "Epoch 00026: val_loss improved from 0.15625 to 0.15543, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4212 - val_loss: 0.1554\n",
      "Epoch 27/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4023\n",
      "Epoch 00027: val_loss improved from 0.15543 to 0.15363, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4022 - val_loss: 0.1536\n",
      "Epoch 28/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3975\n",
      "Epoch 00028: val_loss improved from 0.15363 to 0.14993, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3970 - val_loss: 0.1499\n",
      "Epoch 29/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3890\n",
      "Epoch 00029: val_loss improved from 0.14993 to 0.14813, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3885 - val_loss: 0.1481\n",
      "Epoch 30/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.3957\n",
      "Epoch 00030: val_loss improved from 0.14813 to 0.14632, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3943 - val_loss: 0.1463\n",
      "Epoch 31/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3801\n",
      "Epoch 00031: val_loss improved from 0.14632 to 0.14537, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3804 - val_loss: 0.1454\n",
      "Epoch 32/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.3973\n",
      "Epoch 00032: val_loss improved from 0.14537 to 0.14511, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3959 - val_loss: 0.1451\n",
      "Epoch 33/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3860\n",
      "Epoch 00033: val_loss improved from 0.14511 to 0.14373, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3817 - val_loss: 0.1437\n",
      "Epoch 34/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3714\n",
      "Epoch 00034: val_loss did not improve from 0.14373\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3712 - val_loss: 0.1438\n",
      "Epoch 35/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3842\n",
      "Epoch 00035: val_loss improved from 0.14373 to 0.14256, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3848 - val_loss: 0.1426\n",
      "Epoch 36/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3668\n",
      "Epoch 00036: val_loss improved from 0.14256 to 0.14218, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3676 - val_loss: 0.1422\n",
      "Epoch 37/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.3584\n",
      "Epoch 00037: val_loss improved from 0.14218 to 0.14063, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3577 - val_loss: 0.1406\n",
      "Epoch 38/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3571\n",
      "Epoch 00038: val_loss improved from 0.14063 to 0.13837, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3563 - val_loss: 0.1384\n",
      "Epoch 39/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3577\n",
      "Epoch 00039: val_loss did not improve from 0.13837\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3585 - val_loss: 0.1410\n",
      "Epoch 40/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.3922\n",
      "Epoch 00040: val_loss improved from 0.13837 to 0.13742, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3891 - val_loss: 0.1374\n",
      "Epoch 41/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3521\n",
      "Epoch 00041: val_loss improved from 0.13742 to 0.13653, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3535 - val_loss: 0.1365\n",
      "Epoch 42/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3489\n",
      "Epoch 00042: val_loss did not improve from 0.13653\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3488 - val_loss: 0.1383\n",
      "Epoch 43/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3448\n",
      "Epoch 00043: val_loss improved from 0.13653 to 0.13568, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3439 - val_loss: 0.1357\n",
      "Epoch 44/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3409\n",
      "Epoch 00044: val_loss did not improve from 0.13568\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3410 - val_loss: 0.1365\n",
      "Epoch 45/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3313\n",
      "Epoch 00045: val_loss improved from 0.13568 to 0.13441, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3312 - val_loss: 0.1344\n",
      "Epoch 46/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3542\n",
      "Epoch 00046: val_loss did not improve from 0.13441\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3537 - val_loss: 0.1385\n",
      "Epoch 47/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3245\n",
      "Epoch 00047: val_loss did not improve from 0.13441\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3241 - val_loss: 0.1352\n",
      " ###6 fold : val acc1 0.574, acc3 0.959, mae 0.238###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 19.1366\n",
      "Epoch 00001: val_loss improved from inf to 13.50595, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5085 - val_loss: 13.5059\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 10.1419\n",
      "Epoch 00002: val_loss improved from 13.50595 to 6.32988, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.0093 - val_loss: 6.3299\n",
      "Epoch 3/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 5.0922\n",
      "Epoch 00003: val_loss improved from 6.32988 to 2.90187, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.8839 - val_loss: 2.9019\n",
      "Epoch 4/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 2.7354\n",
      "Epoch 00004: val_loss improved from 2.90187 to 1.54912, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.6260 - val_loss: 1.5491\n",
      "Epoch 5/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 1.9746\n",
      "Epoch 00005: val_loss improved from 1.54912 to 0.98081, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.9024 - val_loss: 0.9808\n",
      "Epoch 6/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.4394\n",
      "Epoch 00006: val_loss improved from 0.98081 to 0.67325, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.3594 - val_loss: 0.6733\n",
      "Epoch 7/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.8836\n",
      "Epoch 00007: val_loss improved from 0.67325 to 0.50164, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0645 - val_loss: 0.5016\n",
      "Epoch 8/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.1784\n",
      "Epoch 00008: val_loss improved from 0.50164 to 0.39213, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0863 - val_loss: 0.3921\n",
      "Epoch 9/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6505\n",
      "Epoch 00009: val_loss improved from 0.39213 to 0.32227, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7349 - val_loss: 0.3223\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8171\n",
      "Epoch 00010: val_loss improved from 0.32227 to 0.28030, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7679 - val_loss: 0.2803\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6793\n",
      "Epoch 00011: val_loss improved from 0.28030 to 0.25460, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6793 - val_loss: 0.2546\n",
      "Epoch 12/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.5800\n",
      "Epoch 00012: val_loss improved from 0.25460 to 0.23286, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5789 - val_loss: 0.2329\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.7004\n",
      "Epoch 00013: val_loss improved from 0.23286 to 0.22160, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7004 - val_loss: 0.2216\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6116\n",
      "Epoch 00014: val_loss improved from 0.22160 to 0.20952, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6116 - val_loss: 0.2095\n",
      "Epoch 15/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6006\n",
      "Epoch 00015: val_loss improved from 0.20952 to 0.20280, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5734 - val_loss: 0.2028\n",
      "Epoch 16/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5579\n",
      "Epoch 00016: val_loss improved from 0.20280 to 0.19418, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5446 - val_loss: 0.1942\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4953\n",
      "Epoch 00017: val_loss improved from 0.19418 to 0.18919, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4953 - val_loss: 0.1892\n",
      "Epoch 18/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4740\n",
      "Epoch 00018: val_loss improved from 0.18919 to 0.18218, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4744 - val_loss: 0.1822\n",
      "Epoch 19/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.5048\n",
      "Epoch 00019: val_loss improved from 0.18218 to 0.17807, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5038 - val_loss: 0.1781\n",
      "Epoch 20/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.5041\n",
      "Epoch 00020: val_loss improved from 0.17807 to 0.17279, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4879 - val_loss: 0.1728\n",
      "Epoch 21/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4437\n",
      "Epoch 00021: val_loss improved from 0.17279 to 0.17031, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4411 - val_loss: 0.1703\n",
      "Epoch 22/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4553\n",
      "Epoch 00022: val_loss improved from 0.17031 to 0.16509, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4493 - val_loss: 0.1651\n",
      "Epoch 23/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4373\n",
      "Epoch 00023: val_loss improved from 0.16509 to 0.16158, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4327 - val_loss: 0.1616\n",
      "Epoch 24/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.4598\n",
      "Epoch 00024: val_loss improved from 0.16158 to 0.15978, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4554 - val_loss: 0.1598\n",
      "Epoch 25/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4258\n",
      "Epoch 00025: val_loss improved from 0.15978 to 0.15526, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4291 - val_loss: 0.1553\n",
      "Epoch 26/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4230\n",
      "Epoch 00026: val_loss improved from 0.15526 to 0.15437, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4204 - val_loss: 0.1544\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4016\n",
      "Epoch 00027: val_loss improved from 0.15437 to 0.15314, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4016 - val_loss: 0.1531\n",
      "Epoch 28/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3981\n",
      "Epoch 00028: val_loss improved from 0.15314 to 0.14866, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3977 - val_loss: 0.1487\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3913\n",
      "Epoch 00029: val_loss improved from 0.14866 to 0.14710, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3913 - val_loss: 0.1471\n",
      "Epoch 30/100\n",
      "58/77 [=====================>........] - ETA: 0s - loss: 0.4051\n",
      "Epoch 00030: val_loss improved from 0.14710 to 0.14579, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3970 - val_loss: 0.1458\n",
      "Epoch 31/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3797\n",
      "Epoch 00031: val_loss improved from 0.14579 to 0.14526, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3822 - val_loss: 0.1453\n",
      "Epoch 32/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3962\n",
      "Epoch 00032: val_loss did not improve from 0.14526\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3949 - val_loss: 0.1453\n",
      "Epoch 33/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3865\n",
      "Epoch 00033: val_loss improved from 0.14526 to 0.14428, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3839 - val_loss: 0.1443\n",
      "Epoch 34/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3764\n",
      "Epoch 00034: val_loss improved from 0.14428 to 0.14402, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3736 - val_loss: 0.1440\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3837\n",
      "Epoch 00035: val_loss improved from 0.14402 to 0.14212, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3837 - val_loss: 0.1421\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3685\n",
      "Epoch 00036: val_loss did not improve from 0.14212\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3685 - val_loss: 0.1431\n",
      "Epoch 37/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3610\n",
      "Epoch 00037: val_loss improved from 0.14212 to 0.14050, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3602 - val_loss: 0.1405\n",
      "Epoch 38/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3561\n",
      "Epoch 00038: val_loss improved from 0.14050 to 0.13835, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3584 - val_loss: 0.1384\n",
      "Epoch 39/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3583\n",
      "Epoch 00039: val_loss did not improve from 0.13835\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3590 - val_loss: 0.1407\n",
      "Epoch 40/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3887\n",
      "Epoch 00040: val_loss improved from 0.13835 to 0.13698, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3881 - val_loss: 0.1370\n",
      "Epoch 41/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3533\n",
      "Epoch 00041: val_loss improved from 0.13698 to 0.13657, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3547 - val_loss: 0.1366\n",
      "Epoch 42/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3531\n",
      "Epoch 00042: val_loss did not improve from 0.13657\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3532 - val_loss: 0.1377\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3424\n",
      "Epoch 00043: val_loss improved from 0.13657 to 0.13568, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3424 - val_loss: 0.1357\n",
      "Epoch 44/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3414\n",
      "Epoch 00044: val_loss did not improve from 0.13568\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3418 - val_loss: 0.1362\n",
      "Epoch 45/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3358\n",
      "Epoch 00045: val_loss improved from 0.13568 to 0.13418, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3332 - val_loss: 0.1342\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3568\n",
      "Epoch 00046: val_loss did not improve from 0.13418\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3568 - val_loss: 0.1375\n",
      "Epoch 47/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3249\n",
      "Epoch 00047: val_loss did not improve from 0.13418\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3249 - val_loss: 0.1347\n",
      " ###7 fold : val acc1 0.588, acc3 0.963, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/77 [=========================>....] - ETA: 0s - loss: 19.0711\n",
      "Epoch 00001: val_loss improved from inf to 13.57709, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5040 - val_loss: 13.5771\n",
      "Epoch 2/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 10.0093\n",
      "Epoch 00002: val_loss improved from 13.57709 to 6.39765, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.0119 - val_loss: 6.3977\n",
      "Epoch 3/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 5.1391\n",
      "Epoch 00003: val_loss improved from 6.39765 to 2.94436, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.8849 - val_loss: 2.9444\n",
      "Epoch 4/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 2.8030\n",
      "Epoch 00004: val_loss improved from 2.94436 to 1.58541, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.6231 - val_loss: 1.5854\n",
      "Epoch 5/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 2.0843\n",
      "Epoch 00005: val_loss improved from 1.58541 to 1.00882, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.9017 - val_loss: 1.0088\n",
      "Epoch 6/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.4599\n",
      "Epoch 00006: val_loss improved from 1.00882 to 0.69470, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3580 - val_loss: 0.6947\n",
      "Epoch 7/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8833\n",
      "Epoch 00007: val_loss improved from 0.69470 to 0.51648, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0631 - val_loss: 0.5165\n",
      "Epoch 8/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.1898\n",
      "Epoch 00008: val_loss improved from 0.51648 to 0.39853, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0803 - val_loss: 0.3985\n",
      "Epoch 9/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6460\n",
      "Epoch 00009: val_loss improved from 0.39853 to 0.32569, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7331 - val_loss: 0.3257\n",
      "Epoch 10/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.8078\n",
      "Epoch 00010: val_loss improved from 0.32569 to 0.27955, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7639 - val_loss: 0.2795\n",
      "Epoch 11/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.5577\n",
      "Epoch 00011: val_loss improved from 0.27955 to 0.25159, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6781 - val_loss: 0.2516\n",
      "Epoch 12/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.5892\n",
      "Epoch 00012: val_loss improved from 0.25159 to 0.22879, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5768 - val_loss: 0.2288\n",
      "Epoch 13/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.7399\n",
      "Epoch 00013: val_loss improved from 0.22879 to 0.21550, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6976 - val_loss: 0.2155\n",
      "Epoch 14/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.6316\n",
      "Epoch 00014: val_loss improved from 0.21550 to 0.20281, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6100 - val_loss: 0.2028\n",
      "Epoch 15/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.5888\n",
      "Epoch 00015: val_loss improved from 0.20281 to 0.19571, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5715 - val_loss: 0.1957\n",
      "Epoch 16/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.5577\n",
      "Epoch 00016: val_loss improved from 0.19571 to 0.18764, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5440 - val_loss: 0.1876\n",
      "Epoch 17/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.5054\n",
      "Epoch 00017: val_loss improved from 0.18764 to 0.18246, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4958 - val_loss: 0.1825\n",
      "Epoch 18/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.4717\n",
      "Epoch 00018: val_loss improved from 0.18246 to 0.17609, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4753 - val_loss: 0.1761\n",
      "Epoch 19/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.5060\n",
      "Epoch 00019: val_loss improved from 0.17609 to 0.17228, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5023 - val_loss: 0.1723\n",
      "Epoch 20/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.5012\n",
      "Epoch 00020: val_loss improved from 0.17228 to 0.16695, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4867 - val_loss: 0.1670\n",
      "Epoch 21/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4417\n",
      "Epoch 00021: val_loss improved from 0.16695 to 0.16501, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4401 - val_loss: 0.1650\n",
      "Epoch 22/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4561\n",
      "Epoch 00022: val_loss improved from 0.16501 to 0.16086, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4499 - val_loss: 0.1609\n",
      "Epoch 23/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4365\n",
      "Epoch 00023: val_loss improved from 0.16086 to 0.15720, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4319 - val_loss: 0.1572\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4558\n",
      "Epoch 00024: val_loss improved from 0.15720 to 0.15513, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4558 - val_loss: 0.1551\n",
      "Epoch 25/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4266\n",
      "Epoch 00025: val_loss improved from 0.15513 to 0.15164, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4308 - val_loss: 0.1516\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4200\n",
      "Epoch 00026: val_loss improved from 0.15164 to 0.15100, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4200 - val_loss: 0.1510\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3990\n",
      "Epoch 00027: val_loss improved from 0.15100 to 0.14974, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4008 - val_loss: 0.1497\n",
      "Epoch 28/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3962\n",
      "Epoch 00028: val_loss improved from 0.14974 to 0.14590, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3974 - val_loss: 0.1459\n",
      "Epoch 29/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3939\n",
      "Epoch 00029: val_loss improved from 0.14590 to 0.14399, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3909 - val_loss: 0.1440\n",
      "Epoch 30/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3995\n",
      "Epoch 00030: val_loss improved from 0.14399 to 0.14335, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3955 - val_loss: 0.1433\n",
      "Epoch 31/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3798\n",
      "Epoch 00031: val_loss improved from 0.14335 to 0.14256, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3821 - val_loss: 0.1426\n",
      "Epoch 32/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.3972\n",
      "Epoch 00032: val_loss improved from 0.14256 to 0.14233, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3969 - val_loss: 0.1423\n",
      "Epoch 33/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.3851\n",
      "Epoch 00033: val_loss improved from 0.14233 to 0.14147, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3812 - val_loss: 0.1415\n",
      "Epoch 34/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.3748\n",
      "Epoch 00034: val_loss improved from 0.14147 to 0.14080, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3730 - val_loss: 0.1408\n",
      "Epoch 35/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3837\n",
      "Epoch 00035: val_loss improved from 0.14080 to 0.14003, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3824 - val_loss: 0.1400\n",
      "Epoch 36/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3680\n",
      "Epoch 00036: val_loss did not improve from 0.14003\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3669 - val_loss: 0.1403\n",
      "Epoch 37/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3640\n",
      "Epoch 00037: val_loss improved from 0.14003 to 0.13803, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3603 - val_loss: 0.1380\n",
      "Epoch 38/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3554\n",
      "Epoch 00038: val_loss improved from 0.13803 to 0.13627, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3590 - val_loss: 0.1363\n",
      "Epoch 39/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3516\n",
      "Epoch 00039: val_loss did not improve from 0.13627\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3569 - val_loss: 0.1376\n",
      "Epoch 40/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3960\n",
      "Epoch 00040: val_loss improved from 0.13627 to 0.13449, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3874 - val_loss: 0.1345\n",
      "Epoch 41/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3509\n",
      "Epoch 00041: val_loss improved from 0.13449 to 0.13419, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3526 - val_loss: 0.1342\n",
      "Epoch 42/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3522\n",
      "Epoch 00042: val_loss did not improve from 0.13419\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3530 - val_loss: 0.1352\n",
      "Epoch 43/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3445\n",
      "Epoch 00043: val_loss improved from 0.13419 to 0.13371, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3414 - val_loss: 0.1337\n",
      "Epoch 44/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3396\n",
      "Epoch 00044: val_loss improved from 0.13371 to 0.13369, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3401 - val_loss: 0.1337\n",
      "Epoch 45/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3338\n",
      "Epoch 00045: val_loss improved from 0.13369 to 0.13202, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3323 - val_loss: 0.1320\n",
      "Epoch 46/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3612\n",
      "Epoch 00046: val_loss did not improve from 0.13202\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3558 - val_loss: 0.1341\n",
      "Epoch 47/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3267\n",
      "Epoch 00047: val_loss did not improve from 0.13202\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3250 - val_loss: 0.1321\n",
      " ###8 fold : val acc1 0.572, acc3 0.956, mae 0.240###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 19.1441\n",
      "Epoch 00001: val_loss improved from inf to 13.59553, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5040 - val_loss: 13.5955\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 10.1395\n",
      "Epoch 00002: val_loss improved from 13.59553 to 6.39376, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.0119 - val_loss: 6.3938\n",
      "Epoch 3/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 5.1391\n",
      "Epoch 00003: val_loss improved from 6.39376 to 2.94915, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.8849 - val_loss: 2.9491\n",
      "Epoch 4/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 2.7354\n",
      "Epoch 00004: val_loss improved from 2.94915 to 1.59161, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.6231 - val_loss: 1.5916\n",
      "Epoch 5/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 1.9742\n",
      "Epoch 00005: val_loss improved from 1.59161 to 1.01244, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.9017 - val_loss: 1.0124\n",
      "Epoch 6/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 1.4094\n",
      "Epoch 00006: val_loss improved from 1.01244 to 0.69228, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.3580 - val_loss: 0.6923\n",
      "Epoch 7/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.8808\n",
      "Epoch 00007: val_loss improved from 0.69228 to 0.51164, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0631 - val_loss: 0.5116\n",
      "Epoch 8/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.1565\n",
      "Epoch 00008: val_loss improved from 0.51164 to 0.39254, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0803 - val_loss: 0.3925\n",
      "Epoch 9/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6462\n",
      "Epoch 00009: val_loss improved from 0.39254 to 0.31788, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7331 - val_loss: 0.3179\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8117\n",
      "Epoch 00010: val_loss improved from 0.31788 to 0.27199, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7639 - val_loss: 0.2720\n",
      "Epoch 11/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5569\n",
      "Epoch 00011: val_loss improved from 0.27199 to 0.24491, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6781 - val_loss: 0.2449\n",
      "Epoch 12/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5927\n",
      "Epoch 00012: val_loss improved from 0.24491 to 0.22221, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5768 - val_loss: 0.2222\n",
      "Epoch 13/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7543\n",
      "Epoch 00013: val_loss improved from 0.22221 to 0.21026, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6976 - val_loss: 0.2103\n",
      "Epoch 14/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.6292\n",
      "Epoch 00014: val_loss improved from 0.21026 to 0.19801, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6100 - val_loss: 0.1980\n",
      "Epoch 15/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.5874\n",
      "Epoch 00015: val_loss improved from 0.19801 to 0.19139, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5715 - val_loss: 0.1914\n",
      "Epoch 16/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5588\n",
      "Epoch 00016: val_loss improved from 0.19139 to 0.18387, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5440 - val_loss: 0.1839\n",
      "Epoch 17/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5080\n",
      "Epoch 00017: val_loss improved from 0.18387 to 0.17932, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4958 - val_loss: 0.1793\n",
      "Epoch 18/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4741\n",
      "Epoch 00018: val_loss improved from 0.17932 to 0.17335, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4753 - val_loss: 0.1734\n",
      "Epoch 19/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.5060\n",
      "Epoch 00019: val_loss improved from 0.17335 to 0.16993, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5023 - val_loss: 0.1699\n",
      "Epoch 20/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5024\n",
      "Epoch 00020: val_loss improved from 0.16993 to 0.16465, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4867 - val_loss: 0.1646\n",
      "Epoch 21/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4419\n",
      "Epoch 00021: val_loss improved from 0.16465 to 0.16322, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4401 - val_loss: 0.1632\n",
      "Epoch 22/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4547\n",
      "Epoch 00022: val_loss improved from 0.16322 to 0.15926, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4499 - val_loss: 0.1593\n",
      "Epoch 23/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4386\n",
      "Epoch 00023: val_loss improved from 0.15926 to 0.15568, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4319 - val_loss: 0.1557\n",
      "Epoch 24/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4602\n",
      "Epoch 00024: val_loss improved from 0.15568 to 0.15400, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4558 - val_loss: 0.1540\n",
      "Epoch 25/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4266\n",
      "Epoch 00025: val_loss improved from 0.15400 to 0.15006, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4308 - val_loss: 0.1501\n",
      "Epoch 26/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4230\n",
      "Epoch 00026: val_loss improved from 0.15006 to 0.14955, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4200 - val_loss: 0.1495\n",
      "Epoch 27/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3985\n",
      "Epoch 00027: val_loss improved from 0.14955 to 0.14879, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4008 - val_loss: 0.1488\n",
      "Epoch 28/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.3980\n",
      "Epoch 00028: val_loss improved from 0.14879 to 0.14477, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3974 - val_loss: 0.1448\n",
      "Epoch 29/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3930\n",
      "Epoch 00029: val_loss improved from 0.14477 to 0.14340, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3909 - val_loss: 0.1434\n",
      "Epoch 30/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4011\n",
      "Epoch 00030: val_loss improved from 0.14340 to 0.14204, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3955 - val_loss: 0.1420\n",
      "Epoch 31/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3805\n",
      "Epoch 00031: val_loss improved from 0.14204 to 0.14186, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3821 - val_loss: 0.1419\n",
      "Epoch 32/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3987\n",
      "Epoch 00032: val_loss did not improve from 0.14186\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3969 - val_loss: 0.1423\n",
      "Epoch 33/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3860\n",
      "Epoch 00033: val_loss improved from 0.14186 to 0.14100, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3812 - val_loss: 0.1410\n",
      "Epoch 34/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.3746\n",
      "Epoch 00034: val_loss did not improve from 0.14100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3730 - val_loss: 0.1410\n",
      "Epoch 35/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 0.3823\n",
      "Epoch 00035: val_loss improved from 0.14100 to 0.13907, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3824 - val_loss: 0.1391\n",
      "Epoch 36/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.3679\n",
      "Epoch 00036: val_loss did not improve from 0.13907\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3669 - val_loss: 0.1405\n",
      "Epoch 37/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.3611\n",
      "Epoch 00037: val_loss improved from 0.13907 to 0.13804, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3603 - val_loss: 0.1380\n",
      "Epoch 38/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.3609\n",
      "Epoch 00038: val_loss improved from 0.13804 to 0.13610, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3590 - val_loss: 0.1361\n",
      "Epoch 39/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.3547\n",
      "Epoch 00039: val_loss did not improve from 0.13610\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3569 - val_loss: 0.1380\n",
      "Epoch 40/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3954\n",
      "Epoch 00040: val_loss improved from 0.13610 to 0.13477, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3874 - val_loss: 0.1348\n",
      "Epoch 41/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.3522\n",
      "Epoch 00041: val_loss improved from 0.13477 to 0.13455, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3526 - val_loss: 0.1346\n",
      "Epoch 42/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.3524\n",
      "Epoch 00042: val_loss did not improve from 0.13455\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3530 - val_loss: 0.1355\n",
      "Epoch 43/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3445\n",
      "Epoch 00043: val_loss improved from 0.13455 to 0.13419, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3414 - val_loss: 0.1342\n",
      "Epoch 44/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3389\n",
      "Epoch 00044: val_loss improved from 0.13419 to 0.13411, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3401 - val_loss: 0.1341\n",
      "Epoch 45/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3340\n",
      "Epoch 00045: val_loss improved from 0.13411 to 0.13213, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3323 - val_loss: 0.1321\n",
      "Epoch 46/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3603\n",
      "Epoch 00046: val_loss did not improve from 0.13213\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3558 - val_loss: 0.1349\n",
      "Epoch 47/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3271\n",
      "Epoch 00047: val_loss did not improve from 0.13213\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3250 - val_loss: 0.1322\n",
      " ###9 fold : val acc1 0.582, acc3 0.960, mae 0.234###\n",
      "acc10.577_acc30.958\n",
      "random search 2/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293/307 [===========================>..] - ETA: 0s - loss: 2.3059\n",
      "Epoch 00001: val_loss improved from inf to 0.29873, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.2248 - val_loss: 0.2987\n",
      "Epoch 2/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.6961\n",
      "Epoch 00002: val_loss improved from 0.29873 to 0.23571, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6914 - val_loss: 0.2357\n",
      "Epoch 3/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.4471\n",
      "Epoch 00003: val_loss did not improve from 0.23571\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4472 - val_loss: 0.4649\n",
      "Epoch 4/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.3459\n",
      "Epoch 00004: val_loss did not improve from 0.23571\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3459 - val_loss: 0.3552\n",
      " ###0 fold : val acc1 0.421, acc3 0.888, mae 0.356###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/307 [============================>.] - ETA: 0s - loss: 2.2612\n",
      "Epoch 00001: val_loss improved from inf to 0.30472, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.2171 - val_loss: 0.3047\n",
      "Epoch 2/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.6851\n",
      "Epoch 00002: val_loss improved from 0.30472 to 0.23432, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6849 - val_loss: 0.2343\n",
      "Epoch 3/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.3296\n",
      "Epoch 00003: val_loss did not improve from 0.23432\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4460 - val_loss: 0.5423\n",
      "Epoch 4/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.3456\n",
      "Epoch 00004: val_loss did not improve from 0.23432\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3436 - val_loss: 0.3320\n",
      " ###1 fold : val acc1 0.414, acc3 0.893, mae 0.354###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/307 [============================>.] - ETA: 0s - loss: 2.2622\n",
      "Epoch 00001: val_loss improved from inf to 0.31248, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.2246 - val_loss: 0.3125\n",
      "Epoch 2/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.6889\n",
      "Epoch 00002: val_loss improved from 0.31248 to 0.24899, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6779 - val_loss: 0.2490\n",
      "Epoch 3/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.4442\n",
      "Epoch 00003: val_loss did not improve from 0.24899\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4442 - val_loss: 0.5783\n",
      "Epoch 4/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.3529\n",
      "Epoch 00004: val_loss did not improve from 0.24899\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3527 - val_loss: 0.3243\n",
      " ###2 fold : val acc1 0.387, acc3 0.885, mae 0.372###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/307 [===========================>..] - ETA: 0s - loss: 2.3247\n",
      "Epoch 00001: val_loss improved from inf to 0.32342, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.2558 - val_loss: 0.3234\n",
      "Epoch 2/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.7023\n",
      "Epoch 00002: val_loss improved from 0.32342 to 0.25477, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6931 - val_loss: 0.2548\n",
      "Epoch 3/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.4390\n",
      "Epoch 00003: val_loss did not improve from 0.25477\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4393 - val_loss: 0.5460\n",
      "Epoch 4/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.3419\n",
      "Epoch 00004: val_loss did not improve from 0.25477\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3419 - val_loss: 0.3193\n",
      " ###3 fold : val acc1 0.408, acc3 0.887, mae 0.360###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/307 [============================>.] - ETA: 0s - loss: 2.5994\n",
      "Epoch 00001: val_loss improved from inf to 0.28460, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.5610 - val_loss: 0.2846\n",
      "Epoch 2/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.7323\n",
      "Epoch 00002: val_loss did not improve from 0.28460\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7323 - val_loss: 0.3070\n",
      "Epoch 3/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.4214\n",
      "Epoch 00003: val_loss improved from 0.28460 to 0.24682, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4171 - val_loss: 0.2468\n",
      "Epoch 4/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.3716\n",
      "Epoch 00004: val_loss did not improve from 0.24682\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3716 - val_loss: 0.3056\n",
      "Epoch 5/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.3224\n",
      "Epoch 00005: val_loss did not improve from 0.24682\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3221 - val_loss: 0.2924\n",
      " ###4 fold : val acc1 0.400, acc3 0.893, mae 0.360###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/307 [============================>.] - ETA: 0s - loss: 1.9631\n",
      "Epoch 00001: val_loss improved from inf to 0.28280, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.9456 - val_loss: 0.2828\n",
      "Epoch 2/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.3715\n",
      "Epoch 00002: val_loss did not improve from 0.28280\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3712 - val_loss: 0.3349\n",
      "Epoch 3/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.3288\n",
      "Epoch 00003: val_loss improved from 0.28280 to 0.25377, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3282 - val_loss: 0.2538\n",
      "Epoch 4/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.2938\n",
      "Epoch 00004: val_loss did not improve from 0.25377\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2935 - val_loss: 0.3533\n",
      "Epoch 5/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.2673\n",
      "Epoch 00005: val_loss did not improve from 0.25377\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2677 - val_loss: 0.3901\n",
      " ###5 fold : val acc1 0.416, acc3 0.897, mae 0.366###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/307 [===========================>..] - ETA: 0s - loss: 3.8029\n",
      "Epoch 00001: val_loss improved from inf to 0.28316, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.6803 - val_loss: 0.2832\n",
      "Epoch 2/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.7779\n",
      "Epoch 00002: val_loss improved from 0.28316 to 0.26019, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7643 - val_loss: 0.2602\n",
      "Epoch 3/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.5137\n",
      "Epoch 00003: val_loss improved from 0.26019 to 0.25654, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5083 - val_loss: 0.2565\n",
      "Epoch 4/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.4139\n",
      "Epoch 00004: val_loss did not improve from 0.25654\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4088 - val_loss: 0.2898\n",
      "Epoch 5/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.3469\n",
      "Epoch 00005: val_loss did not improve from 0.25654\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3460 - val_loss: 0.2972\n",
      " ###6 fold : val acc1 0.404, acc3 0.884, mae 0.364###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/307 [============================>.] - ETA: 0s - loss: 3.7680\n",
      "Epoch 00001: val_loss improved from inf to 0.29138, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.6994 - val_loss: 0.2914\n",
      "Epoch 2/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.8563\n",
      "Epoch 00002: val_loss improved from 0.29138 to 0.28048, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8563 - val_loss: 0.2805\n",
      "Epoch 3/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.5782\n",
      "Epoch 00003: val_loss improved from 0.28048 to 0.26571, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5723 - val_loss: 0.2657\n",
      "Epoch 4/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.4386\n",
      "Epoch 00004: val_loss did not improve from 0.26571\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4340 - val_loss: 0.2783\n",
      "Epoch 5/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.3545\n",
      "Epoch 00005: val_loss did not improve from 0.26571\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3537 - val_loss: 0.3141\n",
      " ###7 fold : val acc1 0.387, acc3 0.879, mae 0.376###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297/307 [============================>.] - ETA: 0s - loss: 3.7854\n",
      "Epoch 00001: val_loss improved from inf to 0.29486, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.6849 - val_loss: 0.2949\n",
      "Epoch 2/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.8429\n",
      "Epoch 00002: val_loss improved from 0.29486 to 0.27795, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8284 - val_loss: 0.2780\n",
      "Epoch 3/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.5581\n",
      "Epoch 00003: val_loss improved from 0.27795 to 0.26602, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5537 - val_loss: 0.2660\n",
      "Epoch 4/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.4334\n",
      "Epoch 00004: val_loss did not improve from 0.26602\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4281 - val_loss: 0.2878\n",
      "Epoch 5/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.3419\n",
      "Epoch 00005: val_loss did not improve from 0.26602\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3408 - val_loss: 0.3158\n",
      " ###8 fold : val acc1 0.355, acc3 0.853, mae 0.406###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/307 [===========================>..] - ETA: 0s - loss: 3.8069\n",
      "Epoch 00001: val_loss improved from inf to 0.29468, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.6849 - val_loss: 0.2947\n",
      "Epoch 2/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.4014\n",
      "Epoch 00002: val_loss improved from 0.29468 to 0.28426, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8284 - val_loss: 0.2843\n",
      "Epoch 3/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.5616\n",
      "Epoch 00003: val_loss improved from 0.28426 to 0.27322, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.5537 - val_loss: 0.2732\n",
      "Epoch 4/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.4309\n",
      "Epoch 00004: val_loss did not improve from 0.27322\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.4281 - val_loss: 0.2970\n",
      "Epoch 5/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.3420\n",
      "Epoch 00005: val_loss did not improve from 0.27322\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3408 - val_loss: 0.3243\n",
      " ###9 fold : val acc1 0.369, acc3 0.866, mae 0.391###\n",
      "acc10.396_acc30.883\n",
      "random search 3/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/39 [============================>.] - ETA: 0s - loss: 15.2906\n",
      "Epoch 00001: val_loss improved from inf to 8.68182, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.2502 - val_loss: 8.6818\n",
      "Epoch 2/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 7.1274\n",
      "Epoch 00002: val_loss improved from 8.68182 to 2.63956, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 7.1095 - val_loss: 2.6396\n",
      "Epoch 3/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 3.7076\n",
      "Epoch 00003: val_loss improved from 2.63956 to 1.23927, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 4.0146 - val_loss: 1.2393\n",
      "Epoch 4/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 3.3467\n",
      "Epoch 00004: val_loss improved from 1.23927 to 0.80318, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.3404 - val_loss: 0.8032\n",
      "Epoch 5/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.6442\n",
      "Epoch 00005: val_loss improved from 0.80318 to 0.56187, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6451 - val_loss: 0.5619\n",
      "Epoch 6/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.7238\n",
      "Epoch 00006: val_loss improved from 0.56187 to 0.50673, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6995 - val_loss: 0.5067\n",
      "Epoch 7/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.7622\n",
      "Epoch 00007: val_loss improved from 0.50673 to 0.44222, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7370 - val_loss: 0.4422\n",
      "Epoch 8/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.8117\n",
      "Epoch 00008: val_loss improved from 0.44222 to 0.40359, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7584 - val_loss: 0.4036\n",
      "Epoch 9/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.2763\n",
      "Epoch 00009: val_loss improved from 0.40359 to 0.32580, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.2549 - val_loss: 0.3258\n",
      "Epoch 10/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.8731\n",
      "Epoch 00010: val_loss did not improve from 0.32580\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.7296 - val_loss: 0.3949\n",
      "Epoch 11/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.8545\n",
      "Epoch 00011: val_loss improved from 0.32580 to 0.30781, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8506 - val_loss: 0.3078\n",
      "Epoch 12/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.7973\n",
      "Epoch 00012: val_loss did not improve from 0.30781\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7958 - val_loss: 0.3308\n",
      "Epoch 13/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8150\n",
      "Epoch 00013: val_loss did not improve from 0.30781\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7926 - val_loss: 0.3260\n",
      " ###0 fold : val acc1 0.363, acc3 0.841, mae 0.415###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/39 [===========================>..] - ETA: 0s - loss: 15.4575\n",
      "Epoch 00001: val_loss improved from inf to 8.66922, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.2572 - val_loss: 8.6692\n",
      "Epoch 2/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 7.2538\n",
      "Epoch 00002: val_loss improved from 8.66922 to 2.63276, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 7.1634 - val_loss: 2.6328\n",
      "Epoch 3/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 3.7019\n",
      "Epoch 00003: val_loss improved from 2.63276 to 1.23048, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 4.0018 - val_loss: 1.2305\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 3.3957\n",
      "Epoch 00004: val_loss improved from 1.23048 to 0.80571, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.3482 - val_loss: 0.8057\n",
      "Epoch 5/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.6390\n",
      "Epoch 00005: val_loss improved from 0.80571 to 0.55450, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6310 - val_loss: 0.5545\n",
      "Epoch 6/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 2.8311\n",
      "Epoch 00006: val_loss improved from 0.55450 to 0.51216, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7168 - val_loss: 0.5122\n",
      "Epoch 7/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 2.8311\n",
      "Epoch 00007: val_loss improved from 0.51216 to 0.43945, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7351 - val_loss: 0.4394\n",
      "Epoch 8/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 2.0147\n",
      "Epoch 00008: val_loss improved from 0.43945 to 0.40354, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7456 - val_loss: 0.4035\n",
      "Epoch 9/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 2.3535\n",
      "Epoch 00009: val_loss improved from 0.40354 to 0.32597, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.2584 - val_loss: 0.3260\n",
      "Epoch 10/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.8742\n",
      "Epoch 00010: val_loss did not improve from 0.32597\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7187 - val_loss: 0.3907\n",
      "Epoch 11/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.8519\n",
      "Epoch 00011: val_loss improved from 0.32597 to 0.30735, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8529 - val_loss: 0.3073\n",
      "Epoch 12/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.8138\n",
      "Epoch 00012: val_loss did not improve from 0.30735\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8007 - val_loss: 0.3255\n",
      "Epoch 13/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.8090\n",
      "Epoch 00013: val_loss did not improve from 0.30735\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7886 - val_loss: 0.3266\n",
      " ###1 fold : val acc1 0.345, acc3 0.854, mae 0.412###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/39 [============================>.] - ETA: 0s - loss: 15.3051\n",
      "Epoch 00001: val_loss improved from inf to 8.66776, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.2638 - val_loss: 8.6678\n",
      "Epoch 2/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 7.1801\n",
      "Epoch 00002: val_loss improved from 8.66776 to 2.65342, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 7.1574 - val_loss: 2.6534\n",
      "Epoch 3/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 4.0358\n",
      "Epoch 00003: val_loss improved from 2.65342 to 1.24771, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 4.0316 - val_loss: 1.2477\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 3.4140\n",
      "Epoch 00004: val_loss improved from 1.24771 to 0.80464, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.3640 - val_loss: 0.8046\n",
      "Epoch 5/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.6646\n",
      "Epoch 00005: val_loss improved from 0.80464 to 0.56203, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6554 - val_loss: 0.5620\n",
      "Epoch 6/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.7400\n",
      "Epoch 00006: val_loss improved from 0.56203 to 0.50719, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7000 - val_loss: 0.5072\n",
      "Epoch 7/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.8003\n",
      "Epoch 00007: val_loss improved from 0.50719 to 0.44197, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7473 - val_loss: 0.4420\n",
      "Epoch 8/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.0044\n",
      "Epoch 00008: val_loss improved from 0.44197 to 0.39887, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7349 - val_loss: 0.3989\n",
      "Epoch 9/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 2.3032\n",
      "Epoch 00009: val_loss improved from 0.39887 to 0.32204, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.2596 - val_loss: 0.3220\n",
      "Epoch 10/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.8597\n",
      "Epoch 00010: val_loss did not improve from 0.32204\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.6953 - val_loss: 0.3915\n",
      "Epoch 11/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8520\n",
      "Epoch 00011: val_loss improved from 0.32204 to 0.30902, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8592 - val_loss: 0.3090\n",
      "Epoch 12/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8128\n",
      "Epoch 00012: val_loss did not improve from 0.30902\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.8024 - val_loss: 0.3279\n",
      "Epoch 13/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.8025\n",
      "Epoch 00013: val_loss did not improve from 0.30902\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7832 - val_loss: 0.3216\n",
      " ###2 fold : val acc1 0.329, acc3 0.841, mae 0.428###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/39 [============================>.] - ETA: 0s - loss: 15.3009\n",
      "Epoch 00001: val_loss improved from inf to 8.68099, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.2608 - val_loss: 8.6810\n",
      "Epoch 2/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 7.1727\n",
      "Epoch 00002: val_loss improved from 8.68099 to 2.66890, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 7.1498 - val_loss: 2.6689\n",
      "Epoch 3/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 3.7196\n",
      "Epoch 00003: val_loss improved from 2.66890 to 1.24204, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 4.0213 - val_loss: 1.2420\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 3.4057\n",
      "Epoch 00004: val_loss improved from 1.24204 to 0.79991, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.3589 - val_loss: 0.7999\n",
      "Epoch 5/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.6704\n",
      "Epoch 00005: val_loss improved from 0.79991 to 0.55628, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6439 - val_loss: 0.5563\n",
      "Epoch 6/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.7473\n",
      "Epoch 00006: val_loss improved from 0.55628 to 0.51110, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7082 - val_loss: 0.5111\n",
      "Epoch 7/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 2.7714\n",
      "Epoch 00007: val_loss improved from 0.51110 to 0.44739, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7634 - val_loss: 0.4474\n",
      "Epoch 8/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.7600\n",
      "Epoch 00008: val_loss improved from 0.44739 to 0.40301, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7294 - val_loss: 0.4030\n",
      "Epoch 9/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.2953\n",
      "Epoch 00009: val_loss improved from 0.40301 to 0.31146, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.2599 - val_loss: 0.3115\n",
      "Epoch 10/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.8528\n",
      "Epoch 00010: val_loss did not improve from 0.31146\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.7093 - val_loss: 0.3883\n",
      "Epoch 11/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8622\n",
      "Epoch 00011: val_loss improved from 0.31146 to 0.30872, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8650 - val_loss: 0.3087\n",
      "Epoch 12/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8101\n",
      "Epoch 00012: val_loss did not improve from 0.30872\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7986 - val_loss: 0.3233\n",
      "Epoch 13/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8080\n",
      "Epoch 00013: val_loss did not improve from 0.30872\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7885 - val_loss: 0.3218\n",
      " ###3 fold : val acc1 0.347, acc3 0.860, mae 0.410###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - ETA: 0s - loss: 15.4773\n",
      "Epoch 00001: val_loss improved from inf to 9.02238, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.4773 - val_loss: 9.0224\n",
      "Epoch 2/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 7.5799\n",
      "Epoch 00002: val_loss improved from 9.02238 to 2.69405, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 7.4430 - val_loss: 2.6940\n",
      "Epoch 3/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 4.9056\n",
      "Epoch 00003: val_loss improved from 2.69405 to 1.34364, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 4.9056 - val_loss: 1.3436\n",
      "Epoch 4/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 3.1681\n",
      "Epoch 00004: val_loss improved from 1.34364 to 0.79884, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.1734 - val_loss: 0.7988\n",
      "Epoch 5/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 3.1786\n",
      "Epoch 00005: val_loss improved from 0.79884 to 0.60419, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.1786 - val_loss: 0.6042\n",
      "Epoch 6/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 3.1338\n",
      "Epoch 00006: val_loss improved from 0.60419 to 0.45954, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.7221 - val_loss: 0.4595\n",
      "Epoch 7/100\n",
      "20/39 [==============>...............] - ETA: 0s - loss: 2.1969\n",
      "Epoch 00007: val_loss improved from 0.45954 to 0.41272, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.1778 - val_loss: 0.4127\n",
      "Epoch 8/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 3.3075\n",
      "Epoch 00008: val_loss did not improve from 0.41272\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.2971 - val_loss: 0.4237\n",
      "Epoch 9/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 3.1882\n",
      "Epoch 00009: val_loss did not improve from 0.41272\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.1065 - val_loss: 0.4295\n",
      " ###4 fold : val acc1 0.299, acc3 0.794, mae 0.474###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/39 [===============>..............] - ETA: 0s - loss: 17.4852 \n",
      "Epoch 00001: val_loss improved from inf to 8.38692, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 14.9323 - val_loss: 8.3869\n",
      "Epoch 2/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 7.1308\n",
      "Epoch 00002: val_loss improved from 8.38692 to 2.36527, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 6.1188 - val_loss: 2.3653\n",
      "Epoch 3/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 3.8135\n",
      "Epoch 00003: val_loss improved from 2.36527 to 1.17219, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.5437 - val_loss: 1.1722\n",
      "Epoch 4/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 2.8719\n",
      "Epoch 00004: val_loss improved from 1.17219 to 0.69629, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.7775 - val_loss: 0.6963\n",
      "Epoch 5/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 2.4360\n",
      "Epoch 00005: val_loss improved from 0.69629 to 0.50345, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.4229 - val_loss: 0.5034\n",
      "Epoch 6/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 2.1326\n",
      "Epoch 00006: val_loss improved from 0.50345 to 0.41668, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.1326 - val_loss: 0.4167\n",
      "Epoch 7/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 2.0306\n",
      "Epoch 00007: val_loss improved from 0.41668 to 0.37766, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.0143 - val_loss: 0.3777\n",
      "Epoch 8/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.8899\n",
      "Epoch 00008: val_loss improved from 0.37766 to 0.35175, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8891 - val_loss: 0.3517\n",
      "Epoch 9/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.8592\n",
      "Epoch 00009: val_loss improved from 0.35175 to 0.30810, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8523 - val_loss: 0.3081\n",
      "Epoch 10/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.7770\n",
      "Epoch 00010: val_loss did not improve from 0.30810\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7777 - val_loss: 0.3157\n",
      "Epoch 11/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.7097\n",
      "Epoch 00011: val_loss improved from 0.30810 to 0.29563, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7054 - val_loss: 0.2956\n",
      "Epoch 12/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.6639\n",
      "Epoch 00012: val_loss did not improve from 0.29563\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.6667 - val_loss: 0.3060\n",
      "Epoch 13/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.6233\n",
      "Epoch 00013: val_loss improved from 0.29563 to 0.28183, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6170 - val_loss: 0.2818\n",
      "Epoch 14/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.6082\n",
      "Epoch 00014: val_loss did not improve from 0.28183\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.6019 - val_loss: 0.2829\n",
      "Epoch 15/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.5856\n",
      "Epoch 00015: val_loss improved from 0.28183 to 0.28078, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.5821 - val_loss: 0.2808\n",
      "Epoch 16/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.5391\n",
      "Epoch 00016: val_loss improved from 0.28078 to 0.24659, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.5378 - val_loss: 0.2466\n",
      "Epoch 17/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.5115\n",
      "Epoch 00017: val_loss did not improve from 0.24659\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.5115 - val_loss: 0.2743\n",
      "Epoch 18/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.4827\n",
      "Epoch 00018: val_loss did not improve from 0.24659\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.4811 - val_loss: 0.2945\n",
      " ###5 fold : val acc1 0.420, acc3 0.907, mae 0.370###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - ETA: 0s - loss: 15.0202\n",
      "Epoch 00001: val_loss improved from inf to 8.56435, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.0202 - val_loss: 8.5644\n",
      "Epoch 2/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 7.1777\n",
      "Epoch 00002: val_loss improved from 8.56435 to 2.41351, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 6.5633 - val_loss: 2.4135\n",
      "Epoch 3/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 8.7343\n",
      "Epoch 00003: val_loss improved from 2.41351 to 1.30823, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 6.2850 - val_loss: 1.3082\n",
      "Epoch 4/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 3.1782\n",
      "Epoch 00004: val_loss improved from 1.30823 to 0.76766, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.1819 - val_loss: 0.7677\n",
      "Epoch 5/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 4.7312\n",
      "Epoch 00005: val_loss improved from 0.76766 to 0.64965, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 4.7312 - val_loss: 0.6496\n",
      "Epoch 6/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 2.6312\n",
      "Epoch 00006: val_loss improved from 0.64965 to 0.44692, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.4443 - val_loss: 0.4469\n",
      "Epoch 7/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 2.2098\n",
      "Epoch 00007: val_loss improved from 0.44692 to 0.42731, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.3172 - val_loss: 0.4273\n",
      "Epoch 8/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 2.0614\n",
      "Epoch 00008: val_loss improved from 0.42731 to 0.41592, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.7923 - val_loss: 0.4159\n",
      "Epoch 9/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 2.0846\n",
      "Epoch 00009: val_loss improved from 0.41592 to 0.38114, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.5932 - val_loss: 0.3811\n",
      "Epoch 10/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.8621\n",
      "Epoch 00010: val_loss improved from 0.38114 to 0.35390, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.8114 - val_loss: 0.3539\n",
      "Epoch 11/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.1721\n",
      "Epoch 00011: val_loss improved from 0.35390 to 0.33308, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.1568 - val_loss: 0.3331\n",
      "Epoch 12/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.3522\n",
      "Epoch 00012: val_loss improved from 0.33308 to 0.31201, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.3062 - val_loss: 0.3120\n",
      "Epoch 13/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.8895\n",
      "Epoch 00013: val_loss improved from 0.31201 to 0.30400, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8685 - val_loss: 0.3040\n",
      "Epoch 14/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.8008\n",
      "Epoch 00014: val_loss improved from 0.30400 to 0.30279, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7929 - val_loss: 0.3028\n",
      "Epoch 15/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.8793\n",
      "Epoch 00015: val_loss did not improve from 0.30279\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.8616 - val_loss: 0.3168\n",
      "Epoch 16/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.6881\n",
      "Epoch 00016: val_loss improved from 0.30279 to 0.26548, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6829 - val_loss: 0.2655\n",
      "Epoch 17/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.6483\n",
      "Epoch 00017: val_loss did not improve from 0.26548\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.6456 - val_loss: 0.2981\n",
      "Epoch 18/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.6229\n",
      "Epoch 00018: val_loss did not improve from 0.26548\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.6202 - val_loss: 0.3090\n",
      " ###6 fold : val acc1 0.397, acc3 0.889, mae 0.367###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/39 [==========================>...] - ETA: 0s - loss: 15.3584\n",
      "Epoch 00001: val_loss improved from inf to 8.56687, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.0224 - val_loss: 8.5669\n",
      "Epoch 2/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 6.5502\n",
      "Epoch 00002: val_loss improved from 8.56687 to 2.39059, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 6.5309 - val_loss: 2.3906\n",
      "Epoch 3/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 6.6024\n",
      "Epoch 00003: val_loss improved from 2.39059 to 1.30556, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 6.3910 - val_loss: 1.3056\n",
      "Epoch 4/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 3.2109\n",
      "Epoch 00004: val_loss improved from 1.30556 to 0.77718, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.1971 - val_loss: 0.7772\n",
      "Epoch 5/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 4.9158\n",
      "Epoch 00005: val_loss improved from 0.77718 to 0.65732, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 4.8947 - val_loss: 0.6573\n",
      "Epoch 6/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.4747\n",
      "Epoch 00006: val_loss improved from 0.65732 to 0.45803, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.4574 - val_loss: 0.4580\n",
      "Epoch 7/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.3816\n",
      "Epoch 00007: val_loss improved from 0.45803 to 0.42099, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.3492 - val_loss: 0.4210\n",
      "Epoch 8/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 2.9660\n",
      "Epoch 00008: val_loss improved from 0.42099 to 0.41962, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.8431 - val_loss: 0.4196\n",
      "Epoch 9/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.7184\n",
      "Epoch 00009: val_loss improved from 0.41962 to 0.37717, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6701 - val_loss: 0.3772\n",
      "Epoch 10/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.8768\n",
      "Epoch 00010: val_loss improved from 0.37717 to 0.35268, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.8445 - val_loss: 0.3527\n",
      "Epoch 11/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 1.8423\n",
      "Epoch 00011: val_loss improved from 0.35268 to 0.32975, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.1692 - val_loss: 0.3297\n",
      "Epoch 12/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.3997\n",
      "Epoch 00012: val_loss improved from 0.32975 to 0.30883, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.3785 - val_loss: 0.3088\n",
      "Epoch 13/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.8745\n",
      "Epoch 00013: val_loss improved from 0.30883 to 0.30356, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.8745 - val_loss: 0.3036\n",
      "Epoch 14/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.8176\n",
      "Epoch 00014: val_loss improved from 0.30356 to 0.30274, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8114 - val_loss: 0.3027\n",
      "Epoch 15/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.9174\n",
      "Epoch 00015: val_loss did not improve from 0.30274\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.9016 - val_loss: 0.3252\n",
      "Epoch 16/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.6813\n",
      "Epoch 00016: val_loss improved from 0.30274 to 0.26606, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6738 - val_loss: 0.2661\n",
      "Epoch 17/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.6711\n",
      "Epoch 00017: val_loss did not improve from 0.26606\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6516 - val_loss: 0.3076\n",
      "Epoch 18/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.6320\n",
      "Epoch 00018: val_loss did not improve from 0.26606\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.6271 - val_loss: 0.3045\n",
      " ###7 fold : val acc1 0.390, acc3 0.887, mae 0.371###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/39 [===========================>..] - ETA: 0s - loss: 15.2231\n",
      "Epoch 00001: val_loss improved from inf to 8.64020, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.0257 - val_loss: 8.6402\n",
      "Epoch 2/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 6.5444\n",
      "Epoch 00002: val_loss improved from 8.64020 to 2.44355, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 6.5444 - val_loss: 2.4435\n",
      "Epoch 3/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 6.3974\n",
      "Epoch 00003: val_loss improved from 2.44355 to 1.35488, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 6.3704 - val_loss: 1.3549\n",
      "Epoch 4/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 3.2152\n",
      "Epoch 00004: val_loss improved from 1.35488 to 0.81522, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.2012 - val_loss: 0.8152\n",
      "Epoch 5/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 4.8909\n",
      "Epoch 00005: val_loss improved from 0.81522 to 0.67275, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 4.8705 - val_loss: 0.6728\n",
      "Epoch 6/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.4623\n",
      "Epoch 00006: val_loss improved from 0.67275 to 0.47141, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.4529 - val_loss: 0.4714\n",
      "Epoch 7/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.3751\n",
      "Epoch 00007: val_loss improved from 0.47141 to 0.43240, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.3467 - val_loss: 0.4324\n",
      "Epoch 8/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 2.8376\n",
      "Epoch 00008: val_loss improved from 0.43240 to 0.42211, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.8317 - val_loss: 0.4221\n",
      "Epoch 9/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 2.6645\n",
      "Epoch 00009: val_loss improved from 0.42211 to 0.37978, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6577 - val_loss: 0.3798\n",
      "Epoch 10/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.8969\n",
      "Epoch 00010: val_loss improved from 0.37978 to 0.35190, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.8434 - val_loss: 0.3519\n",
      "Epoch 11/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.1733\n",
      "Epoch 00011: val_loss improved from 0.35190 to 0.33144, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.1663 - val_loss: 0.3314\n",
      "Epoch 12/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 2.4585\n",
      "Epoch 00012: val_loss improved from 0.33144 to 0.30506, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.3597 - val_loss: 0.3051\n",
      "Epoch 13/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8962\n",
      "Epoch 00013: val_loss improved from 0.30506 to 0.29580, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8657 - val_loss: 0.2958\n",
      "Epoch 14/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.8079\n",
      "Epoch 00014: val_loss did not improve from 0.29580\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.8020 - val_loss: 0.2999\n",
      "Epoch 15/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.9178\n",
      "Epoch 00015: val_loss did not improve from 0.29580\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.8998 - val_loss: 0.3197\n",
      " ###8 fold : val acc1 0.340, acc3 0.849, mae 0.423###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - ETA: 0s - loss: 15.0257\n",
      "Epoch 00001: val_loss improved from inf to 8.65024, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.0257 - val_loss: 8.6502\n",
      "Epoch 2/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 7.1395\n",
      "Epoch 00002: val_loss improved from 8.65024 to 2.44073, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 6.5444 - val_loss: 2.4407\n",
      "Epoch 3/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 6.3974\n",
      "Epoch 00003: val_loss improved from 2.44073 to 1.34777, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 6.3704 - val_loss: 1.3478\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 3.2337\n",
      "Epoch 00004: val_loss improved from 1.34777 to 0.80359, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.2012 - val_loss: 0.8036\n",
      "Epoch 5/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 5.0351\n",
      "Epoch 00005: val_loss improved from 0.80359 to 0.66855, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 4.8705 - val_loss: 0.6686\n",
      "Epoch 6/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.4765\n",
      "Epoch 00006: val_loss improved from 0.66855 to 0.46293, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.4529 - val_loss: 0.4629\n",
      "Epoch 7/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.3751\n",
      "Epoch 00007: val_loss improved from 0.46293 to 0.42337, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.3467 - val_loss: 0.4234\n",
      "Epoch 8/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.9218\n",
      "Epoch 00008: val_loss improved from 0.42337 to 0.41650, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.8317 - val_loss: 0.4165\n",
      "Epoch 9/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.7291\n",
      "Epoch 00009: val_loss improved from 0.41650 to 0.37497, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6577 - val_loss: 0.3750\n",
      "Epoch 10/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.8754\n",
      "Epoch 00010: val_loss improved from 0.37497 to 0.34804, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.8434 - val_loss: 0.3480\n",
      "Epoch 11/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.1733\n",
      "Epoch 00011: val_loss improved from 0.34804 to 0.32925, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.1663 - val_loss: 0.3292\n",
      "Epoch 12/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 2.4400\n",
      "Epoch 00012: val_loss improved from 0.32925 to 0.30361, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.3597 - val_loss: 0.3036\n",
      "Epoch 13/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8962\n",
      "Epoch 00013: val_loss improved from 0.30361 to 0.29446, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8657 - val_loss: 0.2945\n",
      "Epoch 14/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.8092\n",
      "Epoch 00014: val_loss did not improve from 0.29446\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.8020 - val_loss: 0.2997\n",
      "Epoch 15/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.9299\n",
      "Epoch 00015: val_loss did not improve from 0.29446\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8998 - val_loss: 0.3191\n",
      " ###9 fold : val acc1 0.350, acc3 0.847, mae 0.417###\n",
      "acc10.358_acc30.857\n",
      "random search 4/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/77 [===========================>..] - ETA: 0s - loss: 14.3561\n",
      "Epoch 00001: val_loss improved from inf to 5.99854, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 14.0817 - val_loss: 5.9985\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 3.4001\n",
      "Epoch 00002: val_loss improved from 5.99854 to 1.28397, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.1763 - val_loss: 1.2840\n",
      "Epoch 3/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.8492\n",
      "Epoch 00003: val_loss improved from 1.28397 to 0.56727, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0933 - val_loss: 0.5673\n",
      "Epoch 4/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.5602\n",
      "Epoch 00004: val_loss improved from 0.56727 to 0.32787, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5491 - val_loss: 0.3279\n",
      "Epoch 5/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.3526\n",
      "Epoch 00005: val_loss improved from 0.32787 to 0.24742, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3420 - val_loss: 0.2474\n",
      "Epoch 6/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 0.2621\n",
      "Epoch 00006: val_loss improved from 0.24742 to 0.21547, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2555 - val_loss: 0.2155\n",
      "Epoch 7/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.2187\n",
      "Epoch 00007: val_loss improved from 0.21547 to 0.19663, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2128 - val_loss: 0.1966\n",
      "Epoch 8/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.1889\n",
      "Epoch 00008: val_loss improved from 0.19663 to 0.18321, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1869 - val_loss: 0.1832\n",
      "Epoch 9/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.1724\n",
      "Epoch 00009: val_loss improved from 0.18321 to 0.17215, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1699 - val_loss: 0.1721\n",
      "Epoch 10/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.1547\n",
      "Epoch 00010: val_loss improved from 0.17215 to 0.16274, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 0.1627\n",
      "Epoch 11/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.1513\n",
      "Epoch 00011: val_loss improved from 0.16274 to 0.15651, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1501 - val_loss: 0.1565\n",
      "Epoch 12/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.1432\n",
      "Epoch 00012: val_loss improved from 0.15651 to 0.15127, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1444 - val_loss: 0.1513\n",
      "Epoch 13/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1394\n",
      "Epoch 00013: val_loss improved from 0.15127 to 0.14709, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1400 - val_loss: 0.1471\n",
      "Epoch 14/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1390\n",
      "Epoch 00014: val_loss improved from 0.14709 to 0.14281, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1368 - val_loss: 0.1428\n",
      "Epoch 15/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1359\n",
      "Epoch 00015: val_loss improved from 0.14281 to 0.14081, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1348 - val_loss: 0.1408\n",
      "Epoch 16/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00016: val_loss improved from 0.14081 to 0.13847, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1325 - val_loss: 0.1385\n",
      "Epoch 17/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1298\n",
      "Epoch 00017: val_loss improved from 0.13847 to 0.13701, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1311 - val_loss: 0.1370\n",
      "Epoch 18/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1296\n",
      "Epoch 00018: val_loss improved from 0.13701 to 0.13526, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1297 - val_loss: 0.1353\n",
      "Epoch 19/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00019: val_loss improved from 0.13526 to 0.13508, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1286 - val_loss: 0.1351\n",
      "Epoch 20/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00020: val_loss improved from 0.13508 to 0.13418, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1281 - val_loss: 0.1342\n",
      "Epoch 21/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1277\n",
      "Epoch 00021: val_loss improved from 0.13418 to 0.13329, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1274 - val_loss: 0.1333\n",
      "Epoch 22/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00022: val_loss improved from 0.13329 to 0.13193, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1263 - val_loss: 0.1319\n",
      "Epoch 23/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00023: val_loss improved from 0.13193 to 0.13135, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1257 - val_loss: 0.1313\n",
      "Epoch 24/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00024: val_loss improved from 0.13135 to 0.13107, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1254 - val_loss: 0.1311\n",
      "Epoch 25/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00025: val_loss improved from 0.13107 to 0.13097, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1250 - val_loss: 0.1310\n",
      "Epoch 26/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00026: val_loss improved from 0.13097 to 0.12991, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 0.1299\n",
      "Epoch 27/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00027: val_loss improved from 0.12991 to 0.12944, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 0.1294\n",
      "Epoch 28/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00028: val_loss did not improve from 0.12944\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1296\n",
      "Epoch 29/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1219\n",
      "Epoch 00029: val_loss improved from 0.12944 to 0.12884, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1233 - val_loss: 0.1288\n",
      "Epoch 30/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1225\n",
      "Epoch 00030: val_loss improved from 0.12884 to 0.12872, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1232 - val_loss: 0.1287\n",
      "Epoch 31/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00031: val_loss improved from 0.12872 to 0.12853, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1229 - val_loss: 0.1285\n",
      "Epoch 32/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00032: val_loss did not improve from 0.12853\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1228 - val_loss: 0.1289\n",
      "Epoch 33/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1222\n",
      "Epoch 00033: val_loss did not improve from 0.12853\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1226 - val_loss: 0.1290\n",
      " ###0 fold : val acc1 0.576, acc3 0.963, mae 0.235###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/77 [=======================>......] - ETA: 0s - loss: 15.4660\n",
      "Epoch 00001: val_loss improved from inf to 5.99433, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 14.0777 - val_loss: 5.9943\n",
      "Epoch 2/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 3.5419\n",
      "Epoch 00002: val_loss improved from 5.99433 to 1.28372, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 3.1940 - val_loss: 1.2837\n",
      "Epoch 3/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.8794\n",
      "Epoch 00003: val_loss improved from 1.28372 to 0.56599, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1000 - val_loss: 0.5660\n",
      "Epoch 4/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.5976\n",
      "Epoch 00004: val_loss improved from 0.56599 to 0.32584, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5562 - val_loss: 0.3258\n",
      "Epoch 5/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3712\n",
      "Epoch 00005: val_loss improved from 0.32584 to 0.24491, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3496 - val_loss: 0.2449\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.2622\n",
      "Epoch 00006: val_loss improved from 0.24491 to 0.21245, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2622 - val_loss: 0.2124\n",
      "Epoch 7/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.2225\n",
      "Epoch 00007: val_loss improved from 0.21245 to 0.19307, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2177 - val_loss: 0.1931\n",
      "Epoch 8/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1794\n",
      "Epoch 00008: val_loss improved from 0.19307 to 0.17906, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1908 - val_loss: 0.1791\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1723\n",
      "Epoch 00009: val_loss improved from 0.17906 to 0.16826, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1723 - val_loss: 0.1683\n",
      "Epoch 10/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1567\n",
      "Epoch 00010: val_loss improved from 0.16826 to 0.15902, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1609 - val_loss: 0.1590\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1510\n",
      "Epoch 00011: val_loss improved from 0.15902 to 0.15319, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1510 - val_loss: 0.1532\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1452\n",
      "Epoch 00012: val_loss improved from 0.15319 to 0.14833, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1452 - val_loss: 0.1483\n",
      "Epoch 13/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1398\n",
      "Epoch 00013: val_loss improved from 0.14833 to 0.14475, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1407 - val_loss: 0.1448\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1374\n",
      "Epoch 00014: val_loss improved from 0.14475 to 0.14097, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1374 - val_loss: 0.1410\n",
      "Epoch 15/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.1358\n",
      "Epoch 00015: val_loss improved from 0.14097 to 0.13890, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1353 - val_loss: 0.1389\n",
      "Epoch 16/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1330\n",
      "Epoch 00016: val_loss improved from 0.13890 to 0.13699, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1330 - val_loss: 0.1370\n",
      "Epoch 17/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1316\n",
      "Epoch 00017: val_loss improved from 0.13699 to 0.13571, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1315 - val_loss: 0.1357\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1301\n",
      "Epoch 00018: val_loss improved from 0.13571 to 0.13431, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1301 - val_loss: 0.1343\n",
      "Epoch 19/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00019: val_loss improved from 0.13431 to 0.13364, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1291 - val_loss: 0.1336\n",
      "Epoch 20/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00020: val_loss improved from 0.13364 to 0.13344, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1283 - val_loss: 0.1334\n",
      "Epoch 21/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1296\n",
      "Epoch 00021: val_loss improved from 0.13344 to 0.13224, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1278 - val_loss: 0.1322\n",
      "Epoch 22/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1267\n",
      "Epoch 00022: val_loss improved from 0.13224 to 0.13097, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1268 - val_loss: 0.1310\n",
      "Epoch 23/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.1261\n",
      "Epoch 00023: val_loss improved from 0.13097 to 0.13054, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1260 - val_loss: 0.1305\n",
      "Epoch 24/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.1259\n",
      "Epoch 00024: val_loss improved from 0.13054 to 0.13053, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1258 - val_loss: 0.1305\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00025: val_loss improved from 0.13053 to 0.13009, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1253 - val_loss: 0.1301\n",
      "Epoch 26/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00026: val_loss improved from 0.13009 to 0.12921, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1253 - val_loss: 0.1292\n",
      "Epoch 27/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00027: val_loss improved from 0.12921 to 0.12899, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1245 - val_loss: 0.1290\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00028: val_loss did not improve from 0.12899\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1242 - val_loss: 0.1291\n",
      "Epoch 29/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00029: val_loss improved from 0.12899 to 0.12851, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1239 - val_loss: 0.1285\n",
      "Epoch 30/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1234\n",
      "Epoch 00030: val_loss improved from 0.12851 to 0.12842, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1237 - val_loss: 0.1284\n",
      "Epoch 31/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00031: val_loss did not improve from 0.12842\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 0.1286\n",
      "Epoch 32/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1227\n",
      "Epoch 00032: val_loss improved from 0.12842 to 0.12801, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1232 - val_loss: 0.1280\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1230\n",
      "Epoch 00033: val_loss did not improve from 0.12801\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1230 - val_loss: 0.1282\n",
      "Epoch 34/100\n",
      "58/77 [=====================>........] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00034: val_loss improved from 0.12801 to 0.12778, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1231 - val_loss: 0.1278\n",
      "Epoch 35/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00035: val_loss did not improve from 0.12778\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1228 - val_loss: 0.1279\n",
      "Epoch 36/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1228\n",
      "Epoch 00036: val_loss improved from 0.12778 to 0.12732, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1225 - val_loss: 0.1273\n",
      "Epoch 37/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1228\n",
      "Epoch 00037: val_loss did not improve from 0.12732\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1227 - val_loss: 0.1275\n",
      "Epoch 38/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00038: val_loss improved from 0.12732 to 0.12677, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1229 - val_loss: 0.1268\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1232\n",
      "Epoch 00039: val_loss improved from 0.12677 to 0.12673, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1232 - val_loss: 0.1267\n",
      "Epoch 40/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1233\n",
      "Epoch 00040: val_loss did not improve from 0.12673\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1231 - val_loss: 0.1270\n",
      "Epoch 41/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00041: val_loss did not improve from 0.12673\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1245 - val_loss: 0.1275\n",
      " ###1 fold : val acc1 0.588, acc3 0.963, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/77 [=========================>....] - ETA: 0s - loss: 15.0138\n",
      "Epoch 00001: val_loss improved from inf to 5.98259, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 14.0752 - val_loss: 5.9826\n",
      "Epoch 2/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 3.4365\n",
      "Epoch 00002: val_loss improved from 5.98259 to 1.28672, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.1818 - val_loss: 1.2867\n",
      "Epoch 3/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.8984\n",
      "Epoch 00003: val_loss improved from 1.28672 to 0.56612, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1002 - val_loss: 0.5661\n",
      "Epoch 4/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.5957\n",
      "Epoch 00004: val_loss improved from 0.56612 to 0.32564, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5546 - val_loss: 0.3256\n",
      "Epoch 5/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.3644\n",
      "Epoch 00005: val_loss improved from 0.32564 to 0.24543, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3473 - val_loss: 0.2454\n",
      "Epoch 6/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.2744\n",
      "Epoch 00006: val_loss improved from 0.24543 to 0.21342, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2596 - val_loss: 0.2134\n",
      "Epoch 7/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.2217\n",
      "Epoch 00007: val_loss improved from 0.21342 to 0.19448, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2156 - val_loss: 0.1945\n",
      "Epoch 8/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1770\n",
      "Epoch 00008: val_loss improved from 0.19448 to 0.18071, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1892 - val_loss: 0.1807\n",
      "Epoch 9/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1728\n",
      "Epoch 00009: val_loss improved from 0.18071 to 0.16979, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1711 - val_loss: 0.1698\n",
      "Epoch 10/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1553\n",
      "Epoch 00010: val_loss improved from 0.16979 to 0.16060, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1606 - val_loss: 0.1606\n",
      "Epoch 11/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1502\n",
      "Epoch 00011: val_loss improved from 0.16060 to 0.15459, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1509 - val_loss: 0.1546\n",
      "Epoch 12/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1448\n",
      "Epoch 00012: val_loss improved from 0.15459 to 0.14971, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1451 - val_loss: 0.1497\n",
      "Epoch 13/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1386\n",
      "Epoch 00013: val_loss improved from 0.14971 to 0.14597, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1406 - val_loss: 0.1460\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1374\n",
      "Epoch 00014: val_loss improved from 0.14597 to 0.14187, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1374 - val_loss: 0.1419\n",
      "Epoch 15/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00015: val_loss improved from 0.14187 to 0.13961, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1352 - val_loss: 0.1396\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1329\n",
      "Epoch 00016: val_loss improved from 0.13961 to 0.13734, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1329 - val_loss: 0.1373\n",
      "Epoch 17/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1317\n",
      "Epoch 00017: val_loss improved from 0.13734 to 0.13621, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1316 - val_loss: 0.1362\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1303\n",
      "Epoch 00018: val_loss improved from 0.13621 to 0.13468, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1303 - val_loss: 0.1347\n",
      "Epoch 19/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1282\n",
      "Epoch 00019: val_loss improved from 0.13468 to 0.13393, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1292 - val_loss: 0.1339\n",
      "Epoch 20/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1295\n",
      "Epoch 00020: val_loss improved from 0.13393 to 0.13368, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1286 - val_loss: 0.1337\n",
      "Epoch 21/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1297\n",
      "Epoch 00021: val_loss improved from 0.13368 to 0.13239, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1279 - val_loss: 0.1324\n",
      "Epoch 22/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00022: val_loss improved from 0.13239 to 0.13137, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1271 - val_loss: 0.1314\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00023: val_loss improved from 0.13137 to 0.13082, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1263 - val_loss: 0.1308\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00024: val_loss did not improve from 0.13082\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1260 - val_loss: 0.1308\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00025: val_loss improved from 0.13082 to 0.13015, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1254 - val_loss: 0.1301\n",
      "Epoch 26/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00026: val_loss improved from 0.13015 to 0.12934, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1254 - val_loss: 0.1293\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00027: val_loss improved from 0.12934 to 0.12897, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1247 - val_loss: 0.1290\n",
      "Epoch 28/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1246\n",
      "Epoch 00028: val_loss did not improve from 0.12897\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 0.1292\n",
      "Epoch 29/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00029: val_loss improved from 0.12897 to 0.12885, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1242 - val_loss: 0.1288\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00030: val_loss improved from 0.12885 to 0.12848, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1240 - val_loss: 0.1285\n",
      "Epoch 31/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00031: val_loss did not improve from 0.12848\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1238 - val_loss: 0.1287\n",
      "Epoch 32/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1234\n",
      "Epoch 00032: val_loss improved from 0.12848 to 0.12789, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1233 - val_loss: 0.1279\n",
      "Epoch 33/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00033: val_loss did not improve from 0.12789\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 0.1279\n",
      "Epoch 34/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00034: val_loss did not improve from 0.12789\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 0.1285\n",
      " ###2 fold : val acc1 0.585, acc3 0.963, mae 0.230###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/77 [===========================>..] - ETA: 0s - loss: 14.5622\n",
      "Epoch 00001: val_loss improved from inf to 6.00690, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 14.0756 - val_loss: 6.0069\n",
      "Epoch 2/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.3128\n",
      "Epoch 00002: val_loss improved from 6.00690 to 1.28855, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.1968 - val_loss: 1.2886\n",
      "Epoch 3/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.8492\n",
      "Epoch 00003: val_loss improved from 1.28855 to 0.56741, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0984 - val_loss: 0.5674\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.5543\n",
      "Epoch 00004: val_loss improved from 0.56741 to 0.32675, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5543 - val_loss: 0.3268\n",
      "Epoch 5/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.3578\n",
      "Epoch 00005: val_loss improved from 0.32675 to 0.24637, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3473 - val_loss: 0.2464\n",
      "Epoch 6/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 0.2666\n",
      "Epoch 00006: val_loss improved from 0.24637 to 0.21402, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2606 - val_loss: 0.2140\n",
      "Epoch 7/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.2192\n",
      "Epoch 00007: val_loss improved from 0.21402 to 0.19515, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2171 - val_loss: 0.1952\n",
      "Epoch 8/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.1776\n",
      "Epoch 00008: val_loss improved from 0.19515 to 0.18123, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1910 - val_loss: 0.1812\n",
      "Epoch 9/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1753\n",
      "Epoch 00009: val_loss improved from 0.18123 to 0.17063, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1730 - val_loss: 0.1706\n",
      "Epoch 10/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1566\n",
      "Epoch 00010: val_loss improved from 0.17063 to 0.16114, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1618 - val_loss: 0.1611\n",
      "Epoch 11/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.1533\n",
      "Epoch 00011: val_loss improved from 0.16114 to 0.15493, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1523 - val_loss: 0.1549\n",
      "Epoch 12/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1476\n",
      "Epoch 00012: val_loss improved from 0.15493 to 0.15004, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1465 - val_loss: 0.1500\n",
      "Epoch 13/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1404\n",
      "Epoch 00013: val_loss improved from 0.15004 to 0.14601, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1418 - val_loss: 0.1460\n",
      "Epoch 14/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1417\n",
      "Epoch 00014: val_loss improved from 0.14601 to 0.14178, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1385 - val_loss: 0.1418\n",
      "Epoch 15/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1366\n",
      "Epoch 00015: val_loss improved from 0.14178 to 0.13970, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1365 - val_loss: 0.1397\n",
      "Epoch 16/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1353\n",
      "Epoch 00016: val_loss improved from 0.13970 to 0.13742, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1339 - val_loss: 0.1374\n",
      "Epoch 17/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1320\n",
      "Epoch 00017: val_loss improved from 0.13742 to 0.13631, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1327 - val_loss: 0.1363\n",
      "Epoch 18/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1312\n",
      "Epoch 00018: val_loss improved from 0.13631 to 0.13477, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1313 - val_loss: 0.1348\n",
      "Epoch 19/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00019: val_loss improved from 0.13477 to 0.13371, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1301 - val_loss: 0.1337\n",
      "Epoch 20/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1304\n",
      "Epoch 00020: val_loss improved from 0.13371 to 0.13347, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1296 - val_loss: 0.1335\n",
      "Epoch 21/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1297\n",
      "Epoch 00021: val_loss improved from 0.13347 to 0.13205, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1289 - val_loss: 0.1321\n",
      "Epoch 22/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1274\n",
      "Epoch 00022: val_loss improved from 0.13205 to 0.13137, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1279 - val_loss: 0.1314\n",
      "Epoch 23/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1266\n",
      "Epoch 00023: val_loss improved from 0.13137 to 0.13070, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1271 - val_loss: 0.1307\n",
      "Epoch 24/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1271\n",
      "Epoch 00024: val_loss did not improve from 0.13070\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1267 - val_loss: 0.1311\n",
      "Epoch 25/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00025: val_loss improved from 0.13070 to 0.13023, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1263 - val_loss: 0.1302\n",
      "Epoch 26/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00026: val_loss improved from 0.13023 to 0.12965, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1262 - val_loss: 0.1296\n",
      "Epoch 27/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1259\n",
      "Epoch 00027: val_loss improved from 0.12965 to 0.12906, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1256 - val_loss: 0.1291\n",
      "Epoch 28/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00028: val_loss did not improve from 0.12906\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1252 - val_loss: 0.1292\n",
      "Epoch 29/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00029: val_loss improved from 0.12906 to 0.12875, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1250 - val_loss: 0.1288\n",
      "Epoch 30/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00030: val_loss did not improve from 0.12875\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 0.1288\n",
      "Epoch 31/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00031: val_loss improved from 0.12875 to 0.12852, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1246 - val_loss: 0.1285\n",
      "Epoch 32/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00032: val_loss improved from 0.12852 to 0.12792, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1242 - val_loss: 0.1279\n",
      "Epoch 33/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00033: val_loss did not improve from 0.12792\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1243 - val_loss: 0.1282\n",
      "Epoch 34/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00034: val_loss did not improve from 0.12792\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1243 - val_loss: 0.1284\n",
      " ###3 fold : val acc1 0.575, acc3 0.964, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/77 [=========================>....] - ETA: 0s - loss: 14.9715\n",
      "Epoch 00001: val_loss improved from inf to 6.03597, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 14.0630 - val_loss: 6.0360\n",
      "Epoch 2/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 3.3954\n",
      "Epoch 00002: val_loss improved from 6.03597 to 1.27325, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.2180 - val_loss: 1.2732\n",
      "Epoch 3/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 1.1395\n",
      "Epoch 00003: val_loss improved from 1.27325 to 0.55692, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0891 - val_loss: 0.5569\n",
      "Epoch 4/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6164\n",
      "Epoch 00004: val_loss improved from 0.55692 to 0.32526, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5564 - val_loss: 0.3253\n",
      "Epoch 5/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3859\n",
      "Epoch 00005: val_loss improved from 0.32526 to 0.24619, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3503 - val_loss: 0.2462\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.2622\n",
      "Epoch 00006: val_loss improved from 0.24619 to 0.21426, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2622 - val_loss: 0.2143\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.2181\n",
      "Epoch 00007: val_loss improved from 0.21426 to 0.19580, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2181 - val_loss: 0.1958\n",
      "Epoch 8/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.2001\n",
      "Epoch 00008: val_loss improved from 0.19580 to 0.18206, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1913 - val_loss: 0.1821\n",
      "Epoch 9/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1679\n",
      "Epoch 00009: val_loss improved from 0.18206 to 0.17119, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1735 - val_loss: 0.1712\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1621\n",
      "Epoch 00010: val_loss improved from 0.17119 to 0.16280, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1614 - val_loss: 0.1628\n",
      "Epoch 11/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1527\n",
      "Epoch 00011: val_loss improved from 0.16280 to 0.15543, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1526 - val_loss: 0.1554\n",
      "Epoch 12/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1471\n",
      "Epoch 00012: val_loss improved from 0.15543 to 0.14965, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1464 - val_loss: 0.1496\n",
      "Epoch 13/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1389\n",
      "Epoch 00013: val_loss improved from 0.14965 to 0.14598, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1416 - val_loss: 0.1460\n",
      "Epoch 14/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1396\n",
      "Epoch 00014: val_loss improved from 0.14598 to 0.14237, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1384 - val_loss: 0.1424\n",
      "Epoch 15/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1356\n",
      "Epoch 00015: val_loss improved from 0.14237 to 0.13985, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1360 - val_loss: 0.1398\n",
      "Epoch 16/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1355\n",
      "Epoch 00016: val_loss improved from 0.13985 to 0.13744, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1340 - val_loss: 0.1374\n",
      "Epoch 17/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1331\n",
      "Epoch 00017: val_loss improved from 0.13744 to 0.13668, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1327 - val_loss: 0.1367\n",
      "Epoch 18/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1295\n",
      "Epoch 00018: val_loss improved from 0.13668 to 0.13490, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1312 - val_loss: 0.1349\n",
      "Epoch 19/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00019: val_loss improved from 0.13490 to 0.13367, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1303 - val_loss: 0.1337\n",
      "Epoch 20/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1293\n",
      "Epoch 00020: val_loss improved from 0.13367 to 0.13289, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1291 - val_loss: 0.1329\n",
      "Epoch 21/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1306\n",
      "Epoch 00021: val_loss improved from 0.13289 to 0.13223, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1287 - val_loss: 0.1322\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1276\n",
      "Epoch 00022: val_loss improved from 0.13223 to 0.13155, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1276 - val_loss: 0.1316\n",
      "Epoch 23/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1273\n",
      "Epoch 00023: val_loss improved from 0.13155 to 0.13090, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1271 - val_loss: 0.1309\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00024: val_loss improved from 0.13090 to 0.13079, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1265 - val_loss: 0.1308\n",
      "Epoch 25/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00025: val_loss improved from 0.13079 to 0.12988, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1261 - val_loss: 0.1299\n",
      "Epoch 26/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00026: val_loss improved from 0.12988 to 0.12965, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1258 - val_loss: 0.1297\n",
      "Epoch 27/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00027: val_loss improved from 0.12965 to 0.12918, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1256 - val_loss: 0.1292\n",
      "Epoch 28/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00028: val_loss improved from 0.12918 to 0.12883, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1252 - val_loss: 0.1288\n",
      "Epoch 29/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00029: val_loss did not improve from 0.12883\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1249 - val_loss: 0.1294\n",
      "Epoch 30/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00030: val_loss improved from 0.12883 to 0.12816, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1246 - val_loss: 0.1282\n",
      "Epoch 31/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00031: val_loss did not improve from 0.12816\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1246 - val_loss: 0.1283\n",
      "Epoch 32/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00032: val_loss improved from 0.12816 to 0.12810, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1241 - val_loss: 0.1281\n",
      "Epoch 33/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00033: val_loss improved from 0.12810 to 0.12782, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1242 - val_loss: 0.1278\n",
      "Epoch 34/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1230\n",
      "Epoch 00034: val_loss improved from 0.12782 to 0.12769, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1236 - val_loss: 0.1277\n",
      "Epoch 35/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1224\n",
      "Epoch 00035: val_loss did not improve from 0.12769\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 0.1280\n",
      "Epoch 36/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00036: val_loss improved from 0.12769 to 0.12723, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1236 - val_loss: 0.1272\n",
      "Epoch 37/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00037: val_loss improved from 0.12723 to 0.12723, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1237 - val_loss: 0.1272\n",
      "Epoch 38/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00038: val_loss improved from 0.12723 to 0.12686, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1236 - val_loss: 0.1269\n",
      "Epoch 39/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00039: val_loss did not improve from 0.12686\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 0.1272\n",
      "Epoch 40/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1213\n",
      "Epoch 00040: val_loss did not improve from 0.12686\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 0.1271\n",
      " ###4 fold : val acc1 0.589, acc3 0.965, mae 0.227###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/77 [==========================>...] - ETA: 0s - loss: 14.4306\n",
      "Epoch 00001: val_loss improved from inf to 5.67252, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 13.8272 - val_loss: 5.6725\n",
      "Epoch 2/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 2.8557\n",
      "Epoch 00002: val_loss improved from 5.67252 to 1.26020, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.7251 - val_loss: 1.2602\n",
      "Epoch 3/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.8391\n",
      "Epoch 00003: val_loss improved from 1.26020 to 0.50660, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8011 - val_loss: 0.5066\n",
      "Epoch 4/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3574\n",
      "Epoch 00004: val_loss improved from 0.50660 to 0.28925, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3570 - val_loss: 0.2893\n",
      "Epoch 5/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.2320\n",
      "Epoch 00005: val_loss improved from 0.28925 to 0.22517, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2317 - val_loss: 0.2252\n",
      "Epoch 6/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1928\n",
      "Epoch 00006: val_loss improved from 0.22517 to 0.19977, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1931 - val_loss: 0.1998\n",
      "Epoch 7/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.1747\n",
      "Epoch 00007: val_loss improved from 0.19977 to 0.18402, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1756 - val_loss: 0.1840\n",
      "Epoch 8/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.1640\n",
      "Epoch 00008: val_loss improved from 0.18402 to 0.17211, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1639 - val_loss: 0.1721\n",
      "Epoch 9/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.1556\n",
      "Epoch 00009: val_loss improved from 0.17211 to 0.16285, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1552 - val_loss: 0.1629\n",
      "Epoch 10/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1471\n",
      "Epoch 00010: val_loss improved from 0.16285 to 0.15644, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1488 - val_loss: 0.1564\n",
      "Epoch 11/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1434\n",
      "Epoch 00011: val_loss improved from 0.15644 to 0.15138, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1440 - val_loss: 0.1514\n",
      "Epoch 12/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1423\n",
      "Epoch 00012: val_loss improved from 0.15138 to 0.14776, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1405 - val_loss: 0.1478\n",
      "Epoch 13/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1384\n",
      "Epoch 00013: val_loss improved from 0.14776 to 0.14471, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1380 - val_loss: 0.1447\n",
      "Epoch 14/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1365\n",
      "Epoch 00014: val_loss improved from 0.14471 to 0.14264, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1362 - val_loss: 0.1426\n",
      "Epoch 15/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1346\n",
      "Epoch 00015: val_loss improved from 0.14264 to 0.14093, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1343 - val_loss: 0.1409\n",
      "Epoch 16/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1338\n",
      "Epoch 00016: val_loss improved from 0.14093 to 0.13910, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1332 - val_loss: 0.1391\n",
      "Epoch 17/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1328\n",
      "Epoch 00017: val_loss improved from 0.13910 to 0.13778, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1322 - val_loss: 0.1378\n",
      "Epoch 18/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1298\n",
      "Epoch 00018: val_loss improved from 0.13778 to 0.13729, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1313 - val_loss: 0.1373\n",
      "Epoch 19/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00019: val_loss improved from 0.13729 to 0.13557, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1305 - val_loss: 0.1356\n",
      "Epoch 20/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1305\n",
      "Epoch 00020: val_loss improved from 0.13557 to 0.13502, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1296 - val_loss: 0.1350\n",
      "Epoch 21/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1299\n",
      "Epoch 00021: val_loss improved from 0.13502 to 0.13396, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1289 - val_loss: 0.1340\n",
      "Epoch 22/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1289\n",
      "Epoch 00022: val_loss improved from 0.13396 to 0.13387, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1284 - val_loss: 0.1339\n",
      "Epoch 23/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00023: val_loss improved from 0.13387 to 0.13241, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1278 - val_loss: 0.1324\n",
      "Epoch 24/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1266\n",
      "Epoch 00024: val_loss did not improve from 0.13241\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1272 - val_loss: 0.1325\n",
      "Epoch 25/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00025: val_loss improved from 0.13241 to 0.13122, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1267 - val_loss: 0.1312\n",
      "Epoch 26/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1261\n",
      "Epoch 00026: val_loss improved from 0.13122 to 0.13089, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1264 - val_loss: 0.1309\n",
      "Epoch 27/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00027: val_loss improved from 0.13089 to 0.13034, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1261 - val_loss: 0.1303\n",
      "Epoch 28/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00028: val_loss did not improve from 0.13034\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1258 - val_loss: 0.1304\n",
      "Epoch 29/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00029: val_loss improved from 0.13034 to 0.12979, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1254 - val_loss: 0.1298\n",
      "Epoch 30/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00030: val_loss improved from 0.12979 to 0.12915, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1250 - val_loss: 0.1291\n",
      "Epoch 31/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00031: val_loss improved from 0.12915 to 0.12892, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1249 - val_loss: 0.1289\n",
      "Epoch 32/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1261\n",
      "Epoch 00032: val_loss improved from 0.12892 to 0.12864, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 0.1286\n",
      "Epoch 33/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00033: val_loss improved from 0.12864 to 0.12841, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 0.1284\n",
      "Epoch 34/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1232\n",
      "Epoch 00034: val_loss improved from 0.12841 to 0.12796, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 0.1280\n",
      "Epoch 35/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00035: val_loss did not improve from 0.12796\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1239 - val_loss: 0.1282\n",
      "Epoch 36/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00036: val_loss improved from 0.12796 to 0.12789, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1238 - val_loss: 0.1279\n",
      "Epoch 37/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00037: val_loss improved from 0.12789 to 0.12757, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1276\n",
      "Epoch 38/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00038: val_loss improved from 0.12757 to 0.12731, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 0.1273\n",
      "Epoch 39/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00039: val_loss improved from 0.12731 to 0.12691, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1233 - val_loss: 0.1269\n",
      "Epoch 40/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00040: val_loss did not improve from 0.12691\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1231 - val_loss: 0.1270\n",
      "Epoch 41/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00041: val_loss improved from 0.12691 to 0.12673, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1231 - val_loss: 0.1267\n",
      "Epoch 42/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1227\n",
      "Epoch 00042: val_loss did not improve from 0.12673\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1231 - val_loss: 0.1285\n",
      "Epoch 43/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00043: val_loss improved from 0.12673 to 0.12659, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1234 - val_loss: 0.1266\n",
      "Epoch 44/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00044: val_loss improved from 0.12659 to 0.12594, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1226 - val_loss: 0.1259\n",
      "Epoch 45/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00045: val_loss improved from 0.12594 to 0.12592, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1225 - val_loss: 0.1259\n",
      "Epoch 46/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00046: val_loss did not improve from 0.12592\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1224 - val_loss: 0.1263\n",
      "Epoch 47/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00047: val_loss did not improve from 0.12592\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1223 - val_loss: 0.1260\n",
      " ###5 fold : val acc1 0.593, acc3 0.968, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 14.9894\n",
      "Epoch 00001: val_loss improved from inf to 5.80715, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 13.9434 - val_loss: 5.8072\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 2.8885\n",
      "Epoch 00002: val_loss improved from 5.80715 to 1.23732, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.2112 - val_loss: 1.2373\n",
      "Epoch 3/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 1.2368\n",
      "Epoch 00003: val_loss improved from 1.23732 to 0.54275, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1409 - val_loss: 0.5427\n",
      "Epoch 4/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.6572\n",
      "Epoch 00004: val_loss improved from 0.54275 to 0.31962, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6021 - val_loss: 0.3196\n",
      "Epoch 5/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.4206\n",
      "Epoch 00005: val_loss improved from 0.31962 to 0.24252, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3857 - val_loss: 0.2425\n",
      "Epoch 6/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 0.2930\n",
      "Epoch 00006: val_loss improved from 0.24252 to 0.21129, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2843 - val_loss: 0.2113\n",
      "Epoch 7/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.2345\n",
      "Epoch 00007: val_loss improved from 0.21129 to 0.19430, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2312 - val_loss: 0.1943\n",
      "Epoch 8/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.2037\n",
      "Epoch 00008: val_loss improved from 0.19430 to 0.18010, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1975 - val_loss: 0.1801\n",
      "Epoch 9/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1650\n",
      "Epoch 00009: val_loss improved from 0.18010 to 0.16935, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1764 - val_loss: 0.1693\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1623\n",
      "Epoch 00010: val_loss improved from 0.16935 to 0.16097, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1626 - val_loss: 0.1610\n",
      "Epoch 11/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1456\n",
      "Epoch 00011: val_loss improved from 0.16097 to 0.15424, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1529 - val_loss: 0.1542\n",
      "Epoch 12/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1488\n",
      "Epoch 00012: val_loss improved from 0.15424 to 0.14849, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1460 - val_loss: 0.1485\n",
      "Epoch 13/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1436\n",
      "Epoch 00013: val_loss improved from 0.14849 to 0.14456, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1414 - val_loss: 0.1446\n",
      "Epoch 14/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1395\n",
      "Epoch 00014: val_loss improved from 0.14456 to 0.14134, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1379 - val_loss: 0.1413\n",
      "Epoch 15/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1370\n",
      "Epoch 00015: val_loss improved from 0.14134 to 0.13970, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1354 - val_loss: 0.1397\n",
      "Epoch 16/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00016: val_loss improved from 0.13970 to 0.13708, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1332 - val_loss: 0.1371\n",
      "Epoch 17/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00017: val_loss improved from 0.13708 to 0.13561, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1318 - val_loss: 0.1356\n",
      "Epoch 18/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1292\n",
      "Epoch 00018: val_loss improved from 0.13561 to 0.13483, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1304 - val_loss: 0.1348\n",
      "Epoch 19/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1264\n",
      "Epoch 00019: val_loss improved from 0.13483 to 0.13314, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1293 - val_loss: 0.1331\n",
      "Epoch 20/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1300\n",
      "Epoch 00020: val_loss improved from 0.13314 to 0.13277, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1286 - val_loss: 0.1328\n",
      "Epoch 21/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1291\n",
      "Epoch 00021: val_loss improved from 0.13277 to 0.13175, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1275 - val_loss: 0.1318\n",
      "Epoch 22/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1289\n",
      "Epoch 00022: val_loss improved from 0.13175 to 0.13143, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1272 - val_loss: 0.1314\n",
      "Epoch 23/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00023: val_loss improved from 0.13143 to 0.13061, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1264 - val_loss: 0.1306\n",
      "Epoch 24/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00024: val_loss improved from 0.13061 to 0.13024, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1258 - val_loss: 0.1302\n",
      "Epoch 25/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1226\n",
      "Epoch 00025: val_loss improved from 0.13024 to 0.12994, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1254 - val_loss: 0.1299\n",
      "Epoch 26/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00026: val_loss improved from 0.12994 to 0.12943, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1250 - val_loss: 0.1294\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00027: val_loss did not improve from 0.12943\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1251 - val_loss: 0.1295\n",
      "Epoch 28/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00028: val_loss improved from 0.12943 to 0.12899, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1245 - val_loss: 0.1290\n",
      "Epoch 29/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1251\n",
      "Epoch 00029: val_loss improved from 0.12899 to 0.12881, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1243 - val_loss: 0.1288\n",
      "Epoch 30/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00030: val_loss improved from 0.12881 to 0.12824, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1237 - val_loss: 0.1282\n",
      "Epoch 31/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1230\n",
      "Epoch 00031: val_loss did not improve from 0.12824\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1283\n",
      "Epoch 32/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00032: val_loss improved from 0.12824 to 0.12781, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1232 - val_loss: 0.1278\n",
      "Epoch 33/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00033: val_loss improved from 0.12781 to 0.12765, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1236 - val_loss: 0.1276\n",
      "Epoch 34/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1222\n",
      "Epoch 00034: val_loss improved from 0.12765 to 0.12731, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1231 - val_loss: 0.1273\n",
      "Epoch 35/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00035: val_loss did not improve from 0.12731\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1234 - val_loss: 0.1276\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00036: val_loss did not improve from 0.12731\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1229 - val_loss: 0.1273\n",
      " ###6 fold : val acc1 0.580, acc3 0.962, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/77 [==========================>...] - ETA: 0s - loss: 14.6647\n",
      "Epoch 00001: val_loss improved from inf to 5.82032, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 13.9567 - val_loss: 5.8203\n",
      "Epoch 2/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 2.9253\n",
      "Epoch 00002: val_loss improved from 5.82032 to 1.24060, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.2251 - val_loss: 1.2406\n",
      "Epoch 3/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 1.2260\n",
      "Epoch 00003: val_loss improved from 1.24060 to 0.55069, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1568 - val_loss: 0.5507\n",
      "Epoch 4/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6952\n",
      "Epoch 00004: val_loss improved from 0.55069 to 0.32500, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6216 - val_loss: 0.3250\n",
      "Epoch 5/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.4390\n",
      "Epoch 00005: val_loss improved from 0.32500 to 0.24440, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4017 - val_loss: 0.2444\n",
      "Epoch 6/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3147\n",
      "Epoch 00006: val_loss improved from 0.24440 to 0.21108, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2961 - val_loss: 0.2111\n",
      "Epoch 7/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1898\n",
      "Epoch 00007: val_loss improved from 0.21108 to 0.19258, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2388 - val_loss: 0.1926\n",
      "Epoch 8/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.2064\n",
      "Epoch 00008: val_loss improved from 0.19258 to 0.17864, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2018 - val_loss: 0.1786\n",
      "Epoch 9/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1833\n",
      "Epoch 00009: val_loss improved from 0.17864 to 0.16820, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1787 - val_loss: 0.1682\n",
      "Epoch 10/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1644\n",
      "Epoch 00010: val_loss improved from 0.16820 to 0.15999, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1637 - val_loss: 0.1600\n",
      "Epoch 11/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1531\n",
      "Epoch 00011: val_loss improved from 0.15999 to 0.15355, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1536 - val_loss: 0.1535\n",
      "Epoch 12/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1482\n",
      "Epoch 00012: val_loss improved from 0.15355 to 0.14786, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1464 - val_loss: 0.1479\n",
      "Epoch 13/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1432\n",
      "Epoch 00013: val_loss improved from 0.14786 to 0.14429, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1415 - val_loss: 0.1443\n",
      "Epoch 14/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1382\n",
      "Epoch 00014: val_loss improved from 0.14429 to 0.14103, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1380 - val_loss: 0.1410\n",
      "Epoch 15/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1362\n",
      "Epoch 00015: val_loss improved from 0.14103 to 0.13930, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1354 - val_loss: 0.1393\n",
      "Epoch 16/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1320\n",
      "Epoch 00016: val_loss improved from 0.13930 to 0.13699, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1334 - val_loss: 0.1370\n",
      "Epoch 17/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1329\n",
      "Epoch 00017: val_loss improved from 0.13699 to 0.13557, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1319 - val_loss: 0.1356\n",
      "Epoch 18/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1298\n",
      "Epoch 00018: val_loss improved from 0.13557 to 0.13511, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1308 - val_loss: 0.1351\n",
      "Epoch 19/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00019: val_loss improved from 0.13511 to 0.13332, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1297 - val_loss: 0.1333\n",
      "Epoch 20/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1302\n",
      "Epoch 00020: val_loss improved from 0.13332 to 0.13269, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1288 - val_loss: 0.1327\n",
      "Epoch 21/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1288\n",
      "Epoch 00021: val_loss improved from 0.13269 to 0.13180, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1280 - val_loss: 0.1318\n",
      "Epoch 22/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1282\n",
      "Epoch 00022: val_loss improved from 0.13180 to 0.13147, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1276 - val_loss: 0.1315\n",
      "Epoch 23/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1264\n",
      "Epoch 00023: val_loss improved from 0.13147 to 0.13063, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1267 - val_loss: 0.1306\n",
      "Epoch 24/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00024: val_loss improved from 0.13063 to 0.13029, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1262 - val_loss: 0.1303\n",
      "Epoch 25/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00025: val_loss improved from 0.13029 to 0.12983, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1258 - val_loss: 0.1298\n",
      "Epoch 26/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00026: val_loss improved from 0.12983 to 0.12974, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1254 - val_loss: 0.1297\n",
      "Epoch 27/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00027: val_loss improved from 0.12974 to 0.12952, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1255 - val_loss: 0.1295\n",
      "Epoch 28/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00028: val_loss improved from 0.12952 to 0.12894, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1248 - val_loss: 0.1289\n",
      "Epoch 29/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00029: val_loss improved from 0.12894 to 0.12890, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1245 - val_loss: 0.1289\n",
      "Epoch 30/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00030: val_loss improved from 0.12890 to 0.12862, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 0.1286\n",
      "Epoch 31/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00031: val_loss did not improve from 0.12862\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1242 - val_loss: 0.1287\n",
      "Epoch 32/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00032: val_loss improved from 0.12862 to 0.12806, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 0.1281\n",
      "Epoch 33/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00033: val_loss improved from 0.12806 to 0.12773, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1238 - val_loss: 0.1277\n",
      "Epoch 34/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1218\n",
      "Epoch 00034: val_loss improved from 0.12773 to 0.12741, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1234 - val_loss: 0.1274\n",
      "Epoch 35/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00035: val_loss did not improve from 0.12741\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1278\n",
      "Epoch 36/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1221\n",
      "Epoch 00036: val_loss improved from 0.12741 to 0.12730, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1233 - val_loss: 0.1273\n",
      "Epoch 37/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00037: val_loss did not improve from 0.12730\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 0.1288\n",
      "Epoch 38/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00038: val_loss did not improve from 0.12730\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1240 - val_loss: 0.1288\n",
      " ###7 fold : val acc1 0.586, acc3 0.964, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 15.0105\n",
      "Epoch 00001: val_loss improved from inf to 5.87894, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 13.9575 - val_loss: 5.8789\n",
      "Epoch 2/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 2.8599\n",
      "Epoch 00002: val_loss improved from 5.87894 to 1.28191, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.2275 - val_loss: 1.2819\n",
      "Epoch 3/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 1.2404\n",
      "Epoch 00003: val_loss improved from 1.28191 to 0.56495, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1536 - val_loss: 0.5650\n",
      "Epoch 4/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.6471\n",
      "Epoch 00004: val_loss improved from 0.56495 to 0.32696, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6182 - val_loss: 0.3270\n",
      "Epoch 5/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 0.4178\n",
      "Epoch 00005: val_loss improved from 0.32696 to 0.23986, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3991 - val_loss: 0.2399\n",
      "Epoch 6/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.3116\n",
      "Epoch 00006: val_loss improved from 0.23986 to 0.20392, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2944 - val_loss: 0.2039\n",
      "Epoch 7/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1893\n",
      "Epoch 00007: val_loss improved from 0.20392 to 0.18539, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2376 - val_loss: 0.1854\n",
      "Epoch 8/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.2047\n",
      "Epoch 00008: val_loss improved from 0.18539 to 0.17174, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2012 - val_loss: 0.1717\n",
      "Epoch 9/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1641\n",
      "Epoch 00009: val_loss improved from 0.17174 to 0.16250, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1783 - val_loss: 0.1625\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1634\n",
      "Epoch 00010: val_loss improved from 0.16250 to 0.15526, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1634 - val_loss: 0.1553\n",
      "Epoch 11/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1469\n",
      "Epoch 00011: val_loss improved from 0.15526 to 0.14977, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1538 - val_loss: 0.1498\n",
      "Epoch 12/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1475\n",
      "Epoch 00012: val_loss improved from 0.14977 to 0.14534, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1461 - val_loss: 0.1453\n",
      "Epoch 13/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1426\n",
      "Epoch 00013: val_loss improved from 0.14534 to 0.14142, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1411 - val_loss: 0.1414\n",
      "Epoch 14/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1385\n",
      "Epoch 00014: val_loss improved from 0.14142 to 0.13870, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1377 - val_loss: 0.1387\n",
      "Epoch 15/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1356\n",
      "Epoch 00015: val_loss improved from 0.13870 to 0.13769, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1351 - val_loss: 0.1377\n",
      "Epoch 16/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1325\n",
      "Epoch 00016: val_loss improved from 0.13769 to 0.13524, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1330 - val_loss: 0.1352\n",
      "Epoch 17/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00017: val_loss improved from 0.13524 to 0.13388, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1314 - val_loss: 0.1339\n",
      "Epoch 18/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1298\n",
      "Epoch 00018: val_loss improved from 0.13388 to 0.13324, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1302 - val_loss: 0.1332\n",
      "Epoch 19/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1266\n",
      "Epoch 00019: val_loss improved from 0.13324 to 0.13152, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1292 - val_loss: 0.1315\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1284\n",
      "Epoch 00020: val_loss improved from 0.13152 to 0.13114, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1284 - val_loss: 0.1311\n",
      "Epoch 21/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00021: val_loss improved from 0.13114 to 0.13052, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1275 - val_loss: 0.1305\n",
      "Epoch 22/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1284\n",
      "Epoch 00022: val_loss improved from 0.13052 to 0.12989, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1271 - val_loss: 0.1299\n",
      "Epoch 23/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00023: val_loss improved from 0.12989 to 0.12929, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1263 - val_loss: 0.1293\n",
      "Epoch 24/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00024: val_loss improved from 0.12929 to 0.12849, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1257 - val_loss: 0.1285\n",
      "Epoch 25/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1224\n",
      "Epoch 00025: val_loss improved from 0.12849 to 0.12848, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1253 - val_loss: 0.1285\n",
      "Epoch 26/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00026: val_loss did not improve from 0.12848\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1250 - val_loss: 0.1287\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00027: val_loss improved from 0.12848 to 0.12776, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1251 - val_loss: 0.1278\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00028: val_loss improved from 0.12776 to 0.12745, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1245 - val_loss: 0.1274\n",
      "Epoch 29/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00029: val_loss improved from 0.12745 to 0.12697, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1241 - val_loss: 0.1270\n",
      "Epoch 30/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00030: val_loss did not improve from 0.12697\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1273\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00031: val_loss improved from 0.12697 to 0.12669, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1237 - val_loss: 0.1267\n",
      "Epoch 32/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1233\n",
      "Epoch 00032: val_loss improved from 0.12669 to 0.12635, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1232 - val_loss: 0.1264\n",
      "Epoch 33/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1235\n",
      "Epoch 00033: val_loss improved from 0.12635 to 0.12609, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1233 - val_loss: 0.1261\n",
      "Epoch 34/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1213\n",
      "Epoch 00034: val_loss improved from 0.12609 to 0.12585, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1229 - val_loss: 0.1258\n",
      "Epoch 35/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00035: val_loss did not improve from 0.12585\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1231 - val_loss: 0.1265\n",
      "Epoch 36/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1210\n",
      "Epoch 00036: val_loss improved from 0.12585 to 0.12556, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1227 - val_loss: 0.1256\n",
      "Epoch 37/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00037: val_loss did not improve from 0.12556\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1230 - val_loss: 0.1269\n",
      "Epoch 38/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00038: val_loss did not improve from 0.12556\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1227 - val_loss: 0.1276\n",
      " ###8 fold : val acc1 0.578, acc3 0.960, mae 0.235###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/77 [===========================>..] - ETA: 0s - loss: 14.3478\n",
      "Epoch 00001: val_loss improved from inf to 5.87727, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 13.9575 - val_loss: 5.8773\n",
      "Epoch 2/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 3.2966\n",
      "Epoch 00002: val_loss improved from 5.87727 to 1.28024, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.2275 - val_loss: 1.2802\n",
      "Epoch 3/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.1590\n",
      "Epoch 00003: val_loss improved from 1.28024 to 0.55902, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1536 - val_loss: 0.5590\n",
      "Epoch 4/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.6420\n",
      "Epoch 00004: val_loss improved from 0.55902 to 0.31885, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6182 - val_loss: 0.3189\n",
      "Epoch 5/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.4159\n",
      "Epoch 00005: val_loss improved from 0.31885 to 0.23306, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3991 - val_loss: 0.2331\n",
      "Epoch 6/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.3051\n",
      "Epoch 00006: val_loss improved from 0.23306 to 0.19852, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2944 - val_loss: 0.1985\n",
      "Epoch 7/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.2406\n",
      "Epoch 00007: val_loss improved from 0.19852 to 0.18159, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2376 - val_loss: 0.1816\n",
      "Epoch 8/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.2020\n",
      "Epoch 00008: val_loss improved from 0.18159 to 0.16880, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2012 - val_loss: 0.1688\n",
      "Epoch 9/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.1793\n",
      "Epoch 00009: val_loss improved from 0.16880 to 0.16024, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1783 - val_loss: 0.1602\n",
      "Epoch 10/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.1634\n",
      "Epoch 00010: val_loss improved from 0.16024 to 0.15345, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1634 - val_loss: 0.1535\n",
      "Epoch 11/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 0.1530\n",
      "Epoch 00011: val_loss improved from 0.15345 to 0.14851, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1538 - val_loss: 0.1485\n",
      "Epoch 12/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1480\n",
      "Epoch 00012: val_loss improved from 0.14851 to 0.14397, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1461 - val_loss: 0.1440\n",
      "Epoch 13/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1426\n",
      "Epoch 00013: val_loss improved from 0.14397 to 0.14093, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1411 - val_loss: 0.1409\n",
      "Epoch 14/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1385\n",
      "Epoch 00014: val_loss improved from 0.14093 to 0.13829, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1377 - val_loss: 0.1383\n",
      "Epoch 15/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1355\n",
      "Epoch 00015: val_loss improved from 0.13829 to 0.13696, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1351 - val_loss: 0.1370\n",
      "Epoch 16/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1319\n",
      "Epoch 00016: val_loss improved from 0.13696 to 0.13509, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1330 - val_loss: 0.1351\n",
      "Epoch 17/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1321\n",
      "Epoch 00017: val_loss improved from 0.13509 to 0.13381, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1314 - val_loss: 0.1338\n",
      "Epoch 18/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1289\n",
      "Epoch 00018: val_loss improved from 0.13381 to 0.13343, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1302 - val_loss: 0.1334\n",
      "Epoch 19/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00019: val_loss improved from 0.13343 to 0.13183, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1292 - val_loss: 0.1318\n",
      "Epoch 20/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1299\n",
      "Epoch 00020: val_loss improved from 0.13183 to 0.13126, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1284 - val_loss: 0.1313\n",
      "Epoch 21/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00021: val_loss improved from 0.13126 to 0.13036, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1275 - val_loss: 0.1304\n",
      "Epoch 22/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1280\n",
      "Epoch 00022: val_loss improved from 0.13036 to 0.13024, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1271 - val_loss: 0.1302\n",
      "Epoch 23/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00023: val_loss improved from 0.13024 to 0.12944, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1263 - val_loss: 0.1294\n",
      "Epoch 24/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00024: val_loss improved from 0.12944 to 0.12899, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1257 - val_loss: 0.1290\n",
      "Epoch 25/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00025: val_loss improved from 0.12899 to 0.12874, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 0.1287\n",
      "Epoch 26/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1246\n",
      "Epoch 00026: val_loss improved from 0.12874 to 0.12857, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1250 - val_loss: 0.1286\n",
      "Epoch 27/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00027: val_loss improved from 0.12857 to 0.12839, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1251 - val_loss: 0.1284\n",
      "Epoch 28/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00028: val_loss improved from 0.12839 to 0.12790, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1245 - val_loss: 0.1279\n",
      "Epoch 29/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00029: val_loss improved from 0.12790 to 0.12770, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 0.1277\n",
      "Epoch 30/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1251\n",
      "Epoch 00030: val_loss improved from 0.12770 to 0.12755, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1275\n",
      "Epoch 31/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00031: val_loss did not improve from 0.12755\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1276\n",
      "Epoch 32/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1227\n",
      "Epoch 00032: val_loss improved from 0.12755 to 0.12710, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1232 - val_loss: 0.1271\n",
      "Epoch 33/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1232\n",
      "Epoch 00033: val_loss improved from 0.12710 to 0.12661, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1233 - val_loss: 0.1266\n",
      "Epoch 34/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1217\n",
      "Epoch 00034: val_loss improved from 0.12661 to 0.12633, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1229 - val_loss: 0.1263\n",
      "Epoch 35/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1221\n",
      "Epoch 00035: val_loss did not improve from 0.12633\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1231 - val_loss: 0.1268\n",
      "Epoch 36/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1217\n",
      "Epoch 00036: val_loss improved from 0.12633 to 0.12631, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1227 - val_loss: 0.1263\n",
      "Epoch 37/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00037: val_loss did not improve from 0.12631\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1230 - val_loss: 0.1274\n",
      "Epoch 38/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00038: val_loss did not improve from 0.12631\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1227 - val_loss: 0.1276\n",
      " ###9 fold : val acc1 0.583, acc3 0.963, mae 0.231###\n",
      "acc10.583_acc30.964\n",
      "random search 5/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/154 [===========================>..] - ETA: 0s - loss: 11.9084\n",
      "Epoch 00001: val_loss improved from inf to 2.97762, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 11.5580 - val_loss: 2.9776\n",
      "Epoch 2/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.8272\n",
      "Epoch 00002: val_loss improved from 2.97762 to 0.70460, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.7474 - val_loss: 0.7046\n",
      "Epoch 3/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.4540\n",
      "Epoch 00003: val_loss improved from 0.70460 to 0.32775, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.6674 - val_loss: 0.3277\n",
      "Epoch 4/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.3753\n",
      "Epoch 00004: val_loss improved from 0.32775 to 0.22833, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3714 - val_loss: 0.2283\n",
      "Epoch 5/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.2629\n",
      "Epoch 00005: val_loss improved from 0.22833 to 0.19548, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2628 - val_loss: 0.1955\n",
      "Epoch 6/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.2132\n",
      "Epoch 00006: val_loss improved from 0.19548 to 0.17848, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2116 - val_loss: 0.1785\n",
      "Epoch 7/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1858\n",
      "Epoch 00007: val_loss improved from 0.17848 to 0.16643, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1819 - val_loss: 0.1664\n",
      "Epoch 8/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1644\n",
      "Epoch 00008: val_loss improved from 0.16643 to 0.15793, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1620 - val_loss: 0.1579\n",
      "Epoch 9/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1490\n",
      "Epoch 00009: val_loss improved from 0.15793 to 0.14971, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1474 - val_loss: 0.1497\n",
      "Epoch 10/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1415\n",
      "Epoch 00010: val_loss improved from 0.14971 to 0.14436, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1412 - val_loss: 0.1444\n",
      "Epoch 11/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1348\n",
      "Epoch 00011: val_loss improved from 0.14436 to 0.14082, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1346 - val_loss: 0.1408\n",
      "Epoch 12/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1319\n",
      "Epoch 00012: val_loss improved from 0.14082 to 0.13826, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1319 - val_loss: 0.1383\n",
      "Epoch 13/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1291\n",
      "Epoch 00013: val_loss improved from 0.13826 to 0.13618, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1294 - val_loss: 0.1362\n",
      "Epoch 14/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00014: val_loss improved from 0.13618 to 0.13422, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1281 - val_loss: 0.1342\n",
      "Epoch 15/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1270\n",
      "Epoch 00015: val_loss improved from 0.13422 to 0.13290, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1270 - val_loss: 0.1329\n",
      "Epoch 16/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00016: val_loss improved from 0.13290 to 0.13160, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1260 - val_loss: 0.1316\n",
      "Epoch 17/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00017: val_loss improved from 0.13160 to 0.13123, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 0.1312\n",
      "Epoch 18/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1246\n",
      "Epoch 00018: val_loss improved from 0.13123 to 0.13041, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1245 - val_loss: 0.1304\n",
      "Epoch 19/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00019: val_loss did not improve from 0.13041\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1243 - val_loss: 0.1318\n",
      "Epoch 20/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00020: val_loss improved from 0.13041 to 0.12983, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1239 - val_loss: 0.1298\n",
      "Epoch 21/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1235\n",
      "Epoch 00021: val_loss did not improve from 0.12983\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 0.1313\n",
      "Epoch 22/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.1224\n",
      "Epoch 00022: val_loss did not improve from 0.12983\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1230 - val_loss: 0.1301\n",
      " ###0 fold : val acc1 0.575, acc3 0.964, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/154 [============================>.] - ETA: 0s - loss: 11.6896\n",
      "Epoch 00001: val_loss improved from inf to 2.96686, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.5673 - val_loss: 2.9669\n",
      "Epoch 2/100\n",
      "134/154 [=========================>....] - ETA: 0s - loss: 1.9061\n",
      "Epoch 00002: val_loss improved from 2.96686 to 0.70959, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.7607 - val_loss: 0.7096\n",
      "Epoch 3/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.4672\n",
      "Epoch 00003: val_loss improved from 0.70959 to 0.33096, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.6823 - val_loss: 0.3310\n",
      "Epoch 4/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.3935\n",
      "Epoch 00004: val_loss improved from 0.33096 to 0.22928, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3843 - val_loss: 0.2293\n",
      "Epoch 5/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.2822\n",
      "Epoch 00005: val_loss improved from 0.22928 to 0.19429, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2730 - val_loss: 0.1943\n",
      "Epoch 6/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.2194\n",
      "Epoch 00006: val_loss improved from 0.19429 to 0.17621, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2193 - val_loss: 0.1762\n",
      "Epoch 7/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00007: val_loss improved from 0.17621 to 0.16376, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1871 - val_loss: 0.1638\n",
      "Epoch 8/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1649\n",
      "Epoch 00008: val_loss improved from 0.16376 to 0.15518, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1647 - val_loss: 0.1552\n",
      "Epoch 9/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1484\n",
      "Epoch 00009: val_loss improved from 0.15518 to 0.14704, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1485 - val_loss: 0.1470\n",
      "Epoch 10/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1438\n",
      "Epoch 00010: val_loss improved from 0.14704 to 0.14258, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1437 - val_loss: 0.1426\n",
      "Epoch 11/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1362\n",
      "Epoch 00011: val_loss improved from 0.14258 to 0.13963, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1355 - val_loss: 0.1396\n",
      "Epoch 12/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1321\n",
      "Epoch 00012: val_loss improved from 0.13963 to 0.13691, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1327 - val_loss: 0.1369\n",
      "Epoch 13/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1299\n",
      "Epoch 00013: val_loss improved from 0.13691 to 0.13524, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1299 - val_loss: 0.1352\n",
      "Epoch 14/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1294\n",
      "Epoch 00014: val_loss improved from 0.13524 to 0.13362, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1288 - val_loss: 0.1336\n",
      "Epoch 15/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1279\n",
      "Epoch 00015: val_loss improved from 0.13362 to 0.13229, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1278 - val_loss: 0.1323\n",
      "Epoch 16/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1266\n",
      "Epoch 00016: val_loss improved from 0.13229 to 0.13139, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1266 - val_loss: 0.1314\n",
      "Epoch 17/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1259\n",
      "Epoch 00017: val_loss improved from 0.13139 to 0.13077, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1259 - val_loss: 0.1308\n",
      "Epoch 18/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00018: val_loss improved from 0.13077 to 0.13018, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1252 - val_loss: 0.1302\n",
      "Epoch 19/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00019: val_loss did not improve from 0.13018\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1249 - val_loss: 0.1309\n",
      "Epoch 20/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00020: val_loss did not improve from 0.13018\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1246 - val_loss: 0.1304\n",
      " ###1 fold : val acc1 0.577, acc3 0.962, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/154 [==========================>...] - ETA: 0s - loss: 12.3405\n",
      "Epoch 00001: val_loss improved from inf to 2.97901, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.5602 - val_loss: 2.9790\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 1.7675\n",
      "Epoch 00002: val_loss improved from 2.97901 to 0.71033, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.7675 - val_loss: 0.7103\n",
      "Epoch 3/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4660\n",
      "Epoch 00003: val_loss improved from 0.71033 to 0.32990, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.6834 - val_loss: 0.3299\n",
      "Epoch 4/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.3879\n",
      "Epoch 00004: val_loss improved from 0.32990 to 0.22880, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3828 - val_loss: 0.2288\n",
      "Epoch 5/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.2826\n",
      "Epoch 00005: val_loss improved from 0.22880 to 0.19475, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2720 - val_loss: 0.1948\n",
      "Epoch 6/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.2229\n",
      "Epoch 00006: val_loss improved from 0.19475 to 0.17754, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2184 - val_loss: 0.1775\n",
      "Epoch 7/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1892\n",
      "Epoch 00007: val_loss improved from 0.17754 to 0.16482, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1862 - val_loss: 0.1648\n",
      "Epoch 8/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1496\n",
      "Epoch 00008: val_loss improved from 0.16482 to 0.15642, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1650 - val_loss: 0.1564\n",
      "Epoch 9/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1503\n",
      "Epoch 00009: val_loss improved from 0.15642 to 0.14815, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1489 - val_loss: 0.1481\n",
      "Epoch 10/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1367\n",
      "Epoch 00010: val_loss improved from 0.14815 to 0.14347, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1432 - val_loss: 0.1435\n",
      "Epoch 11/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1354\n",
      "Epoch 00011: val_loss improved from 0.14347 to 0.14024, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1354 - val_loss: 0.1402\n",
      "Epoch 12/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1324\n",
      "Epoch 00012: val_loss improved from 0.14024 to 0.13748, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1327 - val_loss: 0.1375\n",
      "Epoch 13/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1305\n",
      "Epoch 00013: val_loss improved from 0.13748 to 0.13566, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1301 - val_loss: 0.1357\n",
      "Epoch 14/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1292\n",
      "Epoch 00014: val_loss improved from 0.13566 to 0.13455, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1288 - val_loss: 0.1345\n",
      "Epoch 15/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00015: val_loss improved from 0.13455 to 0.13244, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1278 - val_loss: 0.1324\n",
      "Epoch 16/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1266\n",
      "Epoch 00016: val_loss improved from 0.13244 to 0.13142, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1266 - val_loss: 0.1314\n",
      "Epoch 17/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1261\n",
      "Epoch 00017: val_loss improved from 0.13142 to 0.13130, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1260 - val_loss: 0.1313\n",
      "Epoch 18/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00018: val_loss improved from 0.13130 to 0.13069, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1254 - val_loss: 0.1307\n",
      "Epoch 19/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00019: val_loss improved from 0.13069 to 0.13037, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1250 - val_loss: 0.1304\n",
      "Epoch 20/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00020: val_loss improved from 0.13037 to 0.13034, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1246 - val_loss: 0.1303\n",
      "Epoch 21/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00021: val_loss did not improve from 0.13034\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1242 - val_loss: 0.1304\n",
      "Epoch 22/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00022: val_loss improved from 0.13034 to 0.12983, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1237 - val_loss: 0.1298\n",
      "Epoch 23/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00023: val_loss improved from 0.12983 to 0.12822, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1236 - val_loss: 0.1282\n",
      "Epoch 24/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00024: val_loss improved from 0.12822 to 0.12820, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1234 - val_loss: 0.1282\n",
      "Epoch 25/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1226\n",
      "Epoch 00025: val_loss improved from 0.12820 to 0.12774, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1228 - val_loss: 0.1277\n",
      "Epoch 26/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1222\n",
      "Epoch 00026: val_loss did not improve from 0.12774\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1232 - val_loss: 0.1298\n",
      "Epoch 27/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1234\n",
      "Epoch 00027: val_loss improved from 0.12774 to 0.12691, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1233 - val_loss: 0.1269\n",
      "Epoch 28/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00028: val_loss did not improve from 0.12691\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1255 - val_loss: 0.1275\n",
      "Epoch 29/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1251\n",
      "Epoch 00029: val_loss did not improve from 0.12691\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1249 - val_loss: 0.1275\n",
      " ###2 fold : val acc1 0.585, acc3 0.963, mae 0.229###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/154 [===========================>..] - ETA: 0s - loss: 11.9167\n",
      "Epoch 00001: val_loss improved from inf to 2.97873, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.5652 - val_loss: 2.9787\n",
      "Epoch 2/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 1.7923\n",
      "Epoch 00002: val_loss improved from 2.97873 to 0.70938, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.7620 - val_loss: 0.7094\n",
      "Epoch 3/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.4715\n",
      "Epoch 00003: val_loss improved from 0.70938 to 0.33052, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6793 - val_loss: 0.3305\n",
      "Epoch 4/100\n",
      "135/154 [=========================>....] - ETA: 0s - loss: 0.4020\n",
      "Epoch 00004: val_loss improved from 0.33052 to 0.22872, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3808 - val_loss: 0.2287\n",
      "Epoch 5/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.2816\n",
      "Epoch 00005: val_loss improved from 0.22872 to 0.19458, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2699 - val_loss: 0.1946\n",
      "Epoch 6/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.2233\n",
      "Epoch 00006: val_loss improved from 0.19458 to 0.17725, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2180 - val_loss: 0.1772\n",
      "Epoch 7/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1887\n",
      "Epoch 00007: val_loss improved from 0.17725 to 0.16473, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1867 - val_loss: 0.1647\n",
      "Epoch 8/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1672\n",
      "Epoch 00008: val_loss improved from 0.16473 to 0.15661, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1649 - val_loss: 0.1566\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1497\n",
      "Epoch 00009: val_loss improved from 0.15661 to 0.14827, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1497 - val_loss: 0.1483\n",
      "Epoch 10/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1444\n",
      "Epoch 00010: val_loss improved from 0.14827 to 0.14345, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1444 - val_loss: 0.1435\n",
      "Epoch 11/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1379\n",
      "Epoch 00011: val_loss improved from 0.14345 to 0.14026, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1366 - val_loss: 0.1403\n",
      "Epoch 12/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1338\n",
      "Epoch 00012: val_loss improved from 0.14026 to 0.13705, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1336 - val_loss: 0.1370\n",
      "Epoch 13/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1311\n",
      "Epoch 00013: val_loss improved from 0.13705 to 0.13556, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1310 - val_loss: 0.1356\n",
      "Epoch 14/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1310\n",
      "Epoch 00014: val_loss improved from 0.13556 to 0.13402, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1297 - val_loss: 0.1340\n",
      "Epoch 15/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1291\n",
      "Epoch 00015: val_loss improved from 0.13402 to 0.13244, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1286 - val_loss: 0.1324\n",
      "Epoch 16/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1276\n",
      "Epoch 00016: val_loss improved from 0.13244 to 0.13153, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1274 - val_loss: 0.1315\n",
      "Epoch 17/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1267\n",
      "Epoch 00017: val_loss improved from 0.13153 to 0.13092, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1269 - val_loss: 0.1309\n",
      "Epoch 18/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00018: val_loss improved from 0.13092 to 0.13069, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1263 - val_loss: 0.1307\n",
      "Epoch 19/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00019: val_loss improved from 0.13069 to 0.13026, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1258 - val_loss: 0.1303\n",
      "Epoch 20/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1269\n",
      "Epoch 00020: val_loss did not improve from 0.13026\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1254 - val_loss: 0.1308\n",
      "Epoch 21/100\n",
      "135/154 [=========================>....] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00021: val_loss improved from 0.13026 to 0.12909, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1251 - val_loss: 0.1291\n",
      "Epoch 22/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00022: val_loss did not improve from 0.12909\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1245 - val_loss: 0.1297\n",
      "Epoch 23/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00023: val_loss improved from 0.12909 to 0.12830, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1244 - val_loss: 0.1283\n",
      "Epoch 24/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00024: val_loss improved from 0.12830 to 0.12797, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1242 - val_loss: 0.1280\n",
      "Epoch 25/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00025: val_loss did not improve from 0.12797\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 0.1282\n",
      "Epoch 26/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00026: val_loss did not improve from 0.12797\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1242 - val_loss: 0.1303\n",
      " ###3 fold : val acc1 0.573, acc3 0.966, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/154 [===========================>..] - ETA: 0s - loss: 11.7680\n",
      "Epoch 00001: val_loss improved from inf to 2.98250, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.4724 - val_loss: 2.9825\n",
      "Epoch 2/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 1.7093\n",
      "Epoch 00002: val_loss improved from 2.98250 to 0.69347, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.6949 - val_loss: 0.6935\n",
      "Epoch 3/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.6225\n",
      "Epoch 00003: val_loss improved from 0.69347 to 0.31815, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.6063 - val_loss: 0.3182\n",
      "Epoch 4/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.3395\n",
      "Epoch 00004: val_loss improved from 0.31815 to 0.22813, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3393 - val_loss: 0.2281\n",
      "Epoch 5/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2469\n",
      "Epoch 00005: val_loss improved from 0.22813 to 0.19806, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2466 - val_loss: 0.1981\n",
      "Epoch 6/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.2024\n",
      "Epoch 00006: val_loss improved from 0.19806 to 0.18024, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2025 - val_loss: 0.1802\n",
      "Epoch 7/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1780\n",
      "Epoch 00007: val_loss improved from 0.18024 to 0.16655, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1776 - val_loss: 0.1666\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1590\n",
      "Epoch 00008: val_loss improved from 0.16655 to 0.15641, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1590 - val_loss: 0.1564\n",
      "Epoch 9/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1494\n",
      "Epoch 00009: val_loss improved from 0.15641 to 0.14963, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1493 - val_loss: 0.1496\n",
      "Epoch 10/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1402\n",
      "Epoch 00010: val_loss improved from 0.14963 to 0.14404, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1408 - val_loss: 0.1440\n",
      "Epoch 11/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1358\n",
      "Epoch 00011: val_loss improved from 0.14404 to 0.13987, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1358 - val_loss: 0.1399\n",
      "Epoch 12/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1337\n",
      "Epoch 00012: val_loss improved from 0.13987 to 0.13689, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1331 - val_loss: 0.1369\n",
      "Epoch 13/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1311\n",
      "Epoch 00013: val_loss improved from 0.13689 to 0.13495, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1307 - val_loss: 0.1349\n",
      "Epoch 14/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1297\n",
      "Epoch 00014: val_loss improved from 0.13495 to 0.13409, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1293 - val_loss: 0.1341\n",
      "Epoch 15/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1283\n",
      "Epoch 00015: val_loss improved from 0.13409 to 0.13253, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1282 - val_loss: 0.1325\n",
      "Epoch 16/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1275\n",
      "Epoch 00016: val_loss improved from 0.13253 to 0.13209, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1272 - val_loss: 0.1321\n",
      "Epoch 17/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1268\n",
      "Epoch 00017: val_loss improved from 0.13209 to 0.13133, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1268 - val_loss: 0.1313\n",
      "Epoch 18/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00018: val_loss improved from 0.13133 to 0.13030, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1262 - val_loss: 0.1303\n",
      "Epoch 19/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00019: val_loss improved from 0.13030 to 0.12963, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1258 - val_loss: 0.1296\n",
      "Epoch 20/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00020: val_loss improved from 0.12963 to 0.12944, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1250 - val_loss: 0.1294\n",
      "Epoch 21/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00021: val_loss improved from 0.12944 to 0.12832, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1247 - val_loss: 0.1283\n",
      "Epoch 22/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00022: val_loss did not improve from 0.12832\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1243 - val_loss: 0.1299\n",
      "Epoch 23/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00023: val_loss improved from 0.12832 to 0.12829, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1241 - val_loss: 0.1283\n",
      "Epoch 24/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00024: val_loss improved from 0.12829 to 0.12751, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1237 - val_loss: 0.1275\n",
      "Epoch 25/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1235\n",
      "Epoch 00025: val_loss did not improve from 0.12751\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1233 - val_loss: 0.1277\n",
      "Epoch 26/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00026: val_loss did not improve from 0.12751\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1236 - val_loss: 0.1279\n",
      " ###4 fold : val acc1 0.592, acc3 0.965, mae 0.225###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/154 [===========================>..] - ETA: 0s - loss: 11.6895\n",
      "Epoch 00001: val_loss improved from inf to 2.74482, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.1650 - val_loss: 2.7448\n",
      "Epoch 2/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 1.3445\n",
      "Epoch 00002: val_loss improved from 2.74482 to 0.63452, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.3435 - val_loss: 0.6345\n",
      "Epoch 3/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.4078\n",
      "Epoch 00003: val_loss improved from 0.63452 to 0.28355, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3932 - val_loss: 0.2836\n",
      "Epoch 4/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2179\n",
      "Epoch 00004: val_loss improved from 0.28355 to 0.21049, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2175 - val_loss: 0.2105\n",
      "Epoch 5/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1769\n",
      "Epoch 00005: val_loss improved from 0.21049 to 0.18465, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1769 - val_loss: 0.1846\n",
      "Epoch 6/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1606\n",
      "Epoch 00006: val_loss improved from 0.18465 to 0.16985, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1607 - val_loss: 0.1699\n",
      "Epoch 7/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1501\n",
      "Epoch 00007: val_loss improved from 0.16985 to 0.15915, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1507 - val_loss: 0.1592\n",
      "Epoch 8/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1435\n",
      "Epoch 00008: val_loss improved from 0.15915 to 0.15214, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1439 - val_loss: 0.1521\n",
      "Epoch 9/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1404\n",
      "Epoch 00009: val_loss improved from 0.15214 to 0.14718, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1395 - val_loss: 0.1472\n",
      "Epoch 10/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1367\n",
      "Epoch 00010: val_loss improved from 0.14718 to 0.14344, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1366 - val_loss: 0.1434\n",
      "Epoch 11/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1344\n",
      "Epoch 00011: val_loss improved from 0.14344 to 0.14085, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1344 - val_loss: 0.1409\n",
      "Epoch 12/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1324\n",
      "Epoch 00012: val_loss improved from 0.14085 to 0.13876, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1326 - val_loss: 0.1388\n",
      "Epoch 13/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1313\n",
      "Epoch 00013: val_loss improved from 0.13876 to 0.13669, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1312 - val_loss: 0.1367\n",
      "Epoch 14/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1302\n",
      "Epoch 00014: val_loss improved from 0.13669 to 0.13639, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1302 - val_loss: 0.1364\n",
      "Epoch 15/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1286\n",
      "Epoch 00015: val_loss improved from 0.13639 to 0.13424, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1290 - val_loss: 0.1342\n",
      "Epoch 16/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1286\n",
      "Epoch 00016: val_loss improved from 0.13424 to 0.13377, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1280 - val_loss: 0.1338\n",
      "Epoch 17/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1275\n",
      "Epoch 00017: val_loss improved from 0.13377 to 0.13209, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1275 - val_loss: 0.1321\n",
      "Epoch 18/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1270\n",
      "Epoch 00018: val_loss improved from 0.13209 to 0.13119, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1267 - val_loss: 0.1312\n",
      "Epoch 19/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00019: val_loss improved from 0.13119 to 0.13043, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1263 - val_loss: 0.1304\n",
      "Epoch 20/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00020: val_loss did not improve from 0.13043\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1255 - val_loss: 0.1310\n",
      "Epoch 21/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00021: val_loss improved from 0.13043 to 0.12898, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1251 - val_loss: 0.1290\n",
      "Epoch 22/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00022: val_loss did not improve from 0.12898\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1248 - val_loss: 0.1309\n",
      "Epoch 23/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00023: val_loss improved from 0.12898 to 0.12861, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1246 - val_loss: 0.1286\n",
      "Epoch 24/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00024: val_loss improved from 0.12861 to 0.12778, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1240 - val_loss: 0.1278\n",
      "Epoch 25/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00025: val_loss did not improve from 0.12778\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1238 - val_loss: 0.1280\n",
      "Epoch 26/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00026: val_loss did not improve from 0.12778\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1239 - val_loss: 0.1284\n",
      " ###5 fold : val acc1 0.589, acc3 0.967, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/154 [============================>.] - ETA: 0s - loss: 11.4712\n",
      "Epoch 00001: val_loss improved from inf to 2.76609, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 11.2917 - val_loss: 2.7661\n",
      "Epoch 2/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 1.4003\n",
      "Epoch 00002: val_loss improved from 2.76609 to 0.63887, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.5863 - val_loss: 0.6389\n",
      "Epoch 3/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.5693\n",
      "Epoch 00003: val_loss improved from 0.63887 to 0.30019, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5523 - val_loss: 0.3002\n",
      "Epoch 4/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.3184\n",
      "Epoch 00004: val_loss improved from 0.30019 to 0.22176, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3127 - val_loss: 0.2218\n",
      "Epoch 5/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.2362\n",
      "Epoch 00005: val_loss improved from 0.22176 to 0.19284, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2319 - val_loss: 0.1928\n",
      "Epoch 6/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1918\n",
      "Epoch 00006: val_loss improved from 0.19284 to 0.17590, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1913 - val_loss: 0.1759\n",
      "Epoch 7/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1741\n",
      "Epoch 00007: val_loss improved from 0.17590 to 0.16446, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1736 - val_loss: 0.1645\n",
      "Epoch 8/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1549\n",
      "Epoch 00008: val_loss improved from 0.16446 to 0.15440, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1536 - val_loss: 0.1544\n",
      "Epoch 9/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1462\n",
      "Epoch 00009: val_loss improved from 0.15440 to 0.14810, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1453 - val_loss: 0.1481\n",
      "Epoch 10/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1375\n",
      "Epoch 00010: val_loss improved from 0.14810 to 0.14234, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1382 - val_loss: 0.1423\n",
      "Epoch 11/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1332\n",
      "Epoch 00011: val_loss improved from 0.14234 to 0.13937, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1346 - val_loss: 0.1394\n",
      "Epoch 12/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1320\n",
      "Epoch 00012: val_loss improved from 0.13937 to 0.13598, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1316 - val_loss: 0.1360\n",
      "Epoch 13/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1312\n",
      "Epoch 00013: val_loss improved from 0.13598 to 0.13457, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1304 - val_loss: 0.1346\n",
      "Epoch 14/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00014: val_loss improved from 0.13457 to 0.13331, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1287 - val_loss: 0.1333\n",
      "Epoch 15/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1278\n",
      "Epoch 00015: val_loss improved from 0.13331 to 0.13286, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1277 - val_loss: 0.1329\n",
      "Epoch 16/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1271\n",
      "Epoch 00016: val_loss improved from 0.13286 to 0.13190, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1268 - val_loss: 0.1319\n",
      "Epoch 17/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00017: val_loss improved from 0.13190 to 0.13102, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1263 - val_loss: 0.1310\n",
      "Epoch 18/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00018: val_loss improved from 0.13102 to 0.13029, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1255 - val_loss: 0.1303\n",
      "Epoch 19/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00019: val_loss improved from 0.13029 to 0.12952, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 0.1295\n",
      "Epoch 20/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00020: val_loss did not improve from 0.12952\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1245 - val_loss: 0.1298\n",
      "Epoch 21/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00021: val_loss improved from 0.12952 to 0.12834, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1242 - val_loss: 0.1283\n",
      "Epoch 22/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00022: val_loss did not improve from 0.12834\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1238 - val_loss: 0.1306\n",
      "Epoch 23/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00023: val_loss did not improve from 0.12834\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1287\n",
      " ###6 fold : val acc1 0.577, acc3 0.962, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/154 [============================>.] - ETA: 0s - loss: 11.4255\n",
      "Epoch 00001: val_loss improved from inf to 2.75367, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.3072 - val_loss: 2.7537\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 1.5957\n",
      "Epoch 00002: val_loss improved from 2.75367 to 0.64698, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.5957 - val_loss: 0.6470\n",
      "Epoch 3/100\n",
      "134/154 [=========================>....] - ETA: 0s - loss: 0.6108\n",
      "Epoch 00003: val_loss improved from 0.64698 to 0.30390, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5711 - val_loss: 0.3039\n",
      "Epoch 4/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3305\n",
      "Epoch 00004: val_loss improved from 0.30390 to 0.22215, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3263 - val_loss: 0.2222\n",
      "Epoch 5/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.2449\n",
      "Epoch 00005: val_loss improved from 0.22215 to 0.19173, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2402 - val_loss: 0.1917\n",
      "Epoch 6/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1963\n",
      "Epoch 00006: val_loss improved from 0.19173 to 0.17444, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1963 - val_loss: 0.1744\n",
      "Epoch 7/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.1769\n",
      "Epoch 00007: val_loss improved from 0.17444 to 0.16280, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1766 - val_loss: 0.1628\n",
      "Epoch 8/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1546\n",
      "Epoch 00008: val_loss improved from 0.16280 to 0.15335, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1547 - val_loss: 0.1533\n",
      "Epoch 9/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1475\n",
      "Epoch 00009: val_loss improved from 0.15335 to 0.14783, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1462 - val_loss: 0.1478\n",
      "Epoch 10/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1380\n",
      "Epoch 00010: val_loss improved from 0.14783 to 0.14239, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1384 - val_loss: 0.1424\n",
      "Epoch 11/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1339\n",
      "Epoch 00011: val_loss improved from 0.14239 to 0.13937, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1348 - val_loss: 0.1394\n",
      "Epoch 12/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1319\n",
      "Epoch 00012: val_loss improved from 0.13937 to 0.13582, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1318 - val_loss: 0.1358\n",
      "Epoch 13/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1317\n",
      "Epoch 00013: val_loss improved from 0.13582 to 0.13453, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1305 - val_loss: 0.1345\n",
      "Epoch 14/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1290\n",
      "Epoch 00014: val_loss improved from 0.13453 to 0.13335, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1288 - val_loss: 0.1333\n",
      "Epoch 15/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1280\n",
      "Epoch 00015: val_loss improved from 0.13335 to 0.13280, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1280 - val_loss: 0.1328\n",
      "Epoch 16/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00016: val_loss improved from 0.13280 to 0.13169, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1270 - val_loss: 0.1317\n",
      "Epoch 17/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00017: val_loss improved from 0.13169 to 0.13066, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1265 - val_loss: 0.1307\n",
      "Epoch 18/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00018: val_loss improved from 0.13066 to 0.13056, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1257 - val_loss: 0.1306\n",
      "Epoch 19/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00019: val_loss improved from 0.13056 to 0.12969, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1255 - val_loss: 0.1297\n",
      "Epoch 20/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00020: val_loss did not improve from 0.12969\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1249 - val_loss: 0.1301\n",
      "Epoch 21/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00021: val_loss improved from 0.12969 to 0.12841, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1245 - val_loss: 0.1284\n",
      "Epoch 22/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00022: val_loss did not improve from 0.12841\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1245 - val_loss: 0.1300\n",
      "Epoch 23/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00023: val_loss improved from 0.12841 to 0.12837, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1241 - val_loss: 0.1284\n",
      "Epoch 24/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00024: val_loss did not improve from 0.12837\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1247 - val_loss: 0.1291\n",
      "Epoch 25/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1251\n",
      "Epoch 00025: val_loss improved from 0.12837 to 0.12778, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1262 - val_loss: 0.1278\n",
      "Epoch 26/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00026: val_loss did not improve from 0.12778\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1258 - val_loss: 0.1302\n",
      "Epoch 27/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00027: val_loss improved from 0.12778 to 0.12715, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1248 - val_loss: 0.1272\n",
      "Epoch 28/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00028: val_loss did not improve from 0.12715\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 0.1280\n",
      "Epoch 29/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00029: val_loss improved from 0.12715 to 0.12691, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1254 - val_loss: 0.1269\n",
      "Epoch 30/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00030: val_loss did not improve from 0.12691\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1251 - val_loss: 0.1272\n",
      "Epoch 31/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00031: val_loss did not improve from 0.12691\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 0.1273\n",
      " ###7 fold : val acc1 0.585, acc3 0.964, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/154 [==========================>...] - ETA: 0s - loss: 12.0341\n",
      "Epoch 00001: val_loss improved from inf to 2.79849, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.3182 - val_loss: 2.7985\n",
      "Epoch 2/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 1.3825\n",
      "Epoch 00002: val_loss improved from 2.79849 to 0.65950, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.5927 - val_loss: 0.6595\n",
      "Epoch 3/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.5743\n",
      "Epoch 00003: val_loss improved from 0.65950 to 0.30077, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5669 - val_loss: 0.3008\n",
      "Epoch 4/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.3241\n",
      "Epoch 00004: val_loss improved from 0.30077 to 0.21380, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3234 - val_loss: 0.2138\n",
      "Epoch 5/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.2455\n",
      "Epoch 00005: val_loss improved from 0.21380 to 0.18310, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2381 - val_loss: 0.1831\n",
      "Epoch 6/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1973\n",
      "Epoch 00006: val_loss improved from 0.18310 to 0.16694, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1950 - val_loss: 0.1669\n",
      "Epoch 7/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1756\n",
      "Epoch 00007: val_loss improved from 0.16694 to 0.15661, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1756 - val_loss: 0.1566\n",
      "Epoch 8/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1547\n",
      "Epoch 00008: val_loss improved from 0.15661 to 0.14852, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1545 - val_loss: 0.1485\n",
      "Epoch 9/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1467\n",
      "Epoch 00009: val_loss improved from 0.14852 to 0.14369, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1456 - val_loss: 0.1437\n",
      "Epoch 10/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1383\n",
      "Epoch 00010: val_loss improved from 0.14369 to 0.13947, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1382 - val_loss: 0.1395\n",
      "Epoch 11/100\n",
      "135/154 [=========================>....] - ETA: 0s - loss: 0.1342\n",
      "Epoch 00011: val_loss improved from 0.13947 to 0.13632, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1345 - val_loss: 0.1363\n",
      "Epoch 12/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1318\n",
      "Epoch 00012: val_loss improved from 0.13632 to 0.13391, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1315 - val_loss: 0.1339\n",
      "Epoch 13/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1301\n",
      "Epoch 00013: val_loss improved from 0.13391 to 0.13236, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1301 - val_loss: 0.1324\n",
      "Epoch 14/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00014: val_loss improved from 0.13236 to 0.13155, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1284 - val_loss: 0.1316\n",
      "Epoch 15/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1274\n",
      "Epoch 00015: val_loss improved from 0.13155 to 0.13086, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1276 - val_loss: 0.1309\n",
      "Epoch 16/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1268\n",
      "Epoch 00016: val_loss improved from 0.13086 to 0.12939, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1266 - val_loss: 0.1294\n",
      "Epoch 17/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00017: val_loss improved from 0.12939 to 0.12864, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1261 - val_loss: 0.1286\n",
      "Epoch 18/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00018: val_loss improved from 0.12864 to 0.12851, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1253 - val_loss: 0.1285\n",
      "Epoch 19/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00019: val_loss improved from 0.12851 to 0.12772, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1250 - val_loss: 0.1277\n",
      "Epoch 20/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00020: val_loss did not improve from 0.12772\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1245 - val_loss: 0.1279\n",
      "Epoch 21/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00021: val_loss improved from 0.12772 to 0.12720, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1241 - val_loss: 0.1272\n",
      "Epoch 22/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00022: val_loss did not improve from 0.12720\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1242 - val_loss: 0.1274\n",
      "Epoch 23/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00023: val_loss improved from 0.12720 to 0.12666, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1237 - val_loss: 0.1267\n",
      "Epoch 24/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00024: val_loss improved from 0.12666 to 0.12656, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1238 - val_loss: 0.1266\n",
      "Epoch 25/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00025: val_loss improved from 0.12656 to 0.12565, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1249 - val_loss: 0.1257\n",
      "Epoch 26/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00026: val_loss did not improve from 0.12565\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 0.1277\n",
      "Epoch 27/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1246\n",
      "Epoch 00027: val_loss did not improve from 0.12565\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1246 - val_loss: 0.1262\n",
      " ###8 fold : val acc1 0.580, acc3 0.961, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/154 [==========================>...] - ETA: 0s - loss: 12.0341\n",
      "Epoch 00001: val_loss improved from inf to 2.80512, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 11.3182 - val_loss: 2.8051\n",
      "Epoch 2/100\n",
      "135/154 [=========================>....] - ETA: 0s - loss: 1.3933\n",
      "Epoch 00002: val_loss improved from 2.80512 to 0.65927, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.5927 - val_loss: 0.6593\n",
      "Epoch 3/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.5762\n",
      "Epoch 00003: val_loss improved from 0.65927 to 0.29438, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5669 - val_loss: 0.2944\n",
      "Epoch 4/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.3272\n",
      "Epoch 00004: val_loss improved from 0.29438 to 0.20898, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3234 - val_loss: 0.2090\n",
      "Epoch 5/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.2381\n",
      "Epoch 00005: val_loss improved from 0.20898 to 0.18004, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2381 - val_loss: 0.1800\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1965\n",
      "Epoch 00006: val_loss improved from 0.18004 to 0.16514, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1950 - val_loss: 0.1651\n",
      "Epoch 7/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1758\n",
      "Epoch 00007: val_loss improved from 0.16514 to 0.15583, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1756 - val_loss: 0.1558\n",
      "Epoch 8/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1549\n",
      "Epoch 00008: val_loss improved from 0.15583 to 0.14825, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1545 - val_loss: 0.1483\n",
      "Epoch 9/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1457\n",
      "Epoch 00009: val_loss improved from 0.14825 to 0.14400, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1456 - val_loss: 0.1440\n",
      "Epoch 10/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1377\n",
      "Epoch 00010: val_loss improved from 0.14400 to 0.13964, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1382 - val_loss: 0.1396\n",
      "Epoch 11/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1334\n",
      "Epoch 00011: val_loss improved from 0.13964 to 0.13690, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1345 - val_loss: 0.1369\n",
      "Epoch 12/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1315\n",
      "Epoch 00012: val_loss improved from 0.13690 to 0.13418, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1315 - val_loss: 0.1342\n",
      "Epoch 13/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1309\n",
      "Epoch 00013: val_loss improved from 0.13418 to 0.13297, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1301 - val_loss: 0.1330\n",
      "Epoch 14/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00014: val_loss improved from 0.13297 to 0.13198, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1284 - val_loss: 0.1320\n",
      "Epoch 15/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1277\n",
      "Epoch 00015: val_loss improved from 0.13198 to 0.13134, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1276 - val_loss: 0.1313\n",
      "Epoch 16/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00016: val_loss improved from 0.13134 to 0.13051, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1266 - val_loss: 0.1305\n",
      "Epoch 17/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00017: val_loss improved from 0.13051 to 0.12945, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1261 - val_loss: 0.1295\n",
      "Epoch 18/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00018: val_loss improved from 0.12945 to 0.12941, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 0.1294\n",
      "Epoch 19/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00019: val_loss improved from 0.12941 to 0.12868, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1250 - val_loss: 0.1287\n",
      "Epoch 20/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00020: val_loss did not improve from 0.12868\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1245 - val_loss: 0.1293\n",
      "Epoch 21/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00021: val_loss improved from 0.12868 to 0.12759, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 0.1276\n",
      "Epoch 22/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00022: val_loss did not improve from 0.12759\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1242 - val_loss: 0.1286\n",
      "Epoch 23/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00023: val_loss improved from 0.12759 to 0.12732, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1273\n",
      "Epoch 24/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00024: val_loss did not improve from 0.12732\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1238 - val_loss: 0.1276\n",
      "Epoch 25/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00025: val_loss improved from 0.12732 to 0.12659, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1249 - val_loss: 0.1266\n",
      "Epoch 26/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00026: val_loss did not improve from 0.12659\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 0.1286\n",
      "Epoch 27/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00027: val_loss improved from 0.12659 to 0.12644, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1246 - val_loss: 0.1264\n",
      "Epoch 28/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1271\n",
      "Epoch 00028: val_loss did not improve from 0.12644\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1261 - val_loss: 0.1274\n",
      "Epoch 29/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1275\n",
      "Epoch 00029: val_loss improved from 0.12644 to 0.12613, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1275 - val_loss: 0.1261\n",
      "Epoch 30/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00030: val_loss did not improve from 0.12613\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1273 - val_loss: 0.1267\n",
      "Epoch 31/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00031: val_loss did not improve from 0.12613\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1257 - val_loss: 0.1265\n",
      " ###9 fold : val acc1 0.581, acc3 0.964, mae 0.231###\n",
      "acc10.582_acc30.964\n",
      "random search 6/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/154 [==========================>...] - ETA: 0s - loss: 14.5634\n",
      "Epoch 00001: val_loss improved from inf to 5.77980, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 13.8157 - val_loss: 5.7798\n",
      "Epoch 2/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 3.6397\n",
      "Epoch 00002: val_loss improved from 5.77980 to 1.26724, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 3.5429 - val_loss: 1.2672\n",
      "Epoch 3/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 1.1079\n",
      "Epoch 00003: val_loss improved from 1.26724 to 0.59965, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.5213 - val_loss: 0.5996\n",
      "Epoch 4/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 1.0746\n",
      "Epoch 00004: val_loss improved from 0.59965 to 0.36768, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.0715 - val_loss: 0.3677\n",
      "Epoch 5/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.6434\n",
      "Epoch 00005: val_loss improved from 0.36768 to 0.27471, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6393 - val_loss: 0.2747\n",
      "Epoch 6/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.7594\n",
      "Epoch 00006: val_loss improved from 0.27471 to 0.23320, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7452 - val_loss: 0.2332\n",
      "Epoch 7/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.5729\n",
      "Epoch 00007: val_loss improved from 0.23320 to 0.21021, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5697 - val_loss: 0.2102\n",
      "Epoch 8/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.6754\n",
      "Epoch 00008: val_loss improved from 0.21021 to 0.20184, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6648 - val_loss: 0.2018\n",
      "Epoch 9/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.4249\n",
      "Epoch 00009: val_loss improved from 0.20184 to 0.18620, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4239 - val_loss: 0.1862\n",
      "Epoch 10/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.4072\n",
      "Epoch 00010: val_loss improved from 0.18620 to 0.17942, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5654 - val_loss: 0.1794\n",
      "Epoch 11/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.4471\n",
      "Epoch 00011: val_loss improved from 0.17942 to 0.17255, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4435 - val_loss: 0.1725\n",
      "Epoch 12/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.4667\n",
      "Epoch 00012: val_loss improved from 0.17255 to 0.16666, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4649 - val_loss: 0.1667\n",
      "Epoch 13/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4278\n",
      "Epoch 00013: val_loss improved from 0.16666 to 0.16347, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4257 - val_loss: 0.1635\n",
      "Epoch 14/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.3778\n",
      "Epoch 00014: val_loss improved from 0.16347 to 0.15888, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3759 - val_loss: 0.1589\n",
      "Epoch 15/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.4103\n",
      "Epoch 00015: val_loss improved from 0.15888 to 0.15411, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4096 - val_loss: 0.1541\n",
      "Epoch 16/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.4109\n",
      "Epoch 00016: val_loss improved from 0.15411 to 0.15190, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4092 - val_loss: 0.1519\n",
      "Epoch 17/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.3540\n",
      "Epoch 00017: val_loss improved from 0.15190 to 0.15060, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3527 - val_loss: 0.1506\n",
      "Epoch 18/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.3487\n",
      "Epoch 00018: val_loss improved from 0.15060 to 0.14639, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3472 - val_loss: 0.1464\n",
      "Epoch 19/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.3336\n",
      "Epoch 00019: val_loss did not improve from 0.14639\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3332 - val_loss: 0.1480\n",
      "Epoch 20/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.3357\n",
      "Epoch 00020: val_loss improved from 0.14639 to 0.14428, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3343 - val_loss: 0.1443\n",
      "Epoch 21/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3192\n",
      "Epoch 00021: val_loss did not improve from 0.14428\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3192 - val_loss: 0.1449\n",
      "Epoch 22/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.3278\n",
      "Epoch 00022: val_loss improved from 0.14428 to 0.14082, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3278 - val_loss: 0.1408\n",
      "Epoch 23/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.3441\n",
      "Epoch 00023: val_loss improved from 0.14082 to 0.13930, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3430 - val_loss: 0.1393\n",
      "Epoch 24/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.3125\n",
      "Epoch 00024: val_loss improved from 0.13930 to 0.13793, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3125 - val_loss: 0.1379\n",
      "Epoch 25/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3088\n",
      "Epoch 00025: val_loss improved from 0.13793 to 0.13727, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3082 - val_loss: 0.1373\n",
      "Epoch 26/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.3042\n",
      "Epoch 00026: val_loss improved from 0.13727 to 0.13618, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3056 - val_loss: 0.1362\n",
      "Epoch 27/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.2943\n",
      "Epoch 00027: val_loss did not improve from 0.13618\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2946 - val_loss: 0.1366\n",
      "Epoch 28/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.3452\n",
      "Epoch 00028: val_loss improved from 0.13618 to 0.13541, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3440 - val_loss: 0.1354\n",
      "Epoch 29/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.2940\n",
      "Epoch 00029: val_loss improved from 0.13541 to 0.13336, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2949 - val_loss: 0.1334\n",
      "Epoch 30/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.2834\n",
      "Epoch 00030: val_loss improved from 0.13336 to 0.13311, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2842 - val_loss: 0.1331\n",
      "Epoch 31/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.2826\n",
      "Epoch 00031: val_loss did not improve from 0.13311\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2831 - val_loss: 0.1336\n",
      "Epoch 32/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.2866\n",
      "Epoch 00032: val_loss improved from 0.13311 to 0.13278, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2885 - val_loss: 0.1328\n",
      "Epoch 33/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.2809\n",
      "Epoch 00033: val_loss did not improve from 0.13278\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2808 - val_loss: 0.1339\n",
      "Epoch 34/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.2845\n",
      "Epoch 00034: val_loss did not improve from 0.13278\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2839 - val_loss: 0.1331\n",
      " ###0 fold : val acc1 0.586, acc3 0.960, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/154 [=========================>....] - ETA: 0s - loss: 14.7270\n",
      "Epoch 00001: val_loss improved from inf to 5.76695, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 13.8072 - val_loss: 5.7669\n",
      "Epoch 2/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 3.5805\n",
      "Epoch 00002: val_loss improved from 5.76695 to 1.25934, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 3.5517 - val_loss: 1.2593\n",
      "Epoch 3/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.1124\n",
      "Epoch 00003: val_loss improved from 1.25934 to 0.59736, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.5190 - val_loss: 0.5974\n",
      "Epoch 4/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.1068\n",
      "Epoch 00004: val_loss improved from 0.59736 to 0.36817, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0714 - val_loss: 0.3682\n",
      "Epoch 5/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.6450\n",
      "Epoch 00005: val_loss improved from 0.36817 to 0.27463, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6427 - val_loss: 0.2746\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.7669\n",
      "Epoch 00006: val_loss improved from 0.27463 to 0.23363, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7535 - val_loss: 0.2336\n",
      "Epoch 7/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.5800\n",
      "Epoch 00007: val_loss improved from 0.23363 to 0.20945, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5757 - val_loss: 0.2094\n",
      "Epoch 8/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.6704\n",
      "Epoch 00008: val_loss improved from 0.20945 to 0.19905, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6684 - val_loss: 0.1991\n",
      "Epoch 9/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.4275\n",
      "Epoch 00009: val_loss improved from 0.19905 to 0.18388, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4255 - val_loss: 0.1839\n",
      "Epoch 10/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.5668\n",
      "Epoch 00010: val_loss improved from 0.18388 to 0.17565, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5637 - val_loss: 0.1757\n",
      "Epoch 11/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.4575\n",
      "Epoch 00011: val_loss improved from 0.17565 to 0.17011, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4491 - val_loss: 0.1701\n",
      "Epoch 12/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.4640\n",
      "Epoch 00012: val_loss improved from 0.17011 to 0.16394, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4614 - val_loss: 0.1639\n",
      "Epoch 13/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.4283\n",
      "Epoch 00013: val_loss improved from 0.16394 to 0.16030, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4270 - val_loss: 0.1603\n",
      "Epoch 14/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.3742\n",
      "Epoch 00014: val_loss improved from 0.16030 to 0.15618, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3738 - val_loss: 0.1562\n",
      "Epoch 15/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.4129\n",
      "Epoch 00015: val_loss improved from 0.15618 to 0.15131, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4111 - val_loss: 0.1513\n",
      "Epoch 16/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4183\n",
      "Epoch 00016: val_loss improved from 0.15131 to 0.14848, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4134 - val_loss: 0.1485\n",
      "Epoch 17/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3530\n",
      "Epoch 00017: val_loss improved from 0.14848 to 0.14794, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3523 - val_loss: 0.1479\n",
      "Epoch 18/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.3515\n",
      "Epoch 00018: val_loss improved from 0.14794 to 0.14419, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3501 - val_loss: 0.1442\n",
      "Epoch 19/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.3333\n",
      "Epoch 00019: val_loss did not improve from 0.14419\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3316 - val_loss: 0.1450\n",
      "Epoch 20/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.3357\n",
      "Epoch 00020: val_loss improved from 0.14419 to 0.14310, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3352 - val_loss: 0.1431\n",
      "Epoch 21/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3223\n",
      "Epoch 00021: val_loss did not improve from 0.14310\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3223 - val_loss: 0.1437\n",
      "Epoch 22/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3312\n",
      "Epoch 00022: val_loss improved from 0.14310 to 0.13921, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3309 - val_loss: 0.1392\n",
      "Epoch 23/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.3452\n",
      "Epoch 00023: val_loss improved from 0.13921 to 0.13848, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3439 - val_loss: 0.1385\n",
      "Epoch 24/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3123\n",
      "Epoch 00024: val_loss improved from 0.13848 to 0.13687, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3124 - val_loss: 0.1369\n",
      "Epoch 25/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3117\n",
      "Epoch 00025: val_loss improved from 0.13687 to 0.13623, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3110 - val_loss: 0.1362\n",
      "Epoch 26/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3087\n",
      "Epoch 00026: val_loss did not improve from 0.13623\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3092 - val_loss: 0.1365\n",
      "Epoch 27/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.2976\n",
      "Epoch 00027: val_loss improved from 0.13623 to 0.13558, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2977 - val_loss: 0.1356\n",
      "Epoch 28/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.3485\n",
      "Epoch 00028: val_loss improved from 0.13558 to 0.13418, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3447 - val_loss: 0.1342\n",
      "Epoch 29/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.2945\n",
      "Epoch 00029: val_loss improved from 0.13418 to 0.13298, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2937 - val_loss: 0.1330\n",
      "Epoch 30/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.2822\n",
      "Epoch 00030: val_loss improved from 0.13298 to 0.13171, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2823 - val_loss: 0.1317\n",
      "Epoch 31/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.2885\n",
      "Epoch 00031: val_loss did not improve from 0.13171\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2881 - val_loss: 0.1321\n",
      "Epoch 32/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.2875\n",
      "Epoch 00032: val_loss improved from 0.13171 to 0.13126, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2891 - val_loss: 0.1313\n",
      "Epoch 33/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.2807\n",
      "Epoch 00033: val_loss did not improve from 0.13126\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2797 - val_loss: 0.1327\n",
      "Epoch 34/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.2861\n",
      "Epoch 00034: val_loss did not improve from 0.13126\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2855 - val_loss: 0.1321\n",
      " ###1 fold : val acc1 0.591, acc3 0.961, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/154 [============================>.] - ETA: 0s - loss: 13.9890\n",
      "Epoch 00001: val_loss improved from inf to 5.76621, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 13.8219 - val_loss: 5.7662\n",
      "Epoch 2/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 3.6875\n",
      "Epoch 00002: val_loss improved from 5.76621 to 1.27506, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 3.5617 - val_loss: 1.2751\n",
      "Epoch 3/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 1.1113\n",
      "Epoch 00003: val_loss improved from 1.27506 to 0.60007, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.5361 - val_loss: 0.6001\n",
      "Epoch 4/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 1.0928\n",
      "Epoch 00004: val_loss improved from 0.60007 to 0.36791, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.0782 - val_loss: 0.3679\n",
      "Epoch 5/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.6537\n",
      "Epoch 00005: val_loss improved from 0.36791 to 0.27440, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.6442 - val_loss: 0.2744\n",
      "Epoch 6/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.7748\n",
      "Epoch 00006: val_loss improved from 0.27440 to 0.23408, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.7482 - val_loss: 0.2341\n",
      "Epoch 7/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.5893\n",
      "Epoch 00007: val_loss improved from 0.23408 to 0.21012, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5772 - val_loss: 0.2101\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.6689\n",
      "Epoch 00008: val_loss improved from 0.21012 to 0.20033, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6689 - val_loss: 0.2003\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.4280\n",
      "Epoch 00009: val_loss improved from 0.20033 to 0.18565, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4280 - val_loss: 0.1856\n",
      "Epoch 10/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.5604\n",
      "Epoch 00010: val_loss improved from 0.18565 to 0.17807, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5604 - val_loss: 0.1781\n",
      "Epoch 11/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.4556\n",
      "Epoch 00011: val_loss improved from 0.17807 to 0.17218, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4479 - val_loss: 0.1722\n",
      "Epoch 12/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.4714\n",
      "Epoch 00012: val_loss improved from 0.17218 to 0.16546, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4631 - val_loss: 0.1655\n",
      "Epoch 13/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.4343\n",
      "Epoch 00013: val_loss improved from 0.16546 to 0.16193, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.4302 - val_loss: 0.1619\n",
      "Epoch 14/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.3773\n",
      "Epoch 00014: val_loss improved from 0.16193 to 0.15865, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3745 - val_loss: 0.1586\n",
      "Epoch 15/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.4164\n",
      "Epoch 00015: val_loss improved from 0.15865 to 0.15264, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4117 - val_loss: 0.1526\n",
      "Epoch 16/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.4224\n",
      "Epoch 00016: val_loss improved from 0.15264 to 0.14920, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.4138 - val_loss: 0.1492\n",
      "Epoch 17/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.3512\n",
      "Epoch 00017: val_loss improved from 0.14920 to 0.14892, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3502 - val_loss: 0.1489\n",
      "Epoch 18/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3495\n",
      "Epoch 00018: val_loss improved from 0.14892 to 0.14534, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3482 - val_loss: 0.1453\n",
      "Epoch 19/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.3311\n",
      "Epoch 00019: val_loss improved from 0.14534 to 0.14473, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3302 - val_loss: 0.1447\n",
      "Epoch 20/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.3351\n",
      "Epoch 00020: val_loss improved from 0.14473 to 0.14341, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3335 - val_loss: 0.1434\n",
      "Epoch 21/100\n",
      "134/154 [=========================>....] - ETA: 0s - loss: 0.3199\n",
      "Epoch 00021: val_loss improved from 0.14341 to 0.14323, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3198 - val_loss: 0.1432\n",
      "Epoch 22/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.3282\n",
      "Epoch 00022: val_loss improved from 0.14323 to 0.14022, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3281 - val_loss: 0.1402\n",
      "Epoch 23/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.3463\n",
      "Epoch 00023: val_loss improved from 0.14022 to 0.13901, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3429 - val_loss: 0.1390\n",
      "Epoch 24/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.3134\n",
      "Epoch 00024: val_loss improved from 0.13901 to 0.13719, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3124 - val_loss: 0.1372\n",
      "Epoch 25/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.3087\n",
      "Epoch 00025: val_loss improved from 0.13719 to 0.13637, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3102 - val_loss: 0.1364\n",
      "Epoch 26/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.3071\n",
      "Epoch 00026: val_loss did not improve from 0.13637\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3084 - val_loss: 0.1373\n",
      "Epoch 27/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.2972\n",
      "Epoch 00027: val_loss improved from 0.13637 to 0.13546, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2985 - val_loss: 0.1355\n",
      "Epoch 28/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.3478\n",
      "Epoch 00028: val_loss improved from 0.13546 to 0.13441, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3450 - val_loss: 0.1344\n",
      "Epoch 29/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.2947\n",
      "Epoch 00029: val_loss improved from 0.13441 to 0.13372, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2941 - val_loss: 0.1337\n",
      "Epoch 30/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.2849\n",
      "Epoch 00030: val_loss improved from 0.13372 to 0.13185, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2851 - val_loss: 0.1318\n",
      "Epoch 31/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.2874\n",
      "Epoch 00031: val_loss did not improve from 0.13185\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2871 - val_loss: 0.1328\n",
      "Epoch 32/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2873\n",
      "Epoch 00032: val_loss did not improve from 0.13185\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2877 - val_loss: 0.1336\n",
      " ###2 fold : val acc1 0.583, acc3 0.964, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/154 [==========================>...] - ETA: 0s - loss: 14.3606\n",
      "Epoch 00001: val_loss improved from inf to 5.78986, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 13.8278 - val_loss: 5.7899\n",
      "Epoch 2/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 3.6070\n",
      "Epoch 00002: val_loss improved from 5.78986 to 1.27671, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 3.5640 - val_loss: 1.2767\n",
      "Epoch 3/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 1.5363\n",
      "Epoch 00003: val_loss improved from 1.27671 to 0.59709, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.5314 - val_loss: 0.5971\n",
      "Epoch 4/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 1.0794\n",
      "Epoch 00004: val_loss improved from 0.59709 to 0.36780, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.0698 - val_loss: 0.3678\n",
      "Epoch 5/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.6456\n",
      "Epoch 00005: val_loss improved from 0.36780 to 0.27558, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6406 - val_loss: 0.2756\n",
      "Epoch 6/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.7549\n",
      "Epoch 00006: val_loss improved from 0.27558 to 0.23481, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.7479 - val_loss: 0.2348\n",
      "Epoch 7/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.5853\n",
      "Epoch 00007: val_loss improved from 0.23481 to 0.21068, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5784 - val_loss: 0.2107\n",
      "Epoch 8/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.6710\n",
      "Epoch 00008: val_loss improved from 0.21068 to 0.20186, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6706 - val_loss: 0.2019\n",
      "Epoch 9/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.4272\n",
      "Epoch 00009: val_loss improved from 0.20186 to 0.18512, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4278 - val_loss: 0.1851\n",
      "Epoch 10/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.5623\n",
      "Epoch 00010: val_loss improved from 0.18512 to 0.17886, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5621 - val_loss: 0.1789\n",
      "Epoch 11/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.4497\n",
      "Epoch 00011: val_loss improved from 0.17886 to 0.17127, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4487 - val_loss: 0.1713\n",
      "Epoch 12/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.4664\n",
      "Epoch 00012: val_loss improved from 0.17127 to 0.16511, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4647 - val_loss: 0.1651\n",
      "Epoch 13/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4358\n",
      "Epoch 00013: val_loss improved from 0.16511 to 0.16230, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4324 - val_loss: 0.1623\n",
      "Epoch 14/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.3774\n",
      "Epoch 00014: val_loss improved from 0.16230 to 0.15879, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3774 - val_loss: 0.1588\n",
      "Epoch 15/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.4124\n",
      "Epoch 00015: val_loss improved from 0.15879 to 0.15301, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4121 - val_loss: 0.1530\n",
      "Epoch 16/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.4177\n",
      "Epoch 00016: val_loss improved from 0.15301 to 0.14893, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4153 - val_loss: 0.1489\n",
      "Epoch 17/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3517\n",
      "Epoch 00017: val_loss did not improve from 0.14893\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3509 - val_loss: 0.1490\n",
      "Epoch 18/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.3485\n",
      "Epoch 00018: val_loss improved from 0.14893 to 0.14513, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3484 - val_loss: 0.1451\n",
      "Epoch 19/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.3293\n",
      "Epoch 00019: val_loss improved from 0.14513 to 0.14382, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3296 - val_loss: 0.1438\n",
      "Epoch 20/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3366\n",
      "Epoch 00020: val_loss improved from 0.14382 to 0.14310, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3342 - val_loss: 0.1431\n",
      "Epoch 21/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.3228\n",
      "Epoch 00021: val_loss improved from 0.14310 to 0.14251, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3224 - val_loss: 0.1425\n",
      "Epoch 22/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3302\n",
      "Epoch 00022: val_loss improved from 0.14251 to 0.14127, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3302 - val_loss: 0.1413\n",
      "Epoch 23/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.3506\n",
      "Epoch 00023: val_loss improved from 0.14127 to 0.13963, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3467 - val_loss: 0.1396\n",
      "Epoch 24/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.3148\n",
      "Epoch 00024: val_loss improved from 0.13963 to 0.13723, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3147 - val_loss: 0.1372\n",
      "Epoch 25/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.3095\n",
      "Epoch 00025: val_loss improved from 0.13723 to 0.13638, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3097 - val_loss: 0.1364\n",
      "Epoch 26/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3092\n",
      "Epoch 00026: val_loss did not improve from 0.13638\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3096 - val_loss: 0.1381\n",
      "Epoch 27/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.2989\n",
      "Epoch 00027: val_loss improved from 0.13638 to 0.13492, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3004 - val_loss: 0.1349\n",
      "Epoch 28/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.3459\n",
      "Epoch 00028: val_loss improved from 0.13492 to 0.13404, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3458 - val_loss: 0.1340\n",
      "Epoch 29/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.2945\n",
      "Epoch 00029: val_loss improved from 0.13404 to 0.13383, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2937 - val_loss: 0.1338\n",
      "Epoch 30/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2854\n",
      "Epoch 00030: val_loss improved from 0.13383 to 0.13210, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2856 - val_loss: 0.1321\n",
      "Epoch 31/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.2894\n",
      "Epoch 00031: val_loss did not improve from 0.13210\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2896 - val_loss: 0.1333\n",
      "Epoch 32/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.2884\n",
      "Epoch 00032: val_loss did not improve from 0.13210\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2887 - val_loss: 0.1329\n",
      " ###3 fold : val acc1 0.575, acc3 0.965, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/154 [=========================>....] - ETA: 0s - loss: 14.7932\n",
      "Epoch 00001: val_loss improved from inf to 5.97838, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 13.9441 - val_loss: 5.9784\n",
      "Epoch 2/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 3.5351\n",
      "Epoch 00002: val_loss improved from 5.97838 to 1.27437, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 3.4529 - val_loss: 1.2744\n",
      "Epoch 3/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.4300\n",
      "Epoch 00003: val_loss improved from 1.27437 to 0.57980, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4011 - val_loss: 0.5798\n",
      "Epoch 4/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.9996\n",
      "Epoch 00004: val_loss improved from 0.57980 to 0.36279, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.9935 - val_loss: 0.3628\n",
      "Epoch 5/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.8239\n",
      "Epoch 00005: val_loss improved from 0.36279 to 0.27245, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8091 - val_loss: 0.2724\n",
      "Epoch 6/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.6575\n",
      "Epoch 00006: val_loss improved from 0.27245 to 0.23338, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6576 - val_loss: 0.2334\n",
      "Epoch 7/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.5386\n",
      "Epoch 00007: val_loss improved from 0.23338 to 0.21225, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5385 - val_loss: 0.2123\n",
      "Epoch 8/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.6379\n",
      "Epoch 00008: val_loss improved from 0.21225 to 0.19879, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6269 - val_loss: 0.1988\n",
      "Epoch 9/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.5672\n",
      "Epoch 00009: val_loss improved from 0.19879 to 0.18990, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5614 - val_loss: 0.1899\n",
      "Epoch 10/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.4104\n",
      "Epoch 00010: val_loss improved from 0.18990 to 0.17954, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4110 - val_loss: 0.1795\n",
      "Epoch 11/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4390\n",
      "Epoch 00011: val_loss improved from 0.17954 to 0.17381, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4352 - val_loss: 0.1738\n",
      "Epoch 12/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.4258\n",
      "Epoch 00012: val_loss improved from 0.17381 to 0.16636, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4257 - val_loss: 0.1664\n",
      "Epoch 13/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4144\n",
      "Epoch 00013: val_loss improved from 0.16636 to 0.16399, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4120 - val_loss: 0.1640\n",
      "Epoch 14/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3825\n",
      "Epoch 00014: val_loss improved from 0.16399 to 0.15831, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3796 - val_loss: 0.1583\n",
      "Epoch 15/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3733\n",
      "Epoch 00015: val_loss improved from 0.15831 to 0.15474, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3720 - val_loss: 0.1547\n",
      "Epoch 16/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3840\n",
      "Epoch 00016: val_loss improved from 0.15474 to 0.14910, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3812 - val_loss: 0.1491\n",
      "Epoch 17/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.3442\n",
      "Epoch 00017: val_loss did not improve from 0.14910\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3433 - val_loss: 0.1492\n",
      "Epoch 18/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.3334\n",
      "Epoch 00018: val_loss improved from 0.14910 to 0.14491, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3339 - val_loss: 0.1449\n",
      "Epoch 19/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3257\n",
      "Epoch 00019: val_loss improved from 0.14491 to 0.14423, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3270 - val_loss: 0.1442\n",
      "Epoch 20/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3301\n",
      "Epoch 00020: val_loss improved from 0.14423 to 0.14198, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3286 - val_loss: 0.1420\n",
      "Epoch 21/100\n",
      " 55/154 [=========>....................] - ETA: 0s - loss: 0.3287"
     ]
    }
   ],
   "source": [
    "from keras import metrics\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "# random search for hyperparameter\n",
    "ntrial = ntest\n",
    "train_errs, val_errs = [] ,[]\n",
    "test_acc, test_roc, test_prc = [], [], []\n",
    "#test_rmse, test_mae, test_auc = [], [], []\n",
    "random_settings = []\n",
    "\n",
    "\n",
    "for itrial in range(ntrial):\n",
    "    # grid search\n",
    "    # test_setting = test_settings[itrial]\n",
    "\n",
    "    # random search\n",
    "    print('random search {}/{}'.format(itrial, ntrial))\n",
    "    \n",
    "    # total conv layers of the model\n",
    "    nlayer = random.choice([1,2]) \n",
    "    # test settings\n",
    "    dnodes[0], dropouts[0], dnodes[1], dropouts[1], batch_size, learning_rate = random.choice(test_settings)\n",
    "    \n",
    "\n",
    "    # 이번 옵션에 대한 결과 디렉토리\n",
    "    odir_f = f'batch{batch_size},'\n",
    "    for i in range(nlayer):\n",
    "        odir_f += f'dnodes{dnodes[i]}_dropout{dropouts[i]},'\n",
    "    odir_f += f'lr{learning_rate}'\n",
    "    random_settings.append(odir_f)\n",
    "    \n",
    "    odir = rootdir + '/' + odir_f\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "\n",
    "    # build a model\n",
    "    inp = Input(shape=(x_train.shape[1],))\n",
    "    out = inp\n",
    "\n",
    "    \n",
    "    for i in range(nlayer):      \n",
    "        out = Dense(dnodes[i], activation='relu')(out)\n",
    "        out = Dropout(dropouts[i])(out)\n",
    "    \n",
    "    out = Dense(1)(out)\n",
    "\n",
    "\n",
    "    model = Model(inputs=[inp], outputs=[out])\n",
    "    model.save_weights(f'{odir}/initial_weights.hdf5')\n",
    "        \n",
    "\n",
    "    # 4-fold cv\n",
    "    kfold = KFold(nfold)\n",
    "    acc1s, acc3s, maes = [], [], []\n",
    "\n",
    "    switch = 0\n",
    "    for fold, (train_mask, test_mask) in enumerate(kfold.split(y_train)):\n",
    "        X_train = x_train_imputed[train_mask]\n",
    "        X_test = x_train_imputed[test_mask] \n",
    "        \n",
    "        Y_train = y_train[train_mask] \n",
    "        Y_test = y_train[test_mask]\n",
    "\n",
    "\n",
    "        # model 학습\n",
    "        try:\n",
    "            weightcache = f\"{odir}/weights_{fold}.hdf5\"\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=learning_rate), metrics=[])\n",
    "            hist = model.fit(X_train, Y_train, validation_split=0.2, epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=2, verbose=0, mode='auto')])\n",
    "\n",
    "            model.load_weights(weightcache)\n",
    "            y_pred = model.predict(X_test).flatten() \n",
    "            y_pred = np.round(y_pred * 2) / 2\n",
    "            \n",
    "            acc1 = np.mean(y_pred==Y_test)\n",
    "            acc3 = np.mean((y_pred >= Y_test-0.5) & (y_pred <= Y_test+0.5))\n",
    "            mae = mean_absolute_error(Y_test, y_pred)           \n",
    "            \n",
    "            acc1s.append(acc1)\n",
    "            acc3s.append(acc3)\n",
    "            maes.append(mae)\n",
    "\n",
    "            print(f' ###{fold} fold : val acc1 {acc1:.3f}, acc3 {acc3:.3f}, mae {mae:.3f}###')\n",
    "            tf.keras.backend.clear_session()\n",
    "            model.load_weights(f'{odir}/initial_weights.hdf5')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            switch = 1\n",
    "            shutil.rmtree(odir)\n",
    "            itrial -= 1\n",
    "            break\n",
    "\n",
    "    if switch:\n",
    "        switch = 0\n",
    "        continue\n",
    "    \n",
    "\n",
    "    print(f'acc1{np.mean(acc1s):.3f}_acc3{np.mean(acc3s):.3f}')\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    os.rename(odir, rootdir+f'/acc1-{np.mean(acc1s):.3f}_acc3-{np.mean(acc3s):.3f}_{odir_f}')\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e9a751-014c-4715-9716-0ad9407a7b8e",
   "metadata": {},
   "source": [
    "# age-based vs xgbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e36a3e6-6d10-4114-8ea1-ac834650cbc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T15:08:04.449944Z",
     "iopub.status.busy": "2023-02-17T15:08:04.449384Z",
     "iopub.status.idle": "2023-02-17T15:08:04.462959Z",
     "shell.execute_reply": "2023-02-17T15:08:04.461906Z",
     "shell.execute_reply.started": "2023-02-17T15:08:04.449889Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xgbr 모델 예측값\n",
    "xgbr = xgb.XGBRegressor()\n",
    "xgbr.load_model('result/size/acc1-0.601_acc3-0.966_XGBR_10fold/model.model')\n",
    "\n",
    "y_pred = xgbr.predict(x_test).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "# age-based formula 값 : y_test_old\n",
    "\n",
    "# 두 공식 간에 다른값 비율\n",
    "#y_pred == y_test_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffe16240-0412-4a80-98e4-9bf1b682cab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T15:08:11.415523Z",
     "iopub.status.busy": "2023-02-17T15:08:11.414967Z",
     "iopub.status.idle": "2023-02-17T15:08:11.423931Z",
     "shell.execute_reply": "2023-02-17T15:08:11.422881Z",
     "shell.execute_reply.started": "2023-02-17T15:08:11.415472Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3641304347826087"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred == y_test_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a069304d-ec75-4969-b1bf-2db8e5c35f73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T15:27:59.363389Z",
     "iopub.status.busy": "2023-02-17T15:27:59.362884Z",
     "iopub.status.idle": "2023-02-17T15:27:59.371137Z",
     "shell.execute_reply": "2023-02-17T15:27:59.370098Z",
     "shell.execute_reply.started": "2023-02-17T15:27:59.363335Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.5, 5.5, 4. , ..., 4. , 5. , 4.5])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8347cd47-d1e8-43b7-87de-66e36900a4dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T15:27:52.959492Z",
     "iopub.status.busy": "2023-02-17T15:27:52.958884Z",
     "iopub.status.idle": "2023-02-17T15:27:52.968095Z",
     "shell.execute_reply": "2023-02-17T15:27:52.966888Z",
     "shell.execute_reply.started": "2023-02-17T15:27:52.959434Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5. , 6. , 4.5, ..., 4.5, 5. , 5. ], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3142a146-9d41-464a-b61d-ca7222a19686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T15:30:41.287359Z",
     "iopub.status.busy": "2023-02-17T15:30:41.286848Z",
     "iopub.status.idle": "2023-02-17T15:30:44.743797Z",
     "shell.execute_reply": "2023-02-17T15:30:44.743175Z",
     "shell.execute_reply.started": "2023-02-17T15:30:41.287304Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f09b426d760>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xvals = []\n",
    "yvals = []\n",
    "cvals = []\n",
    "cvals_old = []\n",
    "for x in np.arange(3, 8, 0.5):\n",
    "    for y in np.arange(3, 8, 0.5):\n",
    "        xvals.append(x)\n",
    "        yvals.append(y)\n",
    "        \n",
    "        cvals.append(sum((y_pred == x) & (y_test_old == y)))\n",
    "        cvals_old.append(sum((y_test_old == x) & (y_test == y)))\n",
    "xvals = np.array(xvals)\n",
    "yvals = np.array(yvals)\n",
    "cvals = np.array(cvals) / 2\n",
    "cvals_old = np.array(cvals_old) / 2\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlim(2, 8)\n",
    "plt.ylim(2, 8)\n",
    "plt.scatter(xvals, yvals, c='red', alpha=0.5, s=cvals, legend=)\n",
    "plt.xlabel('Model Prediction')\n",
    "plt.ylabel('Age-based formula')\n",
    "#lgnd = plt.legend()\n",
    "#lgnd.legendHandles[0]._sizes = [30]\n",
    "plt.plot([2,8], [2,8], 'k-', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb799e3d-1f4f-4608-bd1d-e24652454ccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T15:28:28.919750Z",
     "iopub.status.busy": "2023-02-17T15:28:28.919159Z",
     "iopub.status.idle": "2023-02-17T15:28:29.562161Z",
     "shell.execute_reply": "2023-02-17T15:28:29.561641Z",
     "shell.execute_reply.started": "2023-02-17T15:28:28.919694Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'age-based formula')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_pred, y_test_old)\n",
    "plt.xlabel('XGBR prediction')\n",
    "plt.ylabel('age-based formula')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "painstudy_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
