{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31fd2ee2-ec28-478f-86f8-af4c4bf3619e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T06:53:03.208158Z",
     "iopub.status.busy": "2023-02-17T06:53:03.207645Z",
     "iopub.status.idle": "2023-02-17T06:53:10.351340Z",
     "shell.execute_reply": "2023-02-17T06:53:10.350792Z",
     "shell.execute_reply.started": "2023-02-17T06:53:03.208034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복되는 hid는 첫번째 수술 외 제외: 34042\n",
      "소아 10세 미만에서 cuffed ETT 사용 비율: 0.287\n",
      "소아 10세 미만 최종 opid수: 34042\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import random, os, datetime, pickle\n",
    "import scipy, csv, math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# tube.csv에 수술 시점 weight, height 추가\n",
    "df0 = pd.read_csv('tube.csv')\n",
    "df0.drop(columns=['weight', 'height'], inplace=True)\n",
    "df = pd.read_csv('demography_revised.csv')\n",
    "df = df0.merge(df[['opid', 'weight', 'height']], how='left', on='opid', suffixes=('_o',''))\n",
    "\n",
    "df.loc[df['weight'] <= 0, 'weight'] = None\n",
    "df.loc[df['height'] <= 0, 'height'] = None\n",
    "df['age'] = df['age'].astype(int)\n",
    "df = df.loc[df['airway_tube_type'] == 'plain']\n",
    "# [nan 'plain' 'RAE(oral)' 'reinforced' 'LMA' 'T-tube' 'CobraPLA', 'double lumen tube' 'RAE(nasal)' 'laser' 'combitube' 'univent']\n",
    "\n",
    "# age, sex, airway tube size 값이 없는 경우는 제외\n",
    "df.dropna(subset=['age', 'airway_tube_size'], inplace=True)\n",
    "df['sex'] = (df['sex'] == 'M')\n",
    "\n",
    "# 나이 계산 -> age_cal 열에 추가\n",
    "df_b = pd.read_csv('birth_sex.csv')\n",
    "df_b.rename(columns={'생년월일':'birth_date'}, inplace=True)\n",
    "df_b['birth_date'] = df_b['birth_date'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "\n",
    "df_o = pd.read_csv('opdates.csv')\n",
    "df_o['opdate'] = df_o['opdate'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "\n",
    "df1 = pd.merge(df_o, df_b, how='inner', on='hid')\n",
    "df1['age_cal'] = (df1['opdate'] - df1['birth_date'])/pd.Timedelta(days=365.2425)\n",
    "\n",
    "df = pd.merge(df, df1[['opid', 'age_cal', 'opdate', 'birth_date']], how='inner', on='opid')\n",
    "\n",
    "# inclusion criteria : 소아 10세 미만\n",
    "df = df.loc[df['age_cal'] < 10-0.01]\n",
    "df = df.loc[df['age_cal'] > 0]\n",
    "df['age'] = df['age_cal'].apply(lambda x: math.floor(x))\n",
    "\n",
    "\n",
    "# cuffed 여부와 fixed depth 추가\n",
    "df_t = pd.read_csv('tube_type.csv')\n",
    "df_t['cuffed'] = (df_t['cuffed'] == 1)\n",
    "\n",
    "df_f = pd.read_csv('tube_fixed.csv')\n",
    "\n",
    "# merge 하면서 cuffed 데이터가 없는 경우는 제외\n",
    "df = df.merge(df_f, how='left', on='opid')\n",
    "df = df.merge(df_t[['opid', 'cuffed']], how='inner', on='opid')\n",
    "\n",
    "# 중복되는 hid 경우 제외 (첫번째 수술기록만 가져오기)\n",
    "df = df.merge(df_o[['opid','hid']], how='inner', on='opid')\n",
    "df = df.loc[df[['hid', 'opid']].groupby('hid')['opid'].idxmin()]\n",
    "print(f'중복되는 hid는 첫번째 수술 외 제외: {len(df)}')\n",
    "\n",
    "\n",
    "perc = np.mean(df['cuffed'].values)\n",
    "print(f'소아 10세 미만에서 cuffed ETT 사용 비율: {perc:.3f}')\n",
    "print(f'소아 10세 미만 최종 opid수: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f82e62e-3ee3-4050-ab21-9466dcbd434b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T06:53:10.354937Z",
     "iopub.status.busy": "2023-02-17T06:53:10.354707Z",
     "iopub.status.idle": "2023-02-17T06:53:11.148175Z",
     "shell.execute_reply": "2023-02-17T06:53:11.147500Z",
     "shell.execute_reply.started": "2023-02-17T06:53:10.354913Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# age-based formula에 따른 ETT size\n",
    "OLD_VAR = 'old_tube_size'\n",
    "# df[OLD_VAR] = np.round((df['age'] / 4 + 4) * 2) / 2\n",
    "df[OLD_VAR] = df['age'].apply(lambda x: math.floor((x / 4 + 4) * 2) / 2 if x >= 2 else (3.5 if x < 1 else 4)) \n",
    "df[OLD_VAR] = df.apply(lambda x: x[OLD_VAR] - 0.5 if x['cuffed'] else x[OLD_VAR], axis=1)\n",
    "\n",
    "#y_old = df[[OLD_VAR]].values.flatten().astype(float)\n",
    "#y_test_old = y[-ntest:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f23ca35-a543-4697-9ad9-d14bc15897d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:19:33.414352Z",
     "iopub.status.busy": "2023-02-17T05:19:33.413850Z",
     "iopub.status.idle": "2023-02-17T05:19:33.451649Z",
     "shell.execute_reply": "2023-02-17T05:19:33.450950Z",
     "shell.execute_reply.started": "2023-02-17T05:19:33.414296Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (27234, 5), x_test: (6808, 5)\n"
     ]
    }
   ],
   "source": [
    "SEED = 98\n",
    "INPUT_VARS = ['age_cal','sex','weight','height', 'cuffed']\n",
    "TARGET_VAR = 'airway_tube_size'\n",
    "\n",
    "random.seed(SEED)\n",
    "df = shuffle(df)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "y = df[[TARGET_VAR]].values.flatten().astype(float)\n",
    "c = df['opid'].values.flatten().astype(int)\n",
    "y_old = df[[OLD_VAR]].values.flatten().astype(float)\n",
    "x = df.loc[:, INPUT_VARS].values.astype(float)\n",
    "\n",
    "# 저장하기\n",
    "np.savez(f'dataset/ETT_size.npz', x=x, y=y, y_old=y_old, c=c)\n",
    "\n",
    "\n",
    "# training set의 뒤쪽 20%를 test set 으로 사용\n",
    "nsamp = len(y)\n",
    "ntest = int(nsamp * 0.2)\n",
    "ntrain = nsamp - ntest\n",
    "x_test = x[-ntest:, :]\n",
    "y_test = y[-ntest:]\n",
    "y_test_old = y_old[-ntest:]\n",
    "x_train = x[:ntrain, :]\n",
    "y_train = y[:ntrain]\n",
    "\n",
    "print(f'x_train: {(x_train).shape}, x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d209c7a3-ddf4-41c1-a132-3822ee72da6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T15:07:59.373429Z",
     "iopub.status.busy": "2023-02-17T15:07:59.372880Z",
     "iopub.status.idle": "2023-02-17T15:07:59.391969Z",
     "shell.execute_reply": "2023-02-17T15:07:59.391092Z",
     "shell.execute_reply.started": "2023-02-17T15:07:59.373363Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (27234, 5), x_test: (6808, 5)\n"
     ]
    }
   ],
   "source": [
    "dat = np.load(f'dataset/ETT_size.npz')\n",
    "x, y = dat['x'], dat['y']\n",
    "y_old  = dat['y_old']\n",
    "\n",
    "# training set의 뒤쪽 20%를 test set 으로 사용\n",
    "nsamp = len(y)\n",
    "ntest = int(nsamp * 0.2)\n",
    "ntrain = nsamp - ntest\n",
    "x_test = x[-ntest:, :]\n",
    "y_test = y[-ntest:]\n",
    "y_test_old = y_old[-ntest:]\n",
    "x_train = x[:ntrain, :]\n",
    "y_train = y[:ntrain]\n",
    "\n",
    "print(f'x_train: {(x_train).shape}, x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc467c2-cea5-47b9-99b7-b4a79170e409",
   "metadata": {},
   "source": [
    "# Cuffed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a4f0a97-0e8f-463f-8290-ff5771a1e8dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:29:12.793930Z",
     "iopub.status.busy": "2023-02-17T05:29:12.793434Z",
     "iopub.status.idle": "2023-02-17T05:29:12.802087Z",
     "shell.execute_reply": "2023-02-17T05:29:12.801178Z",
     "shell.execute_reply.started": "2023-02-17T05:29:12.793881Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_mask = (x_train[:,4] == 1)\n",
    "test_mask = (x_test[:,4] == 1)\n",
    "\n",
    "x_train_c = x_train[train_mask][:,0:4]\n",
    "y_train_c = y_train[train_mask]\n",
    "x_test_c = x_test[test_mask][:,0:4]\n",
    "y_test_c = y_test[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40119064-8c2e-4356-ab4b-39531734256b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:29:17.620440Z",
     "iopub.status.busy": "2023-02-17T05:29:17.619956Z",
     "iopub.status.idle": "2023-02-17T05:29:36.222744Z",
     "shell.execute_reply": "2023-02-17T05:29:36.220679Z",
     "shell.execute_reply.started": "2023-02-17T05:29:17.620391Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 80 candidates, totalling 800 fits\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.881 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.880 total time=   2.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.883 total time=   2.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.880 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.874 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.855 total time=   5.5s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.875 total time=   5.2s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.878 total time=   1.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.875 total time=   2.1s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.879 total time=   3.1s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.877 total time=   4.6s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.877 total time=   2.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.875 total time=   2.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.878 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   1.1s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.870 total time=   3.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.852 total time=   7.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.884 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.872 total time=   4.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.874 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.877 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.881 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.882 total time=   4.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.860 total time=   6.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.873 total time=   4.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.879 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.872 total time=   7.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.875 total time=   3.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.872 total time=   6.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.880 total time=   2.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.873 total time=   4.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.874 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.877 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.880 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.874 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.883 total time=   6.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.879 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.888 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.887 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.875 total time=   2.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.876 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.879 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.875 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.868 total time=   2.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.882 total time=   2.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   2.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.873 total time=   4.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.878 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.881 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.878 total time=   3.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.884 total time=   1.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.872 total time=   2.3s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.886 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.879 total time=   5.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.882 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.882 total time=   2.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.864 total time=   8.8s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.0s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.887 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   1.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.4s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.872 total time=   2.6s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.876 total time=   3.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.886 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.879 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.880 total time=   3.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.886 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.887 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.875 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.866 total time=   5.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.883 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.882 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.887 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.887 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.887 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.886 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.884 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.875 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.882 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.885 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.875 total time=   6.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.882 total time=   2.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.870 total time=   7.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.877 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.865 total time=   3.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.863 total time=  11.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.877 total time=   5.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.882 total time=   2.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   2.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.880 total time=   3.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.876 total time=   9.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.872 total time=   3.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.871 total time=   4.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   1.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.882 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.866 total time=   4.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.886 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.884 total time=   3.3s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.877 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.2s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.870 total time=   3.1s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.878 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.6s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.8s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.871 total time=   6.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.865 total time=   2.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.869 total time=   3.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.874 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.885 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.887 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.883 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.886 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.876 total time=   3.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.885 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.876 total time=   4.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.881 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.870 total time=   6.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.875 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.880 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.887 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.887 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.885 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   2.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.883 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.871 total time=   2.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.870 total time=   7.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.872 total time=   2.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.878 total time=   3.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.859 total time=   9.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.875 total time=   4.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.878 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.877 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.878 total time=   6.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.883 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.879 total time=   7.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.876 total time=   3.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.879 total time=   3.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.863 total time=   9.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.881 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.886 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.879 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.876 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   3.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.877 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.877 total time=   4.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.877 total time=   2.4s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.873 total time=   1.4s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.847 total time=   8.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.876 total time=   1.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.886 total time=   1.1s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   2.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.873 total time=   6.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.864 total time=   5.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.877 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.879 total time=   3.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.872 total time=   3.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.883 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.880 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.876 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.875 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.877 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.871 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.858 total time=   9.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.875 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.880 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.867 total time=   5.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.887 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.874 total time=   2.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.864 total time=   8.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.874 total time=   2.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.875 total time=   3.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.881 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.881 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.880 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.883 total time=   5.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.877 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.878 total time=   6.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.873 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   2.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.870 total time=   9.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.876 total time=   3.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.874 total time=   4.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.911 total time=   0.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.886 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.878 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.877 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.878 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.885 total time=   1.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.873 total time=   2.0s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.868 total time=   5.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.874 total time=   4.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.883 total time=   1.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.874 total time=   2.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.882 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.882 total time=   4.7s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.878 total time=   2.1s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.0s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.884 total time=   2.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.870 total time=   2.5s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.874 total time=   3.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.873 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.876 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.884 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   3.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.882 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.885 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.879 total time=   3.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.879 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.879 total time=   5.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.863 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.858 total time=   7.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.880 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.880 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.875 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.887 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.870 total time=   5.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.879 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.877 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.873 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.871 total time=   2.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.881 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.850 total time=  10.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.881 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.886 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.877 total time=   4.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   3.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.884 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.879 total time=   2.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.883 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   2.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.868 total time=   9.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.882 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.875 total time=   2.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.849 total time=  11.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.906 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.909 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.876 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.886 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.882 total time=   4.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.873 total time=   1.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.853 total time=   6.7s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.870 total time=   4.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.871 total time=   5.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.886 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.875 total time=   5.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.870 total time=   8.5s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.873 total time=   2.4s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.855 total time=   6.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.877 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.888 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.874 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.874 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.874 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.887 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.884 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.880 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.885 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.868 total time=   5.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.872 total time=   2.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.869 total time=   6.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.886 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.872 total time=   4.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.873 total time=   2.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.877 total time=   4.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.880 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.878 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.885 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.883 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.872 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.868 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.875 total time=   4.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.886 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.875 total time=   5.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.885 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.882 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.883 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.878 total time=   2.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.872 total time=   8.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.873 total time=   2.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.874 total time=   3.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.861 total time=   9.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.877 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.886 total time=   1.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.881 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.870 total time=   3.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.862 total time=   6.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   2.1s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.885 total time=   2.3s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.875 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.871 total time=   3.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.886 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.877 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.1s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.877 total time=   4.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.886 total time=   1.3s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.877 total time=   6.9s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.882 total time=   1.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.863 total time=   4.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.884 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.882 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.883 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   4.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.886 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.864 total time=   4.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.874 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.875 total time=   2.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.886 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.886 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.875 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.884 total time=   5.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.869 total time=   6.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.881 total time=   2.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.864 total time=   8.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.872 total time=   3.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.861 total time=  11.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.875 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.883 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.883 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.886 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.874 total time=   6.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.879 total time=   2.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.878 total time=   3.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.876 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.886 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.880 total time=   2.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.877 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.872 total time=   5.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.864 total time=   6.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.909 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.909 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.912 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.891 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.913 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.886 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.883 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.886 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.879 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.875 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.872 total time=   2.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.864 total time=   6.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.885 total time=   1.7s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.881 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.869 total time=   6.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.861 total time=   8.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.881 total time=   1.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.873 total time=   2.4s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.878 total time=   7.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.876 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.885 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.884 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.874 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.877 total time=   3.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.884 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.881 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.870 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.877 total time=   1.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.860 total time=   6.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.887 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.882 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.880 total time=   6.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.885 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.878 total time=   3.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.874 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.881 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.882 total time=   1.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.876 total time=   3.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.851 total time=  10.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.879 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.874 total time=   5.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.874 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   7.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.879 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.885 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.878 total time=   2.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.880 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.885 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.883 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.882 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.882 total time=   2.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.845 total time=   8.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.913 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.886 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   1.0s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.881 total time=   1.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.886 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.885 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.880 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.872 total time=   2.5s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.881 total time=   2.0s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.887 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.6s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.3s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.883 total time=   2.1s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.878 total time=   2.4s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.871 total time=   3.3s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.885 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.886 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.0s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.882 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.869 total time=   7.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.874 total time=   1.8s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.869 total time=   3.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.881 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.876 total time=   2.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.886 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.879 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.876 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.884 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.882 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.878 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.876 total time=   3.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.881 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.869 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.862 total time=   6.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.881 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.887 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.885 total time=   5.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   5.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.883 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.876 total time=   1.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.879 total time=   2.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.870 total time=   7.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.867 total time=   3.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.862 total time=   9.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.883 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.885 total time=   5.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.886 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   6.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.881 total time=   1.9s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.880 total time=   2.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.872 total time=   7.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   1.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.866 total time=   4.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.857 total time=  10.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.912 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.874 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.876 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.878 total time=   3.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.871 total time=   4.9s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.884 total time=   4.3s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.0s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.886 total time=   2.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.867 total time=   5.8s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.880 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.882 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.886 total time=   1.6s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.9s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.876 total time=   3.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.870 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.871 total time=   3.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.861 total time=   7.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.878 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.873 total time=   2.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.883 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.885 total time=   3.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.881 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.886 total time=   1.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.873 total time=   3.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.878 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.876 total time=   1.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.875 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.886 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.878 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.884 total time=   4.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.885 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.875 total time=   1.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.882 total time=   6.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.868 total time=   8.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.877 total time=   3.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.865 total time=  10.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.871 total time=   4.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.886 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.877 total time=   8.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   3.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.869 total time=   8.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.864 total time=   3.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.870 total time=   4.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.866 total time=   9.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.912 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.8;, score=0.912 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.876 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.900 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.908 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.890 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.907 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.886 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.880 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.878 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.876 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.875 total time=   1.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.875 total time=   1.9s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.6s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   5.0s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.885 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.875 total time=   3.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.869 total time=   3.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.846 total time=   9.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.871 total time=   6.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.877 total time=   1.0s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.868 total time=   2.6s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.873 total time=   3.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.874 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.886 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.885 total time=   2.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.882 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.877 total time=   4.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.873 total time=   4.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.882 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.879 total time=   2.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.875 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.875 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.885 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.886 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.875 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.886 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.879 total time=   6.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.871 total time=   9.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.875 total time=   2.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.845 total time=  11.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.879 total time=   5.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.877 total time=   3.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.881 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.884 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.873 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.874 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.880 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   2.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.863 total time=   7.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.873 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.879 total time=   2.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.873 total time=   3.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.862 total time=  10.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.906 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.908 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.911 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.868 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.879 total time=   3.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.875 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.876 total time=   1.8s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.877 total time=   4.0s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.877 total time=   1.9s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.880 total time=   1.9s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.877 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.876 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.865 total time=   3.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.877 total time=   4.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.884 total time=   1.9s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.877 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.866 total time=   7.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.886 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.887 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.881 total time=   3.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.870 total time=   3.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.883 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.872 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.884 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.869 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.878 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.858 total time=   6.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.883 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.876 total time=   4.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.880 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.879 total time=   5.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.874 total time=   5.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.879 total time=   2.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.854 total time=  11.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.887 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.880 total time=   2.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.871 total time=   7.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.875 total time=   3.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   8.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.868 total time=   3.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.873 total time=   4.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.909 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.910 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.906 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.900 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.878 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.900 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.910 total time=   0.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.885 total time=   1.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.875 total time=   2.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.883 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.880 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.870 total time=   4.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.866 total time=   5.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.881 total time=   1.5s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.883 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.867 total time=   7.0s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.886 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.879 total time=   3.8s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.887 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.876 total time=   5.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.872 total time=   2.2s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.872 total time=   3.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.882 total time=   2.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.881 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.879 total time=   4.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.879 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.871 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.875 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.874 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.878 total time=   2.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.849 total time=   9.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.882 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.881 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.875 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.882 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.881 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.886 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.882 total time=   2.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.868 total time=   7.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.881 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.870 total time=   3.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.862 total time=  10.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.875 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.880 total time=   4.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.880 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.886 total time=   2.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.879 total time=   6.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.885 total time=   2.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.863 total time=   9.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   2.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.867 total time=   3.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.850 total time=  10.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.894 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.905 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.886 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.911 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.907 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.883 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.886 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.910 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.911 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.906 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.876 total time=   2.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.886 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.881 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.873 total time=   3.6s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.877 total time=   2.1s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.878 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.886 total time=   1.4s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   2.1s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.875 total time=   6.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.887 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.881 total time=   2.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.879 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.876 total time=   5.4s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.873 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.880 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.873 total time=   2.5s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.879 total time=   3.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.874 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.878 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.887 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.879 total time=   2.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.880 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.887 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.887 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.874 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.873 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.886 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.882 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.872 total time=   5.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.866 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.870 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.873 total time=   5.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.887 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.878 total time=   5.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.878 total time=   2.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.880 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.885 total time=   2.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.864 total time=   7.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.872 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.871 total time=   4.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.867 total time=   8.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.880 total time=   6.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.879 total time=   2.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.873 total time=   6.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.872 total time=   2.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.857 total time=   8.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.871 total time=   2.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.870 total time=   4.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.913 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.910 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.911 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.910 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.899 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.884 total time=   3.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.885 total time=   1.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.877 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.874 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.874 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.854 total time=   5.3s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.881 total time=   1.1s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.883 total time=   4.3s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.880 total time=   5.4s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.865 total time=   7.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.881 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.876 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.869 total time=   7.0s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.855 total time=   6.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.883 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.885 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.875 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.879 total time=   3.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.874 total time=   4.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.887 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.873 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.885 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.874 total time=   5.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.876 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.867 total time=   6.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.881 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.878 total time=   3.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.884 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.886 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.875 total time=   5.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.881 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.886 total time=   2.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.881 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.875 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.876 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.879 total time=   2.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.870 total time=   9.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.885 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.884 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.881 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.879 total time=   2.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.877 total time=   6.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.884 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   1.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.879 total time=   2.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.873 total time=   8.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.875 total time=   3.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.845 total time=  10.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.916 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.892 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.907 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.906 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.912 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.909 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.914 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.909 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.904 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.879 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.886 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.877 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.877 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.877 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.869 total time=   3.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.869 total time=   5.3s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.885 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.879 total time=   4.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.876 total time=   2.1s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.870 total time=   4.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.885 total time=   3.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.875 total time=   1.9s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.877 total time=   1.6s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.866 total time=   7.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.878 total time=   3.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.882 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.875 total time=   4.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.872 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   4.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.875 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.872 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.869 total time=   5.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.876 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.886 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   5.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.882 total time=   2.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.883 total time=   4.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.883 total time=   2.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.879 total time=   2.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.882 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.870 total time=   2.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.865 total time=   4.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.887 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.886 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.885 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.886 total time=   4.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.884 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.883 total time=   6.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   2.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.865 total time=   9.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.870 total time=   3.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.881 total time=   4.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.910 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.904 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.916 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.909 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.913 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.875 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.902 total time=   1.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.875 total time=   3.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.877 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.873 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.872 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.885 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.0s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.0s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.887 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.887 total time=   1.4s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.887 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   2.4s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.869 total time=   8.9s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.876 total time=   4.5s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.876 total time=   1.9s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.882 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.882 total time=   3.2s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.868 total time=   8.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.883 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.877 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.877 total time=   3.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.882 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   3.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.874 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.882 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.869 total time=   5.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.871 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.880 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.868 total time=   6.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.875 total time=   4.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.875 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.877 total time=   2.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.882 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.886 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.881 total time=   2.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.879 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.881 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.882 total time=   1.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.868 total time=   2.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.868 total time=   3.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.868 total time=   9.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.881 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.875 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.874 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.874 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.873 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.872 total time=   7.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.868 total time=   3.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.875 total time=   8.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.871 total time=   2.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.864 total time=   3.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.855 total time=   9.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.909 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.904 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.5;, score=0.877 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.5;, score=0.899 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=200, subsample=0.8;, score=0.898 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=200, subsample=0.8;, score=0.896 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   2.1s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.881 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.878 total time=   1.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.880 total time=   3.5s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.877 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.885 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.876 total time=   3.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   5.1s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.881 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.875 total time=   2.1s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.871 total time=   2.6s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.881 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.877 total time=   1.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.887 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.878 total time=   1.1s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.875 total time=   2.3s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.871 total time=   6.3s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.866 total time=   5.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.876 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.887 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.873 total time=   3.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.878 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.882 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.876 total time=   4.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.871 total time=   1.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.877 total time=   4.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.875 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.857 total time=   6.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.880 total time=   5.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.875 total time=   1.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.878 total time=   6.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   2.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.877 total time=   2.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.871 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.873 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.878 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.876 total time=   1.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.875 total time=   1.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.875 total time=   2.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.849 total time=  10.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.886 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.877 total time=   5.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.884 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.872 total time=   2.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.873 total time=   7.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.877 total time=   2.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.867 total time=  10.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.873 total time=   2.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.852 total time=  11.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.897 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.908 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.903 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.901 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.883 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.909 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.889 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.862 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.886 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.886 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.877 total time=   2.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.885 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.875 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.861 total time=   6.8s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.872 total time=   5.5s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.873 total time=   2.1s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.873 total time=   2.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.880 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.876 total time=   4.3s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.7s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.880 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.862 total time=   8.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.876 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.883 total time=   3.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.884 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.883 total time=   4.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.882 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.874 total time=   5.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.874 total time=   2.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.871 total time=   7.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.873 total time=   3.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.886 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.872 total time=   8.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.883 total time=   2.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.885 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.886 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.873 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.870 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.873 total time=   3.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.875 total time=   3.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.880 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.877 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.887 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.884 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.888 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.873 total time=   4.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.887 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.883 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.881 total time=   3.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.863 total time=   9.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.876 total time=   3.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.851 total time=  12.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.891 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.901 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.5;, score=0.902 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.5;, score=0.899 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.881 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.906 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.909 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.899 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.875 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.878 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.882 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.878 total time=   2.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.883 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.858 total time=   5.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.886 total time=   1.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.884 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.876 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.872 total time=   1.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.873 total time=   2.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.847 total time=   8.5s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.882 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.873 total time=   6.1s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.877 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.868 total time=   3.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.843 total time=   9.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.875 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.884 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.878 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.880 total time=   3.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.874 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.878 total time=   4.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.868 total time=   5.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.872 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.872 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.874 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.881 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.885 total time=   3.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.882 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.873 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.876 total time=   5.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.873 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.884 total time=   2.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.869 total time=   8.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.863 total time=   3.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.864 total time=  10.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.878 total time=   5.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.877 total time=   6.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.876 total time=   2.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.866 total time=   9.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.875 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.872 total time=   5.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.858 total time=   9.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.912 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.904 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.910 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.887 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.897 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.894 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.910 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.893 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.870 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.911 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.906 total time=   0.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.886 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.879 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.875 total time=   4.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.870 total time=   1.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.886 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.881 total time=   3.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.874 total time=   5.4s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.866 total time=   2.8s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.863 total time=   9.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.881 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.880 total time=   1.0s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.868 total time=   8.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.883 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.885 total time=   3.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.886 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.884 total time=   3.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.878 total time=   4.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.861 total time=   6.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.877 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.876 total time=   3.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.878 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.877 total time=   5.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   3.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.878 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.876 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.878 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   3.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.853 total time=  10.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.887 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.879 total time=   4.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.888 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.879 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.866 total time=   7.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.886 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   2.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   7.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.868 total time=   2.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.878 total time=   3.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.870 total time=   8.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.881 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.915 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.910 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.884 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.911 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.898 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.907 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.887 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.868 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.891 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.908 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.910 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.892 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.910 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.911 total time=   0.2s[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.886 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.879 total time=   4.3s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.882 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.878 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.874 total time=   3.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   3.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.877 total time=   2.3s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.873 total time=   2.7s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.881 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.880 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.876 total time=   3.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.881 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.874 total time=   6.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.872 total time=   2.3s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.843 total time=   7.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.881 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.876 total time=   4.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.881 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.881 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.873 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.868 total time=   4.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.877 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.877 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.867 total time=   5.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.886 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.882 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.883 total time=   6.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.879 total time=   2.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.875 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.886 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.873 total time=   2.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.884 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.868 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.876 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.874 total time=   3.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.862 total time=  10.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.876 total time=   4.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.887 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.875 total time=   6.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.875 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.876 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.875 total time=   7.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.870 total time=   2.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.868 total time=   4.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.860 total time=  11.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.909 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.904 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.8;, score=0.903 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.8;, score=0.902 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.892 total time=   1.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.878 total time=   2.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.877 total time=   4.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.877 total time=   1.6s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.881 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.876 total time=   3.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.880 total time=   1.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.886 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.877 total time=   1.4s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.864 total time=   6.8s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.884 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.879 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.883 total time=   5.5s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.880 total time=   6.5s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.880 total time=   2.0s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.865 total time=   3.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.875 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.880 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.878 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.878 total time=   3.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.879 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.877 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.871 total time=   4.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.871 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.872 total time=   2.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.862 total time=   6.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.884 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.877 total time=   4.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.880 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.881 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.882 total time=   6.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   2.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.869 total time=   7.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.877 total time=   2.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.870 total time=   5.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.887 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.881 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.881 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.886 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.884 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.887 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.884 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   4.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.881 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.884 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.883 total time=   2.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.875 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.887 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.873 total time=   2.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.866 total time=  10.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   3.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.855 total time=  12.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.908 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.5;, score=0.885 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.8;, score=0.872 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.8;, score=0.894 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.894 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.871 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.904 total time=   0.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.877 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.871 total time=   3.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.885 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.877 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.869 total time=   1.9s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.882 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.874 total time=   4.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   2.1s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.872 total time=   6.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.886 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.876 total time=   5.6s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.872 total time=   7.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.872 total time=   2.6s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.840 total time=   7.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.876 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.887 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.875 total time=   3.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.871 total time=   3.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.884 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.875 total time=   4.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.876 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.862 total time=   5.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.881 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.881 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.880 total time=   4.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.878 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.885 total time=   2.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.875 total time=   4.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.873 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.880 total time=   2.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.879 total time=   7.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.878 total time=   3.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.852 total time=  10.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.879 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.884 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.888 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.888 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.886 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.886 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.876 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.878 total time=   6.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.884 total time=   3.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.876 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.872 total time=   2.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.875 total time=   1.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.879 total time=   3.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.875 total time=   3.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.867 total time=   9.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.910 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.8;, score=0.908 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.8;, score=0.906 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.8;, score=0.901 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.889 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.887 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.883 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.881 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.876 total time=   2.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.869 total time=   5.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.882 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.876 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.873 total time=   5.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.886 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.875 total time=   6.2s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.879 total time=   1.1s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.876 total time=   3.0s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.873 total time=   7.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   3.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.876 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.874 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.879 total time=   3.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.880 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   5.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.877 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.849 total time=   6.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.884 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.878 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.874 total time=   4.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.888 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.879 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.879 total time=   2.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.888 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.882 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.878 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.877 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.877 total time=   7.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.874 total time=   2.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.873 total time=   3.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.861 total time=   9.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.882 total time=   5.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.880 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.875 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.884 total time=   6.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.883 total time=   2.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.872 total time=   7.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.875 total time=   3.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.872 total time=   4.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.912 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.915 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.911 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.893 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.903 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.877 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.896 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.903 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=200, subsample=0.5;, score=0.882 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=200, subsample=0.5;, score=0.874 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.879 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.877 total time=   1.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.875 total time=   1.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.882 total time=   1.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.886 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.886 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.882 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.878 total time=   1.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   2.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.885 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.870 total time=   3.2s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.857 total time=   9.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.874 total time=   5.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.875 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.875 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.866 total time=   3.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.884 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.886 total time=   2.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.874 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.882 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.873 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.880 total time=   4.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.883 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.881 total time=   4.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.875 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.862 total time=   6.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.884 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.888 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.882 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   5.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.875 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.878 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.875 total time=   7.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.870 total time=   3.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.856 total time=  11.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.883 total time=   4.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.883 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.879 total time=   6.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.886 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   5.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.881 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.881 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.876 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.878 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.866 total time=   3.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.870 total time=   3.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.868 total time=   6.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.911 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.908 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.904 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=200, subsample=0.5;, score=0.871 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=200, subsample=0.5;, score=0.890 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.885 total time=   2.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.1s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.884 total time=   2.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.876 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.885 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.874 total time=   4.0s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.875 total time=   4.9s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.881 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   1.1s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.874 total time=   4.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.873 total time=   1.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.865 total time=   3.8s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.879 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.884 total time=   1.9s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.887 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.881 total time=   2.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.886 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.885 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.870 total time=   7.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.875 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.886 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.881 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.887 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.884 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.885 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.885 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.881 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.876 total time=   3.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.880 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.870 total time=   6.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.869 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.869 total time=   5.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.886 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.882 total time=   4.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.875 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.875 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.885 total time=   2.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.863 total time=   8.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.877 total time=   2.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.843 total time=  11.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.874 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   5.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.886 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   7.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.879 total time=   2.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   3.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.871 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.873 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.877 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.872 total time=   2.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.879 total time=   1.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   2.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.848 total time=  11.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.912 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.883 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.907 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.892 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.890 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.895 total time=   1.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.886 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.877 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.877 total time=   2.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.875 total time=   2.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.873 total time=   4.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.886 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.876 total time=   4.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.882 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.876 total time=   2.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.880 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.872 total time=   2.1s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.875 total time=   3.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.878 total time=   1.6s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.878 total time=   4.7s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.871 total time=   7.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.874 total time=   3.5s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.861 total time=   7.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.877 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.887 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.881 total time=   3.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.886 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.877 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.880 total time=   6.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.874 total time=   5.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.882 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.859 total time=   8.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.880 total time=   6.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.874 total time=   5.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.885 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.874 total time=   2.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.881 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.874 total time=   6.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.866 total time=   2.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.874 total time=   3.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.868 total time=   9.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.879 total time=   4.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.886 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.887 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.876 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.874 total time=   5.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.882 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.875 total time=   3.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.880 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.874 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.885 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.871 total time=   2.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.883 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.873 total time=   4.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.855 total time=  10.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.916 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.8;, score=0.905 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.875 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.901 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.906 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.888 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.905 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.904 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.901 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.903 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=200, subsample=0.5;, score=0.882 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=200, subsample=0.5;, score=0.875 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=200, subsample=0.8;, score=0.900 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.904 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.906 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.884 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.877 total time=   1.1s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   2.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   5.0s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.7s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.877 total time=   4.8s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.865 total time=   2.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.869 total time=   9.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   1.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.882 total time=   3.0s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.884 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.878 total time=   2.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.871 total time=   3.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.872 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.874 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.885 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.878 total time=   2.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.877 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.880 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.880 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.879 total time=   2.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.872 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.876 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.875 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.874 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.877 total time=   5.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.887 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.876 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.878 total time=   4.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.884 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.874 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.870 total time=   6.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.878 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.876 total time=   7.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.873 total time=   2.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.873 total time=   3.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.874 total time=   9.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.871 total time=   6.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.888 total time=   1.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.875 total time=   7.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.877 total time=   2.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.871 total time=   8.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.880 total time=   2.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.872 total time=   4.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.915 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.907 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.905 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.891 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.867 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.891 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.845 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.874 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.876 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.891 total time=   1.9s\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'colsample_bytree': 0.8, 'max_depth': 3, 'n_estimators': 25, 'subsample': 0.8}\n",
      "0.9040845258641736\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.894\n",
      "mean_squared_errors: 0.117\n",
      "r2_score: 0.894\n",
      "acc: 0.677\n",
      "acc(+-0.5mm): 0.960\n"
     ]
    }
   ],
   "source": [
    "# age (일단위)\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.05, 0.07, 0.1], #[0.01, 0.03, 0.05],\n",
    "                'max_depth': [3, 4, 5, 7],#[3,4,5],\n",
    "                'n_estimators': [25, 50, 100, 200, 300],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                'subsample': [0.5, 0.8], #[0.5, 0.8, 1],\n",
    "                'colsample_bytree': [0.5, 0.8], #[0.8, 1],\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=xgb.sklearn.XGBRegressor(),\n",
    "                    n_jobs=-1,\n",
    "                    verbose=3,\n",
    "                    param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train_c, y_train_c)\n",
    "model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = gs.predict(x_test_c).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test_c, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test_c, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test_c, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test_c)\n",
    "acc3 = np.mean((y_pred >= y_test_c-0.5) & (y_pred <= y_test_c+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e18b60e-2b2c-4457-a169-dc3a784e4f69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:30:20.138452Z",
     "iopub.status.busy": "2023-02-17T05:30:20.137951Z",
     "iopub.status.idle": "2023-02-17T05:30:20.149537Z",
     "shell.execute_reply": "2023-02-17T05:30:20.148807Z",
     "shell.execute_reply.started": "2023-02-17T05:30:20.138401Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "odir_f = f'acc1-{acc1:.3f}_acc3-{acc3:.3f}_XGBR-cuffed_{nfold}fold'\n",
    "odir = f'result/{odir_f}'\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "model.save_model(f'{odir}/model.model')\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "pickle.dump(param_dict, open(f'{odir}/param_dict', 'wb'))\n",
    "f = open(f'{odir}/result.txt', 'w')\n",
    "f.write(f'classification model')\n",
    "f.write(f'explained_variance_score: {explained_variance_score(y_test_c, y_pred):.3f}')\n",
    "f.write(f'mean_squared_errors: {mean_squared_error(y_test_c, y_pred):.3f}')\n",
    "f.write(f'mean_absolute_errors: {mean_absolute_error(y_test_c, y_pred):.3f}')\n",
    "f.write(f'r2_score: {r2_score(y_test_c, y_pred):.3f}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864a18f-df29-45b3-94e7-eed9558dc149",
   "metadata": {},
   "source": [
    "# Uncuffed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07e91b57-c0b6-458e-8a79-c65429d60086",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:30:28.162814Z",
     "iopub.status.busy": "2023-02-17T05:30:28.162324Z",
     "iopub.status.idle": "2023-02-17T05:30:28.171147Z",
     "shell.execute_reply": "2023-02-17T05:30:28.170242Z",
     "shell.execute_reply.started": "2023-02-17T05:30:28.162763Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_mask = (x_train[:,4] == 0)\n",
    "test_mask = (x_test[:,4] == 0)\n",
    "\n",
    "x_train_c = x_train[train_mask][:,0:4]\n",
    "y_train_c = y_train[train_mask]\n",
    "x_test_c = x_test[test_mask][:,0:4]\n",
    "y_test_c = y_test[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37ac67de-4920-4e99-8881-e1e0220b86a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:30:34.615613Z",
     "iopub.status.busy": "2023-02-17T05:30:34.615111Z",
     "iopub.status.idle": "2023-02-17T05:30:53.447084Z",
     "shell.execute_reply": "2023-02-17T05:30:53.446180Z",
     "shell.execute_reply.started": "2023-02-17T05:30:34.615563Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.877 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.875 total time=   2.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.873 total time=   1.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.866 total time=   4.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.885 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.871 total time=   6.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.873 total time=   3.0s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.858 total time=  11.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.862 total time=   7.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.859 total time=   5.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.885 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.882 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.880 total time=   2.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.883 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.885 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.883 total time=   4.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.874 total time=   4.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.875 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.868 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.863 total time=   6.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.875 total time=   4.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   6.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.882 total time=   2.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.864 total time=   7.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.883 total time=   1.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.872 total time=   4.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.859 total time=   9.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.881 total time=   2.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.880 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.881 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.883 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.881 total time=   2.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   6.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.882 total time=   2.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.878 total time=   2.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.869 total time=   7.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.872 total time=   2.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.877 total time=   4.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.868 total time=   9.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.906 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.8;, score=0.905 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.880 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.906 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.908 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.904 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.908 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=200, subsample=0.8;, score=0.871 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=200, subsample=0.8;, score=0.900 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.853 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.873 total time=   2.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.896 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=200, subsample=0.8;, score=0.901 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.906 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=200, subsample=0.5;, score=0.883 total time=   2.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.869 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.869 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.886 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.875 total time=   3.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.874 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.878 total time=   1.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.860 total time=   7.4s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.877 total time=   1.4s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.866 total time=   6.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.865 total time=  10.0s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.883 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.881 total time=   2.6s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.869 total time=   8.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.873 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.880 total time=   3.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.879 total time=   3.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.883 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.883 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.871 total time=   5.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.870 total time=   2.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.874 total time=   6.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.880 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.887 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.884 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.882 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.885 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   6.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.878 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.861 total time=   7.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.880 total time=   2.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.871 total time=   3.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.866 total time=   9.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.884 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.879 total time=   2.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.869 total time=   6.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.872 total time=   2.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.868 total time=   7.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.875 total time=   2.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.865 total time=   3.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.856 total time=   7.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.911 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.902 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.883 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.862 total time=   1.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.880 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.908 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=200, subsample=0.5;, score=0.902 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.907 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.904 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=200, subsample=0.8;, score=0.880 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.890 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=200, subsample=0.5;, score=0.888 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.905 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.893 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.881 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.863 total time=   2.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.8;, score=0.868 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.868 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   2.1s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.880 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   3.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.871 total time=   3.6s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   1.1s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.876 total time=   3.3s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.871 total time=   5.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.876 total time=   1.4s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.874 total time=   1.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.851 total time=   9.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.883 total time=   5.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.878 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.875 total time=   2.0s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.867 total time=   4.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.875 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.884 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.874 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.886 total time=   3.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.874 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.883 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   3.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.872 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.881 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.881 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.883 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.876 total time=   2.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.864 total time=   5.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.887 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.884 total time=   4.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   6.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   2.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.869 total time=   6.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.879 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.868 total time=   4.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.857 total time=  10.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.877 total time=   6.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   6.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.885 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.884 total time=   2.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.884 total time=   2.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.864 total time=   8.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.869 total time=   3.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.836 total time=  12.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.914 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=200, subsample=0.8;, score=0.882 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.5;, score=0.874 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=200, subsample=0.5;, score=0.894 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.890 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.892 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.906 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.912 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.914 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=200, subsample=0.5;, score=0.887 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.897 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=200, subsample=0.5;, score=0.897 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.870 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.862 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=200, subsample=0.5;, score=0.877 total time=   1.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.866 total time=   2.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.867 total time=   1.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.866 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.859 total time=   0.7s\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'colsample_bytree': 0.8, 'max_depth': 4, 'n_estimators': 25, 'subsample': 0.8}\n",
      "0.8699761780592447\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.849\n",
      "mean_squared_errors: 0.142\n",
      "r2_score: 0.849\n",
      "acc: 0.572\n",
      "acc(+-0.5mm): 0.968\n"
     ]
    }
   ],
   "source": [
    "# age (일단위)\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.05, 0.07, 0.1], #[0.01, 0.03, 0.05],\n",
    "                'max_depth': [3, 4, 5, 7],#[3,4,5],\n",
    "                'n_estimators': [25, 50, 100, 200, 300],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                'subsample': [0.5, 0.8], #[0.5, 0.8, 1],\n",
    "                'colsample_bytree': [0.5, 0.8], #[0.8, 1],\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 5\n",
    "gs = GridSearchCV(estimator=xgb.sklearn.XGBRegressor(),\n",
    "                    n_jobs=-1,\n",
    "                    verbose=3,\n",
    "                    param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train_c, y_train_c)\n",
    "model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = gs.predict(x_test_c).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test_c, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test_c, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test_c, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test_c)\n",
    "acc3 = np.mean((y_pred >= y_test_c-0.5) & (y_pred <= y_test_c+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1a8ecb7-1e6c-4ad3-ba1c-61125b87f9fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:31:00.670552Z",
     "iopub.status.busy": "2023-02-17T05:31:00.670178Z",
     "iopub.status.idle": "2023-02-17T05:31:00.680754Z",
     "shell.execute_reply": "2023-02-17T05:31:00.679982Z",
     "shell.execute_reply.started": "2023-02-17T05:31:00.670516Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "odir_f = f'acc1-{acc1:.3f}_acc3-{acc3:.3f}_XGBR-uncuffed_{nfold}fold'\n",
    "odir = f'result/{odir_f}'\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "model.save_model(f'{odir}/model.model')\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "pickle.dump(param_dict, open(f'{odir}/param_dict', 'wb'))\n",
    "f = open(f'{odir}/result.txt', 'w')\n",
    "f.write(f'classification model')\n",
    "f.write(f'explained_variance_score: {explained_variance_score(y_test_c, y_pred):.3f}')\n",
    "f.write(f'mean_squared_errors: {mean_squared_error(y_test_c, y_pred):.3f}')\n",
    "f.write(f'mean_absolute_errors: {mean_absolute_error(y_test_c, y_pred):.3f}')\n",
    "f.write(f'r2_score: {r2_score(y_test_c, y_pred):.3f}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80084585-3ed7-4b97-9b18-0e7cb53b9d74",
   "metadata": {},
   "source": [
    "# Cuffed + Uncuffed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae122fa-efde-4758-86b9-db8e8195973b",
   "metadata": {},
   "source": [
    "## Age-based (Cole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1cb614-8c3b-45f8-b0a9-c4f35f0fe3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age-based formula - Cole's formula에 따른 ETT size\n",
    "OLD_VAR = 'old_tube_size'\n",
    "# df[OLD_VAR] = np.round((df['age'] / 4 + 4) * 2) / 2\n",
    "df[OLD_VAR] = df['age'].apply(lambda x: math.floor((x / 4 + 4) * 2) / 2 if x >= 2 else (3.5 if x < 1 else 4)) \n",
    "df[OLD_VAR] = df.apply(lambda x: x[OLD_VAR] - 0.5 if x['cuffed'] else x[OLD_VAR], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36e07937-5aa6-4936-a6d6-75dbb8e69172",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:23:40.608237Z",
     "iopub.status.busy": "2023-02-17T05:23:40.607731Z",
     "iopub.status.idle": "2023-02-17T05:23:40.621993Z",
     "shell.execute_reply": "2023-02-17T05:23:40.621252Z",
     "shell.execute_reply.started": "2023-02-17T05:23:40.608185Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model = age/4+4\n",
      "--------------\n",
      "explained_variance_score: 0.819\n",
      "mean_squared_errors: 0.272\n",
      "mean_absolute_errors: 0.395\n",
      "r2_score: 0.725\n",
      "acc: 0.344\n",
      "acc(+-0.5mm): 0.878\n"
     ]
    }
   ],
   "source": [
    "print('--------------')\n",
    "print('old model = age/4+4')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_test_old):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_test_old):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_test_old):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_test_old):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_test_old==y_test)\n",
    "acc3 = np.mean((y_test_old >= y_test-0.5) & (y_test_old <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce733f-07e2-449e-9b57-d5f9e48d4eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khine\n",
    "{0: 3.0, 1:3.5, 2: 3.5, 3: 4, 4: 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d444b-0928-4811-8f5d-16d8d39e738c",
   "metadata": {},
   "source": [
    "### age 내림으로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e496860c-4ed2-489e-9303-30b5ec064ff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T15:37:07.959283Z",
     "iopub.status.busy": "2023-02-19T15:37:07.958698Z",
     "iopub.status.idle": "2023-02-19T15:37:07.969016Z",
     "shell.execute_reply": "2023-02-19T15:37:07.968088Z",
     "shell.execute_reply.started": "2023-02-19T15:37:07.959225Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.5, 4, 4.5, 4.5, 5.0, 5.0]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[math.floor((x / 4 + 4) * 2) / 2 if x >= 2 else (3.5 if x < 1 else 4) for x in range(0,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be1d8de7-8398-4195-80e1-0a2a0c301eef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T06:46:07.565193Z",
     "iopub.status.busy": "2023-02-19T06:46:07.564600Z",
     "iopub.status.idle": "2023-02-19T06:46:07.697046Z",
     "shell.execute_reply": "2023-02-19T06:46:07.696380Z",
     "shell.execute_reply.started": "2023-02-19T06:46:07.565136Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model = age/4+4\n",
      "--------------\n",
      "explained_variance_score: 0.817\n",
      "mean_squared_errors: 0.321\n",
      "mean_absolute_errors: 0.449\n",
      "r2_score: 0.676\n",
      "acc: 0.276\n",
      "acc(+-0.5mm): 0.841\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "df_cole = pd.DataFrame({'age_cal':x_test[:,0], 'cuffed':x_test[:,4]})\n",
    "df_cole['age'] = df_cole['age_cal'].apply(lambda x: math.floor(x))\n",
    "\n",
    "OLD_VAR = 'old_tube_size'\n",
    "df_cole[OLD_VAR] = df_cole['age'].apply(lambda x: math.floor((x / 4 + 4) * 2) / 2 if x >= 2 else (3.5 if x < 1 else 4)) \n",
    "df_cole[OLD_VAR] = df_cole.apply(lambda x: x[OLD_VAR] - 0.5 if x['cuffed'] else x[OLD_VAR], axis=1)\n",
    "\n",
    "y_old = df_cole[[OLD_VAR]].values.flatten().astype(float)\n",
    "y_test_old = y_old[-ntest:]\n",
    "\n",
    "\n",
    "print('--------------')\n",
    "print('old model = age/4+4')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_test_old):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_test_old):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_test_old):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_test_old):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_test_old==y_test)\n",
    "acc3 = np.mean((y_test_old >= y_test-0.5) & (y_test_old <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fbe0ed79-7a4e-4dc3-9377-293bc9a54a36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T16:04:52.629742Z",
     "iopub.status.busy": "2023-02-19T16:04:52.629141Z",
     "iopub.status.idle": "2023-02-19T16:04:52.645865Z",
     "shell.execute_reply": "2023-02-19T16:04:52.644976Z",
     "shell.execute_reply.started": "2023-02-19T16:04:52.629685Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model for cuffed = age/4+3.5\n",
      "--------------\n",
      "explained_variance_score: 0.873\n",
      "mean_squared_errors: 0.302\n",
      "mean_absolute_errors: 0.419\n",
      "r2_score: 0.728\n",
      "acc: 0.318\n",
      "acc(+-0.5mm): 0.870\n"
     ]
    }
   ],
   "source": [
    "cuff_mask = x_test[:,4] == 1\n",
    "\n",
    "y_test_old1 = y_test_old[cuff_mask]\n",
    "y_test1 = y_test[cuff_mask]\n",
    "\n",
    "print('--------------')\n",
    "print('old model for cuffed = age/4+3.5')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test1, y_test_old1):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test1, y_test_old1):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test1, y_test_old1):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test1, y_test_old1):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_test_old1==y_test1)\n",
    "acc3 = np.mean((y_test_old1 >= y_test1-0.5) & (y_test_old1 <= y_test1+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9df3a069-dac0-43c2-9a0e-7e5391b986e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T16:09:36.650450Z",
     "iopub.status.busy": "2023-02-19T16:09:36.649950Z",
     "iopub.status.idle": "2023-02-19T16:09:36.664616Z",
     "shell.execute_reply": "2023-02-19T16:09:36.663986Z",
     "shell.execute_reply.started": "2023-02-19T16:09:36.650399Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "old model for uncuffed = age/4+3.5\n",
      "--------------\n",
      "explained_variance_score: 0.791\n",
      "mean_squared_errors: 0.328\n",
      "mean_absolute_errors: 0.461\n",
      "r2_score: 0.649\n",
      "acc: 0.259\n",
      "acc(+-0.5mm): 0.829\n"
     ]
    }
   ],
   "source": [
    "cuff_mask = x_test[:,4] == 1\n",
    "\n",
    "y_test_old1 = y_test_old[~cuff_mask]\n",
    "y_test1 = y_test[~cuff_mask]\n",
    "\n",
    "print('--------------')\n",
    "print('old model for uncuffed = age/4+3.5')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test1, y_test_old1):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test1, y_test_old1):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test1, y_test_old1):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test1, y_test_old1):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_test_old1==y_test1)\n",
    "acc3 = np.mean((y_test_old1 >= y_test1-0.5) & (y_test_old1 <= y_test1+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a7f6f4d9-6968-485d-a777-a443447746b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T06:46:56.764756Z",
     "iopub.status.busy": "2023-02-19T06:46:56.764270Z",
     "iopub.status.idle": "2023-02-19T06:46:58.562853Z",
     "shell.execute_reply": "2023-02-19T06:46:58.562303Z",
     "shell.execute_reply.started": "2023-02-19T06:46:56.764703Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xvals = []\n",
    "yvals = []\n",
    "cvals = []\n",
    "cvals_old = []\n",
    "for x in np.arange(3, 8, 0.5):\n",
    "    for y in np.arange(3, 8, 0.5):\n",
    "        xvals.append(x)\n",
    "        yvals.append(y)\n",
    "        #cvals.append(sum((y_pred == x) & (y_test == y)))\n",
    "        cvals_old.append(sum((y_test_old == x) & (y_test == y)))\n",
    "xvals = np.array(xvals)\n",
    "yvals = np.array(yvals)\n",
    "#cvals = np.array(cvals) / 2\n",
    "cvals_old = np.array(cvals_old) / 2\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlim(2, 8)\n",
    "plt.ylim(2, 8)\n",
    "plt.scatter(xvals, yvals, c='red', alpha=0.5, s=cvals_old, label='4+AGE/4')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Ground Truth')\n",
    "lgnd = plt.legend()\n",
    "lgnd.legendHandles[0]._sizes = [30]\n",
    "plt.plot([2,8], [2,8], 'k-', alpha=0.1)\n",
    "plt.plot([2.5,8], [2,7.5], '--k', alpha=0.1)\n",
    "plt.plot([2,7.5], [2.5,8], '--k', alpha=0.1)\n",
    "plt.savefig('old_내림.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c472067-3191-445f-878a-32d9d0415341",
   "metadata": {},
   "source": [
    "## XGBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "422e3b07-64c9-41aa-95de-35f9e941c852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:25:34.779719Z",
     "iopub.status.busy": "2023-02-17T05:25:34.779215Z",
     "iopub.status.idle": "2023-02-17T05:27:30.093272Z",
     "shell.execute_reply": "2023-02-17T05:27:30.092544Z",
     "shell.execute_reply.started": "2023-02-17T05:25:34.779668Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 180 candidates, totalling 1800 fits\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.878 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.877 total time=   2.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.875 total time=   2.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.873 total time=   2.1s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.882 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.878 total time=   4.8s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.874 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.881 total time=   2.0s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.851 total time=   8.6s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.883 total time=   1.9s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.8s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.876 total time=   2.2s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.876 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.877 total time=   1.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.880 total time=   2.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.850 total time=   6.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.875 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.882 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.880 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.881 total time=   2.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.882 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.879 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.878 total time=   3.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.875 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.886 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.874 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.876 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.877 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.874 total time=   2.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.885 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.877 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.886 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.884 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.884 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.882 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.882 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.879 total time=   3.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.885 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.887 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.867 total time=   6.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.876 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.880 total time=   7.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.874 total time=   2.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.876 total time=   3.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.875 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.874 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.875 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.875 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.7s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.886 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.879 total time=   2.5s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.876 total time=   1.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.878 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.875 total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.873 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.881 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.882 total time=   1.3s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.873 total time=   5.4s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.875 total time=   2.4s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.859 total time=   8.8s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.878 total time=   1.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.879 total time=   3.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.872 total time=   2.9s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.870 total time=   3.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.883 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.879 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.875 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.876 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.877 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.877 total time=   3.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.880 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.876 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.878 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.876 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.873 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.882 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.870 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.876 total time=   2.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.879 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.881 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.875 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.880 total time=   4.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.887 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.884 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.872 total time=   2.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.873 total time=   7.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.882 total time=   3.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.866 total time=   9.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.877 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.880 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.875 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.877 total time=   1.1s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.878 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.880 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.878 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.876 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.875 total time=   3.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.868 total time=   4.8s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.882 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.880 total time=   1.7s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.8s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.879 total time=   1.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.878 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.869 total time=   2.7s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.873 total time=   2.7s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.5s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   4.9s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.878 total time=   2.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.874 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.880 total time=   2.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   1.6s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.861 total time=   3.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.875 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.886 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.884 total time=   4.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.876 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.881 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.879 total time=   5.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.882 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.882 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.868 total time=   6.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.875 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.880 total time=   4.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.880 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.881 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.875 total time=   6.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.884 total time=   2.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.867 total time=   7.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.875 total time=   2.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.872 total time=   3.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.874 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.881 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.875 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.885 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.886 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.882 total time=   2.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.875 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.876 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.877 total time=   2.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.875 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.884 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.873 total time=   3.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.879 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.869 total time=   1.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.881 total time=   0.3s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.877 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.877 total time=   5.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.875 total time=   2.2s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.869 total time=   7.4s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.878 total time=   1.9s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.883 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.882 total time=   1.3s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.1s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.875 total time=   5.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.874 total time=   1.7s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   2.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.857 total time=   6.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.874 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.882 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.877 total time=   3.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.876 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.885 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.886 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.877 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.885 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.878 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.878 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.875 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.883 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.874 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.874 total time=   2.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.885 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.880 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.875 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.875 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.886 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.878 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.881 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.877 total time=   5.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.881 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.883 total time=   1.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.883 total time=   5.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.876 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   7.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.875 total time=   2.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.879 total time=   3.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.885 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.885 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.885 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.885 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.887 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.886 total time=   2.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.875 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.876 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   6.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.878 total time=   2.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.879 total time=   0.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.881 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.877 total time=   0.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.878 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.881 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.872 total time=   3.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.875 total time=   1.2s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.871 total time=   1.8s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.885 total time=   0.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.882 total time=   2.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.3s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.878 total time=   2.4s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.878 total time=   2.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.862 total time=   8.6s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.886 total time=   1.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.881 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.878 total time=   1.9s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.883 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.878 total time=   1.3s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.879 total time=   3.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.874 total time=   3.0s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.869 total time=   3.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.875 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.884 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.881 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.885 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.884 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.884 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.880 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.877 total time=   3.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.883 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.878 total time=   3.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.884 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.880 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.877 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.872 total time=   6.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.879 total time=   5.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.886 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.886 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.874 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.874 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.879 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.878 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.871 total time=   7.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.883 total time=   2.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.857 total time=  11.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.887 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.888 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.886 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.880 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.874 total time=   2.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.880 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.887 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.880 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.877 total time=   1.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.886 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.879 total time=   1.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.881 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.879 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.880 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.884 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.876 total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.876 total time=   2.3s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.878 total time=   2.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   6.6s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.874 total time=   2.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.857 total time=   8.0s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.7s\n",
      "[CV 1/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.879 total time=   1.0s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.884 total time=   1.7s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.876 total time=   2.0s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   1.5s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.876 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.845 total time=   7.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.887 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.880 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.883 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.876 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.876 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.884 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.886 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.878 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.884 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.878 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.884 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.883 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.871 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.872 total time=   4.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.873 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.869 total time=   1.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.870 total time=   6.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.880 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.887 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.888 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.885 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.874 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.881 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.881 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.871 total time=   7.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.878 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.875 total time=   3.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.858 total time=  12.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.878 total time=   4.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.886 total time=   2.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.880 total time=   7.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.871 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.881 total time=   2.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.871 total time=   1.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.877 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.881 total time=   0.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.885 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.885 total time=   2.2s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.876 total time=   2.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.880 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.875 total time=   1.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.879 total time=   2.7s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.886 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.882 total time=   5.2s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.878 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.876 total time=   2.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.870 total time=   5.5s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.881 total time=   0.4s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.882 total time=   1.0s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.879 total time=   1.3s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.878 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.887 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.886 total time=   1.2s\n",
      "[CV 3/5] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.883 total time=   1.4s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.880 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.882 total time=   1.1s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.879 total time=   1.5s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.862 total time=   7.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.883 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.884 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.875 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.879 total time=   3.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.877 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.875 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   4.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.874 total time=   1.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.871 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.877 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.877 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.875 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.875 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.880 total time=   2.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.883 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.881 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.881 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.878 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.875 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.887 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.877 total time=   5.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.880 total time=   1.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.874 total time=   5.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.881 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.871 total time=   2.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.871 total time=   7.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.884 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.861 total time=   3.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.848 total time=   9.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.885 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.880 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.880 total time=   5.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.879 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.879 total time=   3.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.887 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.881 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.878 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.886 total time=   1.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.877 total time=   3.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.883 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.880 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.886 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.884 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.886 total time=   1.3s\n",
      "[CV 1/5] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.876 total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.886 total time=   0.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.879 total time=   1.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.876 total time=   2.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.860 total time=   5.0s\n",
      "[CV 5/5] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.879 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.884 total time=   1.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.882 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.878 total time=   1.0s\n",
      "[CV 3/5] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.879 total time=   3.4s\n",
      "[CV 1/5] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.868 total time=   3.7s\n",
      "[CV 4/5] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.856 total time=  11.8s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.873 total time=   2.4s\n",
      "[CV 5/5] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.871 total time=   6.4s\n",
      "[CV 4/5] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.861 total time=   5.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.878 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.879 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.879 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.874 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.881 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.877 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.876 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.877 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.874 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.869 total time=   4.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.878 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.882 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.874 total time=   4.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.880 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.876 total time=   2.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.886 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.879 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.874 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.877 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.882 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.880 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.878 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.878 total time=   4.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.886 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.881 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.877 total time=   4.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.887 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.873 total time=   7.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.865 total time=   2.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.875 total time=   4.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.879 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.878 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.883 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.879 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.880 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.881 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.878 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.877 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.881 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.880 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.885 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.882 total time=   4.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.880 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.872 total time=   7.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.877 total time=   2.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.885 total time=   2.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.883 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.879 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.879 total time=   1.7s\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'colsample_bytree': 1, 'max_depth': 5, 'n_estimators': 25, 'subsample': 1}\n",
      "0.8818542105115561\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.864\n",
      "mean_squared_errors: 0.134\n",
      "mean_absolute_errors: 0.219\n",
      "r2_score: 0.864\n",
      "acc: 0.601\n",
      "acc(+-0.5mm): 0.966\n"
     ]
    }
   ],
   "source": [
    "# age (일단위)\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.01, 0.03, 0.05, 0.07] #, #[0.01, 0.03, 0.05],\n",
    "                'max_depth': [3, 4, 5, 7],#[3,4,5],\n",
    "                'n_estimators': [25, 50, 75, 100, 300],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                'subsample': [0.5, 0.8, 1], #[0.5, 0.8, 1],\n",
    "                'colsample_bytree': [0.5, 0.8, 1], #[0.8, 1],\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=xgb.sklearn.XGBRegressor(),\n",
    "                n_jobs=-1,\n",
    "                verbose=3,\n",
    "                param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train, y_train)\n",
    "model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = gs.predict(x_test).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test)\n",
    "acc3 = np.mean((y_pred >= y_test-0.5) & (y_pred <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25839b01-4685-45e4-bed2-ac5e16ba8fc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:28:12.934052Z",
     "iopub.status.busy": "2023-02-17T05:28:12.933554Z",
     "iopub.status.idle": "2023-02-17T05:28:12.947264Z",
     "shell.execute_reply": "2023-02-17T05:28:12.946591Z",
     "shell.execute_reply.started": "2023-02-17T05:28:12.934003Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "odir_f = f'acc1-{acc1:.3f}_acc3-{acc3:.3f}_XGBR_{nfold}fold'\n",
    "odir = f'result/{odir_f}'\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "model.save_model(f'{odir}/model.model')\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "pickle.dump(param_dict, open(f'{odir}/param_dict', 'wb'))\n",
    "f = open(f'{odir}/result.txt', 'w')\n",
    "f.write(f'classification model')\n",
    "f.write(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "f.write(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5f512-6bc7-4a09-a1c1-ee7d08f2ce1f",
   "metadata": {},
   "source": [
    "### feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1edbf70d-655f-450e-bfee-dc89d708cf0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T07:42:14.378513Z",
     "iopub.status.busy": "2023-02-18T07:42:14.377936Z",
     "iopub.status.idle": "2023-02-18T07:42:14.398483Z",
     "shell.execute_reply": "2023-02-18T07:42:14.397717Z",
     "shell.execute_reply.started": "2023-02-18T07:42:14.378455Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of features = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:782: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>0.002337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>0.025891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>0.056897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cuffed</th>\n",
       "      <td>0.098848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.816028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             imp\n",
       "sex     0.002337\n",
       "height  0.025891\n",
       "weight  0.056897\n",
       "cuffed  0.098848\n",
       "age     0.816028"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgbr에 내장된 함수 이용\n",
    "xgbr = xgb.XGBRegressor()\n",
    "xgbr.load_model('result/size/acc1-0.601_acc3-0.966_XGBR_10fold/model.model')\n",
    "\n",
    "index_list = ['age', 'sex', 'weight', 'height', 'cuffed'] # xgbr.get_booster().feature_names\n",
    "df_imp = pd.DataFrame({'imp':xgbr.feature_importances_}, index = index_list)\n",
    "df_imp = df_imp[df_imp.imp > 0].sort_values('imp').copy()\n",
    "\n",
    "feat_num = df_imp.shape[0]\n",
    "print(\"total number of features =\", feat_num)\n",
    "df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12cfa05b-11fc-44c2-82a7-b499e470936e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T15:43:09.580639Z",
     "iopub.status.busy": "2023-02-18T15:43:09.580052Z",
     "iopub.status.idle": "2023-02-18T15:43:09.588886Z",
     "shell.execute_reply": "2023-02-18T15:43:09.587840Z",
     "shell.execute_reply.started": "2023-02-18T15:43:09.580579Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/painstudy/dl_ETT',\n",
       " '/home/painstudy/anaconda3/envs/keras/lib/python38.zip',\n",
       " '/home/painstudy/anaconda3/envs/keras/lib/python3.8',\n",
       " '/home/painstudy/anaconda3/envs/keras/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/painstudy/.local/lib/python3.8/site-packages',\n",
       " '/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages',\n",
       " '/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/home/painstudy/.ipython',\n",
       " '/home/painstudy/anaconda3/lib/python3.9/site-packages']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ccbdccd-3116-4c0f-8614-4e9cf621d5ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T04:14:52.850557Z",
     "iopub.status.busy": "2023-02-19T04:14:52.849971Z",
     "iopub.status.idle": "2023-02-19T04:14:52.856798Z",
     "shell.execute_reply": "2023-02-19T04:14:52.855564Z",
     "shell.execute_reply.started": "2023-02-19T04:14:52.850499Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# 모듈 yaml이 설치된 path를 지정\n",
    "sys.path.append('/home/painstudy/anaconda3/lib/python3.9/site-packages')\n",
    "sys.path.append('/home/painstudy/anaconda3/envs/keras/bin/python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a0c80f0-d8f5-4e46-a014-342cec3ac492",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T04:14:55.332153Z",
     "iopub.status.busy": "2023-02-19T04:14:55.331675Z",
     "iopub.status.idle": "2023-02-19T04:14:55.751407Z",
     "shell.execute_reply": "2023-02-19T04:14:55.750605Z",
     "shell.execute_reply.started": "2023-02-19T04:14:55.332099Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba could not be imported.\nIf you are seeing this message and are undertaking Numba development work, you may need to re-run:\n\npython setup.py build_ext --inplace\n\n(Also, please check the development set up guide https://numba.pydata.org/numba-doc/latest/developer/contributing.html.)\n\nIf you are not working on Numba development:\n\nPlease report the error message and traceback, along with a minimal reproducer\nat: https://github.com/numba/numba/issues/new\n\nIf more help is needed please feel free to speak to the Numba core developers\ndirectly at: https://gitter.im/numba/numba\n\nThanks in advance for your help in improving Numba!\n\nThe original error was: 'cannot import name '_typeconv' from 'numba.core.typeconv' (/home/painstudy/anaconda3/lib/python3.9/site-packages/numba/core/typeconv/__init__.py)'\n--------------------------------------------------------------------------------\nIf possible please include the following in your error report:\n\nsys.executable: /home/painstudy/anaconda3/envs/keras/bin/python\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numba/core/typeconv/typeconv.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Numba, if it fails to import, provide some feedback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_typeconv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_typeconv' from 'numba.core.typeconv' (/home/painstudy/anaconda3/lib/python3.9/site-packages/numba/core/typeconv/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_909053/1320998010.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# feature selection using BorutaShap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mBorutaShap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBorutaShap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_BS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/BorutaShap.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/shap/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"As of version 0.29.0 shap only supports Python 3 (not 2)!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_explanation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplanation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCohorts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# explainers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/shap/_explanation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mslicer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSlicer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAlias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mObj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# from ._order import Order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_general\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDimensionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/shap/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_clustering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhclust_ordering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_tree_shuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_minimization_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhclust\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_general\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapproximate_interactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpotential_interactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_isinstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massert_import\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_import_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_general\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshapley_coefficients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordinal_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpChain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuppress_stderr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_show_progress\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_masked_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaskedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/shap/utils/_clustering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numba/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Re-export typeof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m from numba.misc.special import (\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mtypeof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpndindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgdb_breakpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgdb_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mliterally\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mliteral_unroll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numba/misc/special.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeof\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypeof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumbatype\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mas_numba_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numba/core/typing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from .templates import (signature, make_concrete_template, Signature,\n\u001b[1;32m      3\u001b[0m                         fold_arguments)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numba/core/typing/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtemplates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtypeof\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypeof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPurpose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numba/core/typeconv/rules.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtypeconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTypeManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeCastingRules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numba/core/typeconv/typeconv.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m            \u001b[0;34m\"possible please include the following in your error report:\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m            \"sys.executable: %s\\n\")\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreportme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcastgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Numba could not be imported.\nIf you are seeing this message and are undertaking Numba development work, you may need to re-run:\n\npython setup.py build_ext --inplace\n\n(Also, please check the development set up guide https://numba.pydata.org/numba-doc/latest/developer/contributing.html.)\n\nIf you are not working on Numba development:\n\nPlease report the error message and traceback, along with a minimal reproducer\nat: https://github.com/numba/numba/issues/new\n\nIf more help is needed please feel free to speak to the Numba core developers\ndirectly at: https://gitter.im/numba/numba\n\nThanks in advance for your help in improving Numba!\n\nThe original error was: 'cannot import name '_typeconv' from 'numba.core.typeconv' (/home/painstudy/anaconda3/lib/python3.9/site-packages/numba/core/typeconv/__init__.py)'\n--------------------------------------------------------------------------------\nIf possible please include the following in your error report:\n\nsys.executable: /home/painstudy/anaconda3/envs/keras/bin/python\n"
     ]
    }
   ],
   "source": [
    "# feature selection using BorutaShap\n",
    "from BorutaShap import BorutaShap\n",
    "from sklearn.base import clone\n",
    "\n",
    "model_BS = clone(model)\n",
    "Feature_Selector = BorutaShap(model=xgbr, \n",
    "                              importance_measure='shap', \n",
    "                              classification=False, \n",
    "                              percentile=100, \n",
    "                              pvalue=0.05)\n",
    "\n",
    "Feature_Selector.fit(X=X_train, \n",
    "                     y=y_train, \n",
    "                     n_trials=100, \n",
    "                     sample=False, \n",
    "                     train_or_test = 'train', \n",
    "                     normalize=True, \n",
    "                     verbose=False, \n",
    "                     random_state=SEED)\n",
    "\n",
    "# boruta plot\n",
    "Feature_Selector.plot(X_size=15,\n",
    "                       which_features='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09a696-19bf-475e-84f7-05e7368d65d9",
   "metadata": {},
   "source": [
    "### < 1 years (new born + infant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8a8d4cb-6eb1-4a50-bf6c-e4c76016f0c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T04:21:17.337345Z",
     "iopub.status.busy": "2023-02-19T04:21:17.336751Z",
     "iopub.status.idle": "2023-02-19T04:21:17.345864Z",
     "shell.execute_reply": "2023-02-19T04:21:17.344783Z",
     "shell.execute_reply.started": "2023-02-19T04:21:17.337288Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inf_mask = x_train[:,0] < 1\n",
    "x_train_inf = x_train[inf_mask]\n",
    "y_train_inf = y_train[inf_mask]\n",
    "\n",
    "inf_mask = x_test[:,0] < 1\n",
    "x_test_inf = x_test[inf_mask]\n",
    "y_test_inf = y_test[inf_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7531f7ff-aa6f-455d-b7b4-ee99fab7eb0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T04:21:51.072983Z",
     "iopub.status.busy": "2023-02-19T04:21:51.072392Z",
     "iopub.status.idle": "2023-02-19T04:22:20.942934Z",
     "shell.execute_reply": "2023-02-19T04:22:20.942043Z",
     "shell.execute_reply.started": "2023-02-19T04:21:51.072926Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 180 candidates, totalling 1800 fits\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'colsample_bytree': 0.8, 'max_depth': 3, 'n_estimators': 25, 'subsample': 0.8}\n",
      "0.586124199643337\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.504\n",
      "mean_squared_errors: 0.122\n",
      "mean_absolute_errors: 0.196\n",
      "r2_score: 0.502\n",
      "acc: 0.642\n",
      "acc(+-0.5mm): 0.975\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.583 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.560 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.595 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.563 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.563 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.552 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.579 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.556 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.568 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.580 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.555 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.541 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.565 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.548 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.568 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.513 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.550 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.540 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.500 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.524 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.514 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.507 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.447 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.499 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.488 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.438 total time=   2.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.578 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.548 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.489 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.594 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.536 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.498 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.533 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.564 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.611 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.548 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.579 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.557 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.514 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.598 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.550 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.547 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.582 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.462 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.488 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.606 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.581 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.585 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.587 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.569 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.587 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.564 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.562 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.611 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.615 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.563 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.581 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.580 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.554 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.571 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.519 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.539 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.529 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.456 total time=   2.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.561 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.610 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.500 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.574 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.489 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.486 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.482 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.503 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.547 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.602 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.583 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.538 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.587 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.596 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.536 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.538 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.521 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.524 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.581 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.587 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.554 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.495 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.528 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.572 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.504 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.480 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.552 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.585 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.524 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.557 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.443 total time=   1.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.549 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.499 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.446 total time=   2.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.590 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.587 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.608 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.611 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.593 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.600 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.602 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.575 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.571 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.494 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.498 total time=   2.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.493 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.460 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.583 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.590 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.554 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.560 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.569 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.491 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.582 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.560 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.487 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.532 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.398 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.560 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.544 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.567 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.563 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.513 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.577 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.572 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.503 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.524 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.553 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.484 total time=   2.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.594 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.589 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.550 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.577 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.606 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.557 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.541 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.551 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.543 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.562 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.518 total time=   1.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.542 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.531 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.578 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.587 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.561 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.559 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.607 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.586 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.560 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.585 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.601 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.551 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.564 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.486 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.446 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.482 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.414 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.585 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.579 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.565 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.583 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.525 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.590 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.527 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.478 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.451 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.491 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.612 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.559 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.594 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.582 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.591 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.594 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.542 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.613 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.607 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.609 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.561 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.530 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.532 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.524 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.565 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.589 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.554 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.584 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.533 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.519 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.493 total time=   1.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.509 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.520 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.621 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.527 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.601 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.591 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.548 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.597 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.549 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.548 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.517 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.569 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.525 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.446 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.611 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.562 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.577 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.557 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.527 total time=   1.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.553 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.464 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.542 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.519 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.441 total time=   2.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.550 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.583 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.588 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.586 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.569 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.582 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.590 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.587 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.585 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.596 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.594 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.558 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.547 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.511 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.560 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.504 total time=   1.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.539 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.519 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.590 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.536 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.578 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.548 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.507 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.570 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.556 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.554 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.584 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.615 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.582 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.559 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.519 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.492 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.533 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.599 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.566 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.567 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.559 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.564 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.608 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.606 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.618 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.568 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.559 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.584 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.551 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.533 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.512 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.586 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.438 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.400 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.560 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.550 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.566 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.566 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.547 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.555 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.598 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.587 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.575 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.518 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.533 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.499 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.499 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.564 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.477 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.501 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.528 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.489 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.533 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.487 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.607 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.578 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.555 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.590 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.593 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.543 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.589 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.549 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.539 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.583 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.532 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.493 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.549 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.614 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.605 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.534 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.538 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.537 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.528 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.535 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.592 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.562 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.546 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.480 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.597 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.474 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.420 total time=   1.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.441 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.442 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.478 total time=   2.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.562 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.571 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.580 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.571 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.551 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.559 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.520 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.583 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.537 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.562 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.458 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.483 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.563 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.564 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.597 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.499 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.591 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.514 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.578 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.578 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.445 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.550 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.532 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.587 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.559 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.558 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.559 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.613 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.594 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.571 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.561 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.513 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.585 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.555 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.474 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.589 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.467 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.552 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.524 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.443 total time=   2.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.588 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.553 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.562 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.564 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.547 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.557 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.562 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.540 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.543 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.606 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.607 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.575 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.580 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.564 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.534 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.514 total time=   1.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.560 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.518 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.609 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.614 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.581 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.539 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.575 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.531 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.523 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.558 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.519 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.593 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.615 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.574 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.495 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.473 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.528 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.456 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.591 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.581 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.553 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.555 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.538 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.591 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.521 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.519 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.580 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.529 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.615 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.625 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.622 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.603 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.618 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.615 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.600 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.557 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.582 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.575 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.548 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.562 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.520 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.526 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.518 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.483 total time=   2.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.455 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.502 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.586 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.553 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.602 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.562 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.556 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.573 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.535 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.493 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.584 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.447 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.591 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.587 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.531 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.573 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.515 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.577 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.538 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.506 total time=   1.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.521 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.492 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.488 total time=   2.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.509 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.567 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.548 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.477 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.568 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.562 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.576 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.525 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.572 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.517 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.501 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.525 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.388 total time=   2.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.613 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.578 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.572 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.552 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.535 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.619 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.582 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.480 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.551 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.556 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.539 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.599 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.596 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.582 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.556 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.623 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.561 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.518 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.571 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.604 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.557 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.603 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.561 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.494 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.471 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.476 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.579 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.569 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.561 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.552 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.604 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.563 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.578 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.591 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.591 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.604 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.596 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.578 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.501 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.554 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.512 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.552 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.534 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.534 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.542 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.474 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.544 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.516 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.385 total time=   2.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.615 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.597 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.522 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.561 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.603 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.436 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.503 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.543 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.483 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.585 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.450 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.455 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.537 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.544 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.599 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.454 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.575 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.567 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.504 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.503 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.523 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.452 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.557 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.548 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.573 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.555 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.565 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.552 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.559 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.576 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.578 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.500 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.551 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.476 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.386 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.555 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.602 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.507 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.589 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.546 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.465 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.562 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.575 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.580 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.557 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.601 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.519 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.562 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.516 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.532 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.619 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.592 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.585 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.584 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.587 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.590 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.614 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.597 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.556 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.539 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.528 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.526 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.579 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.536 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.482 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.477 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.517 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.456 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.552 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.555 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.566 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.533 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.569 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.531 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.571 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.584 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.602 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.514 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.587 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.577 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.369 total time=   2.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.557 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.558 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.555 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.561 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.599 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.551 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.512 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.545 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.583 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.523 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.533 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.367 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.554 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.530 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.562 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.582 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.563 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.541 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.545 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.547 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.550 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.590 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.601 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.524 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.538 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.510 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.335 total time=   2.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.591 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.575 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.597 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.528 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.535 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.561 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.485 total time=   2.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.481 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.524 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.379 total time=   2.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.528 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.605 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.565 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.561 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.539 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.576 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.587 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.581 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.572 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.549 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.532 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.564 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.510 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.569 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.461 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.594 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.615 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.553 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.594 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.608 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.557 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.561 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.595 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.555 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.548 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.588 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.587 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.577 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.600 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.576 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.522 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.521 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.540 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.550 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.508 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.564 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.500 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.326 total time=   2.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.537 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.494 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.542 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.431 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.587 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.592 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.515 total time=   2.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.579 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.476 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.447 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.575 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.555 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.558 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.592 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.564 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.558 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.559 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.525 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.583 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.539 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.493 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.536 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.610 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.574 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.575 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.557 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.540 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.529 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.457 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.452 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.565 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.603 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.614 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.610 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.605 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.603 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.598 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.586 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.593 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.604 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.537 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.521 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.525 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.514 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.542 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.586 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.587 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.589 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.550 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.548 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.461 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.424 total time=   2.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.598 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.539 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.576 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.564 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.535 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.461 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.492 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.498 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.512 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.484 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.564 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.558 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.602 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.558 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.591 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.522 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.552 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.533 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.597 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.486 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.469 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.574 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.519 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.543 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.592 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.519 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.564 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.485 total time=   1.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.586 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.496 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.497 total time=   2.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.556 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.547 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.559 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.556 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.594 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.601 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.597 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.526 total time=   1.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.572 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.539 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.376 total time=   2.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.523 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.464 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.423 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.552 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.558 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.573 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.568 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.586 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.561 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.568 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.553 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.602 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.544 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.546 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.589 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.559 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.550 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.529 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.484 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.560 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.555 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.604 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.618 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.540 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.610 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.587 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.584 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.576 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.582 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.610 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.578 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.590 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.555 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.500 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.496 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.560 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.564 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.539 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.524 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.531 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.366 total time=   2.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.562 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.509 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.533 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.587 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.604 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.587 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.582 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.534 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.586 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.553 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.556 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.442 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.459 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.542 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.477 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.561 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.541 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.586 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.627 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.608 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.561 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.558 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.588 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.586 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.589 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.613 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.565 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.587 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.579 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.585 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.585 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.579 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.578 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.570 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.518 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.581 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.555 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.415 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.546 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.572 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.592 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.527 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.490 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.537 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.495 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.537 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.557 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.548 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.560 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.556 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.559 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.563 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.556 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.597 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.563 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.555 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.565 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.545 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.550 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.499 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.556 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.559 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.530 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.492 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.504 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.509 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.592 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.560 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.553 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.581 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.589 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.583 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.580 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.589 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.536 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.581 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.581 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.611 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.503 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.529 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.519 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.447 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.574 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.530 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.569 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.542 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.530 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.536 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.453 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.555 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.450 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.424 total time=   2.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.583 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.582 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.592 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.594 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.568 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.588 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.580 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.588 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.538 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.602 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.597 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.585 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.574 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.479 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.555 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.512 total time=   1.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.579 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.360 total time=   2.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.540 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.557 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.585 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.531 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.536 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.503 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.528 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.511 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.551 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.588 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.608 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.604 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.603 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.622 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.558 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.600 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.606 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.572 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.586 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.587 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.463 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.562 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.566 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.582 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.542 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.547 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.493 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.572 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.522 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.392 total time=   2.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.580 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.538 total time=   1.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.536 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.517 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.549 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.565 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.521 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.552 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.567 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.583 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.533 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.557 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.533 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.305 total time=   2.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.550 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.582 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.590 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.561 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.618 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.569 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.553 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.601 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.604 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.553 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.524 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.517 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.505 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.599 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.547 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.545 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.560 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.593 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.596 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.583 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.610 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.601 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.580 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.593 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.584 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.525 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.533 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.546 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.515 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.524 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.512 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.505 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.567 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.572 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.559 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.464 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.544 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.444 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.607 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.569 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.560 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.523 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.536 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.531 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.546 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.581 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.596 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.595 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.556 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.488 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.519 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.337 total time=   2.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.537 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.586 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.624 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.525 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.581 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.514 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.610 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.544 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.555 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.558 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.602 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.596 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.555 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.596 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.567 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.540 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.602 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.532 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.562 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.500 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.530 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.409 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.551 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.585 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.590 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.595 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.568 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.553 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.562 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.572 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.577 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.581 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.561 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.553 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.543 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.552 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.580 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.557 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.599 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.608 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.561 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.534 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.496 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.437 total time=   2.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.557 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.532 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.586 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.456 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.584 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.575 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.561 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.577 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.598 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.581 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.532 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.542 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.522 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.349 total time=   2.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.598 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.609 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.606 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.539 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.547 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.544 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.568 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.573 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.554 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.437 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.534 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.530 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.506 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.534 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.537 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.562 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.516 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.562 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.478 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.532 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.522 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.542 total time=   2.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.578 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.558 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.545 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.602 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.595 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.579 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.503 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.564 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.539 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.523 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.542 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.517 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.411 total time=   2.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.615 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.602 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.561 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.603 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.584 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.619 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.574 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.591 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.569 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.597 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.622 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.619 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.599 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.621 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.570 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.530 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.593 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.617 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.618 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.532 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.600 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.551 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.511 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.558 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.536 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.567 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.505 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.567 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.596 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.523 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.575 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.591 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.413 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.527 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.426 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.425 total time=   2.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.607 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.601 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.620 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.620 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.530 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.558 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.594 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.476 total time=   1.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.541 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.591 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.439 total time=   2.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.554 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.436 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.393 total time=   2.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.602 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.539 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.582 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.583 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.588 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.576 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.529 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.578 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.497 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.544 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.526 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.462 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.556 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.595 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.554 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.453 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.559 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.520 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.529 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.509 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.505 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.464 total time=   2.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.484 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.571 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.545 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.444 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.567 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.463 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.590 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.506 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.435 total time=   2.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.599 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.555 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.535 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.548 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.530 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.527 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.541 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.553 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.571 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.556 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.599 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.459 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.568 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.501 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.463 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.574 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.501 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.574 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.520 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.556 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.496 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.508 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.514 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.495 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.602 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.606 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.605 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.599 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.561 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.607 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.562 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.606 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.608 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.553 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.519 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.597 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.540 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.503 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.529 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.449 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.564 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.481 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.357 total time=   2.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.592 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.531 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.528 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.615 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.542 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.595 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.515 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.517 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.540 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.558 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.505 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.584 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.547 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.609 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.599 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.547 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.530 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.530 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.520 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.511 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.510 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.584 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.617 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.550 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.534 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.527 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.546 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.524 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.524 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.541 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.522 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.504 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.593 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.572 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.526 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.552 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.416 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.443 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.437 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.380 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.593 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.572 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.499 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.584 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.539 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.554 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.554 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.505 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.546 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.524 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.584 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.558 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.627 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.612 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.598 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.564 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.551 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.558 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.550 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.612 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.505 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.500 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.564 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.590 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.540 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.555 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.557 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.574 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.493 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.561 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.500 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.554 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.589 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.586 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.585 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.611 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.607 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.610 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.604 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.597 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.600 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.565 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.494 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.600 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.553 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.559 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.604 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.576 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.600 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.499 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.508 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.525 total time=   1.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.550 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.450 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.469 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.593 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.586 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.600 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.558 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.601 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.586 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.605 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.597 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.600 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.556 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.560 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.578 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.613 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.611 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.559 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.557 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.558 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.576 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.553 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.573 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.522 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.574 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.573 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.612 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.628 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.564 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.546 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.564 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.563 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.542 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.593 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.512 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.580 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.553 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.525 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.579 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.578 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.525 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.540 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.520 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.548 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.535 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.612 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.612 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.593 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.589 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.545 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.558 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.565 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.552 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.527 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.585 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.595 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.480 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.576 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.462 total time=   2.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.548 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.460 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.437 total time=   2.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.561 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.596 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.536 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.597 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.580 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.585 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.563 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.438 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.563 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.487 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.420 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.605 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.591 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.539 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.569 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.484 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.596 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.429 total time=   1.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.510 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.410 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.379 total time=   2.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.539 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.531 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.541 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.544 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.513 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.526 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.538 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.481 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.551 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.553 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.460 total time=   2.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.561 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.456 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.423 total time=   2.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.554 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.600 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.542 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.532 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.546 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.484 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.572 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.554 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.498 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.478 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.507 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.504 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.609 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.614 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.560 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.558 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.577 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.604 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.558 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.574 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.568 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.556 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.582 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.529 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.569 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.516 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.533 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.563 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.517 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.528 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.466 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.562 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.546 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.410 total time=   2.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.528 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.540 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.510 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.556 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.527 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.546 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.458 total time=   2.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.537 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.462 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.468 total time=   2.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.609 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.563 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.619 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.588 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.592 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.588 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.588 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.589 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.585 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.549 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.585 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.584 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.561 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.543 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.558 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.518 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.582 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.545 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.574 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.415 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.562 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.559 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.604 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.554 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.530 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.527 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.570 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.402 total time=   1.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.560 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.472 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.428 total time=   2.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.586 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.607 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.584 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.590 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.576 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.585 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.588 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.552 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.522 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.549 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.456 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.487 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.512 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.456 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.557 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.574 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.583 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.506 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.585 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.522 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.467 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.539 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.535 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.500 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.595 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.556 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.599 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.503 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.546 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.496 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.464 total time=   1.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.540 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.510 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.551 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.594 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.573 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.547 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.563 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.590 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.581 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.589 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.588 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.586 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.564 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.603 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.501 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.518 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.520 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.434 total time=   2.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.517 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.422 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.387 total time=   2.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.587 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.579 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.611 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.577 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.597 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.560 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.587 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.590 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.574 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.532 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.590 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.507 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.530 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.516 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.428 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.566 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.608 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.594 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.554 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.590 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.607 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.583 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.569 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.569 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.593 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.604 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.604 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.559 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.604 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.543 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.586 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.589 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.567 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.576 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.532 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.541 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.535 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.374 total time=   2.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.519 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.483 total time=   1.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.551 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.583 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.480 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.533 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.434 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.468 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.443 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.462 total time=   2.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.563 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.585 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.567 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.584 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.564 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.617 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.611 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.606 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.605 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.576 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.562 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.540 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.570 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.561 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.451 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.608 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.560 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.558 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.613 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.555 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.599 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.542 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.512 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.521 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.551 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.560 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.604 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.534 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.539 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.569 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.558 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.560 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.562 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.564 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.493 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.514 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.515 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.381 total time=   2.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.577 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.544 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.531 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.519 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.555 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.561 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.593 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.520 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.541 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.550 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.501 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.525 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.460 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.472 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.428 total time=   2.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.598 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.623 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.590 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.602 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.612 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.603 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.596 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.620 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.553 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.501 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.588 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.543 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.533 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.541 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.422 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.617 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.593 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.585 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.563 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.568 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.535 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.583 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.599 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.598 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.541 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.551 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.585 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.557 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.568 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.547 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.560 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.572 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.540 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.529 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.453 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.558 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.605 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.598 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.487 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.529 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.484 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.505 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.508 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.447 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.590 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.584 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.554 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.532 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.592 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.536 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.537 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.619 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.561 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.551 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.555 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.558 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.587 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.599 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.531 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.571 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.553 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.595 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.602 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.583 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.553 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.590 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.584 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.531 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.584 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.577 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.482 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.542 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.575 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.511 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.566 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.545 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.538 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.612 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.589 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.593 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.583 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.588 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.536 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.606 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.554 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.531 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.556 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.484 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.551 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.497 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.435 total time=   2.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.595 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.619 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.604 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.601 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.604 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.612 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.553 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.545 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.608 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.581 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.559 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.560 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.545 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.566 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.580 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.541 total time=   1.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.417 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.454 total time=   2.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.616 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.560 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.599 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.552 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.556 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.553 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.539 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.552 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.551 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.566 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.590 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.562 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.583 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.569 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.612 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.564 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.530 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.540 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.561 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.364 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.607 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.601 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.576 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.513 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.596 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.525 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.528 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.558 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.548 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.563 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.494 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.550 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.510 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.566 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.577 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.592 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.589 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.593 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.573 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.588 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.570 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.567 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.580 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.555 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.586 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.559 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.576 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.582 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.540 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.491 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.507 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.488 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.500 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.502 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.467 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.474 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.459 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.546 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.452 total time=   2.3s\n"
     ]
    }
   ],
   "source": [
    "# age (일단위)\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.01, 0.03, 0.05, 0.07] #, #[0.01, 0.03, 0.05],\n",
    "                'max_depth': [3, 4, 5, 7],#[3,4,5],\n",
    "                'n_estimators': [25, 50, 75, 100, 300],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                'subsample': [0.5, 0.8, 1], #[0.5, 0.8, 1],\n",
    "                'colsample_bytree': [0.5, 0.8, 1], #[0.8, 1],\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=xgb.sklearn.XGBRegressor(),\n",
    "                n_jobs=-1,\n",
    "                verbose=3,\n",
    "                param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train_inf, y_train_inf)\n",
    "model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = gs.predict(x_test_inf).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test_inf, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test_inf, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test_inf, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test_inf, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test_inf)\n",
    "acc3 = np.mean((y_pred >= y_test_inf-0.5) & (y_pred <= y_test_inf+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c432bf26-d3bd-458d-a5ed-45d151d5be8f",
   "metadata": {},
   "source": [
    "### 1 <= age <= 6 years (Toddler, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49ac5650-1964-4c9f-9cc9-c6da9bf04492",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T04:41:06.927120Z",
     "iopub.status.busy": "2023-02-19T04:41:06.926542Z",
     "iopub.status.idle": "2023-02-19T04:41:06.935451Z",
     "shell.execute_reply": "2023-02-19T04:41:06.934548Z",
     "shell.execute_reply.started": "2023-02-19T04:41:06.927064Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "age_mask = (1 <= x_train[:,0]) & (x_train[:,0] < 7)\n",
    "x_train_age = x_train[age_mask]\n",
    "y_train_age = y_train[age_mask]\n",
    "\n",
    "age_mask = (1 <= x_test[:,0]) & (x_test[:,0] < 7)\n",
    "x_test_age = x_test[age_mask]\n",
    "y_test_age = y_test[age_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc99db07-8167-46d5-939e-96b2a20b59f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T04:41:08.451328Z",
     "iopub.status.busy": "2023-02-19T04:41:08.450855Z",
     "iopub.status.idle": "2023-02-19T04:42:03.023345Z",
     "shell.execute_reply": "2023-02-19T04:42:03.022669Z",
     "shell.execute_reply.started": "2023-02-19T04:41:08.451276Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 180 candidates, totalling 1800 fits\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.723 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.688 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.696 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.698 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.690 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.697 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.697 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.696 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.694 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.678 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.707 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.716 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.693 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.737 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.721 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.721 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.731 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.706 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.667 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.700 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.657 total time=   2.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.708 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.708 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.697 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.717 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.715 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.719 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.721 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.711 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.706 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.733 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.736 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.705 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.742 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.716 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.700 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.698 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.673 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.661 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.723 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.714 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.712 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.710 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.689 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.680 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.674 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.678 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.680 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.677 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.695 total time=   2.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.698 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.678 total time=   3.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.699 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.721 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.678 total time=   4.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.678 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.671 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.635 total time=   4.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.700 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.697 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.694 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.698 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.696 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.726 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.744 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.722 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.741 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.698 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.743 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.743 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.744 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.708 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.689 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.702 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.687 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.706 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.692 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.706 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.706 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.706 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.697 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.693 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.705 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.699 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.704 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.696 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.694 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.694 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.689 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.687 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.669 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.703 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.652 total time=   3.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.694 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.714 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.701 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.702 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.702 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.705 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.693 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.687 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.648 total time=   2.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.744 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.698 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.708 total time=   3.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.686 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.656 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.704 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.704 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.706 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.707 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.703 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.710 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.705 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.696 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.747 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.720 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.739 total time=   2.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.712 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.690 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.703 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.700 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.707 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.684 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.719 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.710 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.714 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.716 total time=   3.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.666 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.721 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.636 total time=   4.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.724 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.723 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.714 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.745 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.715 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.717 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.712 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.712 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.706 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.742 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.699 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.704 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.706 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.694 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.706 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.701 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.702 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.745 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.738 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.660 total time=   1.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.672 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.688 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.675 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.683 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.675 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.663 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.675 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.656 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.666 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.678 total time=   2.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.691 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.686 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.712 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.698 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.695 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.725 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.704 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.723 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.706 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.711 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.710 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.697 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.707 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.709 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.699 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.693 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.689 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.689 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.683 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.699 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.728 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.676 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.690 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.687 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.690 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.683 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.681 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.712 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.712 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.746 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.699 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.695 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.714 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.706 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.650 total time=   3.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.709 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.682 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.630 total time=   4.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.700 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.668 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.665 total time=   5.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.705 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.706 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.696 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.702 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.678 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.718 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.714 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.718 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.717 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.742 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.743 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.717 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.739 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.698 total time=   2.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.708 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.723 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.697 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.711 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.699 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.692 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.693 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.674 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.672 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.676 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.679 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.689 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.667 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.677 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.659 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.695 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.680 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.690 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.689 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.689 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.714 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.726 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.706 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.710 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.699 total time=   2.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.698 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.699 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.668 total time=   2.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.741 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.707 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.712 total time=   3.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.695 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.574 total time=   5.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.711 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.674 total time=   2.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.694 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.718 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.703 total time=   3.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.703 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.701 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.698 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.701 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.683 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.662 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.699 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.691 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.699 total time=   1.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.668 total time=   4.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.699 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.702 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.700 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.705 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.690 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.700 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.695 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.723 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.703 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.710 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.678 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.714 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.701 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.720 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.741 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.740 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.731 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.738 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.693 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.697 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.728 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.724 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.726 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.725 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.751 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.749 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.718 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.679 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.748 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.745 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.744 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.724 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.686 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.692 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.683 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.686 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.693 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.702 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.671 total time=   1.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.685 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.736 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.712 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.713 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.719 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.700 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.701 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.687 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.690 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.746 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.751 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.725 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.718 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.712 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.723 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.717 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.690 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.698 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.715 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.707 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.655 total time=   3.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.692 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.692 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.674 total time=   3.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.664 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.689 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.710 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.711 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.714 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.716 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.721 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.749 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.717 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.720 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.715 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.712 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.682 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.749 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.707 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.658 total time=   3.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.697 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.638 total time=   4.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.697 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.668 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.650 total time=   5.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.719 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.745 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.724 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.724 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.721 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.722 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.719 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.746 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.709 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.711 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.732 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.701 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.654 total time=   2.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.687 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.683 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.688 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.722 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.694 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.745 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.696 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.700 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.724 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.707 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.703 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.693 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.691 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.687 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.706 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.698 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.696 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.696 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.698 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.692 total time=   2.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.680 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.602 total time=   3.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.699 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.693 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.700 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.699 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.709 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.697 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.706 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.711 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.695 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.703 total time=   2.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.703 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.658 total time=   3.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.694 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.652 total time=   1.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.651 total time=   5.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.723 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.725 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.719 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.727 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.722 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.743 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.700 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.680 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.668 total time=   3.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.705 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.737 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.707 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.712 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.703 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.690 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.673 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.689 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.689 total time=   1.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.708 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.712 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.695 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.689 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.699 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.721 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.681 total time=   2.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.714 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.660 total time=   2.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.695 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.695 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.717 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.715 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.717 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.718 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.714 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.723 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.718 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.714 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.716 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.719 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.715 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.694 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.688 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.686 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.678 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.694 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.683 total time=   2.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.738 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.693 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.661 total time=   3.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.688 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.708 total time=   2.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.686 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.688 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.702 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.699 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.706 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.697 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.719 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.678 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.701 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.669 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.671 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.673 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.653 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.663 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.708 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.681 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.749 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.751 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.725 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.720 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.724 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.726 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.718 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.721 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.725 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.746 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.705 total time=   2.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.694 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.691 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.668 total time=   3.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.675 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.678 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.677 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.677 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.719 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.657 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.697 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.653 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.649 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.635 total time=   3.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.706 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.717 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.723 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.706 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.716 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.706 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.706 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.705 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.691 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.681 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.686 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.736 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.698 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.669 total time=   2.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.680 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.624 total time=   3.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.710 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.693 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.700 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.697 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.723 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.720 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.715 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.706 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.650 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.712 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.702 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.679 total time=   2.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.705 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.698 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.698 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.707 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.709 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.707 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.700 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.710 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.696 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.722 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.721 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.713 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.690 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.681 total time=   2.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.706 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.689 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.677 total time=   3.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.687 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.681 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.714 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.724 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.727 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.721 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.714 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.726 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.745 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.713 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.712 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.708 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.705 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.709 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.712 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.720 total time=   3.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.687 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.683 total time=   4.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.684 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.630 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.614 total time=   5.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.688 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.681 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.687 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.683 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.707 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.711 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.707 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.708 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.708 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.707 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.715 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.713 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.709 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.704 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.705 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.671 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.711 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.692 total time=   2.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.699 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.652 total time=   3.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.722 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.745 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.714 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.712 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.689 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.723 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.723 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.703 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.715 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.702 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.681 total time=   1.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.689 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.705 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.679 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.680 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.690 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.674 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.681 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.673 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.619 total time=   3.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.747 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.717 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.747 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.748 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.751 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.744 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.744 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.740 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.720 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.697 total time=   2.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.671 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.686 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.695 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.721 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.720 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.700 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.681 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.699 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.727 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.696 total time=   4.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.708 total time=   2.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.676 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.674 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.709 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.711 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.710 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.706 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.707 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.698 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.727 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.693 total time=   3.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.676 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.630 total time=   5.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.707 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.719 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.712 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.681 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.694 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.720 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.685 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.689 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.716 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.737 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.721 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.721 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.709 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.707 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.724 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.690 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.721 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.724 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.725 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.722 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.722 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.717 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.717 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.694 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.697 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.677 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.724 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.716 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.745 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.747 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.719 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.722 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.716 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.706 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.700 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.722 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.724 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.720 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.704 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.687 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.692 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.674 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.708 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.669 total time=   3.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.712 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.711 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.680 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.680 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.679 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.677 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.676 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.675 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.669 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.657 total time=   2.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.689 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.646 total time=   3.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.667 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.702 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.618 total time=   5.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.673 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.656 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.708 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.713 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.742 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.744 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.723 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.692 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.690 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.679 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.680 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.655 total time=   3.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.694 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.590 total time=   4.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.716 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.700 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.701 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.694 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.686 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.693 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.691 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.687 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.696 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.694 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.696 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.687 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.710 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.721 total time=   2.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.708 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.647 total time=   3.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.708 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.714 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.708 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.726 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.711 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.705 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.715 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.718 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.724 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.707 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.717 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.700 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.701 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.705 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.721 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.717 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.712 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.712 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.706 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.742 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.714 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.718 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.691 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.681 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.686 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.675 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.669 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.676 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.702 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.647 total time=   3.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.708 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.676 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.682 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.723 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.743 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.745 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.749 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.722 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.715 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.702 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.668 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.667 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.687 total time=   3.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.733 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.616 total time=   5.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.705 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.699 total time=   2.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.746 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.744 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.693 total time=   3.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.710 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.691 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.686 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.704 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.700 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.688 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.675 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.697 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.619 total time=   5.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.681 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.717 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.724 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.732 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.690 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.673 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.660 total time=   1.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.672 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.672 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.675 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.683 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.689 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.667 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.677 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.659 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.662 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.705 total time=   2.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.714 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.684 total time=   1.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.707 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.724 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.685 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.672 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.721 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.697 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.706 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.686 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.717 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.708 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.711 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.704 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.681 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.689 total time=   2.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.708 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.679 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.646 total time=   3.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.685 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.708 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.735 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.691 total time=   2.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.698 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.650 total time=   3.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.680 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.640 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.605 total time=   5.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.723 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.720 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.725 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.715 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.714 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.708 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.712 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.703 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.693 total time=   3.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.715 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.707 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.705 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.737 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.711 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.721 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.694 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.666 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.687 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.670 total time=   3.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.691 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.677 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.686 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.710 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.706 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.723 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.720 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.702 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.665 total time=   2.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.681 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.731 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.680 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.680 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.682 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.680 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.679 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.680 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.676 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.719 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.710 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.722 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.716 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.709 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.705 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.723 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.704 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.720 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.745 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.705 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.723 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.721 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.683 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.715 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.722 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.695 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.723 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.741 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.718 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.715 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.713 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.709 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.704 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.705 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.705 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.703 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.694 total time=   2.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.699 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.652 total time=   3.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.719 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.712 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.714 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.690 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.689 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.682 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.688 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.680 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.705 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.696 total time=   2.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.705 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.711 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.688 total time=   3.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.677 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.666 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.699 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.702 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.702 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.700 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.700 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.701 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.699 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.704 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.708 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.704 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.688 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.684 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.720 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.697 total time=   3.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.710 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.679 total time=   4.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.643 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.646 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.612 total time=   4.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.697 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.698 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.708 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.741 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.693 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.686 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.664 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.705 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.654 total time=   2.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.707 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.652 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.633 total time=   3.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.714 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.703 total time=   2.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.713 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.705 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.715 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.719 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.714 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.682 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.711 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.710 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.712 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.688 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.679 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.705 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.721 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.708 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.715 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.705 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.710 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.721 total time=   2.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.734 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.634 total time=   3.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.698 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.745 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.722 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.726 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.711 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.705 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.715 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.707 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.709 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.703 total time=   2.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.715 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.641 total time=   3.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.741 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.698 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.637 total time=   5.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.714 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.716 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.714 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.711 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.707 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.709 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.710 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.673 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.690 total time=   3.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.669 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.701 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.734 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.715 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.692 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.693 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.726 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.716 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.669 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.726 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.709 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.715 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.685 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.715 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.738 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.706 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.719 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.688 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.693 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.693 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.693 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.687 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.701 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.677 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.700 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.678 total time=   2.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.689 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.667 total time=   1.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.671 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.689 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.692 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.696 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.706 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.686 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.690 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.746 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.698 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.723 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.681 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.748 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.702 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.707 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.711 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.696 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.714 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.701 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.707 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.711 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.714 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.701 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.707 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.700 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.730 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.711 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.680 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.682 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.680 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.679 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.680 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.676 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.688 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.697 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.687 total time=   1.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.707 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.673 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.681 total time=   2.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.740 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.659 total time=   3.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.703 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.666 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.635 total time=   5.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.711 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.710 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.710 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.727 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.713 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.717 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.705 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.692 total time=   3.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.742 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.664 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.633 total time=   4.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.681 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.678 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.686 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.690 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.749 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.703 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.683 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.668 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.729 total time=   1.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.674 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.688 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.676 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.679 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.675 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.663 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.675 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.656 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.666 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.646 total time=   3.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.685 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.708 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.709 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.723 total time=   2.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.697 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.711 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.700 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.709 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.707 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.711 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.707 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.708 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.708 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.707 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.681 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.682 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.680 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.678 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.677 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.671 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.703 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.655 total time=   2.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.666 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.602 total time=   3.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.678 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.677 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.682 total time=   2.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.682 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.705 total time=   2.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.703 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.703 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.671 total time=   3.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.628 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.624 total time=   5.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.676 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.679 total time=   2.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.677 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.705 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.673 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.680 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.681 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.668 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.675 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.695 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.701 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.684 total time=   3.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.727 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.610 total time=   5.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.716 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.689 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.722 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.711 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.712 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.706 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.671 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.703 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.693 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.679 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.706 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.705 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.692 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.712 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.687 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.664 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.698 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.702 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.701 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.697 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.700 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.700 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.696 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.749 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.679 total time=   2.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.724 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.699 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.671 total time=   2.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.709 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.707 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.689 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.725 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.721 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.698 total time=   1.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.703 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.707 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.707 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.705 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.707 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.703 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.704 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.698 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.690 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.717 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.715 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.750 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.744 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.708 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.690 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.711 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.725 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.689 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.720 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.726 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.728 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.712 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.711 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.703 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.687 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.673 total time=   2.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.711 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.656 total time=   3.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.709 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.665 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.630 total time=   5.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.690 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.677 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.680 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.681 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.674 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.679 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.678 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.704 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.683 total time=   3.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.717 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.672 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.671 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.669 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.669 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.689 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.661 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.680 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.664 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.647 total time=   3.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.745 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.702 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.707 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.702 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.745 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.715 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.703 total time=   1.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.690 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.637 total time=   2.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.714 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.714 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.661 total time=   3.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.740 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.734 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.720 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.686 total time=   2.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.744 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.745 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.704 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.702 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.697 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.689 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.698 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.714 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.715 total time=   2.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.697 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.677 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.707 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.702 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.701 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.697 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.722 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.749 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.718 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.677 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.714 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.703 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.744 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.705 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.700 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.706 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.702 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.698 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.694 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.684 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.684 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.683 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.703 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.700 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.672 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.678 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.688 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.623 total time=   5.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.684 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.703 total time=   2.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.717 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.709 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.731 total time=   3.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.685 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.631 total time=   4.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.712 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.644 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.617 total time=   5.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.680 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.688 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.678 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.678 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.686 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.692 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.683 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.686 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.679 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.677 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.679 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.686 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.721 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.720 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.719 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.705 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.703 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.676 total time=   2.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.683 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.602 total time=   3.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.747 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.676 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.713 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.690 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.669 total time=   2.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.705 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.745 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.695 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.685 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.679 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.682 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.732 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.709 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.716 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.698 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.693 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.693 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.687 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.712 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.677 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.662 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.639 total time=   2.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.698 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.739 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.705 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.693 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.692 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.696 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.699 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.685 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.693 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.690 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.673 total time=   3.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.688 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.682 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.614 total time=   5.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.694 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.710 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.700 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.700 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.687 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.696 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.714 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.667 total time=   3.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.707 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.690 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.645 total time=   4.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.679 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.690 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.718 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.717 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.685 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.710 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.743 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.676 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.742 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.698 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.679 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.673 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.677 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.707 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.707 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.709 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.707 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.708 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.710 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.706 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.725 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.698 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.682 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.686 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.721 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.714 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.713 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.721 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.711 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.715 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.701 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.691 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.698 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.708 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.678 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.724 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.726 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.744 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.746 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.724 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.719 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.725 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.695 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.701 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.694 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.743 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.717 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.711 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.699 total time=   2.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.683 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.624 total time=   3.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.749 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.722 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.707 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.707 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.725 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.715 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.701 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.693 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.707 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.669 total time=   2.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.662 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.640 total time=   3.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.669 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.684 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.634 total time=   4.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.682 total time=   2.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.708 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.712 total time=   3.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.677 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.733 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.660 total time=   4.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.700 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.665 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.640 total time=   5.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.716 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.715 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.748 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.721 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.745 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.747 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.724 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.696 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.721 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.698 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.691 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.701 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.688 total time=   2.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.697 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.677 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.711 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.690 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.689 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.689 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.689 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.690 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.688 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.723 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.698 total time=   2.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.712 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.676 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.691 total time=   2.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.741 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.716 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.743 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.716 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.743 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.698 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.720 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.698 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.696 total time=   2.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.672 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.704 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.728 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.724 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.726 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.725 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.751 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.717 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.688 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.723 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.748 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.684 total time=   1.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.688 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.684 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.706 total time=   2.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.697 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.694 total time=   3.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.703 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.671 total time=   1.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.681 total time=   5.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.745 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.743 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.749 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.751 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.737 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.719 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.722 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.702 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.689 total time=   3.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.741 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.712 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.707 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.710 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.720 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.688 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.704 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.665 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.615 total time=   4.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.710 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.748 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.711 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.707 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.722 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.702 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.693 total time=   1.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.709 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.736 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.698 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.680 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.690 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.674 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.681 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.697 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.679 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.657 total time=   2.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.711 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.703 total time=   1.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.688 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.684 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.697 total time=   2.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.705 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.693 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.705 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.711 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.713 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.719 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.723 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.736 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.712 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.685 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.691 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.664 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.660 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.664 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.664 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.696 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.696 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.697 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.696 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.696 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.695 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.695 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.693 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.725 total time=   2.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.685 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.745 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.691 total time=   2.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.719 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.702 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.679 total time=   3.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.690 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.658 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.635 total time=   4.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.676 total time=   2.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.734 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.709 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.702 total time=   3.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.692 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.676 total time=   4.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.672 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.679 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.668 total time=   4.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.745 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.723 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.711 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.709 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.683 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.683 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.677 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.681 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.673 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.682 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.681 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.682 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.680 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.678 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.677 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.675 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.669 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.694 total time=   2.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.702 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.637 total time=   3.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.688 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.687 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.680 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.680 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.679 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.677 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.676 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.675 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.669 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.673 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.680 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.701 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.695 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.687 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.696 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.680 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.682 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.712 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.725 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.689 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.696 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.715 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.732 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.706 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.690 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.674 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.739 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.681 total time=   2.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.707 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.680 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.646 total time=   3.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.716 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.703 total time=   2.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.673 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.721 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.714 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.713 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.714 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.706 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.715 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.692 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.675 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.661 total time=   3.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.667 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.616 total time=   5.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.714 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.703 total time=   2.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.708 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.711 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.715 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.722 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.725 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.705 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.715 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.700 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.672 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.648 total time=   3.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.659 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.684 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.691 total time=   4.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.706 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.707 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.661 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.706 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.692 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.698 total time=   1.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.685 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.705 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.712 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.713 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.719 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.700 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.706 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.700 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.692 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.639 total time=   2.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.712 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.709 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.711 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.717 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.706 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.711 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.710 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.706 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.706 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.702 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.699 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.693 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.706 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.749 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.677 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.683 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.673 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.703 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.715 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.654 total time=   2.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.714 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.684 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.635 total time=   3.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.719 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.709 total time=   2.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.706 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.652 total time=   2.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.702 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.709 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.732 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.711 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.713 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.684 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.676 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.667 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.695 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.697 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.697 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.699 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.696 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.695 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.697 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.694 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.694 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.695 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.693 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.683 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.687 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.687 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.679 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.679 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.688 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.665 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.659 total time=   3.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.703 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.665 total time=   4.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.711 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.662 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.632 total time=   4.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.697 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.711 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.715 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.713 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.719 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.744 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.696 total time=   1.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.742 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.707 total time=   2.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.671 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.644 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.620 total time=   3.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.672 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.670 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.744 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.688 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.718 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.706 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.702 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.698 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.694 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.692 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.699 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.691 total time=   3.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.716 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.699 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.690 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.721 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.704 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.742 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.732 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.688 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.695 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.697 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.721 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.724 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.717 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.722 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.726 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.700 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.723 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.714 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.725 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.712 total time=   1.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.711 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.707 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.686 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.690 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.686 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.680 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.687 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.675 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.682 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.680 total time=   3.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.700 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.651 total time=   5.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.716 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.677 total time=   2.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.710 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.708 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.697 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.705 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.702 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.716 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.696 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.682 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.683 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.662 total time=   3.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.703 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.580 total time=   4.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.682 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.707 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.699 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.702 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.721 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.688 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.678 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.664 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.644 total time=   2.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.686 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.698 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.696 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.696 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.697 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.696 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.696 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.695 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.695 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.693 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.699 total time=   2.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.698 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.706 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.705 total time=   2.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.689 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.694 total time=   3.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.680 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.744 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.726 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.745 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.711 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.687 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.699 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.663 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.679 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.719 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.688 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.720 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.741 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.740 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.709 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.738 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.724 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.677 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.691 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.712 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.712 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.711 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.689 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.713 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.743 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.714 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.665 total time=   2.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.723 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.676 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.687 total time=   2.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.712 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.625 total time=   3.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.711 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.664 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.645 total time=   5.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.707 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.694 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.712 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.709 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.708 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.710 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.677 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.687 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.684 total time=   3.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.694 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.710 total time=   1.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.674 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.686 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.693 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.656 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.674 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.706 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.643 total time=   5.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.688 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.682 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.675 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.672 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.680 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.719 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.707 total time=   1.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.715 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.656 total time=   2.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.708 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.668 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.671 total time=   3.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.694 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.682 total time=   2.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.696 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.691 total time=   2.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.714 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.698 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.669 total time=   3.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.652 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.723 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.702 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.661 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.712 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.668 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.706 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.742 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.707 total time=   2.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.708 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.652 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.633 total time=   3.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.710 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.670 total time=   2.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.691 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.686 total time=   3.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.670 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.665 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.651 total time=   3.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.700 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.697 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.654 total time=   4.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.705 total time=   2.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.739 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.706 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.695 total time=   2.9s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.678 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.681 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.695 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.690 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.738 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.666 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.692 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.665 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.590 total time=   4.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.689 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.723 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.687 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.706 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.692 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.706 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.697 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.706 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.697 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.707 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.719 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.700 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.695 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.701 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.698 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.704 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.696 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.692 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.674 total time=   2.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.734 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.634 total time=   3.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.711 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.722 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.707 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.702 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.725 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.715 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.701 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.711 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.741 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.669 total time=   2.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.672 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.677 total time=   3.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.708 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.725 total time=   1.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.722 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.726 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.702 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.710 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.711 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.723 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.741 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.678 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.710 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.702 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.701 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.656 total time=   2.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.702 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.683 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.633 total time=   3.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.740 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.734 total time=   2.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.685 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.692 total time=   2.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.701 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.695 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.698 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.683 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.684 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.684 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.697 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.664 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.693 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.679 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.682 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.682 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.676 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.711 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.691 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.687 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.687 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.691 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.684 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.668 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.725 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.734 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.700 total time=   3.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.720 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.682 total time=   4.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.733 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.691 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.629 total time=   4.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.723 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.723 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.721 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.687 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.743 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.678 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.702 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.716 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.681 total time=   2.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.684 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.654 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.628 total time=   3.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.706 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.701 total time=   2.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.670 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.687 total time=   2.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.703 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.702 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.732 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.742 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.744 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.684 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.663 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.693 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.695 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.704 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.704 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.706 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.707 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.706 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.678 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.710 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.719 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.744 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.701 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.695 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.676 total time=   2.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.714 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.714 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.695 total time=   3.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.706 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.701 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.700 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.693 total time=   2.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.700 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.711 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.700 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.711 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.744 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.676 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.708 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.695 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.725 total time=   1.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.722 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.726 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.751 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.746 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.747 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.714 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.711 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.710 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.723 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.717 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.712 total time=   2.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.669 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.674 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.688 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.685 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.689 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.677 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.678 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.663 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.699 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.679 total time=   3.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.692 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.694 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.744 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.746 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.719 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.704 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.712 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.707 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.712 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.694 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.683 total time=   2.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.738 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.693 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.646 total time=   3.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.719 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.709 total time=   2.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.722 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.648 total time=   2.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.683 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.662 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.640 total time=   3.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.673 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.661 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.714 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.711 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.714 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.716 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.747 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.712 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.687 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.681 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.693 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.697 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.708 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.701 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.688 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.735 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.692 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.681 total time=   2.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.679 total time=   2.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.712 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.720 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.683 total time=   2.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.700 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.685 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.651 total time=   3.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.658 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.671 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.661 total time=   4.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.700 total time=   2.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.685 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.700 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.690 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.697 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.696 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.698 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.700 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.696 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.695 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.684 total time=   3.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.674 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.588 total time=   5.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.747 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.745 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.744 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.715 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.693 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.685 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.708 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.709 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.707 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.707 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.705 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.707 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.701 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.704 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.698 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.730 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.746 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.751 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.750 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.744 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.726 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.726 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.723 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.699 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.725 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.694 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.682 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.668 total time=   2.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.671 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.711 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.710 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.721 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.720 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.700 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.699 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.670 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.689 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.726 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.689 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.686 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.706 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.686 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.719 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.712 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.690 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.674 total time=   2.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.684 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.654 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.628 total time=   3.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.694 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.682 total time=   2.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.705 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.671 total time=   2.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.713 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.684 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.710 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.687 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.696 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.720 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.732 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.658 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.695 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.639 total time=   4.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.702 total time=   2.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.706 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.737 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.713 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.713 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.714 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.705 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.710 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.670 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.664 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.653 total time=   3.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.693 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.670 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.718 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.725 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.743 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.715 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.697 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.689 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.698 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.695 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.690 total time=   2.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.701 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.602 total time=   3.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.678 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.677 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.708 total time=   1.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.720 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.743 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.686 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.690 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.677 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.675 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.673 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.670 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.736 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.712 total time=   3.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.700 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.616 total time=   5.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.711 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.703 total time=   2.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.712 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.705 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.673 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.680 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.681 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.706 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.715 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.714 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.713 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.709 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.720 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.685 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.703 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.666 total time=   2.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.671 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.644 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.661 total time=   3.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.672 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.680 total time=   2.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.709 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.684 total time=   2.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.714 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.729 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.683 total time=   3.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.642 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.661 total time=   1.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.668 total time=   4.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.710 total time=   2.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.722 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.697 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.667 total time=   3.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.677 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.642 total time=   4.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.676 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.682 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.693 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.696 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.714 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.736 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.712 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.697 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.712 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.673 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.688 total time=   2.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.667 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.690 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.691 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.712 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.712 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.711 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.712 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.713 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.711 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.677 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.721 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.712 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.673 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.707 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.679 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.678 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.686 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.680 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.687 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.675 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.682 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.687 total time=   3.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.673 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.587 total time=   5.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.694 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.695 total time=   2.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.710 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.737 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.742 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.744 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.746 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.716 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.719 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.680 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.675 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.682 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.680 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.698 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.743 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.743 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.744 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.742 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.719 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.736 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.739 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.698 total time=   2.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.681 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.637 total time=   3.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.711 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.687 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.708 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.716 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.717 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.716 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.723 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.711 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.741 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.731 total time=   2.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.708 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.693 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.658 total time=   3.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.691 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.597 total time=   5.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.697 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.668 total time=   2.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.715 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.683 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.667 total time=   3.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.704 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.621 total time=   4.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.663 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.633 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.597 total time=   4.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.723 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.726 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.701 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.711 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.693 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.690 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.674 total time=   1.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.739 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.715 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.707 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.711 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.714 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.703 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.689 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.664 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.676 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.678 total time=   2.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.748 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.687 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.705 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.711 total time=   2.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.719 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.685 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.651 total time=   3.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.658 total time=   1.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.666 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.661 total time=   4.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.699 total time=   2.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.706 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.720 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.667 total time=   2.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.704 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.688 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.696 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.671 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.743 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.707 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.678 total time=   1.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.673 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.644 total time=   2.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.693 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.662 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.662 total time=   2.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.704 total time=   2.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.724 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.696 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.669 total time=   2.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.675 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.688 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.628 total time=   3.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.680 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.647 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.611 total time=   5.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.698 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.699 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.703 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.703 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.697 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.698 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.677 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.687 total time=   3.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.685 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.686 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.665 total time=   4.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.686 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.669 total time=   2.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.658 total time=   5.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.723 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.700 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.704 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.690 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.720 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.663 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.679 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.689 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.674 total time=   2.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.702 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.680 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.695 total time=   3.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.716 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.710 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.745 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.684 total time=   2.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.675 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.703 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.628 total time=   3.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.694 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.647 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.611 total time=   5.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.694 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.694 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.700 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.674 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.679 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.677 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.711 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.703 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.700 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.707 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.684 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.690 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.679 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.680 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.674 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.705 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.677 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.681 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.684 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.707 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.671 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.706 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.695 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.690 total time=   2.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.681 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.703 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.705 total time=   2.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.679 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.748 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.670 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.723 total time=   2.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.695 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.699 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.672 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.707 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.708 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.659 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.700 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.693 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.690 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.661 total time=   4.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.733 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.720 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.673 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.654 total time=   3.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.712 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.662 total time=   4.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.675 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.644 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.621 total time=   5.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.701 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.706 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.711 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.682 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.688 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.650 total time=   2.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.706 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.683 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.696 total time=   2.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.672 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.619 total time=   3.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.712 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.717 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.747 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.748 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.728 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.744 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.706 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.711 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.695 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.703 total time=   2.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.697 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.673 total time=   3.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.703 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.666 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.635 total time=   5.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.711 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.710 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.710 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.711 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.707 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.709 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.700 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.680 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.668 total time=   3.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.710 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.678 total time=   4.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.701 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.707 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.711 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.722 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.715 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.698 total time=   1.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.709 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.715 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.693 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.706 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.705 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.692 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.689 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.686 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.731 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.678 total time=   2.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.710 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.709 total time=   1.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.720 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.712 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.679 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.678 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.677 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.675 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.673 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.705 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.672 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.654 total time=   3.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.670 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.581 total time=   5.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.713 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.699 total time=   2.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.694 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.654 total time=   3.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.711 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.659 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.688 total time=   4.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.649 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.658 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.656 total time=   4.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.709 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.712 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.720 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.716 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.724 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.706 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.697 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.707 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.699 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.697 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.701 total time=   1.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.703 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.666 total time=   2.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.684 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.694 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.661 total time=   3.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.710 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.708 total time=   2.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.697 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.681 total time=   2.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.713 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.694 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.683 total time=   3.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.664 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.574 total time=   5.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.673 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.656 total time=   2.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.694 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.691 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.668 total time=   3.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.697 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.696 total time=   4.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.681 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.724 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.707 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.690 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.706 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.729 total time=   1.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.705 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.637 total time=   2.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.687 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.668 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.671 total time=   3.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.693 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.667 total time=   1.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.671 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.689 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.718 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.725 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.748 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.739 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.744 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.692 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.702 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.693 total time=   3.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.673 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.587 total time=   5.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.694 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.704 total time=   2.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.706 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.688 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.718 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.724 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.746 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.736 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.737 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.728 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.706 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.681 total time=   3.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.688 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.605 total time=   5.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.723 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.698 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.696 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.684 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.692 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.671 total time=   2.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.723 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.732 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.715 total time=   2.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.735 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.700 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.648 total time=   2.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.698 total time=   2.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.715 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.705 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.692 total time=   2.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.712 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.707 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.700 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.707 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.708 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.720 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.698 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.695 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.658 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.665 total time=   4.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.708 total time=   2.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.677 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.673 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.654 total time=   3.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.685 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.638 total time=   4.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.697 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.700 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.719 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.715 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.692 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.676 total time=   2.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.723 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.683 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.665 total time=   2.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.701 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.676 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.672 total time=   2.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.698 total time=   2.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.675 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.722 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.711 total time=   2.9s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.713 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.694 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.669 total time=   3.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.673 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.650 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.643 total time=   4.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.684 total time=   2.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.690 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.680 total time=   3.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.691 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.674 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.674 total time=   4.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.656 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.655 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.705 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.689 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.681 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.671 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.724 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.705 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.725 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.722 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.724 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.708 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.715 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.704 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.703 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.689 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.703 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.652 total time=   3.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.690 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.697 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.701 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.726 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.717 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.712 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.725 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.740 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.723 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.681 total time=   2.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.715 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.641 total time=   3.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.741 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.671 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.637 total time=   5.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.707 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.710 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.712 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.709 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.708 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.710 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.708 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.685 total time=   3.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.718 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.691 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.648 total time=   4.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.681 total time=   1.5s\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'colsample_bytree': 1, 'max_depth': 4, 'n_estimators': 25, 'subsample': 1}\n",
      "0.711013037921987\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.673\n",
      "mean_squared_errors: 0.141\n",
      "mean_absolute_errors: 0.226\n",
      "r2_score: 0.673\n",
      "acc: 0.592\n",
      "acc(+-0.5mm): 0.961\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.699 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.698 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.695 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.693 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.687 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.712 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.703 total time=   2.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.711 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.656 total time=   3.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.711 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.652 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.614 total time=   5.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.723 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.725 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.719 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.727 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.722 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.717 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.708 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.680 total time=   3.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.742 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.659 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.688 total time=   4.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.679 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.690 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.743 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.714 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.665 total time=   2.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.675 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.691 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.655 total time=   2.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.692 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.672 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.654 total time=   3.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.699 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.671 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.635 total time=   4.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.705 total time=   2.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.694 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.708 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.715 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.722 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.714 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.705 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.707 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.700 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.672 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.648 total time=   3.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.665 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.669 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.700 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.675 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.658 total time=   3.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.733 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.651 total time=   5.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.714 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.703 total time=   2.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.705 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.712 total time=   3.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.685 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.675 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.691 total time=   1.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.734 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.715 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.719 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.666 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.699 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.697 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.721 total time=   1.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.684 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.703 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.698 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.711 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.716 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.684 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.732 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.686 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.681 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.723 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.724 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.712 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.710 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.681 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.680 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.674 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.678 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.680 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.677 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.668 total time=   2.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.688 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.665 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.690 total time=   3.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.678 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.701 total time=   1.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.705 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.712 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.703 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.693 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.697 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.665 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.699 total time=   1.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.703 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.709 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.731 total time=   2.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.688 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.658 total time=   3.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.688 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.698 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.630 total time=   5.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.698 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.699 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.703 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.703 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.697 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.698 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.720 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.692 total time=   3.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.699 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.692 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.679 total time=   4.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.656 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.653 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.697 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.707 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.695 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.665 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.661 total time=   3.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.667 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.618 total time=   5.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.713 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.699 total time=   2.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.684 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.709 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.703 total time=   3.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.694 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.737 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.707 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.710 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.720 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.690 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.704 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.691 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.689 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.749 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.699 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.685 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.693 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.678 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.684 total time=   1.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.683 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.689 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.689 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.672 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.678 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.664 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.697 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.668 total time=   4.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.668 total time=   2.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.749 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.703 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.693 total time=   3.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.717 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.707 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.707 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.737 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.692 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.721 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.675 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.716 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.670 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.714 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.714 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.717 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.696 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.699 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.695 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.714 total time=   2.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.677 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.709 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.667 total time=   3.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.677 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.645 total time=   4.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.676 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.682 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.692 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.674 total time=   3.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.688 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.581 total time=   5.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.743 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.729 total time=   2.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.717 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.678 total time=   3.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.707 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.690 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.674 total time=   4.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.640 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.655 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.711 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.705 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.681 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.655 total time=   2.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.666 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.703 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.681 total time=   3.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.704 total time=   2.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.685 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.707 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.683 total time=   2.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.700 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.729 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.708 total time=   3.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.642 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.650 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.661 total time=   4.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.684 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.714 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.690 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.709 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.685 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.710 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.706 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.678 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.670 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.664 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.679 total time=   3.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.659 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.694 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.695 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.688 total time=   3.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.695 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.623 total time=   5.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.684 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.674 total time=   2.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.708 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.688 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.697 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.724 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.723 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.736 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.715 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.698 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.699 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.681 total time=   3.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.703 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.605 total time=   4.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.681 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.629 total time=   5.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.697 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.704 total time=   2.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.734 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.683 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.731 total time=   3.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.715 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.710 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.695 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.704 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.700 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.688 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.692 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.688 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.580 total time=   4.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.668 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.650 total time=   3.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.659 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.697 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.673 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.676 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.696 total time=   4.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.700 total time=   2.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.694 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.687 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.693 total time=   3.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.697 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.665 total time=   4.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.712 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.691 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.665 total time=   4.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.649 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.646 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.612 total time=   3.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.720 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.724 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.726 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.718 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.746 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.747 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.746 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.739 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.725 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.734 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.700 total time=   3.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.687 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.712 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.677 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.677 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.683 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.657 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.661 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.666 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.649 total time=   1.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.656 total time=   3.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.679 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.635 total time=   3.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.686 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.693 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.656 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.674 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.666 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.619 total time=   4.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.651 total time=   3.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.677 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.695 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.639 total time=   4.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.682 total time=   2.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.698 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.687 total time=   3.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.720 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.705 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.682 total time=   4.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.684 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.630 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.614 total time=   4.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.700 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.700 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.701 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.699 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.721 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.725 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.720 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.688 total time=   2.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.722 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.744 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.695 total time=   2.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.703 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.630 total time=   4.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.663 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.633 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.597 total time=   4.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.708 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.717 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.700 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.709 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.717 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.712 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.721 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.683 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.683 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.677 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.681 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.673 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.677 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.664 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.689 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.654 total time=   2.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.684 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.694 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.620 total time=   3.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.714 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.710 total time=   2.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.713 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.663 total time=   2.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.683 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.672 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.653 total time=   3.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.652 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.678 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.665 total time=   4.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.685 total time=   2.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.690 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.685 total time=   3.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.720 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.691 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.648 total time=   4.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.707 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.707 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.680 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.724 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.723 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.716 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.707 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.704 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.692 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.685 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.664 total time=   2.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.705 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.662 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.662 total time=   2.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.708 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.739 total time=   1.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.723 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.663 total time=   2.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.706 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.689 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.653 total time=   3.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.669 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.693 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.643 total time=   4.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.676 total time=   2.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.669 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.674 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.713 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.713 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.725 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.705 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.700 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.696 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.695 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.662 total time=   3.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.706 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.643 total time=   5.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.697 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.702 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.692 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.686 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.671 total time=   3.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.691 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.597 total time=   5.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.716 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.677 total time=   2.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.685 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.674 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.688 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.711 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.689 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.677 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.710 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.695 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.714 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.684 total time=   3.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.694 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.630 total time=   5.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.676 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.708 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.690 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.690 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.710 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.714 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.727 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.721 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.747 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.749 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.745 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.720 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.723 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.717 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.712 total time=   2.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.706 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.700 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.718 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.697 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.696 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.698 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.696 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.682 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.683 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.693 total time=   3.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.674 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.615 total time=   5.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.634 total time=   4.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.733 total time=   2.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.715 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.713 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.690 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.705 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.702 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.692 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.737 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.728 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.706 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.716 total time=   3.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.727 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.590 total time=   5.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.668 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.675 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.663 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.701 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.653 total time=   3.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.676 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.588 total time=   5.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.710 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.727 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.684 total time=   3.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.693 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.610 total time=   5.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.691 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.686 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.690 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.702 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.697 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.711 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.707 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.705 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.706 total time=   2.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.740 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.650 total time=   3.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.667 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.628 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.605 total time=   5.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.676 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.677 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.687 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.687 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.679 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.679 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.722 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.702 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.689 total time=   3.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.682 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.672 total time=   1.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.655 total time=   3.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.692 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.590 total time=   5.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.737 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.672 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.683 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.684 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.653 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.676 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.687 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.661 total time=   1.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.749 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.751 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.751 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.746 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.721 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.726 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.712 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.713 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.715 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.712 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.705 total time=   2.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.712 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.707 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.683 total time=   3.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.686 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.631 total time=   4.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.711 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.668 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.621 total time=   5.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.633 total time=   3.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.693 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.682 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.700 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.652 total time=   2.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.668 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.667 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.669 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.671 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.673 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.700 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.700 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.658 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.684 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.690 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.687 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.682 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.676 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.689 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.691 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.687 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.687 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.691 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.708 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.705 total time=   2.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.678 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.673 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.659 total time=   3.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.712 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.660 total time=   4.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.700 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.665 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.640 total time=   5.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.664 total time=   1.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.651 total time=   5.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.714 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.716 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.714 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.715 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.714 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.708 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.706 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.654 total time=   3.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.677 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.733 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.662 total time=   4.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.700 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.662 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.632 total time=   5.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.702 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.679 total time=   3.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.667 total time=   1.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.656 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.697 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.697 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.699 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.696 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.703 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.710 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.705 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.704 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.708 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.704 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.702 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.709 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.712 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.720 total time=   3.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.720 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.683 total time=   4.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.686 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.644 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.617 total time=   5.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.699 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.714 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.704 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.720 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.723 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.692 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.685 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.664 total time=   2.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.697 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.700 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.648 total time=   3.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.698 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.694 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.709 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.668 total time=   2.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.680 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.737 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.691 total time=   3.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.681 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.629 total time=   5.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.743 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.729 total time=   2.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.708 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.655 total time=   3.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.718 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.697 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.696 total time=   4.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.640 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.653 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.697 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.679 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.677 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.677 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.725 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.702 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.698 total time=   1.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.695 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.676 total time=   2.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.714 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.684 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.635 total time=   3.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.688 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.680 total time=   2.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.709 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.693 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.702 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.725 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.722 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.739 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.744 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.733 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.709 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.680 total time=   3.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.700 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.727 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.654 total time=   4.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.710 total time=   2.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.739 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.706 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.684 total time=   3.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.664 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.633 total time=   4.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.643 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.658 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.635 total time=   4.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.669 total time=   3.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.698 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.725 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.720 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.707 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.751 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.716 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.744 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.693 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.673 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.696 total time=   2.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.708 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.625 total time=   3.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.709 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.665 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.645 total time=   5.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.745 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.743 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.749 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.751 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.737 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.743 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.690 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.667 total time=   3.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.711 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.741 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.665 total time=   4.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.733 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.669 total time=   2.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.658 total time=   5.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.682 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.688 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.680 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.687 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.657 total time=   2.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.690 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.659 total time=   3.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.680 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.640 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.624 total time=   5.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.690 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.683 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.680 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.681 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.700 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.687 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.696 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.690 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.655 total time=   3.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.691 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.674 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.642 total time=   4.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.707 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.671 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.691 total time=   4.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.679 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.723 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.735 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.693 total time=   2.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.713 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.684 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.693 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.703 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.700 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.683 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.681 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.728 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.681 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.699 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.702 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.725 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.707 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.695 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.697 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.694 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.694 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.695 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.693 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.679 total time=   2.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.708 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.650 total time=   3.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.709 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.710 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.621 total time=   4.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.675 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.644 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.629 total time=   5.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.694 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.689 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.687 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.699 total time=   2.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.681 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.704 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.672 total time=   3.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.698 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.748 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.685 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.668 total time=   2.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.698 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.646 total time=   3.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.703 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.682 total time=   1.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.681 total time=   5.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.723 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.720 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.725 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.727 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.713 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.719 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.676 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.718 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.702 total time=   3.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.703 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.678 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.671 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.669 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.669 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.662 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.673 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.689 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.684 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.670 total time=   4.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.678 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.678 total time=   1.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.647 total time=   4.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.678 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.679 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.682 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.690 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.683 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.711 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.712 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.711 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.710 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.712 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.684 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.682 total time=   2.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.710 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.704 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.658 total time=   3.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.669 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.701 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.698 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.701 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.738 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.689 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.726 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.680 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.687 total time=   1.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.668 total time=   4.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.681 total time=   1.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.686 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.690 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.711 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.688 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.694 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.653 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.664 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.668 total time=   4.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.707 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.700 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.699 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.748 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.715 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.719 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.709 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.711 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.693 total time=   3.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.702 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.616 total time=   5.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.705 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.685 total time=   2.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.746 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.697 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.697 total time=   3.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.721 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.676 total time=   4.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.672 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.707 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.636 total time=   4.6s\n"
     ]
    }
   ],
   "source": [
    "# age (일단위)\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.01, 0.03, 0.05, 0.07] #, #[0.01, 0.03, 0.05],\n",
    "                'max_depth': [3, 4, 5, 7],#[3,4,5],\n",
    "                'n_estimators': [25, 50, 75, 100, 300],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                'subsample': [0.5, 0.8, 1], #[0.5, 0.8, 1],\n",
    "                'colsample_bytree': [0.5, 0.8, 1], #[0.8, 1],\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=xgb.sklearn.XGBRegressor(),\n",
    "                n_jobs=-1,\n",
    "                verbose=3,\n",
    "                param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train_age, y_train_age)\n",
    "model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = gs.predict(x_test_age).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test_age, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test_age, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test_age, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test_age, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test_age)\n",
    "acc3 = np.mean((y_pred >= y_test_age-0.5) & (y_pred <= y_test_age+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb5b7162-39cd-42d0-98be-28c34b9cf544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T04:56:10.426711Z",
     "iopub.status.busy": "2023-02-19T04:56:10.426128Z",
     "iopub.status.idle": "2023-02-19T04:56:10.441722Z",
     "shell.execute_reply": "2023-02-19T04:56:10.440824Z",
     "shell.execute_reply.started": "2023-02-19T04:56:10.426655Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "new model 1<=age<=6 years\n",
      "--------------\n",
      "explained_variance_score: 0.673\n",
      "mean_squared_errors: 0.141\n",
      "mean_absolute_errors: 0.226\n",
      "r2_score: 0.673\n",
      "acc: 0.592\n",
      "acc(+-0.5mm): 0.961\n"
     ]
    }
   ],
   "source": [
    "print('--------------')\n",
    "print('new model 1<=age<=6 years')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test_age, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test_age, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test_age, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test_age, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test_age)\n",
    "acc3 = np.mean((y_pred >= y_test_age-0.5) & (y_pred <= y_test_age+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd4c9ad-cf86-44eb-829b-4d3ffbde3020",
   "metadata": {},
   "source": [
    "### 6 < age < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea08b702-86a4-41cb-885d-7fe3b6750659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T05:00:23.181468Z",
     "iopub.status.busy": "2023-02-19T05:00:23.180882Z",
     "iopub.status.idle": "2023-02-19T05:00:23.189509Z",
     "shell.execute_reply": "2023-02-19T05:00:23.188423Z",
     "shell.execute_reply.started": "2023-02-19T05:00:23.181392Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "age_mask = (6 < x_train[:,0])\n",
    "x_train_age = x_train[age_mask]\n",
    "y_train_age = y_train[age_mask]\n",
    "\n",
    "age_mask = (6 < x_test[:,0])\n",
    "x_test_age = x_test[age_mask]\n",
    "y_test_age = y_test[age_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b7ed641-45d1-43c4-ac24-d91ff1793375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T05:00:31.026381Z",
     "iopub.status.busy": "2023-02-19T05:00:31.025816Z",
     "iopub.status.idle": "2023-02-19T05:00:59.354295Z",
     "shell.execute_reply": "2023-02-19T05:00:59.353515Z",
     "shell.execute_reply.started": "2023-02-19T05:00:31.026325Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 180 candidates, totalling 1800 fits\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'colsample_bytree': 0.5, 'max_depth': 3, 'n_estimators': 25, 'subsample': 1}\n",
      "0.36741212303996496\n",
      "========================================\n",
      "--------------\n",
      "new model 6<age<10\n",
      "--------------\n",
      "explained_variance_score: 0.305\n",
      "mean_squared_errors: 0.147\n",
      "mean_absolute_errors: 0.233\n",
      "r2_score: 0.304\n",
      "acc: 0.578\n",
      "acc(+-0.5mm): 0.962\n"
     ]
    }
   ],
   "source": [
    "# age (일단위)\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.01, 0.03, 0.05, 0.07] #, #[0.01, 0.03, 0.05],\n",
    "                'max_depth': [3, 4, 5, 7],#[3,4,5],\n",
    "                'n_estimators': [25, 50, 75, 100, 300],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                'subsample': [0.5, 0.8, 1], #[0.5, 0.8, 1],\n",
    "                'colsample_bytree': [0.5, 0.8, 1], #[0.8, 1],\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=xgb.sklearn.XGBRegressor(),\n",
    "                n_jobs=-1,\n",
    "                verbose=3,\n",
    "                param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train_age, y_train_age)\n",
    "model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = gs.predict(x_test_age).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "\n",
    "print('--------------')\n",
    "print('new model 6<age<10')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test_age, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test_age, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test_age, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test_age, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test_age)\n",
    "acc3 = np.mean((y_pred >= y_test_age-0.5) & (y_pred <= y_test_age+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc30e9b8-fc39-44ad-be2b-9b13f0ffe16b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T05:03:02.711837Z",
     "iopub.status.busy": "2023-02-19T05:03:02.711241Z",
     "iopub.status.idle": "2023-02-19T05:03:03.270615Z",
     "shell.execute_reply": "2023-02-19T05:03:03.269999Z",
     "shell.execute_reply.started": "2023-02-19T05:03:02.711773Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3905ef9c70>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.349 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.394 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.306 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.339 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.334 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.345 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.350 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.347 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.354 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.341 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.295 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.312 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.315 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.244 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.283 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.081 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.327 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.322 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.283 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.321 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.223 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.322 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.353 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.088 total time=   1.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.262 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.122 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.158 total time=   2.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.250 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.359 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.280 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.212 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.263 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.330 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.379 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.276 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.329 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.248 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.291 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.264 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.313 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.336 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.338 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.366 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.342 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.381 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.398 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.391 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.340 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.323 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.371 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.329 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.392 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.326 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.394 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.372 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.381 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.342 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.318 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.306 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.317 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.315 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.297 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.356 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.309 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.330 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.213 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.283 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.072 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.364 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.328 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.315 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.355 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.300 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.258 total time=   1.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.267 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.216 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.294 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.234 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.142 total time=   2.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.250 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.306 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.333 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.212 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.211 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.172 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.152 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.222 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.358 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.371 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.356 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.337 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.329 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.329 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.348 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.323 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.390 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.408 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.382 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.383 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.323 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.331 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.213 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.272 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.227 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.247 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.233 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.192 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.315 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.297 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.340 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.332 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.325 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.358 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.351 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.313 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.321 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.298 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.280 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.259 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.232 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.065 total time=   2.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.339 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.272 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.349 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.304 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.240 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.274 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.190 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.205 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.262 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.370 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.336 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.313 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.368 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.218 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.366 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.233 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.225 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.301 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.267 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.206 total time=   1.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.308 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.349 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.290 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.210 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.284 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.324 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.356 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.357 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.360 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.198 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.265 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.305 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.259 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.356 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.341 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.348 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.333 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.365 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.368 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.345 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.360 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.364 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.349 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.309 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.335 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.195 total time=   1.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.280 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.257 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.231 total time=   2.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.215 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.187 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.326 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.344 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.385 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.363 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.343 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.371 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.325 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.259 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.352 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.199 total time=   1.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.314 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.178 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.128 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.315 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.273 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.285 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.326 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.315 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.334 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.401 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.345 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.344 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.255 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.302 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.263 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.316 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.313 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.307 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.331 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.227 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.043 total time=   2.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.308 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.386 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.406 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.391 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.414 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.376 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.372 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.351 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.253 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.345 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.269 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.193 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.170 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.164 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.359 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.352 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.348 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.359 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.332 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.369 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.309 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.321 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.301 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.340 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.326 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.286 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.289 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.195 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.258 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.267 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.238 total time=   1.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.365 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.341 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.338 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.180 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.315 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.315 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.115 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.276 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.117 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.142 total time=   2.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.359 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.360 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.378 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.358 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.338 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.352 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.320 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.285 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.342 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.318 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.338 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.294 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.305 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.286 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.299 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.212 total time=   1.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.233 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.259 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.390 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.359 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.376 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.351 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.386 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.335 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.337 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.375 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.362 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.343 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.335 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.346 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.275 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.353 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.342 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.367 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.367 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.374 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.228 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.249 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.244 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.290 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.403 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.364 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.382 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.375 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.340 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.333 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.325 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.375 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.302 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.325 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.366 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.311 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.366 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.381 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.356 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.315 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.352 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.373 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.325 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.274 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.315 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.276 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.274 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.214 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.300 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.302 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.365 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.371 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.389 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.363 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.358 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.354 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.349 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.348 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.349 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.341 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.251 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.362 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.241 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.368 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.275 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.198 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.257 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.249 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.381 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.255 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.374 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.314 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.253 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.346 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.168 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.339 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.154 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.170 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.355 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.275 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.318 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.326 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.239 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.331 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.074 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.282 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.154 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.086 total time=   2.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.366 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.366 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.372 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.363 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.337 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.361 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.342 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.345 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.298 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.354 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.357 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.330 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.300 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.252 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.278 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.209 total time=   1.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.283 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.285 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.370 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.334 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.327 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.334 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.351 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.302 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.316 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.307 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.323 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.238 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.328 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.134 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.320 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.262 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.098 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.346 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.309 total time=   1.0s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.340 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.188 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.300 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.332 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.138 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.276 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.243 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.180 total time=   2.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.334 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.338 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.337 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.343 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.323 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.329 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.318 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.297 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.369 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.325 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.328 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.261 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.312 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.227 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.231 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.191 total time=   1.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.271 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.231 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.392 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.359 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.309 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.302 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.363 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.318 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.322 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.109 total time=   1.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.320 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.228 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.100 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.297 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.267 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.317 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.213 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.276 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.268 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.168 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.250 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.230 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.218 total time=   2.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.309 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.341 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.306 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.301 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.237 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.177 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.169 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.221 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.395 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.384 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.366 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.345 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.353 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.249 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.286 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.326 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.301 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.296 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.327 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.253 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.301 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.232 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.229 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.160 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.340 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.282 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.326 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.319 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.282 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.257 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.135 total time=   1.7s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.149 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.235 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.370 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.350 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.350 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.366 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.338 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.341 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.307 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.337 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.334 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.322 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.290 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.307 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.303 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.294 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.291 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.300 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.314 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.313 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.383 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.216 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.260 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.220 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.024 total time=   2.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.399 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.323 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.276 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.333 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.191 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.277 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.316 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.223 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.303 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.085 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.328 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.359 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.259 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.363 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.202 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.346 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.219 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.200 total time=   1.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.193 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.214 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.347 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.356 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.337 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.336 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.331 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.344 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.320 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.327 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.345 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.337 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.319 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.277 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.292 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.316 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.315 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.307 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.295 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.293 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.264 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.283 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.210 total time=   1.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.249 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.004 total time=   2.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.329 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.367 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.345 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.333 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.334 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.327 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.351 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.332 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.289 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.378 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.376 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.396 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.369 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.362 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.268 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.307 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.191 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.284 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.022 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.393 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.342 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.347 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.331 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.388 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.303 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.347 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.305 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.276 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.376 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.382 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.345 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.302 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.303 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.254 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.304 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.252 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.268 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.027 total time=   2.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.287 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.228 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.335 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.304 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.283 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.320 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.157 total time=   2.0s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.355 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.214 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.189 total time=   1.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.354 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.300 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.364 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.297 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.294 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.331 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.273 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.254 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.314 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.067 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.345 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.337 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.334 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.341 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.197 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.358 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.288 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.256 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.372 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.338 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.319 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.199 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.274 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.260 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.270 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.208 total time=   2.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.290 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.311 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.166 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.310 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.306 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.153 total time=   2.0s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.265 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.146 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.188 total time=   2.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.364 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.330 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.286 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.353 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.289 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.320 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.336 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.329 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.323 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.323 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.379 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.326 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.188 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.261 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.270 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.165 total time=   1.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.321 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.331 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.337 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.343 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.345 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.350 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.284 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.313 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.276 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.335 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.310 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.301 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.322 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.320 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.301 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.269 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.249 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.275 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.414 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.411 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.414 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.402 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.411 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.321 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.348 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.344 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.371 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.339 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.331 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.313 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.162 total time=   1.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.340 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.321 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.127 total time=   2.0s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.328 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.168 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.085 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.372 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.340 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.393 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.340 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.320 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.372 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.341 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.340 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.318 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.329 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.393 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.316 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.320 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.312 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.333 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.356 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.393 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.383 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.385 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.316 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.331 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.263 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.307 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.286 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.308 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=-0.034 total time=   1.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.367 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.376 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.368 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.352 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.344 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.334 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.357 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.296 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.330 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.319 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.334 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.325 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.301 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.316 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.269 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.299 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.241 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.245 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.028 total time=   2.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.348 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.241 total time=   1.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.355 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.296 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.203 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.336 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.081 total time=   2.0s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.269 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.142 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.098 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.393 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.330 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.283 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.365 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.265 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.317 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.297 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.207 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.205 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.258 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.219 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.396 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.403 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.398 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.419 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.306 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.351 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.290 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.243 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.351 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.268 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.156 total time=   1.7s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.276 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.103 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.152 total time=   2.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.340 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.350 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.356 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.365 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.330 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.340 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.303 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.264 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.319 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.327 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.077 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.308 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.201 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.153 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.339 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.341 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.298 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.307 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.245 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.344 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.313 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.292 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.326 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.348 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.348 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.281 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.278 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.262 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.230 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.377 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.395 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.364 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.350 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.355 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.382 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.363 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.372 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.258 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.333 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.268 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.261 total time=   1.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.288 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.313 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.260 total time=   1.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.328 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.070 total time=   2.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.325 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.213 total time=   1.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.324 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.259 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.226 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.281 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.275 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.308 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.347 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.344 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.262 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.248 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.197 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.220 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.156 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.331 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.337 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.282 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.348 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.365 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.339 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.356 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.382 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.346 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.339 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.350 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.253 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.267 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.331 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.105 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.289 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.362 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.335 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.363 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.356 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.341 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.304 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.292 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.315 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.301 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.313 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.339 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.286 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.314 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.314 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.276 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.206 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.232 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.231 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.154 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.284 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.353 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.305 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.203 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.225 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.134 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.212 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.210 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.181 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.365 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.369 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.286 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.359 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.205 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.322 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.316 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.253 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.183 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.260 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.144 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.334 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.364 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.318 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.360 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.342 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.323 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.316 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.331 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.282 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.278 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.184 total time=   1.7s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.188 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.242 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.156 total time=   2.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.290 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.301 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.297 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.272 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.349 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.312 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.273 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.342 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.311 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.202 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.283 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.242 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.228 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.218 total time=   1.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.342 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.388 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.327 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.357 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.343 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.327 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.336 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.296 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.298 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.289 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.351 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.222 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.355 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.236 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.071 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.337 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.288 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.341 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.380 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.343 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.332 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.314 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.292 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.355 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.225 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.308 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.179 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.251 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.039 total time=   2.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.319 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.251 total time=   1.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.368 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.279 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.242 total time=   1.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.296 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.314 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.284 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.361 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.294 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.317 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.229 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.198 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.092 total time=   2.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.340 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.382 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.303 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.350 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.356 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.256 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.291 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.336 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.326 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.348 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.378 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.281 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.297 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.260 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.271 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.378 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.353 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.347 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.328 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.341 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.316 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.309 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.385 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.267 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.330 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.288 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.238 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.308 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.313 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.284 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.301 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.285 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.245 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.333 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.243 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.280 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.205 total time=   2.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.289 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.333 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.332 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.350 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.350 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.371 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.271 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.342 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.293 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.291 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.255 total time=   1.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.220 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.072 total time=   2.7s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.367 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.344 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.377 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.355 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.368 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.382 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.342 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.352 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.375 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.326 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.277 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.329 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.323 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.303 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.314 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.338 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.280 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.342 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.211 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.211 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.341 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.374 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.350 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.315 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.337 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.349 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.318 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.327 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.264 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.348 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.272 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.230 total time=   1.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.291 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.276 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.269 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.307 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.329 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.253 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.283 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.255 total time=   0.6s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.183 total time=   0.8s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.371 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.380 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.378 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.364 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.353 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.352 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.331 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.354 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.342 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.314 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.271 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.314 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.319 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.329 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.336 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.339 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.286 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.340 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.299 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.296 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.178 total time=   1.8s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.241 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.055 total time=   2.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.347 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.299 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.336 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.293 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.286 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.308 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.217 total time=   1.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.316 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.244 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.139 total time=   1.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.342 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.305 total time=   1.0s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.298 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.337 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.258 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.330 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.106 total time=   1.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.342 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.135 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.154 total time=   2.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.327 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.373 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.372 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.345 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.275 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.355 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.282 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.336 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.352 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.333 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.331 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.288 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.299 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.279 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.262 total time=   0.7s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.245 total time=   1.8s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.299 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.074 total time=   2.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.339 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.382 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.381 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.351 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.351 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.330 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.376 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.295 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.367 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.318 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.345 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.353 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.290 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.323 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.353 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.274 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.254 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.316 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.284 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.366 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.408 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.413 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.382 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.379 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.355 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.367 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.403 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.252 total time=   1.1s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.404 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.331 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.242 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.317 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.171 total time=   1.7s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.293 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.204 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.123 total time=   2.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.341 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.340 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.349 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.354 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.313 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.334 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.369 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.282 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.337 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.366 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.344 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.319 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.329 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.303 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.276 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.192 total time=   1.8s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.225 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.012 total time=   2.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.375 total time=   1.0s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.267 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.352 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.292 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.289 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.335 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.225 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.305 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.171 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.139 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.309 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.227 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.284 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.185 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.338 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.224 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.153 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.218 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.273 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.163 total time=   2.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.272 total time=   1.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.314 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.323 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.334 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.377 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.407 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.327 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.332 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.289 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.317 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.233 total time=   1.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.288 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=-0.007 total time=   2.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.318 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.348 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.328 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.369 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.353 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.320 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.339 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.360 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.328 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.308 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.306 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.293 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.257 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.298 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.323 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.159 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.382 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.366 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.385 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.352 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.312 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.341 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.318 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.295 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.261 total time=   1.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.228 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.134 total time=   1.7s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.310 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.214 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.138 total time=   2.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.305 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.314 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.334 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.5;, score=0.329 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=0.8;, score=0.366 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=25, subsample=1;, score=0.355 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.5;, score=0.321 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.339 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.5;, score=0.265 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.256 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=1;, score=0.154 total time=   1.8s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=1;, score=0.328 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.010 total time=   2.7s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.347 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.279 total time=   0.7s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.333 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.330 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.290 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.310 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.303 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.308 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.309 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.316 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.284 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.269 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.260 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.250 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.181 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.365 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.312 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.390 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.201 total time=   1.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.325 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=1;, score=0.293 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.192 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.261 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.167 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.180 total time=   2.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.351 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.325 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.304 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.190 total time=   1.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.309 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.306 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.254 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.327 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.343 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.352 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.268 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.306 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.270 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.5;, score=0.101 total time=   2.8s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.344 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.328 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.287 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.308 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.231 total time=   0.9s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.369 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.290 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.230 total time=   1.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.207 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.135 total time=   1.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.334 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.375 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.326 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.314 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.327 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.365 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.351 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.368 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.307 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.283 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.279 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.243 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.291 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.354 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.349 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.212 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.255 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.122 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.217 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.184 total time=   2.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.315 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.298 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.293 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.236 total time=   1.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.302 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.144 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.306 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.195 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.128 total time=   2.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.5;, score=0.334 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.352 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.8;, score=0.313 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.345 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=1;, score=0.393 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.5;, score=0.330 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.336 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.380 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.288 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.383 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.333 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.293 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.344 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.273 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.276 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.306 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.318 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.253 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.337 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.272 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.291 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.369 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.348 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.344 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.327 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.389 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.359 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.344 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.353 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.274 total time=   1.0s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.362 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.277 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.249 total time=   1.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.259 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.125 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.348 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.199 total time=   0.8s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.201 total time=   2.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.273 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.329 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.282 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.170 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.295 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.139 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.276 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.161 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.079 total time=   2.6s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.355 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.359 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.306 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.326 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.320 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.297 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.338 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.304 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.296 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.306 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.319 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.252 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.314 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.282 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.321 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.329 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.380 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.365 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.344 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.331 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.348 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.306 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.331 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.288 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.326 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.264 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.257 total time=   1.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.290 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.273 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.294 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.292 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.305 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.264 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.321 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.158 total time=   0.6s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.220 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.319 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.319 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.323 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.308 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.315 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.410 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.299 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.309 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.314 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.305 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.306 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.344 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.165 total time=   1.6s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.312 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.270 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.152 total time=   2.0s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.278 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.143 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.084 total time=   2.6s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.320 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.320 total time=   0.7s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.315 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.296 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.292 total time=   0.8s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.335 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.338 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.322 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.367 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.375 total time=   0.1s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.324 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.282 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.245 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.208 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.351 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.318 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.318 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.326 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.304 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.390 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.340 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.342 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.275 total time=   1.0s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.296 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.273 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.194 total time=   1.4s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.329 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.5;, score=0.157 total time=   1.7s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=1;, score=0.332 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.5;, score=0.168 total time=   0.8s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.8;, score=0.160 total time=   2.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.319 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.327 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.326 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.391 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.353 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.323 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.386 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.294 total time=   0.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.233 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.260 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.121 total time=   2.0s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.300 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.165 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.155 total time=   2.6s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.367 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.374 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.346 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.360 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.367 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.338 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.315 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.359 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.8;, score=0.318 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.327 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.331 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.362 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.343 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.351 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.302 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.319 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.163 total time=   1.1s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.302 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.098 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.333 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.328 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=1;, score=0.296 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.328 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.272 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.339 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.362 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.371 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.290 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.335 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.279 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.271 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.185 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.255 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.020 total time=   2.5s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.374 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.280 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.296 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.292 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.261 total time=   1.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.314 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.5;, score=0.189 total time=   2.0s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=1;, score=0.276 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.5;, score=0.123 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=0.8;, score=0.125 total time=   2.6s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.356 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.373 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.270 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.330 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.247 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.342 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.272 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.162 total time=   1.1s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.301 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.187 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.159 total time=   1.5s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.303 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.280 total time=   1.0s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.357 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.5;, score=0.266 total time=   1.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=1;, score=0.377 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.283 total time=   0.6s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.227 total time=   1.7s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.225 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.113 total time=   2.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.342 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.281 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.329 total time=   0.3s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.287 total time=   0.5s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.247 total time=   1.6s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.303 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.285 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.321 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.320 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.334 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.246 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.292 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.246 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.213 total time=   0.9s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.131 total time=   2.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.349 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.380 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.325 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.317 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.372 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.336 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.5;, score=0.338 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=0.8;, score=0.354 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=25, subsample=1;, score=0.339 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.5;, score=0.272 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=0.8;, score=0.346 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.293 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.328 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.300 total time=   1.0s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.287 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.286 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.335 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.348 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.400 total time=   0.1s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.377 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.400 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.410 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.380 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.341 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.302 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.335 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.307 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.213 total time=   1.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.310 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.263 total time=   0.5s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.230 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.310 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=-0.035 total time=   2.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.307 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=1;, score=0.305 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.311 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.324 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.330 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.295 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.310 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.348 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.167 total time=   1.6s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.354 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.274 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.187 total time=   2.0s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.181 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.171 total time=   0.9s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.126 total time=   2.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=0.8;, score=0.366 total time=   0.8s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=0.8;, score=0.341 total time=   0.2s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.5;, score=0.321 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.8;, score=0.311 total time=   0.7s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=1;, score=0.367 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.222 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.5;, score=0.318 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.8;, score=0.313 total time=   0.4s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=1;, score=0.215 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=1;, score=0.331 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.5;, score=0.093 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.8;, score=0.382 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.8;, score=0.281 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.320 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.360 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.371 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.313 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.304 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.319 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.317 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.5;, score=0.336 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=0.8;, score=0.330 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=25, subsample=1;, score=0.366 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.5;, score=0.321 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=50, subsample=0.8;, score=0.321 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.5;, score=0.270 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.8;, score=0.269 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=1;, score=0.193 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=1;, score=0.252 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=0.5;, score=0.058 total time=   2.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.5;, score=0.317 total time=   0.4s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.5;, score=0.293 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.329 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.323 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.219 total time=   1.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.281 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.285 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.317 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.289 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.321 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.236 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.262 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.229 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.270 total time=   0.9s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.163 total time=   2.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.337 total time=   0.8s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.274 total time=   0.7s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.383 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.301 total time=   0.3s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.240 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.325 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.200 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.298 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.225 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.163 total time=   1.5s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.327 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.311 total time=   1.0s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.8;, score=0.303 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.327 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.210 total time=   1.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.244 total time=   0.6s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.178 total time=   1.7s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.170 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.245 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=300, subsample=1;, score=0.130 total time=   2.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.298 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.5;, score=0.295 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.301 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.221 total time=   1.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=1;, score=0.297 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.286 total time=   0.6s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.276 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.279 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.296 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.207 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.264 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.230 total time=   0.7s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.267 total time=   0.9s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.146 total time=   2.2s\n",
      "[CV 8/10] END colsample_bytree=0.5, max_depth=3, n_estimators=50, subsample=0.5;, score=0.378 total time=   0.8s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=1;, score=0.339 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.5;, score=0.342 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=0.8;, score=0.334 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=25, subsample=1;, score=0.351 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.5;, score=0.323 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=0.8;, score=0.340 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.5;, score=0.304 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=1;, score=0.354 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=1;, score=0.283 total time=   0.8s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=1;, score=0.299 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=1;, score=0.336 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.5;, score=0.294 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=0.8;, score=0.324 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=25, subsample=1;, score=0.344 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.5;, score=0.291 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=0.8;, score=0.335 total time=   0.3s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.8;, score=0.282 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=1;, score=0.283 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.5;, score=0.350 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=0.8;, score=0.342 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=25, subsample=1;, score=0.379 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.5;, score=0.376 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=0.8;, score=0.370 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=50, subsample=1;, score=0.376 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=0.5;, score=0.354 total time=   0.3s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=3, n_estimators=75, subsample=1;, score=0.306 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.5;, score=0.263 total time=   1.0s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=1;, score=0.376 total time=   0.2s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.5;, score=0.296 total time=   0.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=0.8;, score=0.257 total time=   1.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.317 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.342 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.256 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.364 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.384 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.275 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.290 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.8;, score=0.259 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.196 total time=   0.8s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.339 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.373 total time=   0.1s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.381 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.317 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.357 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.327 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.312 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.346 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.327 total time=   0.3s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.342 total time=   0.4s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=1;, score=0.282 total time=   1.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=0.8;, score=0.322 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=1;, score=0.352 total time=   0.5s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=1;, score=0.234 total time=   1.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.243 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.169 total time=   2.0s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.229 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.195 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.238 total time=   2.2s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.386 total time=   0.9s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=3, n_estimators=100, subsample=0.8;, score=0.334 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=1;, score=0.327 total time=   0.6s\n",
      "[CV 7/10] END colsample_bytree=0.5, max_depth=4, n_estimators=75, subsample=0.8;, score=0.329 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.5;, score=0.214 total time=   0.9s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=5, n_estimators=50, subsample=1;, score=0.334 total time=   0.2s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=100, subsample=0.5;, score=0.237 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.8;, score=0.173 total time=   1.1s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=75, subsample=0.5;, score=0.249 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.8;, score=0.282 total time=   0.5s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=1;, score=0.169 total time=   1.4s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=1;, score=0.328 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.5;, score=0.316 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=0.8;, score=0.339 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=25, subsample=1;, score=0.339 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.5;, score=0.286 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=50, subsample=0.8;, score=0.323 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=0.5;, score=0.278 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=0.8;, score=0.303 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.232 total time=   1.2s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=75, subsample=0.8;, score=0.294 total time=   0.4s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=1;, score=0.305 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.5;, score=0.337 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=0.8;, score=0.323 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=25, subsample=1;, score=0.319 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.5;, score=0.198 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=0.8, max_depth=7, n_estimators=50, subsample=0.8;, score=0.287 total time=   0.4s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.271 total time=   0.6s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=0.8;, score=0.282 total time=   0.8s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.361 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.360 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.365 total time=   0.1s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.366 total time=   0.2s\n",
      "[CV 7/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.362 total time=   0.2s\n",
      "[CV 8/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.382 total time=   0.2s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.402 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.396 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.399 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.381 total time=   0.4s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.5;, score=0.323 total time=   0.1s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=0.8;, score=0.342 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=25, subsample=1;, score=0.335 total time=   0.1s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.5;, score=0.297 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.324 total time=   0.3s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=4, n_estimators=50, subsample=1;, score=0.320 total time=   0.3s\n",
      "[CV 10/10] END colsample_bytree=1, max_depth=4, n_estimators=100, subsample=0.5;, score=0.349 total time=   0.5s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.8;, score=0.232 total time=   1.6s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=0.5;, score=0.254 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=300, subsample=0.8;, score=0.157 total time=   2.0s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.5;, score=0.219 total time=   0.7s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=0.8;, score=0.231 total time=   0.9s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.191 total time=   2.3s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=3, n_estimators=25, subsample=1;, score=0.406 total time=   0.7s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=3, n_estimators=75, subsample=1;, score=0.325 total time=   0.2s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=3, n_estimators=300, subsample=0.5;, score=0.278 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=0.5, max_depth=4, n_estimators=50, subsample=1;, score=0.340 total time=   0.1s\n",
      "[CV 9/10] END colsample_bytree=0.5, max_depth=4, n_estimators=100, subsample=0.5;, score=0.278 total time=   0.3s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=4, n_estimators=300, subsample=0.8;, score=0.261 total time=   0.9s\n",
      "[CV 10/10] END colsample_bytree=0.5, max_depth=5, n_estimators=75, subsample=0.8;, score=0.332 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.5, max_depth=5, n_estimators=300, subsample=0.5;, score=0.182 total time=   1.1s\n",
      "[CV 5/10] END colsample_bytree=0.5, max_depth=7, n_estimators=50, subsample=1;, score=0.322 total time=   0.2s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=100, subsample=0.5;, score=0.239 total time=   0.5s\n",
      "[CV 3/10] END colsample_bytree=0.5, max_depth=7, n_estimators=300, subsample=0.8;, score=0.129 total time=   1.5s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=3, n_estimators=100, subsample=0.5;, score=0.355 total time=   0.4s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=3, n_estimators=300, subsample=0.8;, score=0.294 total time=   1.0s\n",
      "[CV 4/10] END colsample_bytree=0.8, max_depth=4, n_estimators=75, subsample=1;, score=0.329 total time=   0.3s\n",
      "[CV 6/10] END colsample_bytree=0.8, max_depth=4, n_estimators=100, subsample=1;, score=0.348 total time=   0.4s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=4, n_estimators=300, subsample=1;, score=0.317 total time=   1.2s\n",
      "[CV 7/10] END colsample_bytree=0.8, max_depth=5, n_estimators=100, subsample=0.5;, score=0.235 total time=   0.6s\n",
      "[CV 5/10] END colsample_bytree=0.8, max_depth=5, n_estimators=300, subsample=0.8;, score=0.191 total time=   1.7s\n",
      "[CV 10/10] END colsample_bytree=0.8, max_depth=7, n_estimators=75, subsample=0.5;, score=0.230 total time=   0.6s\n",
      "[CV 3/10] END colsample_bytree=0.8, max_depth=7, n_estimators=100, subsample=1;, score=0.230 total time=   0.7s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.5;, score=0.349 total time=   0.1s\n",
      "[CV 6/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=0.8;, score=0.379 total time=   0.1s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=3, n_estimators=25, subsample=1;, score=0.363 total time=   0.1s\n",
      "[CV 4/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.5;, score=0.331 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.363 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=50, subsample=1;, score=0.376 total time=   0.2s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.5;, score=0.356 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=0.8;, score=0.361 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=75, subsample=1;, score=0.361 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.355 total time=   0.4s\n",
      "[CV 9/10] END colsample_bytree=1, max_depth=3, n_estimators=300, subsample=0.8;, score=0.249 total time=   1.2s\n",
      "[CV 5/10] END colsample_bytree=1, max_depth=4, n_estimators=75, subsample=1;, score=0.316 total time=   0.4s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=4, n_estimators=300, subsample=0.5;, score=0.167 total time=   1.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=50, subsample=1;, score=0.313 total time=   0.3s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=5, n_estimators=75, subsample=0.8;, score=0.314 total time=   0.5s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=5, n_estimators=100, subsample=1;, score=0.286 total time=   0.6s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.5;, score=0.306 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=0.8;, score=0.303 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=25, subsample=1;, score=0.313 total time=   0.2s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.5;, score=0.227 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=50, subsample=0.8;, score=0.310 total time=   0.5s\n",
      "[CV 2/10] END colsample_bytree=1, max_depth=7, n_estimators=75, subsample=0.8;, score=0.260 total time=   0.7s\n",
      "[CV 1/10] END colsample_bytree=1, max_depth=7, n_estimators=100, subsample=1;, score=0.201 total time=   0.9s\n",
      "[CV 3/10] END colsample_bytree=1, max_depth=7, n_estimators=300, subsample=1;, score=0.192 total time=   2.3s\n"
     ]
    }
   ],
   "source": [
    "xvals = []\n",
    "yvals = []\n",
    "cvals = []\n",
    "cvals_old = []\n",
    "for x in np.arange(3, 8, 0.5):\n",
    "    for y in np.arange(3, 8, 0.5):\n",
    "        xvals.append(x)\n",
    "        yvals.append(y)\n",
    "        cvals.append(sum((y_pred == x) & (y_test_age == y)))\n",
    "xvals = np.array(xvals)\n",
    "yvals = np.array(yvals)\n",
    "cvals = np.array(cvals) / 2\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlim(2, 8)\n",
    "plt.ylim(2, 8)\n",
    "plt.scatter(xvals, yvals, c='red', alpha=0.5, s=cvals, label='6<age<!0')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Ground Truth')\n",
    "lgnd = plt.legend()\n",
    "lgnd.legendHandles[0]._sizes = [30]\n",
    "plt.plot([2,8], [2,8], 'k-', alpha=0.1)\n",
    "plt.plot([2.5,8], [2,7.5], '--k', alpha=0.1)\n",
    "plt.plot([2,7.5], [2.5,8], '--k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48966d1a-2ea9-437e-adb6-bb9847a34295",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27cf56d6-429a-426b-8fa4-91934f6b6bea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:32:36.721507Z",
     "iopub.status.busy": "2023-02-17T05:32:36.721129Z",
     "iopub.status.idle": "2023-02-17T05:32:36.908119Z",
     "shell.execute_reply": "2023-02-17T05:32:36.907414Z",
     "shell.execute_reply.started": "2023-02-17T05:32:36.721470Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imp = IterativeImputer().fit(x_train)\n",
    "x_train_imputed = imp.transform(x_train)\n",
    "x_test_imputed = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc997700-db8f-4591-92b9-3856c1aa7b8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T05:58:19.714124Z",
     "iopub.status.busy": "2023-02-17T05:58:19.713622Z",
     "iopub.status.idle": "2023-02-17T06:10:09.690231Z",
     "shell.execute_reply": "2023-02-17T06:10:09.689550Z",
     "shell.execute_reply.started": "2023-02-17T05:58:19.714074Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 216 candidates, totalling 2160 fits\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   6.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.870 total time=   5.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.873 total time=  19.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.874 total time=  30.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.873 total time=  20.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.875 total time=   5.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   9.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.878 total time=   3.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   8.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.876 total time=  29.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.875 total time=   2.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.872 total time=   2.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.878 total time=   4.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.878 total time=  16.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.881 total time=   2.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.877 total time=   4.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.867 total time=   2.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.868 total time=   4.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.879 total time=  14.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=   8.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.874 total time=   2.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   5.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.873 total time=   2.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.871 total time=   5.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.875 total time=   5.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.876 total time=   2.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.875 total time=   4.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.878 total time=   2.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.880 total time=   4.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.876 total time=  14.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.877 total time=  13.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.871 total time=  13.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.764 total time=   7.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.780 total time=  28.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.794 total time=  14.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.809 total time=   9.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.800 total time=  14.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.800 total time=   7.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.798 total time=  15.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.808 total time=  43.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.825 total time=  27.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.820 total time=   6.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.824 total time=  15.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.818 total time=  39.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.869 total time=   3.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.866 total time=  15.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.869 total time=   4.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.871 total time=   7.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.871 total time=   8.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.870 total time=   3.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.873 total time=   6.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.874 total time=   3.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   7.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.867 total time=   4.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.873 total time=   4.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.874 total time=   4.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.875 total time=  10.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.870 total time=   4.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.872 total time=  10.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.872 total time=   4.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.872 total time=  11.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.876 total time=   4.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.877 total time=   9.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.872 total time=  31.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   4.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   9.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.879 total time=   3.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   7.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.876 total time=  26.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.876 total time=   3.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.877 total time=   9.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.880 total time=   8.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.879 total time=  16.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.876 total time=   2.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   4.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.876 total time=   1.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.882 total time=   3.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.880 total time=  11.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.878 total time=   4.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.880 total time=   5.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.867 total time=   2.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.879 total time=   4.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.868 total time=   4.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.879 total time=  14.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.882 total time=  13.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=   7.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.786 total time=  15.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.777 total time=  32.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.778 total time=  33.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.798 total time=   7.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.800 total time=  15.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.814 total time=   6.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.797 total time=  13.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.813 total time=  45.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.825 total time=  27.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.860 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.868 total time=   7.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.866 total time=  14.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.869 total time=   7.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.870 total time=  25.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.872 total time=  20.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.872 total time=  15.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.870 total time=   2.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   6.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.875 total time=   5.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.865 total time=  19.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.869 total time=  15.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.869 total time=  14.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.869 total time=   4.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.870 total time=   9.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.862 total time=  11.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.872 total time=   4.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.873 total time=   8.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   4.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.875 total time=   8.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.875 total time=  21.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.872 total time=   6.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.875 total time=   9.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.875 total time=   4.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.875 total time=   9.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   9.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.875 total time=  25.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.864 total time=  14.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.880 total time=   5.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   2.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.881 total time=   4.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.882 total time=  14.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.877 total time=  13.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.883 total time=   8.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  13.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.875 total time=  12.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.873 total time=   2.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.877 total time=   4.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.875 total time=   2.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.876 total time=   4.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.879 total time=   2.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.876 total time=   4.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.879 total time=  13.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.882 total time=   7.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.877 total time=   7.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.790 total time=  32.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.777 total time=   8.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.783 total time=  14.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.802 total time=   7.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.802 total time=  14.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.792 total time=   6.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.792 total time=  13.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.798 total time=  29.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.814 total time=   7.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.814 total time=  13.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.818 total time=  44.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.819 total time=  27.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.869 total time=   7.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.869 total time=  22.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.872 total time=  15.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.870 total time=  21.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  13.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   2.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   7.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.864 total time=   3.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.865 total time=   6.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.867 total time=  23.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.869 total time=  22.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.872 total time=  14.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   4.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.874 total time=   4.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=  10.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.874 total time=   4.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.875 total time=   9.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   4.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.875 total time=  10.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.875 total time=  26.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.875 total time=  19.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.875 total time=  27.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.875 total time=  17.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.871 total time=   2.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.871 total time=   5.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.872 total time=  15.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.880 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.880 total time=  15.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.879 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.880 total time=   4.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.871 total time=  14.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.864 total time=  15.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.875 total time=   9.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.878 total time=   2.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.879 total time=   4.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.877 total time=  14.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  13.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=   8.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.770 total time=  34.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.794 total time=   8.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.799 total time=  16.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.808 total time=   8.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.807 total time=  13.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.808 total time=   8.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.807 total time=  13.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.795 total time=   6.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.805 total time=  15.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.824 total time=   7.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.820 total time=  12.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.819 total time=  30.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.819 total time=   7.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.814 total time=  28.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.867 total time=  24.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.869 total time=  13.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.872 total time=  14.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.874 total time=   6.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.874 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.875 total time=   6.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.875 total time=  20.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=  15.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.861 total time=   3.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.862 total time=   6.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.868 total time=   3.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.872 total time=   7.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.872 total time=  22.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.871 total time=  14.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   7.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.873 total time=  12.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.870 total time=  19.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.877 total time=  27.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.875 total time=  18.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.876 total time=  29.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.873 total time=  16.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.876 total time=  26.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.875 total time=   6.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.878 total time=   5.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.876 total time=  14.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.882 total time=  13.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.880 total time=  10.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.871 total time=   2.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.872 total time=   5.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.872 total time=   2.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=   4.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.880 total time=   3.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.875 total time=   5.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.875 total time=   2.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.873 total time=   5.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.876 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.878 total time=   8.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.876 total time=   9.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.879 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.880 total time=   4.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  14.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.774 total time=  31.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.786 total time=   7.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.787 total time=  17.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.794 total time=   7.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.794 total time=  15.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.794 total time=   7.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.794 total time=  16.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.816 total time=   7.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.806 total time=  15.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.809 total time=  46.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.819 total time=  41.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.866 total time=  22.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.865 total time=   3.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.873 total time=  14.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.872 total time=  22.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.872 total time=  13.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   6.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.864 total time=   2.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.870 total time=   3.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.871 total time=   5.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.864 total time=   3.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.866 total time=   6.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.869 total time=   4.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.865 total time=   7.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.869 total time=   3.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.870 total time=   6.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.871 total time=  14.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.873 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.870 total time=   7.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.870 total time=   3.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.875 total time=   6.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.869 total time=   4.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.872 total time=   4.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=  10.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  33.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   4.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.874 total time=   9.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.872 total time=   4.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.877 total time=   9.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.875 total time=  25.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.876 total time=  15.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.875 total time=  17.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.876 total time=   2.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.876 total time=   4.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  17.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.880 total time=  14.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.869 total time=  13.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.871 total time=   9.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.876 total time=   2.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.872 total time=   4.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.876 total time=   2.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.876 total time=   4.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.867 total time=   2.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.867 total time=   4.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.877 total time=   9.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   2.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.876 total time=   4.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.880 total time=  13.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=   7.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.883 total time=   7.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.875 total time=   7.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.761 total time=  29.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.777 total time=  41.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.792 total time=  29.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.803 total time=  49.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.824 total time=   6.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.824 total time=  13.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.824 total time=  42.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.868 total time=  23.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.872 total time=   6.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.867 total time=  20.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.868 total time=  13.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   2.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   5.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.871 total time=  13.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   4.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   6.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.863 total time=   4.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.861 total time=   8.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.868 total time=   8.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.865 total time=   3.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.866 total time=   7.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.867 total time=   4.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.867 total time=   8.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.869 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.875 total time=   7.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.874 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   7.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.872 total time=  20.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.876 total time=   4.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.874 total time=  10.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.876 total time=   4.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.877 total time=  10.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.872 total time=  27.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.872 total time=  20.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   5.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.873 total time=   9.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   5.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.877 total time=  17.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.871 total time=  14.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.879 total time=   4.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.867 total time=   2.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.868 total time=   5.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.878 total time=   2.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.875 total time=   4.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.879 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.876 total time=   5.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.876 total time=   2.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.883 total time=   5.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.881 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.878 total time=   4.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.882 total time=  13.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.875 total time=   9.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.874 total time=  13.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.876 total time=   8.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.878 total time=  12.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   4.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.876 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.882 total time=   3.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.876 total time=  14.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.785 total time=  31.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.784 total time=   6.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.789 total time=  13.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.792 total time=   8.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.812 total time=  15.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.799 total time=  43.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.805 total time=  26.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.818 total time=  28.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.818 total time=   8.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.824 total time=  15.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.860 total time=   3.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.866 total time=   8.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.868 total time=  20.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.857 total time=   3.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.873 total time=  14.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.872 total time=  19.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.862 total time=  14.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  18.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.875 total time=  11.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.852 total time=  24.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.869 total time=  15.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   3.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.871 total time=   6.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.862 total time=   6.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.873 total time=  19.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.874 total time=   5.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.876 total time=   9.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.876 total time=   4.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.874 total time=   8.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.877 total time=  16.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.865 total time=  25.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.867 total time=  19.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.875 total time=  19.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.871 total time=   3.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.871 total time=   6.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.873 total time=  15.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.868 total time=  11.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.878 total time=   2.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.879 total time=   4.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.882 total time=  12.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.871 total time=   8.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.875 total time=   8.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   2.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.863 total time=   5.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.874 total time=   2.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.876 total time=   4.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.878 total time=  13.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.878 total time=  10.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.867 total time=   2.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.879 total time=   4.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.870 total time=   3.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.883 total time=   4.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.879 total time=   2.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.883 total time=   4.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.881 total time=   2.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.882 total time=   3.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.884 total time=  13.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.789 total time=   9.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.783 total time=  31.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.799 total time=  27.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.810 total time=  40.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.817 total time=  28.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.820 total time=  28.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.824 total time=   6.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.820 total time=  14.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.814 total time=  40.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.870 total time=   7.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.861 total time=   3.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.861 total time=   6.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.871 total time=  21.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.871 total time=  12.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.864 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.865 total time=   6.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.873 total time=  19.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.868 total time=  16.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.872 total time=   3.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.871 total time=   7.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.869 total time=  21.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.870 total time=  13.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.874 total time=   3.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   7.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.874 total time=   3.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.869 total time=  20.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.875 total time=   5.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.871 total time=  11.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   5.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.871 total time=   9.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   5.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.871 total time=  10.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.874 total time=   4.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.873 total time=   9.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.878 total time=   4.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.879 total time=   8.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.877 total time=  24.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.877 total time=  17.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.863 total time=   2.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.873 total time=   2.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=   5.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  14.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.868 total time=  15.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.881 total time=  14.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.877 total time=  11.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   5.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.874 total time=   2.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.878 total time=  10.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.876 total time=  14.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.876 total time=   9.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.878 total time=   6.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.876 total time=   2.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   4.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.871 total time=   5.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.769 total time=   8.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.762 total time=  52.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.810 total time=   7.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.810 total time=  13.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.803 total time=  44.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.806 total time=  29.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.814 total time=   6.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.814 total time=  13.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.814 total time=  42.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.862 total time=  20.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.870 total time=   3.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.868 total time=   7.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   3.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.870 total time=   6.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.861 total time=   6.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.861 total time=  19.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.875 total time=  15.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   5.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.877 total time=  20.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.870 total time=  16.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.861 total time=   3.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.871 total time=   6.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.871 total time=  12.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.868 total time=  13.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   2.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.865 total time=   6.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.861 total time=  19.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.873 total time=   4.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.874 total time=   9.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.872 total time=  25.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.875 total time=  16.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.875 total time=  18.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.873 total time=  19.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.875 total time=   4.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.875 total time=   8.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   2.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.872 total time=   6.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.874 total time=  16.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.875 total time=   5.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.876 total time=   2.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.876 total time=   4.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.876 total time=   2.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.876 total time=   5.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.879 total time=   4.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.870 total time=   1.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.874 total time=   2.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.875 total time=   4.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.883 total time=   2.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.880 total time=   3.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  12.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.878 total time=  10.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.872 total time=  14.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.877 total time=  10.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.877 total time=   9.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.875 total time=   7.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.884 total time=  12.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.769 total time=  14.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.783 total time=  15.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.779 total time=  45.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.798 total time=  30.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.800 total time=   7.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.804 total time=  15.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.824 total time=   8.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.820 total time=  13.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.825 total time=  41.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.820 total time=  28.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.868 total time=   4.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.855 total time=  14.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.871 total time=  13.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.871 total time=  12.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.872 total time=  19.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  12.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=  12.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.873 total time=  19.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.865 total time=  14.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.873 total time=  15.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   3.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.870 total time=   7.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.869 total time=   4.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   6.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.871 total time=  22.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.871 total time=   5.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.871 total time=   8.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   4.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.873 total time=   9.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.877 total time=  26.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.877 total time=  19.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.875 total time=   3.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.876 total time=   9.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.879 total time=  29.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.878 total time=   5.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.876 total time=  14.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.878 total time=  10.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.880 total time=   4.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.881 total time=   2.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.880 total time=   3.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=   7.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.880 total time=  12.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.871 total time=   4.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=   5.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.872 total time=   2.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.876 total time=   4.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.878 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.878 total time=   4.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.875 total time=   3.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.875 total time=   5.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.875 total time=   2.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.876 total time=   4.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.881 total time=   2.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.880 total time=   4.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   1.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.871 total time=   4.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.880 total time=   6.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.780 total time=   7.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.774 total time=  45.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.797 total time=  53.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.810 total time=  45.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.824 total time=  37.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.819 total time=  12.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.867 total time=   3.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.852 total time=   7.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.851 total time=  15.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.868 total time=   8.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.863 total time=  24.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.867 total time=   4.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.861 total time=  15.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.873 total time=  22.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  22.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.864 total time=  25.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   3.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.875 total time=   6.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.875 total time=  18.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.862 total time=  11.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.865 total time=  11.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.874 total time=  27.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.864 total time=  18.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.865 total time=  19.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.866 total time=   4.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.875 total time=   8.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.876 total time=  28.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.879 total time=  18.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   2.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.869 total time=   2.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   4.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.873 total time=  10.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.875 total time=   8.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.878 total time=  12.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.876 total time=   4.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.876 total time=  14.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.877 total time=  13.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.871 total time=  13.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.875 total time=  11.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.878 total time=   2.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.868 total time=   4.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.878 total time=  12.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.877 total time=   4.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.876 total time=   2.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   4.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.881 total time=   1.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.878 total time=   4.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.877 total time=  14.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.780 total time=  34.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.798 total time=   9.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.776 total time=  27.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.812 total time=  26.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.800 total time=  30.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.818 total time=   6.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.825 total time=  15.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.824 total time=   6.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.824 total time=  13.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.820 total time=  42.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.864 total time=  22.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.870 total time=   3.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.871 total time=  21.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.872 total time=  12.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.872 total time=  11.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  11.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  13.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.865 total time=   3.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.861 total time=   8.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.867 total time=   7.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.868 total time=   4.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.869 total time=   7.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.869 total time=   4.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.870 total time=   8.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.871 total time=   6.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.874 total time=   7.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.874 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.875 total time=   7.2s\n",
      "\n",
      "========= found hyperparameter =========\n",
      "{'bootstrap': True, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "0.8782390066936319\n",
      "========================================\n",
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.860\n",
      "mean_squared_errors: 0.139\n",
      "mean_absolute_errors: 0.224\n",
      "r2_score: 0.860\n",
      "acc: 0.595\n",
      "acc(+-0.5mm): 0.963\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.873 total time=  17.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.862 total time=  28.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.874 total time=  16.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.872 total time=  18.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.876 total time=   4.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.876 total time=   9.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.867 total time=   4.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.875 total time=   8.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.875 total time=  18.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.875 total time=   2.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   4.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.874 total time=   9.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.865 total time=  10.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.873 total time=   8.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.874 total time=  10.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.874 total time=   5.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.882 total time=   4.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.880 total time=  14.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.875 total time=   8.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.873 total time=  14.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.881 total time=   9.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.880 total time=  12.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.874 total time=   4.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.879 total time=   1.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.871 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.871 total time=  13.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.883 total time=   8.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.778 total time=  29.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.776 total time=   7.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.796 total time=  15.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.799 total time=  53.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.809 total time=   8.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.797 total time=  26.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.824 total time=  27.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.814 total time=  27.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.850 total time=   4.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.867 total time=   7.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.868 total time=  16.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.870 total time=  15.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.869 total time=  23.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.870 total time=  14.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   3.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  18.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.865 total time=  13.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.868 total time=  25.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.871 total time=  21.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.861 total time=  23.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  22.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.874 total time=  31.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.873 total time=  28.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.873 total time=  21.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   5.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   8.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.876 total time=   4.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.879 total time=   8.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.868 total time=  26.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.875 total time=  10.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.865 total time=  15.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.876 total time=   9.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.877 total time=  14.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=   7.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  11.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   4.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.871 total time=  16.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.875 total time=   2.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.876 total time=   4.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.874 total time=  15.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.877 total time=  13.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=   8.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.877 total time=  12.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.778 total time=   6.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.779 total time=  17.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.796 total time=   7.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.802 total time=  15.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.812 total time=   7.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.792 total time=  13.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.803 total time=  29.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.806 total time=   6.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.816 total time=  14.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.805 total time=  43.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.820 total time=  28.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.865 total time=   4.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.865 total time=   8.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.866 total time=  16.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.868 total time=  13.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.869 total time=  22.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.872 total time=  14.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   3.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.865 total time=  20.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=  12.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.866 total time=  22.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.859 total time=  13.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.868 total time=  12.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.870 total time=  18.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.875 total time=  11.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.877 total time=  14.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.872 total time=  19.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=  13.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.873 total time=  32.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.872 total time=  31.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.875 total time=  29.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.876 total time=  17.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.879 total time=   7.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.879 total time=  24.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.867 total time=   2.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.872 total time=   2.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.875 total time=   9.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.876 total time=   8.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.874 total time=  14.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.884 total time=  11.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.871 total time=   8.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.873 total time=  10.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.877 total time=   4.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.878 total time=  13.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.868 total time=   9.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.869 total time=  13.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.880 total time=  10.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.761 total time=   7.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.776 total time=  49.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.784 total time=  45.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.803 total time=  34.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.819 total time=   6.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.824 total time=  13.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.814 total time=   7.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.814 total time=  13.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.814 total time=   7.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.824 total time=  14.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.820 total time=  42.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.859 total time=  14.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.868 total time=  14.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.868 total time=   3.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   7.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   3.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.871 total time=   5.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.874 total time=   3.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.875 total time=   6.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  20.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.864 total time=  14.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.857 total time=   3.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.859 total time=   7.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.869 total time=  23.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.872 total time=  22.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.873 total time=  13.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.870 total time=   3.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.871 total time=   6.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.860 total time=  11.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.871 total time=  22.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.866 total time=   5.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.877 total time=  10.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.866 total time=   4.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.866 total time=  10.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.873 total time=  27.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.877 total time=  17.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.876 total time=  27.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.875 total time=  15.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.877 total time=  10.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.878 total time=   9.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.878 total time=   9.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   3.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   3.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.879 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.880 total time=   3.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.883 total time=   7.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.874 total time=  14.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.873 total time=   4.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.881 total time=   2.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.881 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.876 total time=  13.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.882 total time=   8.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=   8.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  12.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.776 total time=  16.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.773 total time=  29.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.797 total time=  30.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.808 total time=  28.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.805 total time=   7.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.795 total time=  15.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.814 total time=   6.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.814 total time=  14.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.820 total time=  42.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.820 total time=  27.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.869 total time=   4.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.867 total time=   7.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.870 total time=  23.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.875 total time=  22.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.872 total time=  21.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  19.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.866 total time=  13.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.866 total time=  15.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   3.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.874 total time=   7.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.870 total time=  21.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.871 total time=  14.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.870 total time=   3.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.871 total time=   6.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   7.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.874 total time=  31.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.876 total time=  28.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.877 total time=  16.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.874 total time=  16.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.879 total time=  17.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.876 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.868 total time=   9.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.875 total time=  26.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.867 total time=   4.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.877 total time=  13.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.874 total time=   9.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   1.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.880 total time=   4.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  13.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.880 total time=   8.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.875 total time=  15.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.878 total time=   9.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.879 total time=  12.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.878 total time=   9.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.880 total time=  12.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=   9.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.761 total time=  30.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.789 total time=   7.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.786 total time=  17.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.798 total time=   7.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.809 total time=  14.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.802 total time=   6.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.812 total time=  13.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.792 total time=  45.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.814 total time=  27.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.824 total time=   6.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.814 total time=  13.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.824 total time=  43.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.870 total time=   6.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.869 total time=   4.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.871 total time=   7.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   3.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.870 total time=   6.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.861 total time=   3.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.862 total time=   6.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.871 total time=  19.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  13.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.871 total time=  19.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.870 total time=  14.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.871 total time=  21.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.870 total time=  14.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.868 total time=  22.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   7.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.869 total time=   9.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.872 total time=   9.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.871 total time=  30.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.873 total time=  31.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.874 total time=  27.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.877 total time=  27.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.874 total time=  16.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.873 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.875 total time=   2.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.877 total time=   4.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.873 total time=   2.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.881 total time=   5.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.877 total time=   4.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   2.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   3.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  12.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=   9.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.878 total time=  14.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.875 total time=   9.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.882 total time=  12.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.881 total time=   9.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.880 total time=  14.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.880 total time=  13.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.783 total time=   9.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.777 total time=  30.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.787 total time=  31.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.808 total time=  42.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.795 total time=  29.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.819 total time=  43.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.814 total time=  42.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.868 total time=   3.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.859 total time=  21.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.870 total time=  14.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.873 total time=  18.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.865 total time=  13.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  19.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.868 total time=  14.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.873 total time=  13.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.875 total time=  21.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.868 total time=  13.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.873 total time=   7.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.873 total time=   7.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.869 total time=   4.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.872 total time=   6.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.862 total time=  18.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.874 total time=  19.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.865 total time=  27.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.878 total time=  20.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.879 total time=   4.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.879 total time=   9.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.879 total time=  26.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.878 total time=  14.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.881 total time=  10.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.877 total time=  13.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.882 total time=   8.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.876 total time=  13.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.877 total time=   9.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.875 total time=  15.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.876 total time=  15.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.882 total time=   4.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.877 total time=  11.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.871 total time=   7.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.880 total time=   7.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.770 total time=  13.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.780 total time=  14.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.761 total time=  46.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.811 total time=  30.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.811 total time=   7.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.800 total time=  15.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.820 total time=   7.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.824 total time=  13.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.814 total time=  41.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.824 total time=  27.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.868 total time=   3.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.861 total time=   4.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.868 total time=  13.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.866 total time=   6.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   3.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.875 total time=   7.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   4.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.871 total time=   6.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.872 total time=   3.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.875 total time=   6.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.874 total time=   3.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   6.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.873 total time=  19.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=  13.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.866 total time=  21.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.871 total time=  14.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.861 total time=   3.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.861 total time=   7.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.871 total time=  21.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  23.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.873 total time=   6.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.864 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.865 total time=   6.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.871 total time=  13.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.873 total time=  11.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.870 total time=  19.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.874 total time=  28.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.874 total time=  19.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.878 total time=  26.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.877 total time=  18.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.875 total time=  26.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.876 total time=  14.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.880 total time=  10.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.879 total time=   1.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.878 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.877 total time=   8.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.881 total time=   2.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.880 total time=   4.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.861 total time=   3.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.876 total time=   6.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.871 total time=   2.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.879 total time=  10.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.877 total time=   3.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.880 total time=   4.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.877 total time=  13.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.877 total time=   8.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.884 total time=  11.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.871 total time=   8.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.761 total time=  16.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.764 total time=  30.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.786 total time=  28.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.792 total time=  44.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.813 total time=  29.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.824 total time=  41.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.814 total time=  28.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.864 total time=   4.8s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.869 total time=  16.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.873 total time=  22.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.871 total time=  12.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.862 total time=  22.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  19.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.867 total time=  15.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.870 total time=  20.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.872 total time=  14.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.875 total time=  23.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   6.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  14.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.872 total time=  20.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.871 total time=   4.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.876 total time=   9.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   4.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.866 total time=   9.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.877 total time=  19.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.875 total time=   4.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.875 total time=   9.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.874 total time=  27.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.876 total time=  20.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   4.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.864 total time=  10.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.876 total time=   9.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.880 total time=  10.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.876 total time=  13.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.877 total time=  10.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   4.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   5.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.873 total time=   2.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.877 total time=   4.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.875 total time=   2.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.865 total time=   4.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  13.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.868 total time=   9.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.877 total time=   9.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.879 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.880 total time=   3.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  11.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.760 total time=   7.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.761 total time=  53.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.803 total time=   7.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.803 total time=  16.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.803 total time=   7.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.802 total time=  13.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.812 total time=  41.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.814 total time=  29.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.814 total time=  45.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.854 total time=   3.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.865 total time=   7.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.868 total time=  22.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.870 total time=  21.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.871 total time=  12.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  20.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.871 total time=  12.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.868 total time=  23.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.859 total time=  22.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.874 total time=  12.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.872 total time=  13.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   3.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   6.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.877 total time=  16.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.871 total time=  18.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.875 total time=  11.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.875 total time=   9.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.860 total time=   4.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.861 total time=   5.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.869 total time=   9.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.864 total time=   4.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.864 total time=   9.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.874 total time=  28.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.874 total time=  19.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.867 total time=   3.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.868 total time=   9.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.868 total time=  28.0s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.876 total time=  17.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.874 total time=   2.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=   5.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.874 total time=  15.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.875 total time=   2.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.868 total time=   5.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.877 total time=  12.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.880 total time=   7.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.880 total time=   7.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=   8.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.877 total time=  14.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.876 total time=   8.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.874 total time=   9.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.881 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.880 total time=   4.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.874 total time=  13.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.880 total time=  13.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.780 total time=  16.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.790 total time=  28.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.789 total time=  29.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.811 total time=  47.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.797 total time=  44.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.824 total time=  27.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.820 total time=  39.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.872 total time=   3.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.871 total time=  14.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.870 total time=  13.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.875 total time=  23.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.871 total time=  18.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.877 total time=  11.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.866 total time=  14.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.867 total time=  21.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.870 total time=  14.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.872 total time=   4.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.868 total time=   7.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.870 total time=   3.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.871 total time=   6.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   2.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   5.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  17.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.869 total time=  10.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.873 total time=  19.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.877 total time=  19.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.875 total time=  26.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.866 total time=  18.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.875 total time=  17.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.876 total time=   5.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   8.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.874 total time=   2.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.873 total time=   2.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.872 total time=   4.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.873 total time=   9.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.878 total time=  11.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.877 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.873 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.873 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.875 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.878 total time=   5.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.878 total time=   4.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.879 total time=  14.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  12.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.877 total time=   9.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.876 total time=  15.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.876 total time=   8.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.882 total time=  13.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.882 total time=   7.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=   7.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.880 total time=  11.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.761 total time=   8.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.761 total time=  16.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.764 total time=  49.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.810 total time=   7.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.803 total time=  14.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.803 total time=  46.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.820 total time=  42.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.818 total time=  27.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.870 total time=   6.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.868 total time=  21.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.861 total time=  12.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.874 total time=  14.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.868 total time=  20.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  13.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.867 total time=   3.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.867 total time=   7.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.854 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.870 total time=   8.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.870 total time=   3.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.869 total time=   7.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.870 total time=   3.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.872 total time=   7.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.870 total time=  21.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.872 total time=  12.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.874 total time=   3.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.875 total time=   7.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  17.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.861 total time=  31.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.875 total time=  29.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.877 total time=  18.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.875 total time=  27.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.877 total time=  19.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.876 total time=   2.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.877 total time=   6.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.875 total time=  15.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.877 total time=   4.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.873 total time=  13.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.877 total time=   9.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.883 total time=   2.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.871 total time=  13.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.880 total time=   9.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.873 total time=  13.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.877 total time=   9.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.882 total time=   8.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.877 total time=  14.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=   8.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.880 total time=  11.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.761 total time=  15.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.777 total time=  15.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.794 total time=  48.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.800 total time=  46.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.804 total time=  31.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.824 total time=   7.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.824 total time=  13.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.825 total time=   7.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.814 total time=  13.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.819 total time=  44.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.869 total time=  20.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.872 total time=  13.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.875 total time=  14.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   4.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.871 total time=   7.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.865 total time=   8.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.864 total time=   4.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.868 total time=   7.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.868 total time=   3.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.864 total time=   7.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.866 total time=  22.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.872 total time=  13.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.872 total time=  19.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=  12.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  18.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.871 total time=  11.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.873 total time=  17.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.871 total time=   9.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.871 total time=   8.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.874 total time=  29.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.873 total time=  18.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.875 total time=   4.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.866 total time=  11.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.872 total time=  29.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.873 total time=  29.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.865 total time=   5.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.877 total time=  15.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.868 total time=   9.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.878 total time=  14.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.882 total time=   8.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.876 total time=  13.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.874 total time=  16.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.878 total time=  14.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.882 total time=   8.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.876 total time=  14.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.870 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.875 total time=   5.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.785 total time=   7.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.785 total time=  47.7s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.786 total time=  45.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.794 total time=  27.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.804 total time=  41.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.824 total time=  28.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.825 total time=  41.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.873 total time=   7.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.869 total time=   3.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.870 total time=   6.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.875 total time=  21.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.875 total time=  14.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   6.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   6.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  23.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.863 total time=  25.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   3.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.874 total time=   7.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.872 total time=   3.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.870 total time=   6.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.875 total time=  22.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  17.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   6.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.875 total time=  19.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.868 total time=   6.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.871 total time=   5.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.875 total time=  18.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.872 total time=  21.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.872 total time=   5.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.872 total time=   8.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.877 total time=  26.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.876 total time=  18.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.875 total time=  25.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.877 total time=  15.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.877 total time=   4.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.878 total time=   2.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.880 total time=   4.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.877 total time=  14.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.877 total time=   8.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.880 total time=  14.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=   9.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.863 total time=   2.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.875 total time=   4.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  14.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.879 total time=   8.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.876 total time=   9.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.883 total time=   2.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.882 total time=   4.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.870 total time=   2.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.874 total time=   1.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   4.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.876 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.880 total time=   9.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.769 total time=  31.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.787 total time=   9.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.777 total time=  16.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.800 total time=   7.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.798 total time=  15.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.809 total time=   7.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.809 total time=  14.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.799 total time=  46.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.814 total time=  43.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.824 total time=  29.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.868 total time=   8.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.867 total time=  22.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.875 total time=  12.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.870 total time=  18.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.865 total time=  11.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.875 total time=  11.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=  14.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.868 total time=   3.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.855 total time=   6.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.869 total time=  19.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.861 total time=  13.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.872 total time=  14.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.862 total time=  18.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.865 total time=  13.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=   9.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  12.3s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  18.0s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.867 total time=   7.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.868 total time=   3.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.871 total time=   6.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.872 total time=  20.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.865 total time=  19.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.870 total time=  19.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.870 total time=  30.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.873 total time=  27.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.873 total time=  18.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.879 total time=  23.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.867 total time=  15.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.878 total time=  13.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  14.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.882 total time=   9.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.875 total time=   2.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.877 total time=   3.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.876 total time=  13.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.877 total time=   8.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.884 total time=  11.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=   5.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.876 total time=   3.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.868 total time=  10.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.867 total time=   2.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.878 total time=   5.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.868 total time=   4.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.877 total time=  14.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.877 total time=   9.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.770 total time=   8.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.780 total time=  49.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.794 total time=  44.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.810 total time=  27.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.801 total time=  44.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.818 total time=  28.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.864 total time=   4.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.861 total time=   7.3s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.852 total time=  22.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.870 total time=   3.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.869 total time=   8.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.867 total time=   3.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.867 total time=   6.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.869 total time=   3.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.875 total time=   7.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.874 total time=   3.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   7.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   3.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  19.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=  13.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.868 total time=  25.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.873 total time=   4.0s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.871 total time=   6.8s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.871 total time=  19.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.872 total time=  13.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.865 total time=  18.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.865 total time=  11.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  11.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.877 total time=  10.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.871 total time=  35.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   5.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.874 total time=   8.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   5.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.874 total time=   9.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.873 total time=  30.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.876 total time=  26.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.873 total time=  15.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.874 total time=   2.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.875 total time=   4.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.872 total time=   8.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.880 total time=  12.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.868 total time=   8.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.879 total time=   8.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.880 total time=   8.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.870 total time=   1.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.871 total time=   5.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  15.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.879 total time=  15.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.868 total time=  15.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.879 total time=  13.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.880 total time=   9.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.790 total time=   8.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.770 total time=  45.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.776 total time=  45.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.812 total time=  27.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.811 total time=  30.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.818 total time=   7.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.818 total time=  14.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.820 total time=   8.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.825 total time=  13.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.824 total time=  42.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.869 total time=  14.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.861 total time=  21.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.873 total time=  14.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.874 total time=   3.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   5.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.874 total time=  16.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.864 total time=   8.1s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.864 total time=   4.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.855 total time=  15.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.871 total time=  14.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.868 total time=  22.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.871 total time=  21.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.871 total time=  18.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.874 total time=  19.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.870 total time=  30.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.874 total time=  31.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.876 total time=   5.1s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   8.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.876 total time=   3.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.868 total time=   9.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.877 total time=  25.8s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.871 total time=  10.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.874 total time=   9.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.876 total time=  14.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.876 total time=  10.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.876 total time=   1.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.882 total time=  15.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.876 total time=  13.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.872 total time=  11.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   2.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.876 total time=   4.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.881 total time=   2.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.881 total time=   4.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.878 total time=   4.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.881 total time=  13.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=   9.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.876 total time=  12.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.779 total time=   7.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.777 total time=  16.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.790 total time=  48.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.812 total time=   8.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.810 total time=  14.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.811 total time=  42.0s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.824 total time=  27.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.824 total time=  28.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.863 total time=   3.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.864 total time=   7.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.865 total time=  18.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.855 total time=  23.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.872 total time=  13.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.872 total time=  20.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  14.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.873 total time=   6.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.860 total time=   3.6s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.865 total time=   7.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.862 total time=  22.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.871 total time=  20.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.861 total time=  14.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.873 total time=  18.6s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  12.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.878 total time=  10.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=  11.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.873 total time=  19.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.874 total time=  21.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.876 total time=   4.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.877 total time=   8.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.877 total time=  27.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.879 total time=  18.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.875 total time=   3.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.875 total time=  10.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.861 total time=   3.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   4.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.875 total time=  11.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.879 total time=   9.5s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.881 total time=  13.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.876 total time=   9.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.874 total time=   9.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.876 total time=   2.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.883 total time=   5.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.875 total time=   4.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   2.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=   4.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.865 total time=  10.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.867 total time=  12.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.877 total time=   4.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.874 total time=  12.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.871 total time=  10.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.879 total time=   2.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.880 total time=   5.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.776 total time=   8.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.770 total time=  48.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.787 total time=  47.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.808 total time=  29.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.825 total time=   7.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.818 total time=  16.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.825 total time=   7.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.825 total time=  12.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.824 total time=  38.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.866 total time=  22.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.868 total time=   3.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.869 total time=   6.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.874 total time=   3.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.871 total time=   8.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   3.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.874 total time=   6.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.872 total time=  19.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=  11.5s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.873 total time=  12.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.850 total time=   4.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.852 total time=   7.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.863 total time=   4.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.869 total time=   7.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.868 total time=  20.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.872 total time=  13.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.870 total time=  22.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  18.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=  11.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.871 total time=  10.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.874 total time=  17.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.875 total time=  19.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.873 total time=  18.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.874 total time=   4.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.875 total time=   9.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.878 total time=  30.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.879 total time=   9.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.873 total time=  25.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.880 total time=   2.8s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.867 total time=  14.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.876 total time=   8.6s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.876 total time=   8.4s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  13.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.882 total time=   7.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.871 total time=  10.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.875 total time=   2.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.877 total time=   4.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.881 total time=  14.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.880 total time=  10.4s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.876 total time=   3.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.875 total time=   5.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.875 total time=   4.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   4.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.777 total time=   7.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.790 total time=  46.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.802 total time=  45.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.792 total time=  28.8s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.801 total time=  31.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.819 total time=   7.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.819 total time=  14.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.824 total time=   6.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.818 total time=  14.6s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.864 total time=   3.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.867 total time=   8.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.864 total time=  14.3s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.865 total time=  15.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.872 total time=  20.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.867 total time=   6.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.870 total time=   5.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.868 total time=   6.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   2.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.873 total time=   7.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   6.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.877 total time=  19.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.868 total time=  16.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.868 total time=   6.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.866 total time=  12.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.872 total time=  21.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.875 total time=  13.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.871 total time=  18.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=  11.5s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.871 total time=  30.3s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.864 total time=  27.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.865 total time=  18.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.875 total time=  18.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.875 total time=  28.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.877 total time=  27.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.875 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.876 total time=  10.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.874 total time=  13.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.877 total time=   9.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.874 total time=   2.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.875 total time=   4.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   5.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.869 total time=   2.4s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.864 total time=  10.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  16.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.876 total time=  15.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.882 total time=  13.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.871 total time=  12.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.778 total time=  16.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.779 total time=  29.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.802 total time=  28.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.803 total time=  27.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.794 total time=  40.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.820 total time=  25.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.820 total time=  28.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.867 total time=   3.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.865 total time=   7.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.867 total time=  16.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.867 total time=  16.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.871 total time=  24.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.868 total time=  20.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.873 total time=  12.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   6.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  20.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.867 total time=  15.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.869 total time=   3.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.870 total time=   6.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.872 total time=  24.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.875 total time=  20.2s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.873 total time=  10.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.871 total time=   4.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.873 total time=   5.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.872 total time=   9.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.873 total time=  31.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   5.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.875 total time=   9.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.875 total time=  28.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.875 total time=  17.5s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.867 total time=   4.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.876 total time=   8.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.879 total time=  26.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.878 total time=   2.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.875 total time=   8.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.879 total time=   8.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.868 total time=  14.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.878 total time=   9.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.879 total time=   2.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.878 total time=   4.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.880 total time=  11.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.878 total time=   4.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.878 total time=   2.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.876 total time=   4.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.876 total time=  13.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.873 total time=   4.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.869 total time=   8.6s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.880 total time=   8.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.875 total time=   8.0s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  12.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.773 total time=   6.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.794 total time=  15.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.777 total time=  45.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.794 total time=  28.6s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.798 total time=   7.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.814 total time=  13.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.811 total time=  45.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.824 total time=  42.9s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.868 total time=  22.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.859 total time=   7.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   4.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.870 total time=  21.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.875 total time=  20.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.877 total time=  12.0s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  16.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.868 total time=   6.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.865 total time=  24.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.872 total time=  19.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.873 total time=  13.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.877 total time=  19.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.874 total time=  11.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  20.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.871 total time=  10.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.873 total time=   4.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.869 total time=   4.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.873 total time=   9.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.873 total time=  27.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.875 total time=  17.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.875 total time=  26.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.877 total time=  16.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.879 total time=  15.6s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.868 total time=  26.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.875 total time=   2.4s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.876 total time=   4.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.881 total time=   2.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.879 total time=   4.2s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.878 total time=  12.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.869 total time=  10.1s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.870 total time=   3.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.882 total time=   4.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   3.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.882 total time=  13.6s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.875 total time=   9.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.880 total time=  14.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.868 total time=  14.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.883 total time=  10.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   3.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.774 total time=   7.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.778 total time=  48.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.789 total time=  45.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.811 total time=  27.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.809 total time=  27.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.820 total time=   6.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.820 total time=  13.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.820 total time=  39.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.861 total time=  14.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.866 total time=  14.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.871 total time=  22.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.875 total time=  13.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.868 total time=  12.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   4.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   5.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.871 total time=  20.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.861 total time=  14.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.870 total time=   4.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.870 total time=   7.4s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   4.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.871 total time=   7.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   3.1s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.871 total time=   6.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.872 total time=  20.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.877 total time=  13.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.873 total time=   3.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   6.5s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  12.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   5.7s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.873 total time=  11.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.875 total time=  13.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.874 total time=  19.3s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.873 total time=  32.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.871 total time=   4.7s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.873 total time=  10.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   5.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.873 total time=  10.0s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   3.9s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.875 total time=   9.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.873 total time=  26.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.877 total time=  28.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.876 total time=   4.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.878 total time=   2.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.875 total time=   4.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   2.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.873 total time=   4.1s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.876 total time=  14.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.879 total time=   2.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.871 total time=   3.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=   9.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.875 total time=   3.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.874 total time=   9.8s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.877 total time=  16.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.877 total time=  12.4s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.879 total time=   9.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.877 total time=   8.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.882 total time=  12.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.790 total time=  15.7s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.761 total time=  32.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.783 total time=  28.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.800 total time=  30.5s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.804 total time=   6.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.811 total time=  12.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.795 total time=  47.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.818 total time=  42.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.862 total time=  24.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.871 total time=  15.9s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.872 total time=   6.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.871 total time=  22.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  19.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.873 total time=  12.5s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.862 total time=  22.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.869 total time=  14.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.870 total time=   3.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.872 total time=   6.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.875 total time=  20.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  13.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.865 total time=   6.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.877 total time=  13.7s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.870 total time=  30.2s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.872 total time=  20.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.874 total time=   4.7s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.875 total time=  10.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.874 total time=  28.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.877 total time=  25.0s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.876 total time=  17.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.873 total time=  15.8s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.878 total time=  10.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.879 total time=  14.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.877 total time=  14.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.884 total time=  12.1s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.875 total time=   8.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.874 total time=  10.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.877 total time=  14.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.876 total time=  15.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.883 total time=   2.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.877 total time=   4.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.883 total time=   2.2s\n",
      "[CV 6/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.883 total time=   3.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  11.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.779 total time=   8.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.764 total time=  16.0s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.783 total time=  46.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.794 total time=  51.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.820 total time=   6.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.824 total time=  13.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.824 total time=  39.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.824 total time=  29.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.863 total time=   4.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.864 total time=   7.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.865 total time=  25.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.861 total time=   4.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.870 total time=   7.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.872 total time=   3.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.870 total time=   6.4s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.871 total time=  20.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.865 total time=  19.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.862 total time=  14.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.868 total time=  22.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.874 total time=  14.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.861 total time=   3.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   7.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.864 total time=   4.2s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   6.3s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.873 total time=  21.8s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  13.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.867 total time=   7.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.869 total time=   7.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.870 total time=  22.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.875 total time=  15.4s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   7.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   3.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.874 total time=   7.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.873 total time=   3.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.874 total time=   6.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.874 total time=   2.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   6.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.865 total time=  14.8s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.874 total time=  10.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.870 total time=   8.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.875 total time=  30.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.877 total time=  28.7s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.866 total time=  27.0s\n",
      "[CV 1/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.867 total time=  16.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.879 total time=  18.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.876 total time=   4.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.875 total time=  10.6s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.875 total time=   2.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.876 total time=   4.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.877 total time=   8.8s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.876 total time=   2.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.882 total time=   4.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.877 total time=  13.3s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.882 total time=   9.9s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   3.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.874 total time=   9.2s\n",
      "[CV 1/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.865 total time=  15.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.880 total time=  10.3s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.877 total time=   5.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.877 total time=   2.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.877 total time=   6.1s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.880 total time=   4.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.879 total time=   2.5s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.878 total time=   3.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.882 total time=  12.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.776 total time=   8.5s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.793 total time=  32.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.794 total time=  27.9s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.803 total time=  47.9s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.817 total time=  46.3s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.819 total time=  26.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.864 total time=   4.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.861 total time=   7.7s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.862 total time=  16.1s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.870 total time=  14.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.869 total time=  12.7s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.874 total time=  14.4s\n",
      "[CV 4/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.869 total time=   3.9s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   6.1s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.864 total time=   2.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.865 total time=   5.6s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.877 total time=  12.2s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.875 total time=  20.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.865 total time=  15.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.870 total time=   4.1s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.873 total time=   7.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.867 total time=  20.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.875 total time=  13.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.875 total time=  22.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   2.8s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   7.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  13.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.873 total time=  20.5s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  11.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  14.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.869 total time=  28.9s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.875 total time=  18.8s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.874 total time=  21.6s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.872 total time=   4.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   9.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.873 total time=   4.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.873 total time=   9.7s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.875 total time=   5.1s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.875 total time=   7.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.878 total time=   5.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.874 total time=   9.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.872 total time=   2.7s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.876 total time=   4.9s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.877 total time=  10.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.876 total time=   9.7s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.878 total time=  14.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.879 total time=   8.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.881 total time=   7.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.875 total time=   7.5s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.880 total time=  13.2s\n",
      "[CV 7/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.873 total time=  10.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.875 total time=   2.6s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.879 total time=   5.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.873 total time=  13.2s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.879 total time=   8.1s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.874 total time=   8.7s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.876 total time=  13.4s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  12.7s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.794 total time=   7.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.773 total time=  14.5s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.780 total time=  46.7s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.810 total time=  28.6s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.800 total time=  43.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.824 total time=  27.4s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.814 total time=   6.2s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.820 total time=  13.1s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.824 total time=  42.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.871 total time=   7.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.870 total time=   3.8s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.872 total time=   7.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.870 total time=   3.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.871 total time=   6.6s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.875 total time=  22.3s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.877 total time=  20.8s\n",
      "[CV 6/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  22.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.855 total time=  23.6s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.870 total time=  13.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=300;, score=0.868 total time=  19.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  13.9s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.864 total time=   3.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.875 total time=   6.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.874 total time=  14.5s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.870 total time=   5.8s\n",
      "[CV 8/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.869 total time=   5.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.874 total time=  18.5s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.876 total time=  18.9s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.872 total time=  18.4s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.877 total time=   4.9s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.878 total time=   9.8s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.879 total time=   4.4s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.879 total time=   8.7s\n",
      "[CV 6/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.879 total time=  26.0s\n",
      "[CV 5/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.879 total time=  18.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.877 total time=   2.8s\n",
      "[CV 7/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.874 total time=   4.6s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.879 total time=  15.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.876 total time=   9.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.881 total time=   2.5s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.876 total time=   4.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.874 total time=  13.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.878 total time=   8.1s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.878 total time=  13.3s\n",
      "[CV 4/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.876 total time=   9.3s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.873 total time=  11.5s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.873 total time=   2.3s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.876 total time=   8.9s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.877 total time=   8.6s\n",
      "[CV 9/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.878 total time=  12.1s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.882 total time=   7.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.774 total time=  14.8s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.790 total time=  15.2s\n",
      "[CV 6/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.773 total time=  46.0s\n",
      "[CV 2/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.812 total time=  39.4s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.809 total time=  15.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.824 total time=   7.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.819 total time=  13.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.820 total time=   7.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.820 total time=  12.2s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.825 total time=  37.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.868 total time=  14.6s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.862 total time=   7.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=300;, score=0.866 total time=  22.0s\n",
      "[CV 8/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.870 total time=  13.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.871 total time=   3.4s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.872 total time=   6.9s\n",
      "[CV 9/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.872 total time=  20.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.871 total time=  13.9s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.860 total time=   4.4s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.851 total time=  15.2s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.863 total time=  13.3s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.873 total time=  19.7s\n",
      "[CV 2/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.872 total time=  12.5s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.875 total time=  12.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.874 total time=  13.0s\n",
      "[CV 5/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.877 total time=   3.0s\n",
      "[CV 6/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.877 total time=   6.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.873 total time=  14.2s\n",
      "[CV 9/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.872 total time=  29.5s\n",
      "[CV 7/10] END bootstrap=True, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.872 total time=  18.9s\n",
      "[CV 2/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.875 total time=  27.0s\n",
      "[CV 4/10] END bootstrap=True, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.876 total time=  18.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.876 total time=  23.4s\n",
      "[CV 10/10] END bootstrap=True, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.873 total time=   9.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.863 total time=   5.4s\n",
      "[CV 4/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.874 total time=  10.2s\n",
      "[CV 3/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.875 total time=  10.9s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.876 total time=  13.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.882 total time=  10.0s\n",
      "[CV 2/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.879 total time=   1.9s\n",
      "[CV 1/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.871 total time=   4.1s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.883 total time=   9.0s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=50;, score=0.883 total time=   2.3s\n",
      "[CV 6/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=100;, score=0.883 total time=   4.3s\n",
      "[CV 8/10] END bootstrap=True, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.877 total time=  12.3s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.876 total time=   9.2s\n",
      "[CV 5/10] END bootstrap=True, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200;, score=0.880 total time=   8.2s\n",
      "[CV 8/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.876 total time=   8.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.874 total time=  11.0s\n",
      "[CV 10/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.874 total time=   2.6s\n",
      "[CV 2/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=200;, score=0.880 total time=   8.7s\n",
      "[CV 3/10] END bootstrap=True, max_features=log2, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.878 total time=  14.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.776 total time=  30.3s\n",
      "[CV 3/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.802 total time=   6.8s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.776 total time=  14.9s\n",
      "[CV 8/10] END bootstrap=False, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300;, score=0.778 total time=  44.1s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=3, n_estimators=200;, score=0.803 total time=  27.2s\n",
      "[CV 4/10] END bootstrap=False, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.806 total time=  38.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.814 total time=  28.3s\n",
      "[CV 7/10] END bootstrap=False, max_features=auto, min_samples_leaf=3, min_samples_split=5, n_estimators=200;, score=0.825 total time=  25.9s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.867 total time=   3.2s\n",
      "[CV 1/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.855 total time=   7.1s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.863 total time=  16.8s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.873 total time=   4.1s\n",
      "[CV 7/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.871 total time=   6.5s\n",
      "[CV 10/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.868 total time=  21.4s\n",
      "[CV 5/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300;, score=0.875 total time=  19.2s\n",
      "[CV 3/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=200;, score=0.874 total time=  12.3s\n",
      "[CV 2/10] END bootstrap=False, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.875 total time=  22.7s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=200;, score=0.868 total time=  14.4s\n",
      "[CV 3/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.871 total time=   3.0s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.861 total time=   7.5s\n",
      "[CV 1/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300;, score=0.861 total time=  20.6s\n",
      "[CV 4/10] END bootstrap=False, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200;, score=0.871 total time=  14.1s\n",
      "[CV 9/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=2, n_estimators=300;, score=0.874 total time=  21.7s\n",
      "[CV 10/10] END bootstrap=False, max_features=log2, min_samples_leaf=3, min_samples_split=5, n_estimators=300;, score=0.871 total time=  14.0s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "SEED = 98\n",
    "param_dict = {\n",
    "                #'learning_rate': [ 0.05, 0.07, 0.1], #[0.01, 0.03, 0.05],\n",
    "                'n_estimators': [50, 100, 200, 300],\n",
    "                'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                #'n_estimators': [100],#[25, 50, 75, 100],\n",
    "                #'n_estimators': [50],\n",
    "                'min_samples_split': [2,3,5],\n",
    "                'min_samples_leaf': [1,2,3],\n",
    "               'bootstrap': [True, False]\n",
    "                #'gamma': [0.9], #[0.3, 0.5, 0.7, 0.9],\n",
    "                #'scale_pos_weight': [5, 10], #[1,10,30,100]\n",
    "            }\n",
    "nfold = 10\n",
    "gs = GridSearchCV(estimator=RandomForestRegressor(random_state = SEED),\n",
    "                    n_jobs=-1,\n",
    "                    verbose=5,\n",
    "                    param_grid=param_dict, cv=nfold)\n",
    "gs.fit(x_train_imputed, y_train)\n",
    "#model = gs.best_estimator_.get_booster()\n",
    "\n",
    "print()\n",
    "print(\"========= found hyperparameter =========\")\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(\"========================================\")\n",
    "\n",
    "y_pred = gs.predict(x_test_imputed).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test)\n",
    "acc3 = np.mean((y_pred >= y_test-0.5) & (y_pred <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da3b3e47-4210-4003-bca3-e541c27756e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T06:25:07.436597Z",
     "iopub.status.busy": "2023-02-17T06:25:07.436082Z",
     "iopub.status.idle": "2023-02-17T06:25:07.451274Z",
     "shell.execute_reply": "2023-02-17T06:25:07.450547Z",
     "shell.execute_reply.started": "2023-02-17T06:25:07.436540Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "new model\n",
      "--------------\n",
      "explained_variance_score: 0.860\n",
      "mean_squared_errors: 0.139\n",
      "mean_absolute_errors: 0.224\n",
      "r2_score: 0.860\n",
      "acc: 0.595\n",
      "acc(+-0.5mm): 0.963\n"
     ]
    }
   ],
   "source": [
    "print('--------------')\n",
    "print('new model')\n",
    "print('--------------')\n",
    "print(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "print(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "# accuracy\n",
    "acc1 = np.mean(y_pred==y_test)\n",
    "acc3 = np.mean((y_pred >= y_test-0.5) & (y_pred <= y_test+0.5))\n",
    "print(f'acc: {acc1:.3f}')\n",
    "print(f'acc(+-0.5mm): {acc3:.3f}')\n",
    "\n",
    "# save model\n",
    "odir_f = f'acc1-{acc1:.3f}_acc3-{acc3:.3f}_RF_{nfold}fold'\n",
    "odir = f'result/{odir_f}'\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "#model.save_model(f'{odir}/model.model')\n",
    "pickle.dump(gs, open(f'{odir}/gridSearch','wb'))\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "pickle.dump(param_dict, open(f'{odir}/param_dict', 'wb'))\n",
    "f = open(f'{odir}/result.txt', 'w')\n",
    "f.write(f'classification model')\n",
    "f.write(f'explained_variance_score: {explained_variance_score(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_squared_errors: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "f.write(f'mean_absolute_errors: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "f.write(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7dd5a1-5bb0-47f7-a345-2db0fe75a0e7",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe6dfa3-2cac-401f-a2fd-2131dd8311e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T01:58:53.028999Z",
     "iopub.status.busy": "2023-02-18T01:58:53.028449Z",
     "iopub.status.idle": "2023-02-18T01:58:56.117327Z",
     "shell.execute_reply": "2023-02-18T01:58:56.116737Z",
     "shell.execute_reply.started": "2023-02-18T01:58:53.028873Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import losses, metrics\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization, Dropout, Activation, Input\n",
    "from keras.layers import GlobalAveragePooling1D, Flatten, SeparableConv1D, concatenate, Add\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats, interp\n",
    "import os, sys, pickle, shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, datetime, time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# tensorflow 사용 시 seed 고정\n",
    "def seed_everything(seed: int = 98):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "SEED = 98\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf224896-6db3-40ad-a6df-549453d8e54d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T01:58:56.118615Z",
     "iopub.status.busy": "2023-02-18T01:58:56.118380Z",
     "iopub.status.idle": "2023-02-18T01:58:56.128196Z",
     "shell.execute_reply": "2023-02-18T01:58:56.127676Z",
     "shell.execute_reply.started": "2023-02-18T01:58:56.118591Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (27234, 5), x_test: (6808, 5)\n"
     ]
    }
   ],
   "source": [
    "dat = np.load(f'dataset/ETT_size.npz')\n",
    "x, y = dat['x'], dat['y']\n",
    "#y_old  = dat['y_old']\n",
    "\n",
    "# training set의 뒤쪽 20%를 test set 으로 사용\n",
    "nsamp = len(y)\n",
    "ntest = int(nsamp * 0.2)\n",
    "ntrain = nsamp - ntest\n",
    "x_test = x[-ntest:, :]\n",
    "y_test = y[-ntest:]\n",
    "#y_test_old = y_old[-ntest:]\n",
    "x_train = x[:ntrain, :]\n",
    "y_train = y[:ntrain]\n",
    "\n",
    "print(f'x_train: {(x_train).shape}, x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81c772d-efb6-4ed1-96a1-cd75fc9dd925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T01:58:58.508581Z",
     "iopub.status.busy": "2023-02-18T01:58:58.508165Z",
     "iopub.status.idle": "2023-02-18T01:58:58.526396Z",
     "shell.execute_reply": "2023-02-18T01:58:58.525757Z",
     "shell.execute_reply.started": "2023-02-18T01:58:58.508533Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(pd.DataFrame(x_train))\n",
    "x_test = sc.transform(pd.DataFrame(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e22a5-fffb-4909-8ca0-942c4abbcd28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T06:55:13.891179Z",
     "iopub.status.busy": "2023-02-17T06:55:13.890746Z",
     "iopub.status.idle": "2023-02-17T06:55:14.216891Z",
     "shell.execute_reply": "2023-02-17T06:55:14.215723Z",
     "shell.execute_reply.started": "2023-02-17T06:55:13.891138Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(df.loc[:ntrain-1,INPUT_VARS])\n",
    "x_test = sc.transform(df.loc[ntrain:,INPUT_VARS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3c74cd-f749-476a-a20b-6fc7ca76c452",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T01:58:59.758569Z",
     "iopub.status.busy": "2023-02-18T01:58:59.758128Z",
     "iopub.status.idle": "2023-02-18T01:58:59.943546Z",
     "shell.execute_reply": "2023-02-18T01:58:59.942822Z",
     "shell.execute_reply.started": "2023-02-18T01:58:59.758520Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "imp = IterativeImputer().fit(x_train)\n",
    "x_train_imputed = imp.transform(x_train)\n",
    "x_test_imputed = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd558086-ada5-465e-b2fd-487f61617d34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T01:59:01.402974Z",
     "iopub.status.busy": "2023-02-18T01:59:01.402627Z",
     "iopub.status.idle": "2023-02-18T01:59:01.427110Z",
     "shell.execute_reply": "2023-02-18T01:59:01.426518Z",
     "shell.execute_reply.started": "2023-02-18T01:59:01.402934Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start making test settings...done\n",
      "2023-02-18 10:59:01.424856\n"
     ]
    }
   ],
   "source": [
    "# folder\n",
    "nfold = 10  # 각각의 hyperparameter에 대해 k-fold 를 시행하고 평균을 구한다.\n",
    "ntest = 500\n",
    "rootdir = f\"result/size/DNN_size_both_y\"\n",
    "\n",
    "if not os.path.exists(rootdir):\n",
    "    os.mkdir(rootdir)\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "f = open(f'{rootdir}/README.txt', 'w')\n",
    "f.write(f'model: DNN 2 layers, regression')\n",
    "f.write(f'input: age, sex, height, weight, cuffed 유무  output: tube size')\n",
    "f.close()\n",
    "    \n",
    "\n",
    "# test_settings\n",
    "layer_settings, test_settings = [], []\n",
    "\n",
    "\n",
    "# hyperparamters pool\n",
    "dropout_opts  = [0, 0.1, 0.2, 0.3, 0.4, 0.5] # dropout rate\n",
    "dense_opts = [16, 32, 64, 128, 256, 512]\n",
    "BATCH_SIZE = [32, 64, 128, 256, 512]\n",
    "lr_opts = [0.001, 0.002, 0.0005]\n",
    "\n",
    "print('start making test settings...', end='', flush=True)\n",
    "# test settings\n",
    "dnodes, dropouts = [], []\n",
    "for i in range(2):\n",
    "    dnodes.append(0)\n",
    "    dropouts.append(0)\n",
    "\n",
    "\n",
    "for dnode1 in dense_opts:\n",
    "    for dropout1 in dropout_opts:\n",
    "        for dnode2 in dense_opts:\n",
    "            for dropout2 in dropout_opts:\n",
    "                for batch_size in BATCH_SIZE:\n",
    "                    for learning_rate in lr_opts:\n",
    "                        test_settings.append([dnode1, dropout1, dnode2, dropout2, batch_size, learning_rate])                                   \n",
    "\n",
    "                        \n",
    "print('done')\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0817b887-6325-4dfb-8f4c-269786b0d41d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T01:59:03.626753Z",
     "iopub.status.busy": "2023-02-18T01:59:03.626023Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random search 0/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 10:59:03.711163: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 10:59:04.435367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30973 MB memory:  -> device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0f:00.0, compute capability: 7.0\n",
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.9278\n",
      "Epoch 00001: val_loss improved from inf to 0.19348, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "613/613 [==============================] - 4s 4ms/step - loss: 0.9244 - val_loss: 0.1935\n",
      "Epoch 2/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.2676\n",
      "Epoch 00002: val_loss improved from 0.19348 to 0.13977, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2676 - val_loss: 0.1398\n",
      "Epoch 3/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.2599\n",
      "Epoch 00003: val_loss did not improve from 0.13977\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2595 - val_loss: 0.1916\n",
      "Epoch 4/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.2632\n",
      "Epoch 00004: val_loss improved from 0.13977 to 0.13927, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2624 - val_loss: 0.1393\n",
      "Epoch 5/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.2518\n",
      "Epoch 00005: val_loss did not improve from 0.13927\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2514 - val_loss: 0.1444\n",
      "Epoch 6/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.2522\n",
      "Epoch 00006: val_loss did not improve from 0.13927\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2515 - val_loss: 0.1553\n",
      " ###0 fold : val acc1 0.559, acc3 0.956, mae 0.249###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612/613 [============================>.] - ETA: 0s - loss: 0.8756\n",
      "Epoch 00001: val_loss improved from inf to 0.20557, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 0.8750 - val_loss: 0.2056\n",
      "Epoch 2/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.2763\n",
      "Epoch 00002: val_loss improved from 0.20557 to 0.13710, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2759 - val_loss: 0.1371\n",
      "Epoch 3/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.2791\n",
      "Epoch 00003: val_loss did not improve from 0.13710\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2789 - val_loss: 0.2821\n",
      "Epoch 4/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.2498\n",
      "Epoch 00004: val_loss did not improve from 0.13710\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2492 - val_loss: 0.1432\n",
      " ###1 fold : val acc1 0.566, acc3 0.959, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - ETA: 0s - loss: 0.8660\n",
      "Epoch 00001: val_loss improved from inf to 0.20387, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 0.8660 - val_loss: 0.2039\n",
      "Epoch 2/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.2742\n",
      "Epoch 00002: val_loss improved from 0.20387 to 0.14028, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2740 - val_loss: 0.1403\n",
      "Epoch 3/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.2838\n",
      "Epoch 00003: val_loss did not improve from 0.14028\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2842 - val_loss: 0.3150\n",
      "Epoch 4/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.2604\n",
      "Epoch 00004: val_loss did not improve from 0.14028\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2604 - val_loss: 0.1680\n",
      " ###2 fold : val acc1 0.544, acc3 0.958, mae 0.252###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599/613 [============================>.] - ETA: 0s - loss: 0.8958\n",
      "Epoch 00001: val_loss improved from inf to 0.18587, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 0.8830 - val_loss: 0.1859\n",
      "Epoch 2/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.2737\n",
      "Epoch 00002: val_loss improved from 0.18587 to 0.14256, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2729 - val_loss: 0.1426\n",
      "Epoch 3/100\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.3089\n",
      "Epoch 00003: val_loss did not improve from 0.14256\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.3089 - val_loss: 0.2841\n",
      "Epoch 4/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.2555\n",
      "Epoch 00004: val_loss did not improve from 0.14256\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2551 - val_loss: 0.1820\n",
      " ###3 fold : val acc1 0.554, acc3 0.960, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - ETA: 0s - loss: 0.9150\n",
      "Epoch 00001: val_loss improved from inf to 0.17072, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 0.9150 - val_loss: 0.1707\n",
      "Epoch 2/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.2974\n",
      "Epoch 00002: val_loss improved from 0.17072 to 0.15626, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2973 - val_loss: 0.1563\n",
      "Epoch 3/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.3098\n",
      "Epoch 00003: val_loss improved from 0.15626 to 0.15126, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3090 - val_loss: 0.1513\n",
      "Epoch 4/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.2597\n",
      "Epoch 00004: val_loss did not improve from 0.15126\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2598 - val_loss: 0.1533\n",
      "Epoch 5/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.2502\n",
      "Epoch 00005: val_loss improved from 0.15126 to 0.13472, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2499 - val_loss: 0.1347\n",
      "Epoch 6/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.2817\n",
      "Epoch 00006: val_loss improved from 0.13472 to 0.13291, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2792 - val_loss: 0.1329\n",
      "Epoch 7/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.2113\n",
      "Epoch 00007: val_loss did not improve from 0.13291\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2109 - val_loss: 0.1425\n",
      "Epoch 8/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.2169\n",
      "Epoch 00008: val_loss improved from 0.13291 to 0.13017, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2165 - val_loss: 0.1302\n",
      "Epoch 9/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.2243\n",
      "Epoch 00009: val_loss did not improve from 0.13017\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2239 - val_loss: 0.1321\n",
      "Epoch 10/100\n",
      "608/613 [============================>.] - ETA: 0s - loss: 0.2404\n",
      "Epoch 00010: val_loss did not improve from 0.13017\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2405 - val_loss: 0.1326\n",
      " ###4 fold : val acc1 0.565, acc3 0.965, mae 0.238###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611/613 [============================>.] - ETA: 0s - loss: 0.5446\n",
      "Epoch 00001: val_loss improved from inf to 0.20312, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 0.5436 - val_loss: 0.2031\n",
      "Epoch 2/100\n",
      "608/613 [============================>.] - ETA: 0s - loss: 0.2506\n",
      "Epoch 00002: val_loss improved from 0.20312 to 0.17047, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2505 - val_loss: 0.1705\n",
      "Epoch 3/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.2181\n",
      "Epoch 00003: val_loss improved from 0.17047 to 0.14858, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2179 - val_loss: 0.1486\n",
      "Epoch 4/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.2055\n",
      "Epoch 00004: val_loss did not improve from 0.14858\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2056 - val_loss: 0.1520\n",
      "Epoch 5/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.1894\n",
      "Epoch 00005: val_loss did not improve from 0.14858\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.1893 - val_loss: 0.2021\n",
      " ###5 fold : val acc1 0.547, acc3 0.958, mae 0.259###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609/613 [============================>.] - ETA: 0s - loss: 1.1688\n",
      "Epoch 00001: val_loss improved from inf to 0.14863, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 1.1633 - val_loss: 0.1486\n",
      "Epoch 2/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.2978\n",
      "Epoch 00002: val_loss did not improve from 0.14863\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2978 - val_loss: 0.1860\n",
      "Epoch 3/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.2587\n",
      "Epoch 00003: val_loss improved from 0.14863 to 0.14522, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2577 - val_loss: 0.1452\n",
      "Epoch 4/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.2847\n",
      "Epoch 00004: val_loss did not improve from 0.14522\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2846 - val_loss: 0.1459\n",
      "Epoch 5/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.3208\n",
      "Epoch 00005: val_loss did not improve from 0.14522\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3192 - val_loss: 0.1499\n",
      " ###6 fold : val acc1 0.545, acc3 0.950, mae 0.257###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601/613 [============================>.] - ETA: 0s - loss: 1.1516\n",
      "Epoch 00001: val_loss improved from inf to 0.15978, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 1.1346 - val_loss: 0.1598\n",
      "Epoch 2/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.3361\n",
      "Epoch 00002: val_loss did not improve from 0.15978\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3354 - val_loss: 0.1656\n",
      "Epoch 3/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.3062\n",
      "Epoch 00003: val_loss improved from 0.15978 to 0.14360, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3046 - val_loss: 0.1436\n",
      "Epoch 4/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.3030\n",
      "Epoch 00004: val_loss did not improve from 0.14360\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3016 - val_loss: 0.1659\n",
      "Epoch 5/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.3114\n",
      "Epoch 00005: val_loss did not improve from 0.14360\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3107 - val_loss: 0.1438\n",
      " ###7 fold : val acc1 0.546, acc3 0.954, mae 0.254###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - ETA: 0s - loss: 1.1416\n",
      "Epoch 00001: val_loss improved from inf to 0.15102, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 1.1416 - val_loss: 0.1510\n",
      "Epoch 2/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.3356\n",
      "Epoch 00002: val_loss did not improve from 0.15102\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3348 - val_loss: 0.1543\n",
      "Epoch 3/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.3109\n",
      "Epoch 00003: val_loss improved from 0.15102 to 0.14850, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3098 - val_loss: 0.1485\n",
      "Epoch 4/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.3055\n",
      "Epoch 00004: val_loss did not improve from 0.14850\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3046 - val_loss: 0.1565\n",
      "Epoch 5/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.3216\n",
      "Epoch 00005: val_loss improved from 0.14850 to 0.13994, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3203 - val_loss: 0.1399\n",
      "Epoch 6/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.2523\n",
      "Epoch 00006: val_loss did not improve from 0.13994\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2522 - val_loss: 0.1402\n",
      "Epoch 7/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.2281\n",
      "Epoch 00007: val_loss improved from 0.13994 to 0.13547, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2279 - val_loss: 0.1355\n",
      "Epoch 8/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.1940\n",
      "Epoch 00008: val_loss improved from 0.13547 to 0.12956, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.1939 - val_loss: 0.1296\n",
      "Epoch 9/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.1904\n",
      "Epoch 00009: val_loss did not improve from 0.12956\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.1905 - val_loss: 0.1388\n",
      "Epoch 10/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.2009\n",
      "Epoch 00010: val_loss did not improve from 0.12956\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2013 - val_loss: 0.1389\n",
      " ###8 fold : val acc1 0.572, acc3 0.957, mae 0.239###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612/613 [============================>.] - ETA: 0s - loss: 1.1428\n",
      "Epoch 00001: val_loss improved from inf to 0.14796, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 1.1416 - val_loss: 0.1480\n",
      "Epoch 2/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.3350\n",
      "Epoch 00002: val_loss did not improve from 0.14796\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3348 - val_loss: 0.1540\n",
      "Epoch 3/100\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.3098\n",
      "Epoch 00003: val_loss improved from 0.14796 to 0.14688, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3098 - val_loss: 0.1469\n",
      "Epoch 4/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.3061\n",
      "Epoch 00004: val_loss did not improve from 0.14688\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3046 - val_loss: 0.1575\n",
      "Epoch 5/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.3225\n",
      "Epoch 00005: val_loss improved from 0.14688 to 0.14155, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.3203 - val_loss: 0.1416\n",
      "Epoch 6/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.2528\n",
      "Epoch 00006: val_loss improved from 0.14155 to 0.13911, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2522 - val_loss: 0.1391\n",
      "Epoch 7/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.2282\n",
      "Epoch 00007: val_loss improved from 0.13911 to 0.13635, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2279 - val_loss: 0.1364\n",
      "Epoch 8/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.1941\n",
      "Epoch 00008: val_loss improved from 0.13635 to 0.12956, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.4,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.1939 - val_loss: 0.1296\n",
      "Epoch 9/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.1902\n",
      "Epoch 00009: val_loss did not improve from 0.12956\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.1905 - val_loss: 0.1388\n",
      "Epoch 10/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.2014\n",
      "Epoch 00010: val_loss did not improve from 0.12956\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2013 - val_loss: 0.1369\n",
      " ###9 fold : val acc1 0.580, acc3 0.961, mae 0.234###\n",
      "acc10.558_acc30.958\n",
      "random search 1/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/77 [========================>.....] - ETA: 0s - loss: 19.2984\n",
      "Epoch 00001: val_loss improved from inf to 13.59509, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5708 - val_loss: 13.5951\n",
      "Epoch 2/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 10.5923\n",
      "Epoch 00002: val_loss improved from 13.59509 to 6.51969, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.0363 - val_loss: 6.5197\n",
      "Epoch 3/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 4.8782\n",
      "Epoch 00003: val_loss improved from 6.51969 to 2.76211, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 4.7532 - val_loss: 2.7621\n",
      "Epoch 4/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 2.3928\n",
      "Epoch 00004: val_loss improved from 2.76211 to 1.47162, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.6058 - val_loss: 1.4716\n",
      "Epoch 5/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.8734\n",
      "Epoch 00005: val_loss improved from 1.47162 to 0.92223, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7302 - val_loss: 0.9222\n",
      "Epoch 6/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.2339\n",
      "Epoch 00006: val_loss improved from 0.92223 to 0.63317, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2233 - val_loss: 0.6332\n",
      "Epoch 7/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.0257\n",
      "Epoch 00007: val_loss improved from 0.63317 to 0.46899, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0112 - val_loss: 0.4690\n",
      "Epoch 8/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.8263\n",
      "Epoch 00008: val_loss improved from 0.46899 to 0.37659, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8155 - val_loss: 0.3766\n",
      "Epoch 9/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.6934\n",
      "Epoch 00009: val_loss improved from 0.37659 to 0.31443, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6863 - val_loss: 0.3144\n",
      "Epoch 10/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6112\n",
      "Epoch 00010: val_loss improved from 0.31443 to 0.27736, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6115 - val_loss: 0.2774\n",
      "Epoch 11/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6322\n",
      "Epoch 00011: val_loss improved from 0.27736 to 0.25121, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6271 - val_loss: 0.2512\n",
      "Epoch 12/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6171\n",
      "Epoch 00012: val_loss improved from 0.25121 to 0.23599, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6143 - val_loss: 0.2360\n",
      "Epoch 13/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.5821\n",
      "Epoch 00013: val_loss improved from 0.23599 to 0.22436, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5796 - val_loss: 0.2244\n",
      "Epoch 14/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5124\n",
      "Epoch 00014: val_loss improved from 0.22436 to 0.20965, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5111 - val_loss: 0.2096\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4983\n",
      "Epoch 00015: val_loss improved from 0.20965 to 0.20056, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4983 - val_loss: 0.2006\n",
      "Epoch 16/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4822\n",
      "Epoch 00016: val_loss improved from 0.20056 to 0.19279, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4824 - val_loss: 0.1928\n",
      "Epoch 17/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4884\n",
      "Epoch 00017: val_loss improved from 0.19279 to 0.18764, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4875 - val_loss: 0.1876\n",
      "Epoch 18/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.5179\n",
      "Epoch 00018: val_loss improved from 0.18764 to 0.18198, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5165 - val_loss: 0.1820\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4666\n",
      "Epoch 00019: val_loss improved from 0.18198 to 0.17927, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4666 - val_loss: 0.1793\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4494\n",
      "Epoch 00020: val_loss improved from 0.17927 to 0.17377, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4494 - val_loss: 0.1738\n",
      "Epoch 21/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.4365\n",
      "Epoch 00021: val_loss improved from 0.17377 to 0.17204, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4356 - val_loss: 0.1720\n",
      "Epoch 22/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.4365\n",
      "Epoch 00022: val_loss improved from 0.17204 to 0.16477, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4377 - val_loss: 0.1648\n",
      "Epoch 23/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.4319\n",
      "Epoch 00023: val_loss improved from 0.16477 to 0.16327, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4312 - val_loss: 0.1633\n",
      "Epoch 24/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.4235\n",
      "Epoch 00024: val_loss improved from 0.16327 to 0.15864, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4241 - val_loss: 0.1586\n",
      "Epoch 25/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4028\n",
      "Epoch 00025: val_loss improved from 0.15864 to 0.15863, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4172 - val_loss: 0.1586\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4216\n",
      "Epoch 00026: val_loss improved from 0.15863 to 0.15695, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4216 - val_loss: 0.1570\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4062\n",
      "Epoch 00027: val_loss improved from 0.15695 to 0.15318, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4059 - val_loss: 0.1532\n",
      "Epoch 28/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4017\n",
      "Epoch 00028: val_loss improved from 0.15318 to 0.15149, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4004 - val_loss: 0.1515\n",
      "Epoch 29/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3777\n",
      "Epoch 00029: val_loss improved from 0.15149 to 0.14829, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3834 - val_loss: 0.1483\n",
      "Epoch 30/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4253\n",
      "Epoch 00030: val_loss did not improve from 0.14829\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4236 - val_loss: 0.1498\n",
      "Epoch 31/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3798\n",
      "Epoch 00031: val_loss improved from 0.14829 to 0.14567, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3806 - val_loss: 0.1457\n",
      "Epoch 32/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3721\n",
      "Epoch 00032: val_loss improved from 0.14567 to 0.14508, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3730 - val_loss: 0.1451\n",
      "Epoch 33/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3730\n",
      "Epoch 00033: val_loss improved from 0.14508 to 0.14374, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3726 - val_loss: 0.1437\n",
      "Epoch 34/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3675\n",
      "Epoch 00034: val_loss did not improve from 0.14374\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3688 - val_loss: 0.1461\n",
      "Epoch 35/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3828\n",
      "Epoch 00035: val_loss did not improve from 0.14374\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3828 - val_loss: 0.1439\n",
      " ###0 fold : val acc1 0.566, acc3 0.953, mae 0.247###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/77 [==========================>...] - ETA: 0s - loss: 18.9252\n",
      "Epoch 00001: val_loss improved from inf to 13.58276, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 18.5561 - val_loss: 13.5828\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 10.3997\n",
      "Epoch 00002: val_loss improved from 13.58276 to 6.51094, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.0374 - val_loss: 6.5109\n",
      "Epoch 3/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 4.8014\n",
      "Epoch 00003: val_loss improved from 6.51094 to 2.75632, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.7617 - val_loss: 2.7563\n",
      "Epoch 4/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 2.3692\n",
      "Epoch 00004: val_loss improved from 2.75632 to 1.46880, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.6110 - val_loss: 1.4688\n",
      "Epoch 5/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.8291\n",
      "Epoch 00005: val_loss improved from 1.46880 to 0.92119, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.7304 - val_loss: 0.9212\n",
      "Epoch 6/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.2864\n",
      "Epoch 00006: val_loss improved from 0.92119 to 0.63334, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.2234 - val_loss: 0.6333\n",
      "Epoch 7/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.0806\n",
      "Epoch 00007: val_loss improved from 0.63334 to 0.46965, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0208 - val_loss: 0.4697\n",
      "Epoch 8/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7252\n",
      "Epoch 00008: val_loss improved from 0.46965 to 0.37583, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8210 - val_loss: 0.3758\n",
      "Epoch 9/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.7160\n",
      "Epoch 00009: val_loss improved from 0.37583 to 0.31443, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6940 - val_loss: 0.3144\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5990\n",
      "Epoch 00010: val_loss improved from 0.31443 to 0.27667, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6168 - val_loss: 0.2767\n",
      "Epoch 11/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6489\n",
      "Epoch 00011: val_loss improved from 0.27667 to 0.25051, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6352 - val_loss: 0.2505\n",
      "Epoch 12/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6386\n",
      "Epoch 00012: val_loss improved from 0.25051 to 0.23407, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6223 - val_loss: 0.2341\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.5860\n",
      "Epoch 00013: val_loss improved from 0.23407 to 0.22291, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5860 - val_loss: 0.2229\n",
      "Epoch 14/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5180\n",
      "Epoch 00014: val_loss improved from 0.22291 to 0.20716, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5168 - val_loss: 0.2072\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4984\n",
      "Epoch 00015: val_loss improved from 0.20716 to 0.19744, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4984 - val_loss: 0.1974\n",
      "Epoch 16/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4962\n",
      "Epoch 00016: val_loss improved from 0.19744 to 0.18939, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4889 - val_loss: 0.1894\n",
      "Epoch 17/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4791\n",
      "Epoch 00017: val_loss improved from 0.18939 to 0.18494, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4906 - val_loss: 0.1849\n",
      "Epoch 18/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.5309\n",
      "Epoch 00018: val_loss improved from 0.18494 to 0.17807, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5214 - val_loss: 0.1781\n",
      "Epoch 19/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4778\n",
      "Epoch 00019: val_loss improved from 0.17807 to 0.17618, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4718 - val_loss: 0.1762\n",
      "Epoch 20/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4481\n",
      "Epoch 00020: val_loss improved from 0.17618 to 0.17059, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4497 - val_loss: 0.1706\n",
      "Epoch 21/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4418\n",
      "Epoch 00021: val_loss improved from 0.17059 to 0.16870, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4376 - val_loss: 0.1687\n",
      "Epoch 22/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4449\n",
      "Epoch 00022: val_loss improved from 0.16870 to 0.16221, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4418 - val_loss: 0.1622\n",
      "Epoch 23/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4385\n",
      "Epoch 00023: val_loss improved from 0.16221 to 0.16110, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4337 - val_loss: 0.1611\n",
      "Epoch 24/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4209\n",
      "Epoch 00024: val_loss improved from 0.16110 to 0.15634, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4244 - val_loss: 0.1563\n",
      "Epoch 25/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4049\n",
      "Epoch 00025: val_loss improved from 0.15634 to 0.15608, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4199 - val_loss: 0.1561\n",
      "Epoch 26/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4089\n",
      "Epoch 00026: val_loss improved from 0.15608 to 0.15455, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4228 - val_loss: 0.1545\n",
      "Epoch 27/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4053\n",
      "Epoch 00027: val_loss improved from 0.15455 to 0.15088, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4072 - val_loss: 0.1509\n",
      "Epoch 28/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4053\n",
      "Epoch 00028: val_loss improved from 0.15088 to 0.14917, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4038 - val_loss: 0.1492\n",
      "Epoch 29/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3849\n",
      "Epoch 00029: val_loss improved from 0.14917 to 0.14649, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3851 - val_loss: 0.1465\n",
      "Epoch 30/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4409\n",
      "Epoch 00030: val_loss did not improve from 0.14649\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4301 - val_loss: 0.1472\n",
      "Epoch 31/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3827\n",
      "Epoch 00031: val_loss improved from 0.14649 to 0.14382, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3832 - val_loss: 0.1438\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3749\n",
      "Epoch 00032: val_loss improved from 0.14382 to 0.14325, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3749 - val_loss: 0.1433\n",
      "Epoch 33/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3730\n",
      "Epoch 00033: val_loss improved from 0.14325 to 0.14200, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3726 - val_loss: 0.1420\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3684\n",
      "Epoch 00034: val_loss did not improve from 0.14200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3684 - val_loss: 0.1453\n",
      "Epoch 35/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3854\n",
      "Epoch 00035: val_loss did not improve from 0.14200\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3841 - val_loss: 0.1427\n",
      " ###1 fold : val acc1 0.569, acc3 0.960, mae 0.239###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 19.2794\n",
      "Epoch 00001: val_loss improved from inf to 13.57704, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5766 - val_loss: 13.5770\n",
      "Epoch 2/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 10.4306\n",
      "Epoch 00002: val_loss improved from 13.57704 to 6.51213, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.0256 - val_loss: 6.5121\n",
      "Epoch 3/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 4.8058\n",
      "Epoch 00003: val_loss improved from 6.51213 to 2.75964, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.7449 - val_loss: 2.7596\n",
      "Epoch 4/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 2.7508\n",
      "Epoch 00004: val_loss improved from 2.75964 to 1.47530, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.6081 - val_loss: 1.4753\n",
      "Epoch 5/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.8332\n",
      "Epoch 00005: val_loss improved from 1.47530 to 0.92420, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.7259 - val_loss: 0.9242\n",
      "Epoch 6/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.2776\n",
      "Epoch 00006: val_loss improved from 0.92420 to 0.63452, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.2214 - val_loss: 0.6345\n",
      "Epoch 7/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.0727\n",
      "Epoch 00007: val_loss improved from 0.63452 to 0.47093, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0242 - val_loss: 0.4709\n",
      "Epoch 8/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7288\n",
      "Epoch 00008: val_loss improved from 0.47093 to 0.37501, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8194 - val_loss: 0.3750\n",
      "Epoch 9/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7146\n",
      "Epoch 00009: val_loss improved from 0.37501 to 0.31192, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6912 - val_loss: 0.3119\n",
      "Epoch 10/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5890\n",
      "Epoch 00010: val_loss improved from 0.31192 to 0.27462, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6075 - val_loss: 0.2746\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6266\n",
      "Epoch 00011: val_loss improved from 0.27462 to 0.24916, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6266 - val_loss: 0.2492\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6186\n",
      "Epoch 00012: val_loss improved from 0.24916 to 0.23238, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6186 - val_loss: 0.2324\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.5830\n",
      "Epoch 00013: val_loss improved from 0.23238 to 0.22241, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5830 - val_loss: 0.2224\n",
      "Epoch 14/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.5201\n",
      "Epoch 00014: val_loss improved from 0.22241 to 0.20735, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5139 - val_loss: 0.2074\n",
      "Epoch 15/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4932\n",
      "Epoch 00015: val_loss improved from 0.20735 to 0.19864, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4976 - val_loss: 0.1986\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4889\n",
      "Epoch 00016: val_loss improved from 0.19864 to 0.19029, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4889 - val_loss: 0.1903\n",
      "Epoch 17/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.4892\n",
      "Epoch 00017: val_loss improved from 0.19029 to 0.18614, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4870 - val_loss: 0.1861\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.5181\n",
      "Epoch 00018: val_loss improved from 0.18614 to 0.17990, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5181 - val_loss: 0.1799\n",
      "Epoch 19/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4746\n",
      "Epoch 00019: val_loss improved from 0.17990 to 0.17732, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4688 - val_loss: 0.1773\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4491\n",
      "Epoch 00020: val_loss improved from 0.17732 to 0.17245, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4491 - val_loss: 0.1724\n",
      "Epoch 21/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4418\n",
      "Epoch 00021: val_loss improved from 0.17245 to 0.16994, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4374 - val_loss: 0.1699\n",
      "Epoch 22/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4431\n",
      "Epoch 00022: val_loss improved from 0.16994 to 0.16360, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4389 - val_loss: 0.1636\n",
      "Epoch 23/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.4358\n",
      "Epoch 00023: val_loss improved from 0.16360 to 0.16256, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4316 - val_loss: 0.1626\n",
      "Epoch 24/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4194\n",
      "Epoch 00024: val_loss improved from 0.16256 to 0.15839, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4231 - val_loss: 0.1584\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4208\n",
      "Epoch 00025: val_loss improved from 0.15839 to 0.15754, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4208 - val_loss: 0.1575\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4221\n",
      "Epoch 00026: val_loss improved from 0.15754 to 0.15575, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4221 - val_loss: 0.1558\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4083\n",
      "Epoch 00027: val_loss improved from 0.15575 to 0.15202, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4093 - val_loss: 0.1520\n",
      "Epoch 28/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.4038\n",
      "Epoch 00028: val_loss improved from 0.15202 to 0.15044, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4016 - val_loss: 0.1504\n",
      "Epoch 29/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3842\n",
      "Epoch 00029: val_loss improved from 0.15044 to 0.14813, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3862 - val_loss: 0.1481\n",
      "Epoch 30/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4428\n",
      "Epoch 00030: val_loss improved from 0.14813 to 0.14719, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4295 - val_loss: 0.1472\n",
      "Epoch 31/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3796\n",
      "Epoch 00031: val_loss improved from 0.14719 to 0.14490, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3801 - val_loss: 0.1449\n",
      "Epoch 32/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3770\n",
      "Epoch 00032: val_loss improved from 0.14490 to 0.14474, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3753 - val_loss: 0.1447\n",
      "Epoch 33/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3735\n",
      "Epoch 00033: val_loss improved from 0.14474 to 0.14293, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3727 - val_loss: 0.1429\n",
      "Epoch 34/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.3687\n",
      "Epoch 00034: val_loss did not improve from 0.14293\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3682 - val_loss: 0.1455\n",
      "Epoch 35/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3863\n",
      "Epoch 00035: val_loss did not improve from 0.14293\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3854 - val_loss: 0.1434\n",
      " ###2 fold : val acc1 0.569, acc3 0.956, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 19.2071\n",
      "Epoch 00001: val_loss improved from inf to 13.59920, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5740 - val_loss: 13.5992\n",
      "Epoch 2/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 10.2361\n",
      "Epoch 00002: val_loss improved from 13.59920 to 6.52496, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.0392 - val_loss: 6.5250\n",
      "Epoch 3/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 4.8193\n",
      "Epoch 00003: val_loss improved from 6.52496 to 2.76727, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.7580 - val_loss: 2.7673\n",
      "Epoch 4/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 2.3649\n",
      "Epoch 00004: val_loss improved from 2.76727 to 1.47361, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.6066 - val_loss: 1.4736\n",
      "Epoch 5/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.8494\n",
      "Epoch 00005: val_loss improved from 1.47361 to 0.92054, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7264 - val_loss: 0.9205\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.2152\n",
      "Epoch 00006: val_loss improved from 0.92054 to 0.63269, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2152 - val_loss: 0.6327\n",
      "Epoch 7/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.0935\n",
      "Epoch 00007: val_loss improved from 0.63269 to 0.47055, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0224 - val_loss: 0.4706\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8174\n",
      "Epoch 00008: val_loss improved from 0.47055 to 0.37473, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8174 - val_loss: 0.3747\n",
      "Epoch 9/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7077\n",
      "Epoch 00009: val_loss improved from 0.37473 to 0.31248, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6881 - val_loss: 0.3125\n",
      "Epoch 10/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.5896\n",
      "Epoch 00010: val_loss improved from 0.31248 to 0.27581, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6086 - val_loss: 0.2758\n",
      "Epoch 11/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.6390\n",
      "Epoch 00011: val_loss improved from 0.27581 to 0.25035, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6227 - val_loss: 0.2504\n",
      "Epoch 12/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6406\n",
      "Epoch 00012: val_loss improved from 0.25035 to 0.23394, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6188 - val_loss: 0.2339\n",
      "Epoch 13/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5958\n",
      "Epoch 00013: val_loss improved from 0.23394 to 0.22292, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5849 - val_loss: 0.2229\n",
      "Epoch 14/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.5215\n",
      "Epoch 00014: val_loss improved from 0.22292 to 0.20867, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5145 - val_loss: 0.2087\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4943\n",
      "Epoch 00015: val_loss improved from 0.20867 to 0.19928, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4943 - val_loss: 0.1993\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4900\n",
      "Epoch 00016: val_loss improved from 0.19928 to 0.19114, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4900 - val_loss: 0.1911\n",
      "Epoch 17/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4802\n",
      "Epoch 00017: val_loss improved from 0.19114 to 0.18755, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4906 - val_loss: 0.1875\n",
      "Epoch 18/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5292\n",
      "Epoch 00018: val_loss improved from 0.18755 to 0.18045, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5181 - val_loss: 0.1804\n",
      "Epoch 19/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4673\n",
      "Epoch 00019: val_loss improved from 0.18045 to 0.17709, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4656 - val_loss: 0.1771\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4491\n",
      "Epoch 00020: val_loss improved from 0.17709 to 0.17273, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4491 - val_loss: 0.1727\n",
      "Epoch 21/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4402\n",
      "Epoch 00021: val_loss improved from 0.17273 to 0.17018, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4370 - val_loss: 0.1702\n",
      "Epoch 22/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4448\n",
      "Epoch 00022: val_loss improved from 0.17018 to 0.16443, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4407 - val_loss: 0.1644\n",
      "Epoch 23/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.4331\n",
      "Epoch 00023: val_loss improved from 0.16443 to 0.16248, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4325 - val_loss: 0.1625\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4241\n",
      "Epoch 00024: val_loss improved from 0.16248 to 0.15900, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4241 - val_loss: 0.1590\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4227\n",
      "Epoch 00025: val_loss improved from 0.15900 to 0.15699, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4227 - val_loss: 0.1570\n",
      "Epoch 26/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4099\n",
      "Epoch 00026: val_loss improved from 0.15699 to 0.15575, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4226 - val_loss: 0.1558\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4081\n",
      "Epoch 00027: val_loss improved from 0.15575 to 0.15240, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4081 - val_loss: 0.1524\n",
      "Epoch 28/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4025\n",
      "Epoch 00028: val_loss improved from 0.15240 to 0.15160, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4019 - val_loss: 0.1516\n",
      "Epoch 29/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3837\n",
      "Epoch 00029: val_loss improved from 0.15160 to 0.14831, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3849 - val_loss: 0.1483\n",
      "Epoch 30/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4393\n",
      "Epoch 00030: val_loss improved from 0.14831 to 0.14732, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4271 - val_loss: 0.1473\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3825\n",
      "Epoch 00031: val_loss improved from 0.14732 to 0.14497, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3825 - val_loss: 0.1450\n",
      "Epoch 32/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3783\n",
      "Epoch 00032: val_loss did not improve from 0.14497\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3783 - val_loss: 0.1455\n",
      "Epoch 33/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3774\n",
      "Epoch 00033: val_loss improved from 0.14497 to 0.14285, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3746 - val_loss: 0.1428\n",
      "Epoch 34/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3697\n",
      "Epoch 00034: val_loss did not improve from 0.14285\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3693 - val_loss: 0.1457\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3864\n",
      "Epoch 00035: val_loss did not improve from 0.14285\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3864 - val_loss: 0.1431\n",
      " ###3 fold : val acc1 0.582, acc3 0.955, mae 0.235###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/77 [=========================>....] - ETA: 0s - loss: 19.1370\n",
      "Epoch 00001: val_loss improved from inf to 13.62650, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5864 - val_loss: 13.6265\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 10.5958\n",
      "Epoch 00002: val_loss improved from 13.62650 to 6.56603, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.1806 - val_loss: 6.5660\n",
      "Epoch 3/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 5.3245\n",
      "Epoch 00003: val_loss improved from 6.56603 to 2.93432, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 5.0215 - val_loss: 2.9343\n",
      "Epoch 4/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 2.8258\n",
      "Epoch 00004: val_loss improved from 2.93432 to 1.52784, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.6141 - val_loss: 1.5278\n",
      "Epoch 5/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.9322\n",
      "Epoch 00005: val_loss improved from 1.52784 to 0.95404, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7930 - val_loss: 0.9540\n",
      "Epoch 6/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.7358\n",
      "Epoch 00006: val_loss improved from 0.95404 to 0.65672, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5751 - val_loss: 0.6567\n",
      "Epoch 7/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.2499\n",
      "Epoch 00007: val_loss improved from 0.65672 to 0.48737, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1508 - val_loss: 0.4874\n",
      "Epoch 8/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.0105\n",
      "Epoch 00008: val_loss improved from 0.48737 to 0.38431, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9431 - val_loss: 0.3843\n",
      "Epoch 9/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6549\n",
      "Epoch 00009: val_loss improved from 0.38431 to 0.31984, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7302 - val_loss: 0.3198\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6188\n",
      "Epoch 00010: val_loss improved from 0.31984 to 0.28005, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6114 - val_loss: 0.2801\n",
      "Epoch 11/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5803\n",
      "Epoch 00011: val_loss improved from 0.28005 to 0.25495, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5744 - val_loss: 0.2549\n",
      "Epoch 12/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5349\n",
      "Epoch 00012: val_loss improved from 0.25495 to 0.23682, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6402 - val_loss: 0.2368\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.5354\n",
      "Epoch 00013: val_loss improved from 0.23682 to 0.22430, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5354 - val_loss: 0.2243\n",
      "Epoch 14/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6187\n",
      "Epoch 00014: val_loss improved from 0.22430 to 0.21283, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5950 - val_loss: 0.2128\n",
      "Epoch 15/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4955\n",
      "Epoch 00015: val_loss improved from 0.21283 to 0.20407, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4994 - val_loss: 0.2041\n",
      "Epoch 16/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.5475\n",
      "Epoch 00016: val_loss improved from 0.20407 to 0.19576, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5357 - val_loss: 0.1958\n",
      "Epoch 17/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.5882\n",
      "Epoch 00017: val_loss improved from 0.19576 to 0.19364, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5678 - val_loss: 0.1936\n",
      "Epoch 18/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4957\n",
      "Epoch 00018: val_loss improved from 0.19364 to 0.18525, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4885 - val_loss: 0.1853\n",
      "Epoch 19/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4624\n",
      "Epoch 00019: val_loss improved from 0.18525 to 0.18076, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4664 - val_loss: 0.1808\n",
      "Epoch 20/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4786\n",
      "Epoch 00020: val_loss improved from 0.18076 to 0.17643, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4702 - val_loss: 0.1764\n",
      "Epoch 21/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4617\n",
      "Epoch 00021: val_loss improved from 0.17643 to 0.17356, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4556 - val_loss: 0.1736\n",
      "Epoch 22/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4657\n",
      "Epoch 00022: val_loss improved from 0.17356 to 0.17048, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4584 - val_loss: 0.1705\n",
      "Epoch 23/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4289\n",
      "Epoch 00023: val_loss improved from 0.17048 to 0.16636, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4381 - val_loss: 0.1664\n",
      "Epoch 24/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4277\n",
      "Epoch 00024: val_loss improved from 0.16636 to 0.16483, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4383 - val_loss: 0.1648\n",
      "Epoch 25/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4118\n",
      "Epoch 00025: val_loss improved from 0.16483 to 0.15938, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4162 - val_loss: 0.1594\n",
      "Epoch 26/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4184\n",
      "Epoch 00026: val_loss improved from 0.15938 to 0.15543, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4195 - val_loss: 0.1554\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4059\n",
      "Epoch 00027: val_loss improved from 0.15543 to 0.15406, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4072 - val_loss: 0.1541\n",
      "Epoch 28/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4040\n",
      "Epoch 00028: val_loss improved from 0.15406 to 0.15185, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3998 - val_loss: 0.1519\n",
      "Epoch 29/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4154\n",
      "Epoch 00029: val_loss improved from 0.15185 to 0.15157, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4070 - val_loss: 0.1516\n",
      "Epoch 30/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3976\n",
      "Epoch 00030: val_loss improved from 0.15157 to 0.14767, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3936 - val_loss: 0.1477\n",
      "Epoch 31/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3957\n",
      "Epoch 00031: val_loss improved from 0.14767 to 0.14715, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3933 - val_loss: 0.1472\n",
      "Epoch 32/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3988\n",
      "Epoch 00032: val_loss did not improve from 0.14715\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3947 - val_loss: 0.1475\n",
      "Epoch 33/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3812\n",
      "Epoch 00033: val_loss improved from 0.14715 to 0.14548, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3802 - val_loss: 0.1455\n",
      "Epoch 34/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3727\n",
      "Epoch 00034: val_loss improved from 0.14548 to 0.14511, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3711 - val_loss: 0.1451\n",
      "Epoch 35/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3760\n",
      "Epoch 00035: val_loss improved from 0.14511 to 0.14312, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3803 - val_loss: 0.1431\n",
      "Epoch 36/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3743\n",
      "Epoch 00036: val_loss improved from 0.14312 to 0.14015, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3729 - val_loss: 0.1401\n",
      "Epoch 37/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3660\n",
      "Epoch 00037: val_loss did not improve from 0.14015\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3634 - val_loss: 0.1407\n",
      "Epoch 38/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3645\n",
      "Epoch 00038: val_loss improved from 0.14015 to 0.13781, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3597 - val_loss: 0.1378\n",
      "Epoch 39/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.3585\n",
      "Epoch 00039: val_loss did not improve from 0.13781\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3592 - val_loss: 0.1380\n",
      "Epoch 40/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3605\n",
      "Epoch 00040: val_loss did not improve from 0.13781\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3823 - val_loss: 0.1394\n",
      " ###4 fold : val acc1 0.580, acc3 0.961, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/77 [=========================>....] - ETA: 0s - loss: 19.0090\n",
      "Epoch 00001: val_loss improved from inf to 13.47621, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5166 - val_loss: 13.4762\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 10.1652\n",
      "Epoch 00002: val_loss improved from 13.47621 to 6.26303, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 9.7668 - val_loss: 6.2630\n",
      "Epoch 3/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 4.6294\n",
      "Epoch 00003: val_loss improved from 6.26303 to 2.72390, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.4389 - val_loss: 2.7239\n",
      "Epoch 4/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 2.3670\n",
      "Epoch 00004: val_loss improved from 2.72390 to 1.45767, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.2763 - val_loss: 1.4577\n",
      "Epoch 5/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.4814\n",
      "Epoch 00005: val_loss improved from 1.45767 to 0.88988, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.4282 - val_loss: 0.8899\n",
      "Epoch 6/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.0487\n",
      "Epoch 00006: val_loss improved from 0.88988 to 0.59162, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0264 - val_loss: 0.5916\n",
      "Epoch 7/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.7986\n",
      "Epoch 00007: val_loss improved from 0.59162 to 0.42763, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7874 - val_loss: 0.4276\n",
      "Epoch 8/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6728\n",
      "Epoch 00008: val_loss improved from 0.42763 to 0.33362, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6652 - val_loss: 0.3336\n",
      "Epoch 9/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6007\n",
      "Epoch 00009: val_loss improved from 0.33362 to 0.27957, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5915 - val_loss: 0.2796\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5530\n",
      "Epoch 00010: val_loss improved from 0.27957 to 0.24978, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5515 - val_loss: 0.2498\n",
      "Epoch 11/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.5260\n",
      "Epoch 00011: val_loss improved from 0.24978 to 0.23060, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5254 - val_loss: 0.2306\n",
      "Epoch 12/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5084\n",
      "Epoch 00012: val_loss improved from 0.23060 to 0.21652, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5085 - val_loss: 0.2165\n",
      "Epoch 13/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4945\n",
      "Epoch 00013: val_loss improved from 0.21652 to 0.20830, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4922 - val_loss: 0.2083\n",
      "Epoch 14/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4839\n",
      "Epoch 00014: val_loss improved from 0.20830 to 0.19891, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4806 - val_loss: 0.1989\n",
      "Epoch 15/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4598\n",
      "Epoch 00015: val_loss improved from 0.19891 to 0.19193, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4598 - val_loss: 0.1919\n",
      "Epoch 16/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4580\n",
      "Epoch 00016: val_loss improved from 0.19193 to 0.18506, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4600 - val_loss: 0.1851\n",
      "Epoch 17/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4556\n",
      "Epoch 00017: val_loss improved from 0.18506 to 0.18167, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4513 - val_loss: 0.1817\n",
      "Epoch 18/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.4350\n",
      "Epoch 00018: val_loss improved from 0.18167 to 0.17597, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4359 - val_loss: 0.1760\n",
      "Epoch 19/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.4312\n",
      "Epoch 00019: val_loss improved from 0.17597 to 0.17227, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4366 - val_loss: 0.1723\n",
      "Epoch 20/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.4300\n",
      "Epoch 00020: val_loss improved from 0.17227 to 0.16830, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4264 - val_loss: 0.1683\n",
      "Epoch 21/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.4231\n",
      "Epoch 00021: val_loss improved from 0.16830 to 0.16649, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4218 - val_loss: 0.1665\n",
      "Epoch 22/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4200\n",
      "Epoch 00022: val_loss improved from 0.16649 to 0.16256, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4186 - val_loss: 0.1626\n",
      "Epoch 23/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4069\n",
      "Epoch 00023: val_loss improved from 0.16256 to 0.15924, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4034 - val_loss: 0.1592\n",
      "Epoch 24/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4024\n",
      "Epoch 00024: val_loss improved from 0.15924 to 0.15748, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4052 - val_loss: 0.1575\n",
      "Epoch 25/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3949\n",
      "Epoch 00025: val_loss improved from 0.15748 to 0.15423, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3997 - val_loss: 0.1542\n",
      "Epoch 26/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3918\n",
      "Epoch 00026: val_loss improved from 0.15423 to 0.15302, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3939 - val_loss: 0.1530\n",
      "Epoch 27/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.3822\n",
      "Epoch 00027: val_loss improved from 0.15302 to 0.15257, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3825 - val_loss: 0.1526\n",
      "Epoch 28/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3825\n",
      "Epoch 00028: val_loss improved from 0.15257 to 0.15061, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3810 - val_loss: 0.1506\n",
      "Epoch 29/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3764\n",
      "Epoch 00029: val_loss improved from 0.15061 to 0.14910, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3702 - val_loss: 0.1491\n",
      "Epoch 30/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3702\n",
      "Epoch 00030: val_loss improved from 0.14910 to 0.14736, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3700 - val_loss: 0.1474\n",
      "Epoch 31/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3626\n",
      "Epoch 00031: val_loss improved from 0.14736 to 0.14699, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3645 - val_loss: 0.1470\n",
      "Epoch 32/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3668\n",
      "Epoch 00032: val_loss did not improve from 0.14699\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3668 - val_loss: 0.1472\n",
      "Epoch 33/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3658\n",
      "Epoch 00033: val_loss improved from 0.14699 to 0.14632, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3635 - val_loss: 0.1463\n",
      "Epoch 34/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3579\n",
      "Epoch 00034: val_loss improved from 0.14632 to 0.14574, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3570 - val_loss: 0.1457\n",
      "Epoch 35/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3535\n",
      "Epoch 00035: val_loss improved from 0.14574 to 0.14388, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3564 - val_loss: 0.1439\n",
      "Epoch 36/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3550\n",
      "Epoch 00036: val_loss improved from 0.14388 to 0.14289, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3544 - val_loss: 0.1429\n",
      "Epoch 37/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3456\n",
      "Epoch 00037: val_loss did not improve from 0.14289\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3457 - val_loss: 0.1431\n",
      "Epoch 38/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3416\n",
      "Epoch 00038: val_loss improved from 0.14289 to 0.14040, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3403 - val_loss: 0.1404\n",
      "Epoch 39/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3443\n",
      "Epoch 00039: val_loss did not improve from 0.14040\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3439 - val_loss: 0.1412\n",
      "Epoch 40/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3475\n",
      "Epoch 00040: val_loss improved from 0.14040 to 0.13967, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3465 - val_loss: 0.1397\n",
      "Epoch 41/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3374\n",
      "Epoch 00041: val_loss did not improve from 0.13967\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3384 - val_loss: 0.1399\n",
      "Epoch 42/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3355\n",
      "Epoch 00042: val_loss did not improve from 0.13967\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3368 - val_loss: 0.1416\n",
      " ###5 fold : val acc1 0.591, acc3 0.961, mae 0.243###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/77 [======================>.......] - ETA: 0s - loss: 19.5526\n",
      "Epoch 00001: val_loss improved from inf to 13.48230, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 18.5092 - val_loss: 13.4823\n",
      "Epoch 2/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 10.4086\n",
      "Epoch 00002: val_loss improved from 13.48230 to 6.32192, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 9.9932 - val_loss: 6.3219\n",
      "Epoch 3/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 5.2128\n",
      "Epoch 00003: val_loss improved from 6.32192 to 2.90692, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 4.8909 - val_loss: 2.9069\n",
      "Epoch 4/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 2.6499\n",
      "Epoch 00004: val_loss improved from 2.90692 to 1.54957, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.6319 - val_loss: 1.5496\n",
      "Epoch 5/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.9349\n",
      "Epoch 00005: val_loss improved from 1.54957 to 0.97609, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.9036 - val_loss: 0.9761\n",
      "Epoch 6/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.3762\n",
      "Epoch 00006: val_loss improved from 0.97609 to 0.66664, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3605 - val_loss: 0.6666\n",
      "Epoch 7/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.0637\n",
      "Epoch 00007: val_loss improved from 0.66664 to 0.49519, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0536 - val_loss: 0.4952\n",
      "Epoch 8/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.0939\n",
      "Epoch 00008: val_loss improved from 0.49519 to 0.38703, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0696 - val_loss: 0.3870\n",
      "Epoch 9/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7399\n",
      "Epoch 00009: val_loss improved from 0.38703 to 0.31840, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7361 - val_loss: 0.3184\n",
      "Epoch 10/100\n",
      "58/77 [=====================>........] - ETA: 0s - loss: 0.8136\n",
      "Epoch 00010: val_loss improved from 0.31840 to 0.27836, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7589 - val_loss: 0.2784\n",
      "Epoch 11/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6761\n",
      "Epoch 00011: val_loss improved from 0.27836 to 0.25379, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6752 - val_loss: 0.2538\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.5770\n",
      "Epoch 00012: val_loss improved from 0.25379 to 0.23271, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5770 - val_loss: 0.2327\n",
      "Epoch 13/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6907\n",
      "Epoch 00013: val_loss improved from 0.23271 to 0.22231, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6890 - val_loss: 0.2223\n",
      "Epoch 14/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6066\n",
      "Epoch 00014: val_loss improved from 0.22231 to 0.21063, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6053 - val_loss: 0.2106\n",
      "Epoch 15/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5597\n",
      "Epoch 00015: val_loss improved from 0.21063 to 0.20423, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5595 - val_loss: 0.2042\n",
      "Epoch 16/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.5417\n",
      "Epoch 00016: val_loss improved from 0.20423 to 0.19576, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5369 - val_loss: 0.1958\n",
      "Epoch 17/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.4973\n",
      "Epoch 00017: val_loss improved from 0.19576 to 0.19125, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4954 - val_loss: 0.1912\n",
      "Epoch 18/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.4647\n",
      "Epoch 00018: val_loss improved from 0.19125 to 0.18430, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4693 - val_loss: 0.1843\n",
      "Epoch 19/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.4988\n",
      "Epoch 00019: val_loss improved from 0.18430 to 0.17934, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5000 - val_loss: 0.1793\n",
      "Epoch 20/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4859\n",
      "Epoch 00020: val_loss improved from 0.17934 to 0.17435, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4815 - val_loss: 0.1743\n",
      "Epoch 21/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.4411\n",
      "Epoch 00021: val_loss improved from 0.17435 to 0.17129, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4417 - val_loss: 0.1713\n",
      "Epoch 22/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4483\n",
      "Epoch 00022: val_loss improved from 0.17129 to 0.16602, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4479 - val_loss: 0.1660\n",
      "Epoch 23/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4329\n",
      "Epoch 00023: val_loss improved from 0.16602 to 0.16249, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4312 - val_loss: 0.1625\n",
      "Epoch 24/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.4521\n",
      "Epoch 00024: val_loss improved from 0.16249 to 0.16067, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4508 - val_loss: 0.1607\n",
      "Epoch 25/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.4318\n",
      "Epoch 00025: val_loss improved from 0.16067 to 0.15625, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4327 - val_loss: 0.1562\n",
      "Epoch 26/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4213\n",
      "Epoch 00026: val_loss improved from 0.15625 to 0.15543, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4212 - val_loss: 0.1554\n",
      "Epoch 27/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.4023\n",
      "Epoch 00027: val_loss improved from 0.15543 to 0.15363, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4022 - val_loss: 0.1536\n",
      "Epoch 28/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3975\n",
      "Epoch 00028: val_loss improved from 0.15363 to 0.14993, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3970 - val_loss: 0.1499\n",
      "Epoch 29/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3890\n",
      "Epoch 00029: val_loss improved from 0.14993 to 0.14813, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3885 - val_loss: 0.1481\n",
      "Epoch 30/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.3957\n",
      "Epoch 00030: val_loss improved from 0.14813 to 0.14632, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3943 - val_loss: 0.1463\n",
      "Epoch 31/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3801\n",
      "Epoch 00031: val_loss improved from 0.14632 to 0.14537, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3804 - val_loss: 0.1454\n",
      "Epoch 32/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.3973\n",
      "Epoch 00032: val_loss improved from 0.14537 to 0.14511, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3959 - val_loss: 0.1451\n",
      "Epoch 33/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3860\n",
      "Epoch 00033: val_loss improved from 0.14511 to 0.14373, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3817 - val_loss: 0.1437\n",
      "Epoch 34/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3714\n",
      "Epoch 00034: val_loss did not improve from 0.14373\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3712 - val_loss: 0.1438\n",
      "Epoch 35/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3842\n",
      "Epoch 00035: val_loss improved from 0.14373 to 0.14256, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3848 - val_loss: 0.1426\n",
      "Epoch 36/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3668\n",
      "Epoch 00036: val_loss improved from 0.14256 to 0.14218, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3676 - val_loss: 0.1422\n",
      "Epoch 37/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.3584\n",
      "Epoch 00037: val_loss improved from 0.14218 to 0.14063, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3577 - val_loss: 0.1406\n",
      "Epoch 38/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3571\n",
      "Epoch 00038: val_loss improved from 0.14063 to 0.13837, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3563 - val_loss: 0.1384\n",
      "Epoch 39/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3577\n",
      "Epoch 00039: val_loss did not improve from 0.13837\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3585 - val_loss: 0.1410\n",
      "Epoch 40/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.3922\n",
      "Epoch 00040: val_loss improved from 0.13837 to 0.13742, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3891 - val_loss: 0.1374\n",
      "Epoch 41/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3521\n",
      "Epoch 00041: val_loss improved from 0.13742 to 0.13653, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3535 - val_loss: 0.1365\n",
      "Epoch 42/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3489\n",
      "Epoch 00042: val_loss did not improve from 0.13653\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3488 - val_loss: 0.1383\n",
      "Epoch 43/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3448\n",
      "Epoch 00043: val_loss improved from 0.13653 to 0.13568, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3439 - val_loss: 0.1357\n",
      "Epoch 44/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3409\n",
      "Epoch 00044: val_loss did not improve from 0.13568\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3410 - val_loss: 0.1365\n",
      "Epoch 45/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3313\n",
      "Epoch 00045: val_loss improved from 0.13568 to 0.13441, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3312 - val_loss: 0.1344\n",
      "Epoch 46/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3542\n",
      "Epoch 00046: val_loss did not improve from 0.13441\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3537 - val_loss: 0.1385\n",
      "Epoch 47/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3245\n",
      "Epoch 00047: val_loss did not improve from 0.13441\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3241 - val_loss: 0.1352\n",
      " ###6 fold : val acc1 0.574, acc3 0.959, mae 0.238###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 19.1366\n",
      "Epoch 00001: val_loss improved from inf to 13.50595, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5085 - val_loss: 13.5059\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 10.1419\n",
      "Epoch 00002: val_loss improved from 13.50595 to 6.32988, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.0093 - val_loss: 6.3299\n",
      "Epoch 3/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 5.0922\n",
      "Epoch 00003: val_loss improved from 6.32988 to 2.90187, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.8839 - val_loss: 2.9019\n",
      "Epoch 4/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 2.7354\n",
      "Epoch 00004: val_loss improved from 2.90187 to 1.54912, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.6260 - val_loss: 1.5491\n",
      "Epoch 5/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 1.9746\n",
      "Epoch 00005: val_loss improved from 1.54912 to 0.98081, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.9024 - val_loss: 0.9808\n",
      "Epoch 6/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.4394\n",
      "Epoch 00006: val_loss improved from 0.98081 to 0.67325, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.3594 - val_loss: 0.6733\n",
      "Epoch 7/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.8836\n",
      "Epoch 00007: val_loss improved from 0.67325 to 0.50164, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0645 - val_loss: 0.5016\n",
      "Epoch 8/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.1784\n",
      "Epoch 00008: val_loss improved from 0.50164 to 0.39213, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0863 - val_loss: 0.3921\n",
      "Epoch 9/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6505\n",
      "Epoch 00009: val_loss improved from 0.39213 to 0.32227, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7349 - val_loss: 0.3223\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8171\n",
      "Epoch 00010: val_loss improved from 0.32227 to 0.28030, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7679 - val_loss: 0.2803\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6793\n",
      "Epoch 00011: val_loss improved from 0.28030 to 0.25460, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6793 - val_loss: 0.2546\n",
      "Epoch 12/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.5800\n",
      "Epoch 00012: val_loss improved from 0.25460 to 0.23286, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5789 - val_loss: 0.2329\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.7004\n",
      "Epoch 00013: val_loss improved from 0.23286 to 0.22160, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7004 - val_loss: 0.2216\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6116\n",
      "Epoch 00014: val_loss improved from 0.22160 to 0.20952, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6116 - val_loss: 0.2095\n",
      "Epoch 15/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6006\n",
      "Epoch 00015: val_loss improved from 0.20952 to 0.20280, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5734 - val_loss: 0.2028\n",
      "Epoch 16/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5579\n",
      "Epoch 00016: val_loss improved from 0.20280 to 0.19418, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5446 - val_loss: 0.1942\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4953\n",
      "Epoch 00017: val_loss improved from 0.19418 to 0.18919, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4953 - val_loss: 0.1892\n",
      "Epoch 18/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4740\n",
      "Epoch 00018: val_loss improved from 0.18919 to 0.18218, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4744 - val_loss: 0.1822\n",
      "Epoch 19/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.5048\n",
      "Epoch 00019: val_loss improved from 0.18218 to 0.17807, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5038 - val_loss: 0.1781\n",
      "Epoch 20/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.5041\n",
      "Epoch 00020: val_loss improved from 0.17807 to 0.17279, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4879 - val_loss: 0.1728\n",
      "Epoch 21/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4437\n",
      "Epoch 00021: val_loss improved from 0.17279 to 0.17031, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4411 - val_loss: 0.1703\n",
      "Epoch 22/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4553\n",
      "Epoch 00022: val_loss improved from 0.17031 to 0.16509, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4493 - val_loss: 0.1651\n",
      "Epoch 23/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4373\n",
      "Epoch 00023: val_loss improved from 0.16509 to 0.16158, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4327 - val_loss: 0.1616\n",
      "Epoch 24/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.4598\n",
      "Epoch 00024: val_loss improved from 0.16158 to 0.15978, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4554 - val_loss: 0.1598\n",
      "Epoch 25/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4258\n",
      "Epoch 00025: val_loss improved from 0.15978 to 0.15526, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4291 - val_loss: 0.1553\n",
      "Epoch 26/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4230\n",
      "Epoch 00026: val_loss improved from 0.15526 to 0.15437, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4204 - val_loss: 0.1544\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4016\n",
      "Epoch 00027: val_loss improved from 0.15437 to 0.15314, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4016 - val_loss: 0.1531\n",
      "Epoch 28/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3981\n",
      "Epoch 00028: val_loss improved from 0.15314 to 0.14866, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3977 - val_loss: 0.1487\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3913\n",
      "Epoch 00029: val_loss improved from 0.14866 to 0.14710, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3913 - val_loss: 0.1471\n",
      "Epoch 30/100\n",
      "58/77 [=====================>........] - ETA: 0s - loss: 0.4051\n",
      "Epoch 00030: val_loss improved from 0.14710 to 0.14579, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3970 - val_loss: 0.1458\n",
      "Epoch 31/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3797\n",
      "Epoch 00031: val_loss improved from 0.14579 to 0.14526, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3822 - val_loss: 0.1453\n",
      "Epoch 32/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3962\n",
      "Epoch 00032: val_loss did not improve from 0.14526\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3949 - val_loss: 0.1453\n",
      "Epoch 33/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3865\n",
      "Epoch 00033: val_loss improved from 0.14526 to 0.14428, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3839 - val_loss: 0.1443\n",
      "Epoch 34/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3764\n",
      "Epoch 00034: val_loss improved from 0.14428 to 0.14402, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3736 - val_loss: 0.1440\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3837\n",
      "Epoch 00035: val_loss improved from 0.14402 to 0.14212, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3837 - val_loss: 0.1421\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3685\n",
      "Epoch 00036: val_loss did not improve from 0.14212\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3685 - val_loss: 0.1431\n",
      "Epoch 37/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3610\n",
      "Epoch 00037: val_loss improved from 0.14212 to 0.14050, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3602 - val_loss: 0.1405\n",
      "Epoch 38/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3561\n",
      "Epoch 00038: val_loss improved from 0.14050 to 0.13835, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3584 - val_loss: 0.1384\n",
      "Epoch 39/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3583\n",
      "Epoch 00039: val_loss did not improve from 0.13835\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3590 - val_loss: 0.1407\n",
      "Epoch 40/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3887\n",
      "Epoch 00040: val_loss improved from 0.13835 to 0.13698, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3881 - val_loss: 0.1370\n",
      "Epoch 41/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.3533\n",
      "Epoch 00041: val_loss improved from 0.13698 to 0.13657, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3547 - val_loss: 0.1366\n",
      "Epoch 42/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.3531\n",
      "Epoch 00042: val_loss did not improve from 0.13657\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3532 - val_loss: 0.1377\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3424\n",
      "Epoch 00043: val_loss improved from 0.13657 to 0.13568, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3424 - val_loss: 0.1357\n",
      "Epoch 44/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3414\n",
      "Epoch 00044: val_loss did not improve from 0.13568\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3418 - val_loss: 0.1362\n",
      "Epoch 45/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3358\n",
      "Epoch 00045: val_loss improved from 0.13568 to 0.13418, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3332 - val_loss: 0.1342\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3568\n",
      "Epoch 00046: val_loss did not improve from 0.13418\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3568 - val_loss: 0.1375\n",
      "Epoch 47/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3249\n",
      "Epoch 00047: val_loss did not improve from 0.13418\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3249 - val_loss: 0.1347\n",
      " ###7 fold : val acc1 0.588, acc3 0.963, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/77 [=========================>....] - ETA: 0s - loss: 19.0711\n",
      "Epoch 00001: val_loss improved from inf to 13.57709, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5040 - val_loss: 13.5771\n",
      "Epoch 2/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 10.0093\n",
      "Epoch 00002: val_loss improved from 13.57709 to 6.39765, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.0119 - val_loss: 6.3977\n",
      "Epoch 3/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 5.1391\n",
      "Epoch 00003: val_loss improved from 6.39765 to 2.94436, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.8849 - val_loss: 2.9444\n",
      "Epoch 4/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 2.8030\n",
      "Epoch 00004: val_loss improved from 2.94436 to 1.58541, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.6231 - val_loss: 1.5854\n",
      "Epoch 5/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 2.0843\n",
      "Epoch 00005: val_loss improved from 1.58541 to 1.00882, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.9017 - val_loss: 1.0088\n",
      "Epoch 6/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.4599\n",
      "Epoch 00006: val_loss improved from 1.00882 to 0.69470, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3580 - val_loss: 0.6947\n",
      "Epoch 7/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8833\n",
      "Epoch 00007: val_loss improved from 0.69470 to 0.51648, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0631 - val_loss: 0.5165\n",
      "Epoch 8/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.1898\n",
      "Epoch 00008: val_loss improved from 0.51648 to 0.39853, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0803 - val_loss: 0.3985\n",
      "Epoch 9/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6460\n",
      "Epoch 00009: val_loss improved from 0.39853 to 0.32569, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7331 - val_loss: 0.3257\n",
      "Epoch 10/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.8078\n",
      "Epoch 00010: val_loss improved from 0.32569 to 0.27955, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7639 - val_loss: 0.2795\n",
      "Epoch 11/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.5577\n",
      "Epoch 00011: val_loss improved from 0.27955 to 0.25159, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6781 - val_loss: 0.2516\n",
      "Epoch 12/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.5892\n",
      "Epoch 00012: val_loss improved from 0.25159 to 0.22879, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5768 - val_loss: 0.2288\n",
      "Epoch 13/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.7399\n",
      "Epoch 00013: val_loss improved from 0.22879 to 0.21550, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6976 - val_loss: 0.2155\n",
      "Epoch 14/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.6316\n",
      "Epoch 00014: val_loss improved from 0.21550 to 0.20281, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6100 - val_loss: 0.2028\n",
      "Epoch 15/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.5888\n",
      "Epoch 00015: val_loss improved from 0.20281 to 0.19571, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5715 - val_loss: 0.1957\n",
      "Epoch 16/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.5577\n",
      "Epoch 00016: val_loss improved from 0.19571 to 0.18764, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5440 - val_loss: 0.1876\n",
      "Epoch 17/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.5054\n",
      "Epoch 00017: val_loss improved from 0.18764 to 0.18246, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4958 - val_loss: 0.1825\n",
      "Epoch 18/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.4717\n",
      "Epoch 00018: val_loss improved from 0.18246 to 0.17609, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4753 - val_loss: 0.1761\n",
      "Epoch 19/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.5060\n",
      "Epoch 00019: val_loss improved from 0.17609 to 0.17228, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5023 - val_loss: 0.1723\n",
      "Epoch 20/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.5012\n",
      "Epoch 00020: val_loss improved from 0.17228 to 0.16695, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4867 - val_loss: 0.1670\n",
      "Epoch 21/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4417\n",
      "Epoch 00021: val_loss improved from 0.16695 to 0.16501, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4401 - val_loss: 0.1650\n",
      "Epoch 22/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4561\n",
      "Epoch 00022: val_loss improved from 0.16501 to 0.16086, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4499 - val_loss: 0.1609\n",
      "Epoch 23/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4365\n",
      "Epoch 00023: val_loss improved from 0.16086 to 0.15720, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4319 - val_loss: 0.1572\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4558\n",
      "Epoch 00024: val_loss improved from 0.15720 to 0.15513, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4558 - val_loss: 0.1551\n",
      "Epoch 25/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4266\n",
      "Epoch 00025: val_loss improved from 0.15513 to 0.15164, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4308 - val_loss: 0.1516\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.4200\n",
      "Epoch 00026: val_loss improved from 0.15164 to 0.15100, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4200 - val_loss: 0.1510\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3990\n",
      "Epoch 00027: val_loss improved from 0.15100 to 0.14974, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4008 - val_loss: 0.1497\n",
      "Epoch 28/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3962\n",
      "Epoch 00028: val_loss improved from 0.14974 to 0.14590, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3974 - val_loss: 0.1459\n",
      "Epoch 29/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3939\n",
      "Epoch 00029: val_loss improved from 0.14590 to 0.14399, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3909 - val_loss: 0.1440\n",
      "Epoch 30/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3995\n",
      "Epoch 00030: val_loss improved from 0.14399 to 0.14335, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3955 - val_loss: 0.1433\n",
      "Epoch 31/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3798\n",
      "Epoch 00031: val_loss improved from 0.14335 to 0.14256, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3821 - val_loss: 0.1426\n",
      "Epoch 32/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.3972\n",
      "Epoch 00032: val_loss improved from 0.14256 to 0.14233, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3969 - val_loss: 0.1423\n",
      "Epoch 33/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.3851\n",
      "Epoch 00033: val_loss improved from 0.14233 to 0.14147, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3812 - val_loss: 0.1415\n",
      "Epoch 34/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.3748\n",
      "Epoch 00034: val_loss improved from 0.14147 to 0.14080, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3730 - val_loss: 0.1408\n",
      "Epoch 35/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3837\n",
      "Epoch 00035: val_loss improved from 0.14080 to 0.14003, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3824 - val_loss: 0.1400\n",
      "Epoch 36/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.3680\n",
      "Epoch 00036: val_loss did not improve from 0.14003\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3669 - val_loss: 0.1403\n",
      "Epoch 37/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3640\n",
      "Epoch 00037: val_loss improved from 0.14003 to 0.13803, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3603 - val_loss: 0.1380\n",
      "Epoch 38/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3554\n",
      "Epoch 00038: val_loss improved from 0.13803 to 0.13627, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3590 - val_loss: 0.1363\n",
      "Epoch 39/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3516\n",
      "Epoch 00039: val_loss did not improve from 0.13627\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3569 - val_loss: 0.1376\n",
      "Epoch 40/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3960\n",
      "Epoch 00040: val_loss improved from 0.13627 to 0.13449, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3874 - val_loss: 0.1345\n",
      "Epoch 41/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3509\n",
      "Epoch 00041: val_loss improved from 0.13449 to 0.13419, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3526 - val_loss: 0.1342\n",
      "Epoch 42/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3522\n",
      "Epoch 00042: val_loss did not improve from 0.13419\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3530 - val_loss: 0.1352\n",
      "Epoch 43/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3445\n",
      "Epoch 00043: val_loss improved from 0.13419 to 0.13371, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3414 - val_loss: 0.1337\n",
      "Epoch 44/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3396\n",
      "Epoch 00044: val_loss improved from 0.13371 to 0.13369, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3401 - val_loss: 0.1337\n",
      "Epoch 45/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3338\n",
      "Epoch 00045: val_loss improved from 0.13369 to 0.13202, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3323 - val_loss: 0.1320\n",
      "Epoch 46/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3612\n",
      "Epoch 00046: val_loss did not improve from 0.13202\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3558 - val_loss: 0.1341\n",
      "Epoch 47/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3267\n",
      "Epoch 00047: val_loss did not improve from 0.13202\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3250 - val_loss: 0.1321\n",
      " ###8 fold : val acc1 0.572, acc3 0.956, mae 0.240###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 19.1441\n",
      "Epoch 00001: val_loss improved from inf to 13.59553, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 18.5040 - val_loss: 13.5955\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 10.1395\n",
      "Epoch 00002: val_loss improved from 13.59553 to 6.39376, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 10.0119 - val_loss: 6.3938\n",
      "Epoch 3/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 5.1391\n",
      "Epoch 00003: val_loss improved from 6.39376 to 2.94915, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.8849 - val_loss: 2.9491\n",
      "Epoch 4/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 2.7354\n",
      "Epoch 00004: val_loss improved from 2.94915 to 1.59161, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.6231 - val_loss: 1.5916\n",
      "Epoch 5/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 1.9742\n",
      "Epoch 00005: val_loss improved from 1.59161 to 1.01244, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.9017 - val_loss: 1.0124\n",
      "Epoch 6/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 1.4094\n",
      "Epoch 00006: val_loss improved from 1.01244 to 0.69228, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.3580 - val_loss: 0.6923\n",
      "Epoch 7/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.8808\n",
      "Epoch 00007: val_loss improved from 0.69228 to 0.51164, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0631 - val_loss: 0.5116\n",
      "Epoch 8/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.1565\n",
      "Epoch 00008: val_loss improved from 0.51164 to 0.39254, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0803 - val_loss: 0.3925\n",
      "Epoch 9/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6462\n",
      "Epoch 00009: val_loss improved from 0.39254 to 0.31788, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7331 - val_loss: 0.3179\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8117\n",
      "Epoch 00010: val_loss improved from 0.31788 to 0.27199, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7639 - val_loss: 0.2720\n",
      "Epoch 11/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5569\n",
      "Epoch 00011: val_loss improved from 0.27199 to 0.24491, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6781 - val_loss: 0.2449\n",
      "Epoch 12/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5927\n",
      "Epoch 00012: val_loss improved from 0.24491 to 0.22221, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5768 - val_loss: 0.2222\n",
      "Epoch 13/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7543\n",
      "Epoch 00013: val_loss improved from 0.22221 to 0.21026, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6976 - val_loss: 0.2103\n",
      "Epoch 14/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.6292\n",
      "Epoch 00014: val_loss improved from 0.21026 to 0.19801, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6100 - val_loss: 0.1980\n",
      "Epoch 15/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.5874\n",
      "Epoch 00015: val_loss improved from 0.19801 to 0.19139, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5715 - val_loss: 0.1914\n",
      "Epoch 16/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5588\n",
      "Epoch 00016: val_loss improved from 0.19139 to 0.18387, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5440 - val_loss: 0.1839\n",
      "Epoch 17/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5080\n",
      "Epoch 00017: val_loss improved from 0.18387 to 0.17932, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4958 - val_loss: 0.1793\n",
      "Epoch 18/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.4741\n",
      "Epoch 00018: val_loss improved from 0.17932 to 0.17335, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4753 - val_loss: 0.1734\n",
      "Epoch 19/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.5060\n",
      "Epoch 00019: val_loss improved from 0.17335 to 0.16993, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5023 - val_loss: 0.1699\n",
      "Epoch 20/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5024\n",
      "Epoch 00020: val_loss improved from 0.16993 to 0.16465, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4867 - val_loss: 0.1646\n",
      "Epoch 21/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4419\n",
      "Epoch 00021: val_loss improved from 0.16465 to 0.16322, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4401 - val_loss: 0.1632\n",
      "Epoch 22/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4547\n",
      "Epoch 00022: val_loss improved from 0.16322 to 0.15926, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4499 - val_loss: 0.1593\n",
      "Epoch 23/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4386\n",
      "Epoch 00023: val_loss improved from 0.15926 to 0.15568, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4319 - val_loss: 0.1557\n",
      "Epoch 24/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.4602\n",
      "Epoch 00024: val_loss improved from 0.15568 to 0.15400, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4558 - val_loss: 0.1540\n",
      "Epoch 25/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4266\n",
      "Epoch 00025: val_loss improved from 0.15400 to 0.15006, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4308 - val_loss: 0.1501\n",
      "Epoch 26/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.4230\n",
      "Epoch 00026: val_loss improved from 0.15006 to 0.14955, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4200 - val_loss: 0.1495\n",
      "Epoch 27/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3985\n",
      "Epoch 00027: val_loss improved from 0.14955 to 0.14879, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4008 - val_loss: 0.1488\n",
      "Epoch 28/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.3980\n",
      "Epoch 00028: val_loss improved from 0.14879 to 0.14477, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3974 - val_loss: 0.1448\n",
      "Epoch 29/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3930\n",
      "Epoch 00029: val_loss improved from 0.14477 to 0.14340, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3909 - val_loss: 0.1434\n",
      "Epoch 30/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.4011\n",
      "Epoch 00030: val_loss improved from 0.14340 to 0.14204, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3955 - val_loss: 0.1420\n",
      "Epoch 31/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3805\n",
      "Epoch 00031: val_loss improved from 0.14204 to 0.14186, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3821 - val_loss: 0.1419\n",
      "Epoch 32/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3987\n",
      "Epoch 00032: val_loss did not improve from 0.14186\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3969 - val_loss: 0.1423\n",
      "Epoch 33/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3860\n",
      "Epoch 00033: val_loss improved from 0.14186 to 0.14100, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3812 - val_loss: 0.1410\n",
      "Epoch 34/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.3746\n",
      "Epoch 00034: val_loss did not improve from 0.14100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3730 - val_loss: 0.1410\n",
      "Epoch 35/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 0.3823\n",
      "Epoch 00035: val_loss improved from 0.14100 to 0.13907, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3824 - val_loss: 0.1391\n",
      "Epoch 36/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.3679\n",
      "Epoch 00036: val_loss did not improve from 0.13907\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3669 - val_loss: 0.1405\n",
      "Epoch 37/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.3611\n",
      "Epoch 00037: val_loss improved from 0.13907 to 0.13804, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3603 - val_loss: 0.1380\n",
      "Epoch 38/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.3609\n",
      "Epoch 00038: val_loss improved from 0.13804 to 0.13610, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3590 - val_loss: 0.1361\n",
      "Epoch 39/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.3547\n",
      "Epoch 00039: val_loss did not improve from 0.13610\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3569 - val_loss: 0.1380\n",
      "Epoch 40/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3954\n",
      "Epoch 00040: val_loss improved from 0.13610 to 0.13477, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3874 - val_loss: 0.1348\n",
      "Epoch 41/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.3522\n",
      "Epoch 00041: val_loss improved from 0.13477 to 0.13455, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3526 - val_loss: 0.1346\n",
      "Epoch 42/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.3524\n",
      "Epoch 00042: val_loss did not improve from 0.13455\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3530 - val_loss: 0.1355\n",
      "Epoch 43/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.3445\n",
      "Epoch 00043: val_loss improved from 0.13455 to 0.13419, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3414 - val_loss: 0.1342\n",
      "Epoch 44/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3389\n",
      "Epoch 00044: val_loss improved from 0.13419 to 0.13411, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3401 - val_loss: 0.1341\n",
      "Epoch 45/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3340\n",
      "Epoch 00045: val_loss improved from 0.13411 to 0.13213, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3323 - val_loss: 0.1321\n",
      "Epoch 46/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.3603\n",
      "Epoch 00046: val_loss did not improve from 0.13213\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3558 - val_loss: 0.1349\n",
      "Epoch 47/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3271\n",
      "Epoch 00047: val_loss did not improve from 0.13213\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3250 - val_loss: 0.1322\n",
      " ###9 fold : val acc1 0.582, acc3 0.960, mae 0.234###\n",
      "acc10.577_acc30.958\n",
      "random search 2/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293/307 [===========================>..] - ETA: 0s - loss: 2.3059\n",
      "Epoch 00001: val_loss improved from inf to 0.29873, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.2248 - val_loss: 0.2987\n",
      "Epoch 2/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.6961\n",
      "Epoch 00002: val_loss improved from 0.29873 to 0.23571, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6914 - val_loss: 0.2357\n",
      "Epoch 3/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.4471\n",
      "Epoch 00003: val_loss did not improve from 0.23571\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4472 - val_loss: 0.4649\n",
      "Epoch 4/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.3459\n",
      "Epoch 00004: val_loss did not improve from 0.23571\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3459 - val_loss: 0.3552\n",
      " ###0 fold : val acc1 0.421, acc3 0.888, mae 0.356###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/307 [============================>.] - ETA: 0s - loss: 2.2612\n",
      "Epoch 00001: val_loss improved from inf to 0.30472, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.2171 - val_loss: 0.3047\n",
      "Epoch 2/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.6851\n",
      "Epoch 00002: val_loss improved from 0.30472 to 0.23432, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6849 - val_loss: 0.2343\n",
      "Epoch 3/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.3296\n",
      "Epoch 00003: val_loss did not improve from 0.23432\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4460 - val_loss: 0.5423\n",
      "Epoch 4/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.3456\n",
      "Epoch 00004: val_loss did not improve from 0.23432\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3436 - val_loss: 0.3320\n",
      " ###1 fold : val acc1 0.414, acc3 0.893, mae 0.354###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/307 [============================>.] - ETA: 0s - loss: 2.2622\n",
      "Epoch 00001: val_loss improved from inf to 0.31248, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.2246 - val_loss: 0.3125\n",
      "Epoch 2/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.6889\n",
      "Epoch 00002: val_loss improved from 0.31248 to 0.24899, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6779 - val_loss: 0.2490\n",
      "Epoch 3/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.4442\n",
      "Epoch 00003: val_loss did not improve from 0.24899\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4442 - val_loss: 0.5783\n",
      "Epoch 4/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.3529\n",
      "Epoch 00004: val_loss did not improve from 0.24899\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3527 - val_loss: 0.3243\n",
      " ###2 fold : val acc1 0.387, acc3 0.885, mae 0.372###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/307 [===========================>..] - ETA: 0s - loss: 2.3247\n",
      "Epoch 00001: val_loss improved from inf to 0.32342, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.2558 - val_loss: 0.3234\n",
      "Epoch 2/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.7023\n",
      "Epoch 00002: val_loss improved from 0.32342 to 0.25477, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6931 - val_loss: 0.2548\n",
      "Epoch 3/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.4390\n",
      "Epoch 00003: val_loss did not improve from 0.25477\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4393 - val_loss: 0.5460\n",
      "Epoch 4/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.3419\n",
      "Epoch 00004: val_loss did not improve from 0.25477\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3419 - val_loss: 0.3193\n",
      " ###3 fold : val acc1 0.408, acc3 0.887, mae 0.360###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/307 [============================>.] - ETA: 0s - loss: 2.5994\n",
      "Epoch 00001: val_loss improved from inf to 0.28460, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.5610 - val_loss: 0.2846\n",
      "Epoch 2/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.7323\n",
      "Epoch 00002: val_loss did not improve from 0.28460\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7323 - val_loss: 0.3070\n",
      "Epoch 3/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.4214\n",
      "Epoch 00003: val_loss improved from 0.28460 to 0.24682, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4171 - val_loss: 0.2468\n",
      "Epoch 4/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.3716\n",
      "Epoch 00004: val_loss did not improve from 0.24682\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3716 - val_loss: 0.3056\n",
      "Epoch 5/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.3224\n",
      "Epoch 00005: val_loss did not improve from 0.24682\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3221 - val_loss: 0.2924\n",
      " ###4 fold : val acc1 0.400, acc3 0.893, mae 0.360###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/307 [============================>.] - ETA: 0s - loss: 1.9631\n",
      "Epoch 00001: val_loss improved from inf to 0.28280, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.9456 - val_loss: 0.2828\n",
      "Epoch 2/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.3715\n",
      "Epoch 00002: val_loss did not improve from 0.28280\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3712 - val_loss: 0.3349\n",
      "Epoch 3/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.3288\n",
      "Epoch 00003: val_loss improved from 0.28280 to 0.25377, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3282 - val_loss: 0.2538\n",
      "Epoch 4/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.2938\n",
      "Epoch 00004: val_loss did not improve from 0.25377\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2935 - val_loss: 0.3533\n",
      "Epoch 5/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.2673\n",
      "Epoch 00005: val_loss did not improve from 0.25377\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2677 - val_loss: 0.3901\n",
      " ###5 fold : val acc1 0.416, acc3 0.897, mae 0.366###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/307 [===========================>..] - ETA: 0s - loss: 3.8029\n",
      "Epoch 00001: val_loss improved from inf to 0.28316, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.6803 - val_loss: 0.2832\n",
      "Epoch 2/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.7779\n",
      "Epoch 00002: val_loss improved from 0.28316 to 0.26019, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7643 - val_loss: 0.2602\n",
      "Epoch 3/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.5137\n",
      "Epoch 00003: val_loss improved from 0.26019 to 0.25654, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5083 - val_loss: 0.2565\n",
      "Epoch 4/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.4139\n",
      "Epoch 00004: val_loss did not improve from 0.25654\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4088 - val_loss: 0.2898\n",
      "Epoch 5/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.3469\n",
      "Epoch 00005: val_loss did not improve from 0.25654\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3460 - val_loss: 0.2972\n",
      " ###6 fold : val acc1 0.404, acc3 0.884, mae 0.364###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/307 [============================>.] - ETA: 0s - loss: 3.7680\n",
      "Epoch 00001: val_loss improved from inf to 0.29138, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.6994 - val_loss: 0.2914\n",
      "Epoch 2/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.8563\n",
      "Epoch 00002: val_loss improved from 0.29138 to 0.28048, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8563 - val_loss: 0.2805\n",
      "Epoch 3/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.5782\n",
      "Epoch 00003: val_loss improved from 0.28048 to 0.26571, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5723 - val_loss: 0.2657\n",
      "Epoch 4/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.4386\n",
      "Epoch 00004: val_loss did not improve from 0.26571\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4340 - val_loss: 0.2783\n",
      "Epoch 5/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.3545\n",
      "Epoch 00005: val_loss did not improve from 0.26571\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3537 - val_loss: 0.3141\n",
      " ###7 fold : val acc1 0.387, acc3 0.879, mae 0.376###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297/307 [============================>.] - ETA: 0s - loss: 3.7854\n",
      "Epoch 00001: val_loss improved from inf to 0.29486, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.6849 - val_loss: 0.2949\n",
      "Epoch 2/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.8429\n",
      "Epoch 00002: val_loss improved from 0.29486 to 0.27795, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8284 - val_loss: 0.2780\n",
      "Epoch 3/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.5581\n",
      "Epoch 00003: val_loss improved from 0.27795 to 0.26602, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5537 - val_loss: 0.2660\n",
      "Epoch 4/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.4334\n",
      "Epoch 00004: val_loss did not improve from 0.26602\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4281 - val_loss: 0.2878\n",
      "Epoch 5/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.3419\n",
      "Epoch 00005: val_loss did not improve from 0.26602\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3408 - val_loss: 0.3158\n",
      " ###8 fold : val acc1 0.355, acc3 0.853, mae 0.406###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/307 [===========================>..] - ETA: 0s - loss: 3.8069\n",
      "Epoch 00001: val_loss improved from inf to 0.29468, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.6849 - val_loss: 0.2947\n",
      "Epoch 2/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.4014\n",
      "Epoch 00002: val_loss improved from 0.29468 to 0.28426, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8284 - val_loss: 0.2843\n",
      "Epoch 3/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.5616\n",
      "Epoch 00003: val_loss improved from 0.28426 to 0.27322, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0.3,dnodes128_dropout0.2,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.5537 - val_loss: 0.2732\n",
      "Epoch 4/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.4309\n",
      "Epoch 00004: val_loss did not improve from 0.27322\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.4281 - val_loss: 0.2970\n",
      "Epoch 5/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.3420\n",
      "Epoch 00005: val_loss did not improve from 0.27322\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3408 - val_loss: 0.3243\n",
      " ###9 fold : val acc1 0.369, acc3 0.866, mae 0.391###\n",
      "acc10.396_acc30.883\n",
      "random search 3/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/39 [============================>.] - ETA: 0s - loss: 15.2906\n",
      "Epoch 00001: val_loss improved from inf to 8.68182, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.2502 - val_loss: 8.6818\n",
      "Epoch 2/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 7.1274\n",
      "Epoch 00002: val_loss improved from 8.68182 to 2.63956, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 7.1095 - val_loss: 2.6396\n",
      "Epoch 3/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 3.7076\n",
      "Epoch 00003: val_loss improved from 2.63956 to 1.23927, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 4.0146 - val_loss: 1.2393\n",
      "Epoch 4/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 3.3467\n",
      "Epoch 00004: val_loss improved from 1.23927 to 0.80318, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.3404 - val_loss: 0.8032\n",
      "Epoch 5/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.6442\n",
      "Epoch 00005: val_loss improved from 0.80318 to 0.56187, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6451 - val_loss: 0.5619\n",
      "Epoch 6/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.7238\n",
      "Epoch 00006: val_loss improved from 0.56187 to 0.50673, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6995 - val_loss: 0.5067\n",
      "Epoch 7/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.7622\n",
      "Epoch 00007: val_loss improved from 0.50673 to 0.44222, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7370 - val_loss: 0.4422\n",
      "Epoch 8/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.8117\n",
      "Epoch 00008: val_loss improved from 0.44222 to 0.40359, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7584 - val_loss: 0.4036\n",
      "Epoch 9/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.2763\n",
      "Epoch 00009: val_loss improved from 0.40359 to 0.32580, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.2549 - val_loss: 0.3258\n",
      "Epoch 10/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.8731\n",
      "Epoch 00010: val_loss did not improve from 0.32580\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.7296 - val_loss: 0.3949\n",
      "Epoch 11/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.8545\n",
      "Epoch 00011: val_loss improved from 0.32580 to 0.30781, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8506 - val_loss: 0.3078\n",
      "Epoch 12/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.7973\n",
      "Epoch 00012: val_loss did not improve from 0.30781\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7958 - val_loss: 0.3308\n",
      "Epoch 13/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8150\n",
      "Epoch 00013: val_loss did not improve from 0.30781\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7926 - val_loss: 0.3260\n",
      " ###0 fold : val acc1 0.363, acc3 0.841, mae 0.415###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/39 [===========================>..] - ETA: 0s - loss: 15.4575\n",
      "Epoch 00001: val_loss improved from inf to 8.66922, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.2572 - val_loss: 8.6692\n",
      "Epoch 2/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 7.2538\n",
      "Epoch 00002: val_loss improved from 8.66922 to 2.63276, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 7.1634 - val_loss: 2.6328\n",
      "Epoch 3/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 3.7019\n",
      "Epoch 00003: val_loss improved from 2.63276 to 1.23048, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 4.0018 - val_loss: 1.2305\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 3.3957\n",
      "Epoch 00004: val_loss improved from 1.23048 to 0.80571, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.3482 - val_loss: 0.8057\n",
      "Epoch 5/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.6390\n",
      "Epoch 00005: val_loss improved from 0.80571 to 0.55450, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6310 - val_loss: 0.5545\n",
      "Epoch 6/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 2.8311\n",
      "Epoch 00006: val_loss improved from 0.55450 to 0.51216, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7168 - val_loss: 0.5122\n",
      "Epoch 7/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 2.8311\n",
      "Epoch 00007: val_loss improved from 0.51216 to 0.43945, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7351 - val_loss: 0.4394\n",
      "Epoch 8/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 2.0147\n",
      "Epoch 00008: val_loss improved from 0.43945 to 0.40354, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7456 - val_loss: 0.4035\n",
      "Epoch 9/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 2.3535\n",
      "Epoch 00009: val_loss improved from 0.40354 to 0.32597, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.2584 - val_loss: 0.3260\n",
      "Epoch 10/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.8742\n",
      "Epoch 00010: val_loss did not improve from 0.32597\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7187 - val_loss: 0.3907\n",
      "Epoch 11/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.8519\n",
      "Epoch 00011: val_loss improved from 0.32597 to 0.30735, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8529 - val_loss: 0.3073\n",
      "Epoch 12/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.8138\n",
      "Epoch 00012: val_loss did not improve from 0.30735\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8007 - val_loss: 0.3255\n",
      "Epoch 13/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.8090\n",
      "Epoch 00013: val_loss did not improve from 0.30735\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7886 - val_loss: 0.3266\n",
      " ###1 fold : val acc1 0.345, acc3 0.854, mae 0.412###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/39 [============================>.] - ETA: 0s - loss: 15.3051\n",
      "Epoch 00001: val_loss improved from inf to 8.66776, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.2638 - val_loss: 8.6678\n",
      "Epoch 2/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 7.1801\n",
      "Epoch 00002: val_loss improved from 8.66776 to 2.65342, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 7.1574 - val_loss: 2.6534\n",
      "Epoch 3/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 4.0358\n",
      "Epoch 00003: val_loss improved from 2.65342 to 1.24771, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 4.0316 - val_loss: 1.2477\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 3.4140\n",
      "Epoch 00004: val_loss improved from 1.24771 to 0.80464, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.3640 - val_loss: 0.8046\n",
      "Epoch 5/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.6646\n",
      "Epoch 00005: val_loss improved from 0.80464 to 0.56203, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6554 - val_loss: 0.5620\n",
      "Epoch 6/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.7400\n",
      "Epoch 00006: val_loss improved from 0.56203 to 0.50719, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7000 - val_loss: 0.5072\n",
      "Epoch 7/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.8003\n",
      "Epoch 00007: val_loss improved from 0.50719 to 0.44197, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7473 - val_loss: 0.4420\n",
      "Epoch 8/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.0044\n",
      "Epoch 00008: val_loss improved from 0.44197 to 0.39887, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7349 - val_loss: 0.3989\n",
      "Epoch 9/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 2.3032\n",
      "Epoch 00009: val_loss improved from 0.39887 to 0.32204, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.2596 - val_loss: 0.3220\n",
      "Epoch 10/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.8597\n",
      "Epoch 00010: val_loss did not improve from 0.32204\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.6953 - val_loss: 0.3915\n",
      "Epoch 11/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8520\n",
      "Epoch 00011: val_loss improved from 0.32204 to 0.30902, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8592 - val_loss: 0.3090\n",
      "Epoch 12/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8128\n",
      "Epoch 00012: val_loss did not improve from 0.30902\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.8024 - val_loss: 0.3279\n",
      "Epoch 13/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.8025\n",
      "Epoch 00013: val_loss did not improve from 0.30902\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7832 - val_loss: 0.3216\n",
      " ###2 fold : val acc1 0.329, acc3 0.841, mae 0.428###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/39 [============================>.] - ETA: 0s - loss: 15.3009\n",
      "Epoch 00001: val_loss improved from inf to 8.68099, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.2608 - val_loss: 8.6810\n",
      "Epoch 2/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 7.1727\n",
      "Epoch 00002: val_loss improved from 8.68099 to 2.66890, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 7.1498 - val_loss: 2.6689\n",
      "Epoch 3/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 3.7196\n",
      "Epoch 00003: val_loss improved from 2.66890 to 1.24204, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 4.0213 - val_loss: 1.2420\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 3.4057\n",
      "Epoch 00004: val_loss improved from 1.24204 to 0.79991, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.3589 - val_loss: 0.7999\n",
      "Epoch 5/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.6704\n",
      "Epoch 00005: val_loss improved from 0.79991 to 0.55628, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6439 - val_loss: 0.5563\n",
      "Epoch 6/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.7473\n",
      "Epoch 00006: val_loss improved from 0.55628 to 0.51110, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7082 - val_loss: 0.5111\n",
      "Epoch 7/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 2.7714\n",
      "Epoch 00007: val_loss improved from 0.51110 to 0.44739, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7634 - val_loss: 0.4474\n",
      "Epoch 8/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.7600\n",
      "Epoch 00008: val_loss improved from 0.44739 to 0.40301, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7294 - val_loss: 0.4030\n",
      "Epoch 9/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.2953\n",
      "Epoch 00009: val_loss improved from 0.40301 to 0.31146, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.2599 - val_loss: 0.3115\n",
      "Epoch 10/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.8528\n",
      "Epoch 00010: val_loss did not improve from 0.31146\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.7093 - val_loss: 0.3883\n",
      "Epoch 11/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8622\n",
      "Epoch 00011: val_loss improved from 0.31146 to 0.30872, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8650 - val_loss: 0.3087\n",
      "Epoch 12/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8101\n",
      "Epoch 00012: val_loss did not improve from 0.30872\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7986 - val_loss: 0.3233\n",
      "Epoch 13/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8080\n",
      "Epoch 00013: val_loss did not improve from 0.30872\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7885 - val_loss: 0.3218\n",
      " ###3 fold : val acc1 0.347, acc3 0.860, mae 0.410###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - ETA: 0s - loss: 15.4773\n",
      "Epoch 00001: val_loss improved from inf to 9.02238, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.4773 - val_loss: 9.0224\n",
      "Epoch 2/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 7.5799\n",
      "Epoch 00002: val_loss improved from 9.02238 to 2.69405, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 7.4430 - val_loss: 2.6940\n",
      "Epoch 3/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 4.9056\n",
      "Epoch 00003: val_loss improved from 2.69405 to 1.34364, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 4.9056 - val_loss: 1.3436\n",
      "Epoch 4/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 3.1681\n",
      "Epoch 00004: val_loss improved from 1.34364 to 0.79884, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.1734 - val_loss: 0.7988\n",
      "Epoch 5/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 3.1786\n",
      "Epoch 00005: val_loss improved from 0.79884 to 0.60419, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.1786 - val_loss: 0.6042\n",
      "Epoch 6/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 3.1338\n",
      "Epoch 00006: val_loss improved from 0.60419 to 0.45954, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.7221 - val_loss: 0.4595\n",
      "Epoch 7/100\n",
      "20/39 [==============>...............] - ETA: 0s - loss: 2.1969\n",
      "Epoch 00007: val_loss improved from 0.45954 to 0.41272, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.1778 - val_loss: 0.4127\n",
      "Epoch 8/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 3.3075\n",
      "Epoch 00008: val_loss did not improve from 0.41272\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.2971 - val_loss: 0.4237\n",
      "Epoch 9/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 3.1882\n",
      "Epoch 00009: val_loss did not improve from 0.41272\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.1065 - val_loss: 0.4295\n",
      " ###4 fold : val acc1 0.299, acc3 0.794, mae 0.474###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/39 [===============>..............] - ETA: 0s - loss: 17.4852 \n",
      "Epoch 00001: val_loss improved from inf to 8.38692, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 14.9323 - val_loss: 8.3869\n",
      "Epoch 2/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 7.1308\n",
      "Epoch 00002: val_loss improved from 8.38692 to 2.36527, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 6.1188 - val_loss: 2.3653\n",
      "Epoch 3/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 3.8135\n",
      "Epoch 00003: val_loss improved from 2.36527 to 1.17219, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.5437 - val_loss: 1.1722\n",
      "Epoch 4/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 2.8719\n",
      "Epoch 00004: val_loss improved from 1.17219 to 0.69629, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.7775 - val_loss: 0.6963\n",
      "Epoch 5/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 2.4360\n",
      "Epoch 00005: val_loss improved from 0.69629 to 0.50345, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.4229 - val_loss: 0.5034\n",
      "Epoch 6/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 2.1326\n",
      "Epoch 00006: val_loss improved from 0.50345 to 0.41668, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.1326 - val_loss: 0.4167\n",
      "Epoch 7/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 2.0306\n",
      "Epoch 00007: val_loss improved from 0.41668 to 0.37766, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.0143 - val_loss: 0.3777\n",
      "Epoch 8/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.8899\n",
      "Epoch 00008: val_loss improved from 0.37766 to 0.35175, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8891 - val_loss: 0.3517\n",
      "Epoch 9/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.8592\n",
      "Epoch 00009: val_loss improved from 0.35175 to 0.30810, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8523 - val_loss: 0.3081\n",
      "Epoch 10/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.7770\n",
      "Epoch 00010: val_loss did not improve from 0.30810\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7777 - val_loss: 0.3157\n",
      "Epoch 11/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.7097\n",
      "Epoch 00011: val_loss improved from 0.30810 to 0.29563, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7054 - val_loss: 0.2956\n",
      "Epoch 12/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.6639\n",
      "Epoch 00012: val_loss did not improve from 0.29563\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.6667 - val_loss: 0.3060\n",
      "Epoch 13/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.6233\n",
      "Epoch 00013: val_loss improved from 0.29563 to 0.28183, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6170 - val_loss: 0.2818\n",
      "Epoch 14/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.6082\n",
      "Epoch 00014: val_loss did not improve from 0.28183\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.6019 - val_loss: 0.2829\n",
      "Epoch 15/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.5856\n",
      "Epoch 00015: val_loss improved from 0.28183 to 0.28078, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.5821 - val_loss: 0.2808\n",
      "Epoch 16/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.5391\n",
      "Epoch 00016: val_loss improved from 0.28078 to 0.24659, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.5378 - val_loss: 0.2466\n",
      "Epoch 17/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.5115\n",
      "Epoch 00017: val_loss did not improve from 0.24659\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.5115 - val_loss: 0.2743\n",
      "Epoch 18/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.4827\n",
      "Epoch 00018: val_loss did not improve from 0.24659\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.4811 - val_loss: 0.2945\n",
      " ###5 fold : val acc1 0.420, acc3 0.907, mae 0.370###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - ETA: 0s - loss: 15.0202\n",
      "Epoch 00001: val_loss improved from inf to 8.56435, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.0202 - val_loss: 8.5644\n",
      "Epoch 2/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 7.1777\n",
      "Epoch 00002: val_loss improved from 8.56435 to 2.41351, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 6.5633 - val_loss: 2.4135\n",
      "Epoch 3/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 8.7343\n",
      "Epoch 00003: val_loss improved from 2.41351 to 1.30823, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 6.2850 - val_loss: 1.3082\n",
      "Epoch 4/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 3.1782\n",
      "Epoch 00004: val_loss improved from 1.30823 to 0.76766, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.1819 - val_loss: 0.7677\n",
      "Epoch 5/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 4.7312\n",
      "Epoch 00005: val_loss improved from 0.76766 to 0.64965, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 4.7312 - val_loss: 0.6496\n",
      "Epoch 6/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 2.6312\n",
      "Epoch 00006: val_loss improved from 0.64965 to 0.44692, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.4443 - val_loss: 0.4469\n",
      "Epoch 7/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 2.2098\n",
      "Epoch 00007: val_loss improved from 0.44692 to 0.42731, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.3172 - val_loss: 0.4273\n",
      "Epoch 8/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 2.0614\n",
      "Epoch 00008: val_loss improved from 0.42731 to 0.41592, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.7923 - val_loss: 0.4159\n",
      "Epoch 9/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 2.0846\n",
      "Epoch 00009: val_loss improved from 0.41592 to 0.38114, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.5932 - val_loss: 0.3811\n",
      "Epoch 10/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.8621\n",
      "Epoch 00010: val_loss improved from 0.38114 to 0.35390, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.8114 - val_loss: 0.3539\n",
      "Epoch 11/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.1721\n",
      "Epoch 00011: val_loss improved from 0.35390 to 0.33308, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.1568 - val_loss: 0.3331\n",
      "Epoch 12/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.3522\n",
      "Epoch 00012: val_loss improved from 0.33308 to 0.31201, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.3062 - val_loss: 0.3120\n",
      "Epoch 13/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.8895\n",
      "Epoch 00013: val_loss improved from 0.31201 to 0.30400, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8685 - val_loss: 0.3040\n",
      "Epoch 14/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.8008\n",
      "Epoch 00014: val_loss improved from 0.30400 to 0.30279, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7929 - val_loss: 0.3028\n",
      "Epoch 15/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.8793\n",
      "Epoch 00015: val_loss did not improve from 0.30279\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.8616 - val_loss: 0.3168\n",
      "Epoch 16/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.6881\n",
      "Epoch 00016: val_loss improved from 0.30279 to 0.26548, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6829 - val_loss: 0.2655\n",
      "Epoch 17/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.6483\n",
      "Epoch 00017: val_loss did not improve from 0.26548\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.6456 - val_loss: 0.2981\n",
      "Epoch 18/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.6229\n",
      "Epoch 00018: val_loss did not improve from 0.26548\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.6202 - val_loss: 0.3090\n",
      " ###6 fold : val acc1 0.397, acc3 0.889, mae 0.367###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/39 [==========================>...] - ETA: 0s - loss: 15.3584\n",
      "Epoch 00001: val_loss improved from inf to 8.56687, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.0224 - val_loss: 8.5669\n",
      "Epoch 2/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 6.5502\n",
      "Epoch 00002: val_loss improved from 8.56687 to 2.39059, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 6.5309 - val_loss: 2.3906\n",
      "Epoch 3/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 6.6024\n",
      "Epoch 00003: val_loss improved from 2.39059 to 1.30556, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 6.3910 - val_loss: 1.3056\n",
      "Epoch 4/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 3.2109\n",
      "Epoch 00004: val_loss improved from 1.30556 to 0.77718, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.1971 - val_loss: 0.7772\n",
      "Epoch 5/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 4.9158\n",
      "Epoch 00005: val_loss improved from 0.77718 to 0.65732, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 4.8947 - val_loss: 0.6573\n",
      "Epoch 6/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.4747\n",
      "Epoch 00006: val_loss improved from 0.65732 to 0.45803, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.4574 - val_loss: 0.4580\n",
      "Epoch 7/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.3816\n",
      "Epoch 00007: val_loss improved from 0.45803 to 0.42099, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.3492 - val_loss: 0.4210\n",
      "Epoch 8/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 2.9660\n",
      "Epoch 00008: val_loss improved from 0.42099 to 0.41962, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.8431 - val_loss: 0.4196\n",
      "Epoch 9/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.7184\n",
      "Epoch 00009: val_loss improved from 0.41962 to 0.37717, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6701 - val_loss: 0.3772\n",
      "Epoch 10/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.8768\n",
      "Epoch 00010: val_loss improved from 0.37717 to 0.35268, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.8445 - val_loss: 0.3527\n",
      "Epoch 11/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 1.8423\n",
      "Epoch 00011: val_loss improved from 0.35268 to 0.32975, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.1692 - val_loss: 0.3297\n",
      "Epoch 12/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.3997\n",
      "Epoch 00012: val_loss improved from 0.32975 to 0.30883, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.3785 - val_loss: 0.3088\n",
      "Epoch 13/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.8745\n",
      "Epoch 00013: val_loss improved from 0.30883 to 0.30356, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.8745 - val_loss: 0.3036\n",
      "Epoch 14/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.8176\n",
      "Epoch 00014: val_loss improved from 0.30356 to 0.30274, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8114 - val_loss: 0.3027\n",
      "Epoch 15/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.9174\n",
      "Epoch 00015: val_loss did not improve from 0.30274\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.9016 - val_loss: 0.3252\n",
      "Epoch 16/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.6813\n",
      "Epoch 00016: val_loss improved from 0.30274 to 0.26606, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6738 - val_loss: 0.2661\n",
      "Epoch 17/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.6711\n",
      "Epoch 00017: val_loss did not improve from 0.26606\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6516 - val_loss: 0.3076\n",
      "Epoch 18/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.6320\n",
      "Epoch 00018: val_loss did not improve from 0.26606\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.6271 - val_loss: 0.3045\n",
      " ###7 fold : val acc1 0.390, acc3 0.887, mae 0.371###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/39 [===========================>..] - ETA: 0s - loss: 15.2231\n",
      "Epoch 00001: val_loss improved from inf to 8.64020, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.0257 - val_loss: 8.6402\n",
      "Epoch 2/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 6.5444\n",
      "Epoch 00002: val_loss improved from 8.64020 to 2.44355, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 6.5444 - val_loss: 2.4435\n",
      "Epoch 3/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 6.3974\n",
      "Epoch 00003: val_loss improved from 2.44355 to 1.35488, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 6.3704 - val_loss: 1.3549\n",
      "Epoch 4/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 3.2152\n",
      "Epoch 00004: val_loss improved from 1.35488 to 0.81522, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.2012 - val_loss: 0.8152\n",
      "Epoch 5/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 4.8909\n",
      "Epoch 00005: val_loss improved from 0.81522 to 0.67275, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 4.8705 - val_loss: 0.6728\n",
      "Epoch 6/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.4623\n",
      "Epoch 00006: val_loss improved from 0.67275 to 0.47141, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.4529 - val_loss: 0.4714\n",
      "Epoch 7/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.3751\n",
      "Epoch 00007: val_loss improved from 0.47141 to 0.43240, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.3467 - val_loss: 0.4324\n",
      "Epoch 8/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 2.8376\n",
      "Epoch 00008: val_loss improved from 0.43240 to 0.42211, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.8317 - val_loss: 0.4221\n",
      "Epoch 9/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 2.6645\n",
      "Epoch 00009: val_loss improved from 0.42211 to 0.37978, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6577 - val_loss: 0.3798\n",
      "Epoch 10/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.8969\n",
      "Epoch 00010: val_loss improved from 0.37978 to 0.35190, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.8434 - val_loss: 0.3519\n",
      "Epoch 11/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.1733\n",
      "Epoch 00011: val_loss improved from 0.35190 to 0.33144, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.1663 - val_loss: 0.3314\n",
      "Epoch 12/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 2.4585\n",
      "Epoch 00012: val_loss improved from 0.33144 to 0.30506, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.3597 - val_loss: 0.3051\n",
      "Epoch 13/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8962\n",
      "Epoch 00013: val_loss improved from 0.30506 to 0.29580, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8657 - val_loss: 0.2958\n",
      "Epoch 14/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.8079\n",
      "Epoch 00014: val_loss did not improve from 0.29580\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.8020 - val_loss: 0.2999\n",
      "Epoch 15/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.9178\n",
      "Epoch 00015: val_loss did not improve from 0.29580\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.8998 - val_loss: 0.3197\n",
      " ###8 fold : val acc1 0.340, acc3 0.849, mae 0.423###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - ETA: 0s - loss: 15.0257\n",
      "Epoch 00001: val_loss improved from inf to 8.65024, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 15.0257 - val_loss: 8.6502\n",
      "Epoch 2/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 7.1395\n",
      "Epoch 00002: val_loss improved from 8.65024 to 2.44073, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 6.5444 - val_loss: 2.4407\n",
      "Epoch 3/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 6.3974\n",
      "Epoch 00003: val_loss improved from 2.44073 to 1.34777, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 6.3704 - val_loss: 1.3478\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 3.2337\n",
      "Epoch 00004: val_loss improved from 1.34777 to 0.80359, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.2012 - val_loss: 0.8036\n",
      "Epoch 5/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 5.0351\n",
      "Epoch 00005: val_loss improved from 0.80359 to 0.66855, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 4.8705 - val_loss: 0.6686\n",
      "Epoch 6/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.4765\n",
      "Epoch 00006: val_loss improved from 0.66855 to 0.46293, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.4529 - val_loss: 0.4629\n",
      "Epoch 7/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.3751\n",
      "Epoch 00007: val_loss improved from 0.46293 to 0.42337, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.3467 - val_loss: 0.4234\n",
      "Epoch 8/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.9218\n",
      "Epoch 00008: val_loss improved from 0.42337 to 0.41650, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.8317 - val_loss: 0.4165\n",
      "Epoch 9/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 2.7291\n",
      "Epoch 00009: val_loss improved from 0.41650 to 0.37497, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.6577 - val_loss: 0.3750\n",
      "Epoch 10/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.8754\n",
      "Epoch 00010: val_loss improved from 0.37497 to 0.34804, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.8434 - val_loss: 0.3480\n",
      "Epoch 11/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.1733\n",
      "Epoch 00011: val_loss improved from 0.34804 to 0.32925, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.1663 - val_loss: 0.3292\n",
      "Epoch 12/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 2.4400\n",
      "Epoch 00012: val_loss improved from 0.32925 to 0.30361, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.3597 - val_loss: 0.3036\n",
      "Epoch 13/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.8962\n",
      "Epoch 00013: val_loss improved from 0.30361 to 0.29446, saving model to result/size/DNN_size_both_y/batch512,dnodes64_dropout0.5,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8657 - val_loss: 0.2945\n",
      "Epoch 14/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.8092\n",
      "Epoch 00014: val_loss did not improve from 0.29446\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.8020 - val_loss: 0.2997\n",
      "Epoch 15/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.9299\n",
      "Epoch 00015: val_loss did not improve from 0.29446\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.8998 - val_loss: 0.3191\n",
      " ###9 fold : val acc1 0.350, acc3 0.847, mae 0.417###\n",
      "acc10.358_acc30.857\n",
      "random search 4/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/77 [===========================>..] - ETA: 0s - loss: 14.3561\n",
      "Epoch 00001: val_loss improved from inf to 5.99854, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 14.0817 - val_loss: 5.9985\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 3.4001\n",
      "Epoch 00002: val_loss improved from 5.99854 to 1.28397, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.1763 - val_loss: 1.2840\n",
      "Epoch 3/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.8492\n",
      "Epoch 00003: val_loss improved from 1.28397 to 0.56727, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0933 - val_loss: 0.5673\n",
      "Epoch 4/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.5602\n",
      "Epoch 00004: val_loss improved from 0.56727 to 0.32787, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5491 - val_loss: 0.3279\n",
      "Epoch 5/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.3526\n",
      "Epoch 00005: val_loss improved from 0.32787 to 0.24742, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3420 - val_loss: 0.2474\n",
      "Epoch 6/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 0.2621\n",
      "Epoch 00006: val_loss improved from 0.24742 to 0.21547, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2555 - val_loss: 0.2155\n",
      "Epoch 7/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.2187\n",
      "Epoch 00007: val_loss improved from 0.21547 to 0.19663, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2128 - val_loss: 0.1966\n",
      "Epoch 8/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.1889\n",
      "Epoch 00008: val_loss improved from 0.19663 to 0.18321, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1869 - val_loss: 0.1832\n",
      "Epoch 9/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.1724\n",
      "Epoch 00009: val_loss improved from 0.18321 to 0.17215, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1699 - val_loss: 0.1721\n",
      "Epoch 10/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.1547\n",
      "Epoch 00010: val_loss improved from 0.17215 to 0.16274, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 0.1627\n",
      "Epoch 11/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.1513\n",
      "Epoch 00011: val_loss improved from 0.16274 to 0.15651, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1501 - val_loss: 0.1565\n",
      "Epoch 12/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.1432\n",
      "Epoch 00012: val_loss improved from 0.15651 to 0.15127, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1444 - val_loss: 0.1513\n",
      "Epoch 13/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1394\n",
      "Epoch 00013: val_loss improved from 0.15127 to 0.14709, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1400 - val_loss: 0.1471\n",
      "Epoch 14/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1390\n",
      "Epoch 00014: val_loss improved from 0.14709 to 0.14281, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1368 - val_loss: 0.1428\n",
      "Epoch 15/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1359\n",
      "Epoch 00015: val_loss improved from 0.14281 to 0.14081, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1348 - val_loss: 0.1408\n",
      "Epoch 16/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00016: val_loss improved from 0.14081 to 0.13847, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1325 - val_loss: 0.1385\n",
      "Epoch 17/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1298\n",
      "Epoch 00017: val_loss improved from 0.13847 to 0.13701, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1311 - val_loss: 0.1370\n",
      "Epoch 18/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1296\n",
      "Epoch 00018: val_loss improved from 0.13701 to 0.13526, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1297 - val_loss: 0.1353\n",
      "Epoch 19/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00019: val_loss improved from 0.13526 to 0.13508, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1286 - val_loss: 0.1351\n",
      "Epoch 20/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00020: val_loss improved from 0.13508 to 0.13418, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1281 - val_loss: 0.1342\n",
      "Epoch 21/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1277\n",
      "Epoch 00021: val_loss improved from 0.13418 to 0.13329, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1274 - val_loss: 0.1333\n",
      "Epoch 22/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00022: val_loss improved from 0.13329 to 0.13193, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1263 - val_loss: 0.1319\n",
      "Epoch 23/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00023: val_loss improved from 0.13193 to 0.13135, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1257 - val_loss: 0.1313\n",
      "Epoch 24/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00024: val_loss improved from 0.13135 to 0.13107, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1254 - val_loss: 0.1311\n",
      "Epoch 25/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00025: val_loss improved from 0.13107 to 0.13097, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1250 - val_loss: 0.1310\n",
      "Epoch 26/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00026: val_loss improved from 0.13097 to 0.12991, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 0.1299\n",
      "Epoch 27/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00027: val_loss improved from 0.12991 to 0.12944, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 0.1294\n",
      "Epoch 28/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00028: val_loss did not improve from 0.12944\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1296\n",
      "Epoch 29/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1219\n",
      "Epoch 00029: val_loss improved from 0.12944 to 0.12884, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1233 - val_loss: 0.1288\n",
      "Epoch 30/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1225\n",
      "Epoch 00030: val_loss improved from 0.12884 to 0.12872, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1232 - val_loss: 0.1287\n",
      "Epoch 31/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00031: val_loss improved from 0.12872 to 0.12853, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1229 - val_loss: 0.1285\n",
      "Epoch 32/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00032: val_loss did not improve from 0.12853\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1228 - val_loss: 0.1289\n",
      "Epoch 33/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1222\n",
      "Epoch 00033: val_loss did not improve from 0.12853\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1226 - val_loss: 0.1290\n",
      " ###0 fold : val acc1 0.576, acc3 0.963, mae 0.235###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/77 [=======================>......] - ETA: 0s - loss: 15.4660\n",
      "Epoch 00001: val_loss improved from inf to 5.99433, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 14.0777 - val_loss: 5.9943\n",
      "Epoch 2/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 3.5419\n",
      "Epoch 00002: val_loss improved from 5.99433 to 1.28372, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 3.1940 - val_loss: 1.2837\n",
      "Epoch 3/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.8794\n",
      "Epoch 00003: val_loss improved from 1.28372 to 0.56599, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1000 - val_loss: 0.5660\n",
      "Epoch 4/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.5976\n",
      "Epoch 00004: val_loss improved from 0.56599 to 0.32584, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5562 - val_loss: 0.3258\n",
      "Epoch 5/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3712\n",
      "Epoch 00005: val_loss improved from 0.32584 to 0.24491, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3496 - val_loss: 0.2449\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.2622\n",
      "Epoch 00006: val_loss improved from 0.24491 to 0.21245, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2622 - val_loss: 0.2124\n",
      "Epoch 7/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.2225\n",
      "Epoch 00007: val_loss improved from 0.21245 to 0.19307, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2177 - val_loss: 0.1931\n",
      "Epoch 8/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1794\n",
      "Epoch 00008: val_loss improved from 0.19307 to 0.17906, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1908 - val_loss: 0.1791\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1723\n",
      "Epoch 00009: val_loss improved from 0.17906 to 0.16826, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1723 - val_loss: 0.1683\n",
      "Epoch 10/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1567\n",
      "Epoch 00010: val_loss improved from 0.16826 to 0.15902, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1609 - val_loss: 0.1590\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1510\n",
      "Epoch 00011: val_loss improved from 0.15902 to 0.15319, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1510 - val_loss: 0.1532\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1452\n",
      "Epoch 00012: val_loss improved from 0.15319 to 0.14833, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1452 - val_loss: 0.1483\n",
      "Epoch 13/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1398\n",
      "Epoch 00013: val_loss improved from 0.14833 to 0.14475, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1407 - val_loss: 0.1448\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1374\n",
      "Epoch 00014: val_loss improved from 0.14475 to 0.14097, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1374 - val_loss: 0.1410\n",
      "Epoch 15/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.1358\n",
      "Epoch 00015: val_loss improved from 0.14097 to 0.13890, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1353 - val_loss: 0.1389\n",
      "Epoch 16/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1330\n",
      "Epoch 00016: val_loss improved from 0.13890 to 0.13699, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1330 - val_loss: 0.1370\n",
      "Epoch 17/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1316\n",
      "Epoch 00017: val_loss improved from 0.13699 to 0.13571, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1315 - val_loss: 0.1357\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1301\n",
      "Epoch 00018: val_loss improved from 0.13571 to 0.13431, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1301 - val_loss: 0.1343\n",
      "Epoch 19/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00019: val_loss improved from 0.13431 to 0.13364, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1291 - val_loss: 0.1336\n",
      "Epoch 20/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00020: val_loss improved from 0.13364 to 0.13344, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1283 - val_loss: 0.1334\n",
      "Epoch 21/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1296\n",
      "Epoch 00021: val_loss improved from 0.13344 to 0.13224, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1278 - val_loss: 0.1322\n",
      "Epoch 22/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1267\n",
      "Epoch 00022: val_loss improved from 0.13224 to 0.13097, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1268 - val_loss: 0.1310\n",
      "Epoch 23/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.1261\n",
      "Epoch 00023: val_loss improved from 0.13097 to 0.13054, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1260 - val_loss: 0.1305\n",
      "Epoch 24/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.1259\n",
      "Epoch 00024: val_loss improved from 0.13054 to 0.13053, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1258 - val_loss: 0.1305\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00025: val_loss improved from 0.13053 to 0.13009, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1253 - val_loss: 0.1301\n",
      "Epoch 26/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00026: val_loss improved from 0.13009 to 0.12921, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1253 - val_loss: 0.1292\n",
      "Epoch 27/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00027: val_loss improved from 0.12921 to 0.12899, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1245 - val_loss: 0.1290\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00028: val_loss did not improve from 0.12899\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1242 - val_loss: 0.1291\n",
      "Epoch 29/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00029: val_loss improved from 0.12899 to 0.12851, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1239 - val_loss: 0.1285\n",
      "Epoch 30/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1234\n",
      "Epoch 00030: val_loss improved from 0.12851 to 0.12842, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1237 - val_loss: 0.1284\n",
      "Epoch 31/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00031: val_loss did not improve from 0.12842\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 0.1286\n",
      "Epoch 32/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1227\n",
      "Epoch 00032: val_loss improved from 0.12842 to 0.12801, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1232 - val_loss: 0.1280\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1230\n",
      "Epoch 00033: val_loss did not improve from 0.12801\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1230 - val_loss: 0.1282\n",
      "Epoch 34/100\n",
      "58/77 [=====================>........] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00034: val_loss improved from 0.12801 to 0.12778, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1231 - val_loss: 0.1278\n",
      "Epoch 35/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00035: val_loss did not improve from 0.12778\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1228 - val_loss: 0.1279\n",
      "Epoch 36/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1228\n",
      "Epoch 00036: val_loss improved from 0.12778 to 0.12732, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1225 - val_loss: 0.1273\n",
      "Epoch 37/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1228\n",
      "Epoch 00037: val_loss did not improve from 0.12732\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1227 - val_loss: 0.1275\n",
      "Epoch 38/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00038: val_loss improved from 0.12732 to 0.12677, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1229 - val_loss: 0.1268\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1232\n",
      "Epoch 00039: val_loss improved from 0.12677 to 0.12673, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1232 - val_loss: 0.1267\n",
      "Epoch 40/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1233\n",
      "Epoch 00040: val_loss did not improve from 0.12673\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1231 - val_loss: 0.1270\n",
      "Epoch 41/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00041: val_loss did not improve from 0.12673\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1245 - val_loss: 0.1275\n",
      " ###1 fold : val acc1 0.588, acc3 0.963, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/77 [=========================>....] - ETA: 0s - loss: 15.0138\n",
      "Epoch 00001: val_loss improved from inf to 5.98259, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 14.0752 - val_loss: 5.9826\n",
      "Epoch 2/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 3.4365\n",
      "Epoch 00002: val_loss improved from 5.98259 to 1.28672, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.1818 - val_loss: 1.2867\n",
      "Epoch 3/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.8984\n",
      "Epoch 00003: val_loss improved from 1.28672 to 0.56612, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1002 - val_loss: 0.5661\n",
      "Epoch 4/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.5957\n",
      "Epoch 00004: val_loss improved from 0.56612 to 0.32564, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5546 - val_loss: 0.3256\n",
      "Epoch 5/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.3644\n",
      "Epoch 00005: val_loss improved from 0.32564 to 0.24543, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3473 - val_loss: 0.2454\n",
      "Epoch 6/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.2744\n",
      "Epoch 00006: val_loss improved from 0.24543 to 0.21342, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2596 - val_loss: 0.2134\n",
      "Epoch 7/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.2217\n",
      "Epoch 00007: val_loss improved from 0.21342 to 0.19448, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2156 - val_loss: 0.1945\n",
      "Epoch 8/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1770\n",
      "Epoch 00008: val_loss improved from 0.19448 to 0.18071, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1892 - val_loss: 0.1807\n",
      "Epoch 9/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1728\n",
      "Epoch 00009: val_loss improved from 0.18071 to 0.16979, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1711 - val_loss: 0.1698\n",
      "Epoch 10/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1553\n",
      "Epoch 00010: val_loss improved from 0.16979 to 0.16060, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1606 - val_loss: 0.1606\n",
      "Epoch 11/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1502\n",
      "Epoch 00011: val_loss improved from 0.16060 to 0.15459, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1509 - val_loss: 0.1546\n",
      "Epoch 12/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1448\n",
      "Epoch 00012: val_loss improved from 0.15459 to 0.14971, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1451 - val_loss: 0.1497\n",
      "Epoch 13/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1386\n",
      "Epoch 00013: val_loss improved from 0.14971 to 0.14597, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1406 - val_loss: 0.1460\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1374\n",
      "Epoch 00014: val_loss improved from 0.14597 to 0.14187, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1374 - val_loss: 0.1419\n",
      "Epoch 15/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00015: val_loss improved from 0.14187 to 0.13961, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1352 - val_loss: 0.1396\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1329\n",
      "Epoch 00016: val_loss improved from 0.13961 to 0.13734, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1329 - val_loss: 0.1373\n",
      "Epoch 17/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1317\n",
      "Epoch 00017: val_loss improved from 0.13734 to 0.13621, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1316 - val_loss: 0.1362\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1303\n",
      "Epoch 00018: val_loss improved from 0.13621 to 0.13468, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1303 - val_loss: 0.1347\n",
      "Epoch 19/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1282\n",
      "Epoch 00019: val_loss improved from 0.13468 to 0.13393, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1292 - val_loss: 0.1339\n",
      "Epoch 20/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1295\n",
      "Epoch 00020: val_loss improved from 0.13393 to 0.13368, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1286 - val_loss: 0.1337\n",
      "Epoch 21/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1297\n",
      "Epoch 00021: val_loss improved from 0.13368 to 0.13239, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1279 - val_loss: 0.1324\n",
      "Epoch 22/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00022: val_loss improved from 0.13239 to 0.13137, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1271 - val_loss: 0.1314\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00023: val_loss improved from 0.13137 to 0.13082, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1263 - val_loss: 0.1308\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00024: val_loss did not improve from 0.13082\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1260 - val_loss: 0.1308\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00025: val_loss improved from 0.13082 to 0.13015, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1254 - val_loss: 0.1301\n",
      "Epoch 26/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00026: val_loss improved from 0.13015 to 0.12934, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1254 - val_loss: 0.1293\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00027: val_loss improved from 0.12934 to 0.12897, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1247 - val_loss: 0.1290\n",
      "Epoch 28/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1246\n",
      "Epoch 00028: val_loss did not improve from 0.12897\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 0.1292\n",
      "Epoch 29/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00029: val_loss improved from 0.12897 to 0.12885, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1242 - val_loss: 0.1288\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00030: val_loss improved from 0.12885 to 0.12848, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1240 - val_loss: 0.1285\n",
      "Epoch 31/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00031: val_loss did not improve from 0.12848\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1238 - val_loss: 0.1287\n",
      "Epoch 32/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1234\n",
      "Epoch 00032: val_loss improved from 0.12848 to 0.12789, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1233 - val_loss: 0.1279\n",
      "Epoch 33/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00033: val_loss did not improve from 0.12789\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 0.1279\n",
      "Epoch 34/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00034: val_loss did not improve from 0.12789\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 0.1285\n",
      " ###2 fold : val acc1 0.585, acc3 0.963, mae 0.230###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/77 [===========================>..] - ETA: 0s - loss: 14.5622\n",
      "Epoch 00001: val_loss improved from inf to 6.00690, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 14.0756 - val_loss: 6.0069\n",
      "Epoch 2/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.3128\n",
      "Epoch 00002: val_loss improved from 6.00690 to 1.28855, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.1968 - val_loss: 1.2886\n",
      "Epoch 3/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.8492\n",
      "Epoch 00003: val_loss improved from 1.28855 to 0.56741, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0984 - val_loss: 0.5674\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.5543\n",
      "Epoch 00004: val_loss improved from 0.56741 to 0.32675, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5543 - val_loss: 0.3268\n",
      "Epoch 5/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.3578\n",
      "Epoch 00005: val_loss improved from 0.32675 to 0.24637, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3473 - val_loss: 0.2464\n",
      "Epoch 6/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 0.2666\n",
      "Epoch 00006: val_loss improved from 0.24637 to 0.21402, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2606 - val_loss: 0.2140\n",
      "Epoch 7/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.2192\n",
      "Epoch 00007: val_loss improved from 0.21402 to 0.19515, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2171 - val_loss: 0.1952\n",
      "Epoch 8/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.1776\n",
      "Epoch 00008: val_loss improved from 0.19515 to 0.18123, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1910 - val_loss: 0.1812\n",
      "Epoch 9/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1753\n",
      "Epoch 00009: val_loss improved from 0.18123 to 0.17063, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1730 - val_loss: 0.1706\n",
      "Epoch 10/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1566\n",
      "Epoch 00010: val_loss improved from 0.17063 to 0.16114, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1618 - val_loss: 0.1611\n",
      "Epoch 11/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.1533\n",
      "Epoch 00011: val_loss improved from 0.16114 to 0.15493, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1523 - val_loss: 0.1549\n",
      "Epoch 12/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1476\n",
      "Epoch 00012: val_loss improved from 0.15493 to 0.15004, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1465 - val_loss: 0.1500\n",
      "Epoch 13/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1404\n",
      "Epoch 00013: val_loss improved from 0.15004 to 0.14601, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1418 - val_loss: 0.1460\n",
      "Epoch 14/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1417\n",
      "Epoch 00014: val_loss improved from 0.14601 to 0.14178, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1385 - val_loss: 0.1418\n",
      "Epoch 15/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1366\n",
      "Epoch 00015: val_loss improved from 0.14178 to 0.13970, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1365 - val_loss: 0.1397\n",
      "Epoch 16/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1353\n",
      "Epoch 00016: val_loss improved from 0.13970 to 0.13742, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1339 - val_loss: 0.1374\n",
      "Epoch 17/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1320\n",
      "Epoch 00017: val_loss improved from 0.13742 to 0.13631, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1327 - val_loss: 0.1363\n",
      "Epoch 18/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1312\n",
      "Epoch 00018: val_loss improved from 0.13631 to 0.13477, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1313 - val_loss: 0.1348\n",
      "Epoch 19/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00019: val_loss improved from 0.13477 to 0.13371, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1301 - val_loss: 0.1337\n",
      "Epoch 20/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1304\n",
      "Epoch 00020: val_loss improved from 0.13371 to 0.13347, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1296 - val_loss: 0.1335\n",
      "Epoch 21/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1297\n",
      "Epoch 00021: val_loss improved from 0.13347 to 0.13205, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1289 - val_loss: 0.1321\n",
      "Epoch 22/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1274\n",
      "Epoch 00022: val_loss improved from 0.13205 to 0.13137, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1279 - val_loss: 0.1314\n",
      "Epoch 23/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1266\n",
      "Epoch 00023: val_loss improved from 0.13137 to 0.13070, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1271 - val_loss: 0.1307\n",
      "Epoch 24/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1271\n",
      "Epoch 00024: val_loss did not improve from 0.13070\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1267 - val_loss: 0.1311\n",
      "Epoch 25/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00025: val_loss improved from 0.13070 to 0.13023, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1263 - val_loss: 0.1302\n",
      "Epoch 26/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00026: val_loss improved from 0.13023 to 0.12965, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1262 - val_loss: 0.1296\n",
      "Epoch 27/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1259\n",
      "Epoch 00027: val_loss improved from 0.12965 to 0.12906, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1256 - val_loss: 0.1291\n",
      "Epoch 28/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00028: val_loss did not improve from 0.12906\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1252 - val_loss: 0.1292\n",
      "Epoch 29/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00029: val_loss improved from 0.12906 to 0.12875, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1250 - val_loss: 0.1288\n",
      "Epoch 30/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00030: val_loss did not improve from 0.12875\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 0.1288\n",
      "Epoch 31/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00031: val_loss improved from 0.12875 to 0.12852, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1246 - val_loss: 0.1285\n",
      "Epoch 32/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00032: val_loss improved from 0.12852 to 0.12792, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1242 - val_loss: 0.1279\n",
      "Epoch 33/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00033: val_loss did not improve from 0.12792\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1243 - val_loss: 0.1282\n",
      "Epoch 34/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00034: val_loss did not improve from 0.12792\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1243 - val_loss: 0.1284\n",
      " ###3 fold : val acc1 0.575, acc3 0.964, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/77 [=========================>....] - ETA: 0s - loss: 14.9715\n",
      "Epoch 00001: val_loss improved from inf to 6.03597, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 14.0630 - val_loss: 6.0360\n",
      "Epoch 2/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 3.3954\n",
      "Epoch 00002: val_loss improved from 6.03597 to 1.27325, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.2180 - val_loss: 1.2732\n",
      "Epoch 3/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 1.1395\n",
      "Epoch 00003: val_loss improved from 1.27325 to 0.55692, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0891 - val_loss: 0.5569\n",
      "Epoch 4/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6164\n",
      "Epoch 00004: val_loss improved from 0.55692 to 0.32526, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5564 - val_loss: 0.3253\n",
      "Epoch 5/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.3859\n",
      "Epoch 00005: val_loss improved from 0.32526 to 0.24619, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3503 - val_loss: 0.2462\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.2622\n",
      "Epoch 00006: val_loss improved from 0.24619 to 0.21426, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2622 - val_loss: 0.2143\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.2181\n",
      "Epoch 00007: val_loss improved from 0.21426 to 0.19580, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2181 - val_loss: 0.1958\n",
      "Epoch 8/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.2001\n",
      "Epoch 00008: val_loss improved from 0.19580 to 0.18206, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1913 - val_loss: 0.1821\n",
      "Epoch 9/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1679\n",
      "Epoch 00009: val_loss improved from 0.18206 to 0.17119, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1735 - val_loss: 0.1712\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1621\n",
      "Epoch 00010: val_loss improved from 0.17119 to 0.16280, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1614 - val_loss: 0.1628\n",
      "Epoch 11/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1527\n",
      "Epoch 00011: val_loss improved from 0.16280 to 0.15543, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1526 - val_loss: 0.1554\n",
      "Epoch 12/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1471\n",
      "Epoch 00012: val_loss improved from 0.15543 to 0.14965, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1464 - val_loss: 0.1496\n",
      "Epoch 13/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1389\n",
      "Epoch 00013: val_loss improved from 0.14965 to 0.14598, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1416 - val_loss: 0.1460\n",
      "Epoch 14/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1396\n",
      "Epoch 00014: val_loss improved from 0.14598 to 0.14237, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1384 - val_loss: 0.1424\n",
      "Epoch 15/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1356\n",
      "Epoch 00015: val_loss improved from 0.14237 to 0.13985, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1360 - val_loss: 0.1398\n",
      "Epoch 16/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1355\n",
      "Epoch 00016: val_loss improved from 0.13985 to 0.13744, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1340 - val_loss: 0.1374\n",
      "Epoch 17/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1331\n",
      "Epoch 00017: val_loss improved from 0.13744 to 0.13668, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1327 - val_loss: 0.1367\n",
      "Epoch 18/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1295\n",
      "Epoch 00018: val_loss improved from 0.13668 to 0.13490, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1312 - val_loss: 0.1349\n",
      "Epoch 19/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00019: val_loss improved from 0.13490 to 0.13367, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1303 - val_loss: 0.1337\n",
      "Epoch 20/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1293\n",
      "Epoch 00020: val_loss improved from 0.13367 to 0.13289, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1291 - val_loss: 0.1329\n",
      "Epoch 21/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1306\n",
      "Epoch 00021: val_loss improved from 0.13289 to 0.13223, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1287 - val_loss: 0.1322\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1276\n",
      "Epoch 00022: val_loss improved from 0.13223 to 0.13155, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1276 - val_loss: 0.1316\n",
      "Epoch 23/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1273\n",
      "Epoch 00023: val_loss improved from 0.13155 to 0.13090, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1271 - val_loss: 0.1309\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00024: val_loss improved from 0.13090 to 0.13079, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1265 - val_loss: 0.1308\n",
      "Epoch 25/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00025: val_loss improved from 0.13079 to 0.12988, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1261 - val_loss: 0.1299\n",
      "Epoch 26/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00026: val_loss improved from 0.12988 to 0.12965, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1258 - val_loss: 0.1297\n",
      "Epoch 27/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00027: val_loss improved from 0.12965 to 0.12918, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1256 - val_loss: 0.1292\n",
      "Epoch 28/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00028: val_loss improved from 0.12918 to 0.12883, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1252 - val_loss: 0.1288\n",
      "Epoch 29/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00029: val_loss did not improve from 0.12883\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1249 - val_loss: 0.1294\n",
      "Epoch 30/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00030: val_loss improved from 0.12883 to 0.12816, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1246 - val_loss: 0.1282\n",
      "Epoch 31/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00031: val_loss did not improve from 0.12816\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1246 - val_loss: 0.1283\n",
      "Epoch 32/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00032: val_loss improved from 0.12816 to 0.12810, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1241 - val_loss: 0.1281\n",
      "Epoch 33/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00033: val_loss improved from 0.12810 to 0.12782, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1242 - val_loss: 0.1278\n",
      "Epoch 34/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1230\n",
      "Epoch 00034: val_loss improved from 0.12782 to 0.12769, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1236 - val_loss: 0.1277\n",
      "Epoch 35/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1224\n",
      "Epoch 00035: val_loss did not improve from 0.12769\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 0.1280\n",
      "Epoch 36/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00036: val_loss improved from 0.12769 to 0.12723, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1236 - val_loss: 0.1272\n",
      "Epoch 37/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00037: val_loss improved from 0.12723 to 0.12723, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1237 - val_loss: 0.1272\n",
      "Epoch 38/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00038: val_loss improved from 0.12723 to 0.12686, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1236 - val_loss: 0.1269\n",
      "Epoch 39/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00039: val_loss did not improve from 0.12686\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 0.1272\n",
      "Epoch 40/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1213\n",
      "Epoch 00040: val_loss did not improve from 0.12686\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 0.1271\n",
      " ###4 fold : val acc1 0.589, acc3 0.965, mae 0.227###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/77 [==========================>...] - ETA: 0s - loss: 14.4306\n",
      "Epoch 00001: val_loss improved from inf to 5.67252, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 13.8272 - val_loss: 5.6725\n",
      "Epoch 2/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 2.8557\n",
      "Epoch 00002: val_loss improved from 5.67252 to 1.26020, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.7251 - val_loss: 1.2602\n",
      "Epoch 3/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.8391\n",
      "Epoch 00003: val_loss improved from 1.26020 to 0.50660, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8011 - val_loss: 0.5066\n",
      "Epoch 4/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3574\n",
      "Epoch 00004: val_loss improved from 0.50660 to 0.28925, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3570 - val_loss: 0.2893\n",
      "Epoch 5/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.2320\n",
      "Epoch 00005: val_loss improved from 0.28925 to 0.22517, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2317 - val_loss: 0.2252\n",
      "Epoch 6/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.1928\n",
      "Epoch 00006: val_loss improved from 0.22517 to 0.19977, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1931 - val_loss: 0.1998\n",
      "Epoch 7/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.1747\n",
      "Epoch 00007: val_loss improved from 0.19977 to 0.18402, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1756 - val_loss: 0.1840\n",
      "Epoch 8/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.1640\n",
      "Epoch 00008: val_loss improved from 0.18402 to 0.17211, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1639 - val_loss: 0.1721\n",
      "Epoch 9/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.1556\n",
      "Epoch 00009: val_loss improved from 0.17211 to 0.16285, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1552 - val_loss: 0.1629\n",
      "Epoch 10/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1471\n",
      "Epoch 00010: val_loss improved from 0.16285 to 0.15644, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1488 - val_loss: 0.1564\n",
      "Epoch 11/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1434\n",
      "Epoch 00011: val_loss improved from 0.15644 to 0.15138, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1440 - val_loss: 0.1514\n",
      "Epoch 12/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1423\n",
      "Epoch 00012: val_loss improved from 0.15138 to 0.14776, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1405 - val_loss: 0.1478\n",
      "Epoch 13/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1384\n",
      "Epoch 00013: val_loss improved from 0.14776 to 0.14471, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1380 - val_loss: 0.1447\n",
      "Epoch 14/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1365\n",
      "Epoch 00014: val_loss improved from 0.14471 to 0.14264, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1362 - val_loss: 0.1426\n",
      "Epoch 15/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1346\n",
      "Epoch 00015: val_loss improved from 0.14264 to 0.14093, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1343 - val_loss: 0.1409\n",
      "Epoch 16/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1338\n",
      "Epoch 00016: val_loss improved from 0.14093 to 0.13910, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1332 - val_loss: 0.1391\n",
      "Epoch 17/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1328\n",
      "Epoch 00017: val_loss improved from 0.13910 to 0.13778, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1322 - val_loss: 0.1378\n",
      "Epoch 18/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1298\n",
      "Epoch 00018: val_loss improved from 0.13778 to 0.13729, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1313 - val_loss: 0.1373\n",
      "Epoch 19/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00019: val_loss improved from 0.13729 to 0.13557, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1305 - val_loss: 0.1356\n",
      "Epoch 20/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1305\n",
      "Epoch 00020: val_loss improved from 0.13557 to 0.13502, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1296 - val_loss: 0.1350\n",
      "Epoch 21/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1299\n",
      "Epoch 00021: val_loss improved from 0.13502 to 0.13396, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1289 - val_loss: 0.1340\n",
      "Epoch 22/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1289\n",
      "Epoch 00022: val_loss improved from 0.13396 to 0.13387, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1284 - val_loss: 0.1339\n",
      "Epoch 23/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00023: val_loss improved from 0.13387 to 0.13241, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1278 - val_loss: 0.1324\n",
      "Epoch 24/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1266\n",
      "Epoch 00024: val_loss did not improve from 0.13241\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1272 - val_loss: 0.1325\n",
      "Epoch 25/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00025: val_loss improved from 0.13241 to 0.13122, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1267 - val_loss: 0.1312\n",
      "Epoch 26/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1261\n",
      "Epoch 00026: val_loss improved from 0.13122 to 0.13089, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1264 - val_loss: 0.1309\n",
      "Epoch 27/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00027: val_loss improved from 0.13089 to 0.13034, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1261 - val_loss: 0.1303\n",
      "Epoch 28/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00028: val_loss did not improve from 0.13034\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1258 - val_loss: 0.1304\n",
      "Epoch 29/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00029: val_loss improved from 0.13034 to 0.12979, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1254 - val_loss: 0.1298\n",
      "Epoch 30/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00030: val_loss improved from 0.12979 to 0.12915, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1250 - val_loss: 0.1291\n",
      "Epoch 31/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00031: val_loss improved from 0.12915 to 0.12892, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1249 - val_loss: 0.1289\n",
      "Epoch 32/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1261\n",
      "Epoch 00032: val_loss improved from 0.12892 to 0.12864, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 0.1286\n",
      "Epoch 33/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00033: val_loss improved from 0.12864 to 0.12841, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 0.1284\n",
      "Epoch 34/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1232\n",
      "Epoch 00034: val_loss improved from 0.12841 to 0.12796, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 0.1280\n",
      "Epoch 35/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00035: val_loss did not improve from 0.12796\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1239 - val_loss: 0.1282\n",
      "Epoch 36/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00036: val_loss improved from 0.12796 to 0.12789, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1238 - val_loss: 0.1279\n",
      "Epoch 37/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00037: val_loss improved from 0.12789 to 0.12757, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1276\n",
      "Epoch 38/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00038: val_loss improved from 0.12757 to 0.12731, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 0.1273\n",
      "Epoch 39/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00039: val_loss improved from 0.12731 to 0.12691, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1233 - val_loss: 0.1269\n",
      "Epoch 40/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00040: val_loss did not improve from 0.12691\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1231 - val_loss: 0.1270\n",
      "Epoch 41/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00041: val_loss improved from 0.12691 to 0.12673, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1231 - val_loss: 0.1267\n",
      "Epoch 42/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1227\n",
      "Epoch 00042: val_loss did not improve from 0.12673\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1231 - val_loss: 0.1285\n",
      "Epoch 43/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00043: val_loss improved from 0.12673 to 0.12659, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1234 - val_loss: 0.1266\n",
      "Epoch 44/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00044: val_loss improved from 0.12659 to 0.12594, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1226 - val_loss: 0.1259\n",
      "Epoch 45/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00045: val_loss improved from 0.12594 to 0.12592, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1225 - val_loss: 0.1259\n",
      "Epoch 46/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00046: val_loss did not improve from 0.12592\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1224 - val_loss: 0.1263\n",
      "Epoch 47/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00047: val_loss did not improve from 0.12592\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1223 - val_loss: 0.1260\n",
      " ###5 fold : val acc1 0.593, acc3 0.968, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 14.9894\n",
      "Epoch 00001: val_loss improved from inf to 5.80715, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 13.9434 - val_loss: 5.8072\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 2.8885\n",
      "Epoch 00002: val_loss improved from 5.80715 to 1.23732, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.2112 - val_loss: 1.2373\n",
      "Epoch 3/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 1.2368\n",
      "Epoch 00003: val_loss improved from 1.23732 to 0.54275, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1409 - val_loss: 0.5427\n",
      "Epoch 4/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.6572\n",
      "Epoch 00004: val_loss improved from 0.54275 to 0.31962, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6021 - val_loss: 0.3196\n",
      "Epoch 5/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.4206\n",
      "Epoch 00005: val_loss improved from 0.31962 to 0.24252, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.3857 - val_loss: 0.2425\n",
      "Epoch 6/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 0.2930\n",
      "Epoch 00006: val_loss improved from 0.24252 to 0.21129, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2843 - val_loss: 0.2113\n",
      "Epoch 7/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.2345\n",
      "Epoch 00007: val_loss improved from 0.21129 to 0.19430, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2312 - val_loss: 0.1943\n",
      "Epoch 8/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.2037\n",
      "Epoch 00008: val_loss improved from 0.19430 to 0.18010, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1975 - val_loss: 0.1801\n",
      "Epoch 9/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1650\n",
      "Epoch 00009: val_loss improved from 0.18010 to 0.16935, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1764 - val_loss: 0.1693\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1623\n",
      "Epoch 00010: val_loss improved from 0.16935 to 0.16097, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1626 - val_loss: 0.1610\n",
      "Epoch 11/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1456\n",
      "Epoch 00011: val_loss improved from 0.16097 to 0.15424, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1529 - val_loss: 0.1542\n",
      "Epoch 12/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1488\n",
      "Epoch 00012: val_loss improved from 0.15424 to 0.14849, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1460 - val_loss: 0.1485\n",
      "Epoch 13/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1436\n",
      "Epoch 00013: val_loss improved from 0.14849 to 0.14456, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1414 - val_loss: 0.1446\n",
      "Epoch 14/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1395\n",
      "Epoch 00014: val_loss improved from 0.14456 to 0.14134, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1379 - val_loss: 0.1413\n",
      "Epoch 15/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1370\n",
      "Epoch 00015: val_loss improved from 0.14134 to 0.13970, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1354 - val_loss: 0.1397\n",
      "Epoch 16/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00016: val_loss improved from 0.13970 to 0.13708, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1332 - val_loss: 0.1371\n",
      "Epoch 17/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00017: val_loss improved from 0.13708 to 0.13561, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1318 - val_loss: 0.1356\n",
      "Epoch 18/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1292\n",
      "Epoch 00018: val_loss improved from 0.13561 to 0.13483, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1304 - val_loss: 0.1348\n",
      "Epoch 19/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1264\n",
      "Epoch 00019: val_loss improved from 0.13483 to 0.13314, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1293 - val_loss: 0.1331\n",
      "Epoch 20/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1300\n",
      "Epoch 00020: val_loss improved from 0.13314 to 0.13277, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1286 - val_loss: 0.1328\n",
      "Epoch 21/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1291\n",
      "Epoch 00021: val_loss improved from 0.13277 to 0.13175, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1275 - val_loss: 0.1318\n",
      "Epoch 22/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1289\n",
      "Epoch 00022: val_loss improved from 0.13175 to 0.13143, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1272 - val_loss: 0.1314\n",
      "Epoch 23/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00023: val_loss improved from 0.13143 to 0.13061, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1264 - val_loss: 0.1306\n",
      "Epoch 24/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00024: val_loss improved from 0.13061 to 0.13024, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1258 - val_loss: 0.1302\n",
      "Epoch 25/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1226\n",
      "Epoch 00025: val_loss improved from 0.13024 to 0.12994, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1254 - val_loss: 0.1299\n",
      "Epoch 26/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00026: val_loss improved from 0.12994 to 0.12943, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1250 - val_loss: 0.1294\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00027: val_loss did not improve from 0.12943\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1251 - val_loss: 0.1295\n",
      "Epoch 28/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00028: val_loss improved from 0.12943 to 0.12899, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1245 - val_loss: 0.1290\n",
      "Epoch 29/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1251\n",
      "Epoch 00029: val_loss improved from 0.12899 to 0.12881, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1243 - val_loss: 0.1288\n",
      "Epoch 30/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00030: val_loss improved from 0.12881 to 0.12824, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1237 - val_loss: 0.1282\n",
      "Epoch 31/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1230\n",
      "Epoch 00031: val_loss did not improve from 0.12824\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1283\n",
      "Epoch 32/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00032: val_loss improved from 0.12824 to 0.12781, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1232 - val_loss: 0.1278\n",
      "Epoch 33/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00033: val_loss improved from 0.12781 to 0.12765, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1236 - val_loss: 0.1276\n",
      "Epoch 34/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1222\n",
      "Epoch 00034: val_loss improved from 0.12765 to 0.12731, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1231 - val_loss: 0.1273\n",
      "Epoch 35/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00035: val_loss did not improve from 0.12731\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1234 - val_loss: 0.1276\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00036: val_loss did not improve from 0.12731\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1229 - val_loss: 0.1273\n",
      " ###6 fold : val acc1 0.580, acc3 0.962, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/77 [==========================>...] - ETA: 0s - loss: 14.6647\n",
      "Epoch 00001: val_loss improved from inf to 5.82032, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 13.9567 - val_loss: 5.8203\n",
      "Epoch 2/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 2.9253\n",
      "Epoch 00002: val_loss improved from 5.82032 to 1.24060, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.2251 - val_loss: 1.2406\n",
      "Epoch 3/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 1.2260\n",
      "Epoch 00003: val_loss improved from 1.24060 to 0.55069, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1568 - val_loss: 0.5507\n",
      "Epoch 4/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6952\n",
      "Epoch 00004: val_loss improved from 0.55069 to 0.32500, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6216 - val_loss: 0.3250\n",
      "Epoch 5/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.4390\n",
      "Epoch 00005: val_loss improved from 0.32500 to 0.24440, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4017 - val_loss: 0.2444\n",
      "Epoch 6/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.3147\n",
      "Epoch 00006: val_loss improved from 0.24440 to 0.21108, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2961 - val_loss: 0.2111\n",
      "Epoch 7/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1898\n",
      "Epoch 00007: val_loss improved from 0.21108 to 0.19258, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2388 - val_loss: 0.1926\n",
      "Epoch 8/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.2064\n",
      "Epoch 00008: val_loss improved from 0.19258 to 0.17864, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2018 - val_loss: 0.1786\n",
      "Epoch 9/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1833\n",
      "Epoch 00009: val_loss improved from 0.17864 to 0.16820, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1787 - val_loss: 0.1682\n",
      "Epoch 10/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1644\n",
      "Epoch 00010: val_loss improved from 0.16820 to 0.15999, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1637 - val_loss: 0.1600\n",
      "Epoch 11/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1531\n",
      "Epoch 00011: val_loss improved from 0.15999 to 0.15355, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1536 - val_loss: 0.1535\n",
      "Epoch 12/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1482\n",
      "Epoch 00012: val_loss improved from 0.15355 to 0.14786, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1464 - val_loss: 0.1479\n",
      "Epoch 13/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1432\n",
      "Epoch 00013: val_loss improved from 0.14786 to 0.14429, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1415 - val_loss: 0.1443\n",
      "Epoch 14/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1382\n",
      "Epoch 00014: val_loss improved from 0.14429 to 0.14103, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1380 - val_loss: 0.1410\n",
      "Epoch 15/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1362\n",
      "Epoch 00015: val_loss improved from 0.14103 to 0.13930, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1354 - val_loss: 0.1393\n",
      "Epoch 16/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1320\n",
      "Epoch 00016: val_loss improved from 0.13930 to 0.13699, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1334 - val_loss: 0.1370\n",
      "Epoch 17/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1329\n",
      "Epoch 00017: val_loss improved from 0.13699 to 0.13557, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1319 - val_loss: 0.1356\n",
      "Epoch 18/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1298\n",
      "Epoch 00018: val_loss improved from 0.13557 to 0.13511, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1308 - val_loss: 0.1351\n",
      "Epoch 19/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00019: val_loss improved from 0.13511 to 0.13332, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1297 - val_loss: 0.1333\n",
      "Epoch 20/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1302\n",
      "Epoch 00020: val_loss improved from 0.13332 to 0.13269, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1288 - val_loss: 0.1327\n",
      "Epoch 21/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1288\n",
      "Epoch 00021: val_loss improved from 0.13269 to 0.13180, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1280 - val_loss: 0.1318\n",
      "Epoch 22/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1282\n",
      "Epoch 00022: val_loss improved from 0.13180 to 0.13147, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1276 - val_loss: 0.1315\n",
      "Epoch 23/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1264\n",
      "Epoch 00023: val_loss improved from 0.13147 to 0.13063, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1267 - val_loss: 0.1306\n",
      "Epoch 24/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00024: val_loss improved from 0.13063 to 0.13029, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1262 - val_loss: 0.1303\n",
      "Epoch 25/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00025: val_loss improved from 0.13029 to 0.12983, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1258 - val_loss: 0.1298\n",
      "Epoch 26/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00026: val_loss improved from 0.12983 to 0.12974, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1254 - val_loss: 0.1297\n",
      "Epoch 27/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00027: val_loss improved from 0.12974 to 0.12952, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1255 - val_loss: 0.1295\n",
      "Epoch 28/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00028: val_loss improved from 0.12952 to 0.12894, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1248 - val_loss: 0.1289\n",
      "Epoch 29/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00029: val_loss improved from 0.12894 to 0.12890, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1245 - val_loss: 0.1289\n",
      "Epoch 30/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00030: val_loss improved from 0.12890 to 0.12862, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 0.1286\n",
      "Epoch 31/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00031: val_loss did not improve from 0.12862\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1242 - val_loss: 0.1287\n",
      "Epoch 32/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00032: val_loss improved from 0.12862 to 0.12806, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 0.1281\n",
      "Epoch 33/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00033: val_loss improved from 0.12806 to 0.12773, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1238 - val_loss: 0.1277\n",
      "Epoch 34/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1218\n",
      "Epoch 00034: val_loss improved from 0.12773 to 0.12741, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1234 - val_loss: 0.1274\n",
      "Epoch 35/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00035: val_loss did not improve from 0.12741\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1278\n",
      "Epoch 36/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1221\n",
      "Epoch 00036: val_loss improved from 0.12741 to 0.12730, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1233 - val_loss: 0.1273\n",
      "Epoch 37/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00037: val_loss did not improve from 0.12730\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 0.1288\n",
      "Epoch 38/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00038: val_loss did not improve from 0.12730\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1240 - val_loss: 0.1288\n",
      " ###7 fold : val acc1 0.586, acc3 0.964, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 15.0105\n",
      "Epoch 00001: val_loss improved from inf to 5.87894, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 13.9575 - val_loss: 5.8789\n",
      "Epoch 2/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 2.8599\n",
      "Epoch 00002: val_loss improved from 5.87894 to 1.28191, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.2275 - val_loss: 1.2819\n",
      "Epoch 3/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 1.2404\n",
      "Epoch 00003: val_loss improved from 1.28191 to 0.56495, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1536 - val_loss: 0.5650\n",
      "Epoch 4/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.6471\n",
      "Epoch 00004: val_loss improved from 0.56495 to 0.32696, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6182 - val_loss: 0.3270\n",
      "Epoch 5/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 0.4178\n",
      "Epoch 00005: val_loss improved from 0.32696 to 0.23986, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3991 - val_loss: 0.2399\n",
      "Epoch 6/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.3116\n",
      "Epoch 00006: val_loss improved from 0.23986 to 0.20392, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2944 - val_loss: 0.2039\n",
      "Epoch 7/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1893\n",
      "Epoch 00007: val_loss improved from 0.20392 to 0.18539, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.2376 - val_loss: 0.1854\n",
      "Epoch 8/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.2047\n",
      "Epoch 00008: val_loss improved from 0.18539 to 0.17174, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2012 - val_loss: 0.1717\n",
      "Epoch 9/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1641\n",
      "Epoch 00009: val_loss improved from 0.17174 to 0.16250, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1783 - val_loss: 0.1625\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1634\n",
      "Epoch 00010: val_loss improved from 0.16250 to 0.15526, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1634 - val_loss: 0.1553\n",
      "Epoch 11/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1469\n",
      "Epoch 00011: val_loss improved from 0.15526 to 0.14977, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1538 - val_loss: 0.1498\n",
      "Epoch 12/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1475\n",
      "Epoch 00012: val_loss improved from 0.14977 to 0.14534, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1461 - val_loss: 0.1453\n",
      "Epoch 13/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1426\n",
      "Epoch 00013: val_loss improved from 0.14534 to 0.14142, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1411 - val_loss: 0.1414\n",
      "Epoch 14/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1385\n",
      "Epoch 00014: val_loss improved from 0.14142 to 0.13870, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1377 - val_loss: 0.1387\n",
      "Epoch 15/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1356\n",
      "Epoch 00015: val_loss improved from 0.13870 to 0.13769, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1351 - val_loss: 0.1377\n",
      "Epoch 16/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1325\n",
      "Epoch 00016: val_loss improved from 0.13769 to 0.13524, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1330 - val_loss: 0.1352\n",
      "Epoch 17/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00017: val_loss improved from 0.13524 to 0.13388, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1314 - val_loss: 0.1339\n",
      "Epoch 18/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1298\n",
      "Epoch 00018: val_loss improved from 0.13388 to 0.13324, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1302 - val_loss: 0.1332\n",
      "Epoch 19/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1266\n",
      "Epoch 00019: val_loss improved from 0.13324 to 0.13152, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1292 - val_loss: 0.1315\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1284\n",
      "Epoch 00020: val_loss improved from 0.13152 to 0.13114, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1284 - val_loss: 0.1311\n",
      "Epoch 21/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00021: val_loss improved from 0.13114 to 0.13052, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1275 - val_loss: 0.1305\n",
      "Epoch 22/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1284\n",
      "Epoch 00022: val_loss improved from 0.13052 to 0.12989, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1271 - val_loss: 0.1299\n",
      "Epoch 23/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00023: val_loss improved from 0.12989 to 0.12929, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1263 - val_loss: 0.1293\n",
      "Epoch 24/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00024: val_loss improved from 0.12929 to 0.12849, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1257 - val_loss: 0.1285\n",
      "Epoch 25/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1224\n",
      "Epoch 00025: val_loss improved from 0.12849 to 0.12848, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1253 - val_loss: 0.1285\n",
      "Epoch 26/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00026: val_loss did not improve from 0.12848\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1250 - val_loss: 0.1287\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00027: val_loss improved from 0.12848 to 0.12776, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1251 - val_loss: 0.1278\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00028: val_loss improved from 0.12776 to 0.12745, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1245 - val_loss: 0.1274\n",
      "Epoch 29/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00029: val_loss improved from 0.12745 to 0.12697, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1241 - val_loss: 0.1270\n",
      "Epoch 30/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00030: val_loss did not improve from 0.12697\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1273\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00031: val_loss improved from 0.12697 to 0.12669, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1237 - val_loss: 0.1267\n",
      "Epoch 32/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1233\n",
      "Epoch 00032: val_loss improved from 0.12669 to 0.12635, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1232 - val_loss: 0.1264\n",
      "Epoch 33/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1235\n",
      "Epoch 00033: val_loss improved from 0.12635 to 0.12609, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1233 - val_loss: 0.1261\n",
      "Epoch 34/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1213\n",
      "Epoch 00034: val_loss improved from 0.12609 to 0.12585, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1229 - val_loss: 0.1258\n",
      "Epoch 35/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00035: val_loss did not improve from 0.12585\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1231 - val_loss: 0.1265\n",
      "Epoch 36/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.1210\n",
      "Epoch 00036: val_loss improved from 0.12585 to 0.12556, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1227 - val_loss: 0.1256\n",
      "Epoch 37/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00037: val_loss did not improve from 0.12556\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1230 - val_loss: 0.1269\n",
      "Epoch 38/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00038: val_loss did not improve from 0.12556\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1227 - val_loss: 0.1276\n",
      " ###8 fold : val acc1 0.578, acc3 0.960, mae 0.235###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/77 [===========================>..] - ETA: 0s - loss: 14.3478\n",
      "Epoch 00001: val_loss improved from inf to 5.87727, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 13.9575 - val_loss: 5.8773\n",
      "Epoch 2/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 3.2966\n",
      "Epoch 00002: val_loss improved from 5.87727 to 1.28024, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.2275 - val_loss: 1.2802\n",
      "Epoch 3/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.1590\n",
      "Epoch 00003: val_loss improved from 1.28024 to 0.55902, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1536 - val_loss: 0.5590\n",
      "Epoch 4/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.6420\n",
      "Epoch 00004: val_loss improved from 0.55902 to 0.31885, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6182 - val_loss: 0.3189\n",
      "Epoch 5/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.4159\n",
      "Epoch 00005: val_loss improved from 0.31885 to 0.23306, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3991 - val_loss: 0.2331\n",
      "Epoch 6/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.3051\n",
      "Epoch 00006: val_loss improved from 0.23306 to 0.19852, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2944 - val_loss: 0.1985\n",
      "Epoch 7/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.2406\n",
      "Epoch 00007: val_loss improved from 0.19852 to 0.18159, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2376 - val_loss: 0.1816\n",
      "Epoch 8/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.2020\n",
      "Epoch 00008: val_loss improved from 0.18159 to 0.16880, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2012 - val_loss: 0.1688\n",
      "Epoch 9/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.1793\n",
      "Epoch 00009: val_loss improved from 0.16880 to 0.16024, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1783 - val_loss: 0.1602\n",
      "Epoch 10/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.1634\n",
      "Epoch 00010: val_loss improved from 0.16024 to 0.15345, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1634 - val_loss: 0.1535\n",
      "Epoch 11/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 0.1530\n",
      "Epoch 00011: val_loss improved from 0.15345 to 0.14851, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1538 - val_loss: 0.1485\n",
      "Epoch 12/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1480\n",
      "Epoch 00012: val_loss improved from 0.14851 to 0.14397, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1461 - val_loss: 0.1440\n",
      "Epoch 13/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1426\n",
      "Epoch 00013: val_loss improved from 0.14397 to 0.14093, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1411 - val_loss: 0.1409\n",
      "Epoch 14/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1385\n",
      "Epoch 00014: val_loss improved from 0.14093 to 0.13829, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1377 - val_loss: 0.1383\n",
      "Epoch 15/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1355\n",
      "Epoch 00015: val_loss improved from 0.13829 to 0.13696, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1351 - val_loss: 0.1370\n",
      "Epoch 16/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1319\n",
      "Epoch 00016: val_loss improved from 0.13696 to 0.13509, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1330 - val_loss: 0.1351\n",
      "Epoch 17/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1321\n",
      "Epoch 00017: val_loss improved from 0.13509 to 0.13381, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1314 - val_loss: 0.1338\n",
      "Epoch 18/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1289\n",
      "Epoch 00018: val_loss improved from 0.13381 to 0.13343, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1302 - val_loss: 0.1334\n",
      "Epoch 19/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00019: val_loss improved from 0.13343 to 0.13183, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1292 - val_loss: 0.1318\n",
      "Epoch 20/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1299\n",
      "Epoch 00020: val_loss improved from 0.13183 to 0.13126, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1284 - val_loss: 0.1313\n",
      "Epoch 21/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00021: val_loss improved from 0.13126 to 0.13036, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1275 - val_loss: 0.1304\n",
      "Epoch 22/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1280\n",
      "Epoch 00022: val_loss improved from 0.13036 to 0.13024, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1271 - val_loss: 0.1302\n",
      "Epoch 23/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00023: val_loss improved from 0.13024 to 0.12944, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1263 - val_loss: 0.1294\n",
      "Epoch 24/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00024: val_loss improved from 0.12944 to 0.12899, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1257 - val_loss: 0.1290\n",
      "Epoch 25/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00025: val_loss improved from 0.12899 to 0.12874, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 0.1287\n",
      "Epoch 26/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1246\n",
      "Epoch 00026: val_loss improved from 0.12874 to 0.12857, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1250 - val_loss: 0.1286\n",
      "Epoch 27/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00027: val_loss improved from 0.12857 to 0.12839, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1251 - val_loss: 0.1284\n",
      "Epoch 28/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00028: val_loss improved from 0.12839 to 0.12790, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1245 - val_loss: 0.1279\n",
      "Epoch 29/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00029: val_loss improved from 0.12790 to 0.12770, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 0.1277\n",
      "Epoch 30/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1251\n",
      "Epoch 00030: val_loss improved from 0.12770 to 0.12755, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1275\n",
      "Epoch 31/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00031: val_loss did not improve from 0.12755\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1276\n",
      "Epoch 32/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1227\n",
      "Epoch 00032: val_loss improved from 0.12755 to 0.12710, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1232 - val_loss: 0.1271\n",
      "Epoch 33/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.1232\n",
      "Epoch 00033: val_loss improved from 0.12710 to 0.12661, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1233 - val_loss: 0.1266\n",
      "Epoch 34/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1217\n",
      "Epoch 00034: val_loss improved from 0.12661 to 0.12633, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1229 - val_loss: 0.1263\n",
      "Epoch 35/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.1221\n",
      "Epoch 00035: val_loss did not improve from 0.12633\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1231 - val_loss: 0.1268\n",
      "Epoch 36/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1217\n",
      "Epoch 00036: val_loss improved from 0.12633 to 0.12631, saving model to result/size/DNN_size_both_y/batch256,dnodes512_dropout0,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1227 - val_loss: 0.1263\n",
      "Epoch 37/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00037: val_loss did not improve from 0.12631\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1230 - val_loss: 0.1274\n",
      "Epoch 38/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00038: val_loss did not improve from 0.12631\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1227 - val_loss: 0.1276\n",
      " ###9 fold : val acc1 0.583, acc3 0.963, mae 0.231###\n",
      "acc10.583_acc30.964\n",
      "random search 5/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/154 [===========================>..] - ETA: 0s - loss: 11.9084\n",
      "Epoch 00001: val_loss improved from inf to 2.97762, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 11.5580 - val_loss: 2.9776\n",
      "Epoch 2/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.8272\n",
      "Epoch 00002: val_loss improved from 2.97762 to 0.70460, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.7474 - val_loss: 0.7046\n",
      "Epoch 3/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.4540\n",
      "Epoch 00003: val_loss improved from 0.70460 to 0.32775, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.6674 - val_loss: 0.3277\n",
      "Epoch 4/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.3753\n",
      "Epoch 00004: val_loss improved from 0.32775 to 0.22833, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3714 - val_loss: 0.2283\n",
      "Epoch 5/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.2629\n",
      "Epoch 00005: val_loss improved from 0.22833 to 0.19548, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2628 - val_loss: 0.1955\n",
      "Epoch 6/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.2132\n",
      "Epoch 00006: val_loss improved from 0.19548 to 0.17848, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2116 - val_loss: 0.1785\n",
      "Epoch 7/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1858\n",
      "Epoch 00007: val_loss improved from 0.17848 to 0.16643, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1819 - val_loss: 0.1664\n",
      "Epoch 8/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1644\n",
      "Epoch 00008: val_loss improved from 0.16643 to 0.15793, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1620 - val_loss: 0.1579\n",
      "Epoch 9/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1490\n",
      "Epoch 00009: val_loss improved from 0.15793 to 0.14971, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1474 - val_loss: 0.1497\n",
      "Epoch 10/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1415\n",
      "Epoch 00010: val_loss improved from 0.14971 to 0.14436, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1412 - val_loss: 0.1444\n",
      "Epoch 11/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1348\n",
      "Epoch 00011: val_loss improved from 0.14436 to 0.14082, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1346 - val_loss: 0.1408\n",
      "Epoch 12/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1319\n",
      "Epoch 00012: val_loss improved from 0.14082 to 0.13826, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1319 - val_loss: 0.1383\n",
      "Epoch 13/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1291\n",
      "Epoch 00013: val_loss improved from 0.13826 to 0.13618, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1294 - val_loss: 0.1362\n",
      "Epoch 14/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00014: val_loss improved from 0.13618 to 0.13422, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1281 - val_loss: 0.1342\n",
      "Epoch 15/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1270\n",
      "Epoch 00015: val_loss improved from 0.13422 to 0.13290, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1270 - val_loss: 0.1329\n",
      "Epoch 16/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00016: val_loss improved from 0.13290 to 0.13160, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1260 - val_loss: 0.1316\n",
      "Epoch 17/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00017: val_loss improved from 0.13160 to 0.13123, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 0.1312\n",
      "Epoch 18/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1246\n",
      "Epoch 00018: val_loss improved from 0.13123 to 0.13041, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1245 - val_loss: 0.1304\n",
      "Epoch 19/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00019: val_loss did not improve from 0.13041\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1243 - val_loss: 0.1318\n",
      "Epoch 20/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00020: val_loss improved from 0.13041 to 0.12983, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1239 - val_loss: 0.1298\n",
      "Epoch 21/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1235\n",
      "Epoch 00021: val_loss did not improve from 0.12983\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 0.1313\n",
      "Epoch 22/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.1224\n",
      "Epoch 00022: val_loss did not improve from 0.12983\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1230 - val_loss: 0.1301\n",
      " ###0 fold : val acc1 0.575, acc3 0.964, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/154 [============================>.] - ETA: 0s - loss: 11.6896\n",
      "Epoch 00001: val_loss improved from inf to 2.96686, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.5673 - val_loss: 2.9669\n",
      "Epoch 2/100\n",
      "134/154 [=========================>....] - ETA: 0s - loss: 1.9061\n",
      "Epoch 00002: val_loss improved from 2.96686 to 0.70959, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.7607 - val_loss: 0.7096\n",
      "Epoch 3/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.4672\n",
      "Epoch 00003: val_loss improved from 0.70959 to 0.33096, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.6823 - val_loss: 0.3310\n",
      "Epoch 4/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.3935\n",
      "Epoch 00004: val_loss improved from 0.33096 to 0.22928, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3843 - val_loss: 0.2293\n",
      "Epoch 5/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.2822\n",
      "Epoch 00005: val_loss improved from 0.22928 to 0.19429, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2730 - val_loss: 0.1943\n",
      "Epoch 6/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.2194\n",
      "Epoch 00006: val_loss improved from 0.19429 to 0.17621, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2193 - val_loss: 0.1762\n",
      "Epoch 7/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00007: val_loss improved from 0.17621 to 0.16376, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1871 - val_loss: 0.1638\n",
      "Epoch 8/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1649\n",
      "Epoch 00008: val_loss improved from 0.16376 to 0.15518, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1647 - val_loss: 0.1552\n",
      "Epoch 9/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1484\n",
      "Epoch 00009: val_loss improved from 0.15518 to 0.14704, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1485 - val_loss: 0.1470\n",
      "Epoch 10/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1438\n",
      "Epoch 00010: val_loss improved from 0.14704 to 0.14258, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1437 - val_loss: 0.1426\n",
      "Epoch 11/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1362\n",
      "Epoch 00011: val_loss improved from 0.14258 to 0.13963, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1355 - val_loss: 0.1396\n",
      "Epoch 12/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1321\n",
      "Epoch 00012: val_loss improved from 0.13963 to 0.13691, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1327 - val_loss: 0.1369\n",
      "Epoch 13/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1299\n",
      "Epoch 00013: val_loss improved from 0.13691 to 0.13524, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1299 - val_loss: 0.1352\n",
      "Epoch 14/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1294\n",
      "Epoch 00014: val_loss improved from 0.13524 to 0.13362, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1288 - val_loss: 0.1336\n",
      "Epoch 15/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1279\n",
      "Epoch 00015: val_loss improved from 0.13362 to 0.13229, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1278 - val_loss: 0.1323\n",
      "Epoch 16/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1266\n",
      "Epoch 00016: val_loss improved from 0.13229 to 0.13139, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1266 - val_loss: 0.1314\n",
      "Epoch 17/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1259\n",
      "Epoch 00017: val_loss improved from 0.13139 to 0.13077, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1259 - val_loss: 0.1308\n",
      "Epoch 18/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00018: val_loss improved from 0.13077 to 0.13018, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1252 - val_loss: 0.1302\n",
      "Epoch 19/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00019: val_loss did not improve from 0.13018\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1249 - val_loss: 0.1309\n",
      "Epoch 20/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00020: val_loss did not improve from 0.13018\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1246 - val_loss: 0.1304\n",
      " ###1 fold : val acc1 0.577, acc3 0.962, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/154 [==========================>...] - ETA: 0s - loss: 12.3405\n",
      "Epoch 00001: val_loss improved from inf to 2.97901, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.5602 - val_loss: 2.9790\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 1.7675\n",
      "Epoch 00002: val_loss improved from 2.97901 to 0.71033, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.7675 - val_loss: 0.7103\n",
      "Epoch 3/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4660\n",
      "Epoch 00003: val_loss improved from 0.71033 to 0.32990, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.6834 - val_loss: 0.3299\n",
      "Epoch 4/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.3879\n",
      "Epoch 00004: val_loss improved from 0.32990 to 0.22880, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3828 - val_loss: 0.2288\n",
      "Epoch 5/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.2826\n",
      "Epoch 00005: val_loss improved from 0.22880 to 0.19475, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2720 - val_loss: 0.1948\n",
      "Epoch 6/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.2229\n",
      "Epoch 00006: val_loss improved from 0.19475 to 0.17754, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2184 - val_loss: 0.1775\n",
      "Epoch 7/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1892\n",
      "Epoch 00007: val_loss improved from 0.17754 to 0.16482, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1862 - val_loss: 0.1648\n",
      "Epoch 8/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1496\n",
      "Epoch 00008: val_loss improved from 0.16482 to 0.15642, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1650 - val_loss: 0.1564\n",
      "Epoch 9/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1503\n",
      "Epoch 00009: val_loss improved from 0.15642 to 0.14815, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1489 - val_loss: 0.1481\n",
      "Epoch 10/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1367\n",
      "Epoch 00010: val_loss improved from 0.14815 to 0.14347, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1432 - val_loss: 0.1435\n",
      "Epoch 11/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1354\n",
      "Epoch 00011: val_loss improved from 0.14347 to 0.14024, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1354 - val_loss: 0.1402\n",
      "Epoch 12/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1324\n",
      "Epoch 00012: val_loss improved from 0.14024 to 0.13748, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1327 - val_loss: 0.1375\n",
      "Epoch 13/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1305\n",
      "Epoch 00013: val_loss improved from 0.13748 to 0.13566, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1301 - val_loss: 0.1357\n",
      "Epoch 14/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1292\n",
      "Epoch 00014: val_loss improved from 0.13566 to 0.13455, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1288 - val_loss: 0.1345\n",
      "Epoch 15/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00015: val_loss improved from 0.13455 to 0.13244, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1278 - val_loss: 0.1324\n",
      "Epoch 16/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1266\n",
      "Epoch 00016: val_loss improved from 0.13244 to 0.13142, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1266 - val_loss: 0.1314\n",
      "Epoch 17/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1261\n",
      "Epoch 00017: val_loss improved from 0.13142 to 0.13130, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1260 - val_loss: 0.1313\n",
      "Epoch 18/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00018: val_loss improved from 0.13130 to 0.13069, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1254 - val_loss: 0.1307\n",
      "Epoch 19/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00019: val_loss improved from 0.13069 to 0.13037, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1250 - val_loss: 0.1304\n",
      "Epoch 20/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00020: val_loss improved from 0.13037 to 0.13034, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1246 - val_loss: 0.1303\n",
      "Epoch 21/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00021: val_loss did not improve from 0.13034\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1242 - val_loss: 0.1304\n",
      "Epoch 22/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00022: val_loss improved from 0.13034 to 0.12983, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1237 - val_loss: 0.1298\n",
      "Epoch 23/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00023: val_loss improved from 0.12983 to 0.12822, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1236 - val_loss: 0.1282\n",
      "Epoch 24/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00024: val_loss improved from 0.12822 to 0.12820, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1234 - val_loss: 0.1282\n",
      "Epoch 25/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1226\n",
      "Epoch 00025: val_loss improved from 0.12820 to 0.12774, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1228 - val_loss: 0.1277\n",
      "Epoch 26/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1222\n",
      "Epoch 00026: val_loss did not improve from 0.12774\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1232 - val_loss: 0.1298\n",
      "Epoch 27/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1234\n",
      "Epoch 00027: val_loss improved from 0.12774 to 0.12691, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1233 - val_loss: 0.1269\n",
      "Epoch 28/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00028: val_loss did not improve from 0.12691\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1255 - val_loss: 0.1275\n",
      "Epoch 29/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1251\n",
      "Epoch 00029: val_loss did not improve from 0.12691\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1249 - val_loss: 0.1275\n",
      " ###2 fold : val acc1 0.585, acc3 0.963, mae 0.229###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/154 [===========================>..] - ETA: 0s - loss: 11.9167\n",
      "Epoch 00001: val_loss improved from inf to 2.97873, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.5652 - val_loss: 2.9787\n",
      "Epoch 2/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 1.7923\n",
      "Epoch 00002: val_loss improved from 2.97873 to 0.70938, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.7620 - val_loss: 0.7094\n",
      "Epoch 3/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.4715\n",
      "Epoch 00003: val_loss improved from 0.70938 to 0.33052, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6793 - val_loss: 0.3305\n",
      "Epoch 4/100\n",
      "135/154 [=========================>....] - ETA: 0s - loss: 0.4020\n",
      "Epoch 00004: val_loss improved from 0.33052 to 0.22872, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3808 - val_loss: 0.2287\n",
      "Epoch 5/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.2816\n",
      "Epoch 00005: val_loss improved from 0.22872 to 0.19458, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2699 - val_loss: 0.1946\n",
      "Epoch 6/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.2233\n",
      "Epoch 00006: val_loss improved from 0.19458 to 0.17725, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2180 - val_loss: 0.1772\n",
      "Epoch 7/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1887\n",
      "Epoch 00007: val_loss improved from 0.17725 to 0.16473, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1867 - val_loss: 0.1647\n",
      "Epoch 8/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1672\n",
      "Epoch 00008: val_loss improved from 0.16473 to 0.15661, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1649 - val_loss: 0.1566\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1497\n",
      "Epoch 00009: val_loss improved from 0.15661 to 0.14827, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1497 - val_loss: 0.1483\n",
      "Epoch 10/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1444\n",
      "Epoch 00010: val_loss improved from 0.14827 to 0.14345, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1444 - val_loss: 0.1435\n",
      "Epoch 11/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1379\n",
      "Epoch 00011: val_loss improved from 0.14345 to 0.14026, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1366 - val_loss: 0.1403\n",
      "Epoch 12/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1338\n",
      "Epoch 00012: val_loss improved from 0.14026 to 0.13705, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1336 - val_loss: 0.1370\n",
      "Epoch 13/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1311\n",
      "Epoch 00013: val_loss improved from 0.13705 to 0.13556, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1310 - val_loss: 0.1356\n",
      "Epoch 14/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1310\n",
      "Epoch 00014: val_loss improved from 0.13556 to 0.13402, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1297 - val_loss: 0.1340\n",
      "Epoch 15/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1291\n",
      "Epoch 00015: val_loss improved from 0.13402 to 0.13244, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1286 - val_loss: 0.1324\n",
      "Epoch 16/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1276\n",
      "Epoch 00016: val_loss improved from 0.13244 to 0.13153, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1274 - val_loss: 0.1315\n",
      "Epoch 17/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1267\n",
      "Epoch 00017: val_loss improved from 0.13153 to 0.13092, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1269 - val_loss: 0.1309\n",
      "Epoch 18/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00018: val_loss improved from 0.13092 to 0.13069, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1263 - val_loss: 0.1307\n",
      "Epoch 19/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00019: val_loss improved from 0.13069 to 0.13026, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1258 - val_loss: 0.1303\n",
      "Epoch 20/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1269\n",
      "Epoch 00020: val_loss did not improve from 0.13026\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1254 - val_loss: 0.1308\n",
      "Epoch 21/100\n",
      "135/154 [=========================>....] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00021: val_loss improved from 0.13026 to 0.12909, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1251 - val_loss: 0.1291\n",
      "Epoch 22/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00022: val_loss did not improve from 0.12909\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1245 - val_loss: 0.1297\n",
      "Epoch 23/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00023: val_loss improved from 0.12909 to 0.12830, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1244 - val_loss: 0.1283\n",
      "Epoch 24/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00024: val_loss improved from 0.12830 to 0.12797, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1242 - val_loss: 0.1280\n",
      "Epoch 25/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00025: val_loss did not improve from 0.12797\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 0.1282\n",
      "Epoch 26/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00026: val_loss did not improve from 0.12797\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1242 - val_loss: 0.1303\n",
      " ###3 fold : val acc1 0.573, acc3 0.966, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/154 [===========================>..] - ETA: 0s - loss: 11.7680\n",
      "Epoch 00001: val_loss improved from inf to 2.98250, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.4724 - val_loss: 2.9825\n",
      "Epoch 2/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 1.7093\n",
      "Epoch 00002: val_loss improved from 2.98250 to 0.69347, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.6949 - val_loss: 0.6935\n",
      "Epoch 3/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.6225\n",
      "Epoch 00003: val_loss improved from 0.69347 to 0.31815, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.6063 - val_loss: 0.3182\n",
      "Epoch 4/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.3395\n",
      "Epoch 00004: val_loss improved from 0.31815 to 0.22813, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3393 - val_loss: 0.2281\n",
      "Epoch 5/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2469\n",
      "Epoch 00005: val_loss improved from 0.22813 to 0.19806, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2466 - val_loss: 0.1981\n",
      "Epoch 6/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.2024\n",
      "Epoch 00006: val_loss improved from 0.19806 to 0.18024, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2025 - val_loss: 0.1802\n",
      "Epoch 7/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1780\n",
      "Epoch 00007: val_loss improved from 0.18024 to 0.16655, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1776 - val_loss: 0.1666\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1590\n",
      "Epoch 00008: val_loss improved from 0.16655 to 0.15641, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1590 - val_loss: 0.1564\n",
      "Epoch 9/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1494\n",
      "Epoch 00009: val_loss improved from 0.15641 to 0.14963, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1493 - val_loss: 0.1496\n",
      "Epoch 10/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1402\n",
      "Epoch 00010: val_loss improved from 0.14963 to 0.14404, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1408 - val_loss: 0.1440\n",
      "Epoch 11/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1358\n",
      "Epoch 00011: val_loss improved from 0.14404 to 0.13987, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1358 - val_loss: 0.1399\n",
      "Epoch 12/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1337\n",
      "Epoch 00012: val_loss improved from 0.13987 to 0.13689, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1331 - val_loss: 0.1369\n",
      "Epoch 13/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1311\n",
      "Epoch 00013: val_loss improved from 0.13689 to 0.13495, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1307 - val_loss: 0.1349\n",
      "Epoch 14/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1297\n",
      "Epoch 00014: val_loss improved from 0.13495 to 0.13409, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1293 - val_loss: 0.1341\n",
      "Epoch 15/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1283\n",
      "Epoch 00015: val_loss improved from 0.13409 to 0.13253, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1282 - val_loss: 0.1325\n",
      "Epoch 16/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1275\n",
      "Epoch 00016: val_loss improved from 0.13253 to 0.13209, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1272 - val_loss: 0.1321\n",
      "Epoch 17/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1268\n",
      "Epoch 00017: val_loss improved from 0.13209 to 0.13133, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1268 - val_loss: 0.1313\n",
      "Epoch 18/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00018: val_loss improved from 0.13133 to 0.13030, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1262 - val_loss: 0.1303\n",
      "Epoch 19/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00019: val_loss improved from 0.13030 to 0.12963, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1258 - val_loss: 0.1296\n",
      "Epoch 20/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00020: val_loss improved from 0.12963 to 0.12944, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1250 - val_loss: 0.1294\n",
      "Epoch 21/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00021: val_loss improved from 0.12944 to 0.12832, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1247 - val_loss: 0.1283\n",
      "Epoch 22/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00022: val_loss did not improve from 0.12832\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1243 - val_loss: 0.1299\n",
      "Epoch 23/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00023: val_loss improved from 0.12832 to 0.12829, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1241 - val_loss: 0.1283\n",
      "Epoch 24/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00024: val_loss improved from 0.12829 to 0.12751, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1237 - val_loss: 0.1275\n",
      "Epoch 25/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1235\n",
      "Epoch 00025: val_loss did not improve from 0.12751\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1233 - val_loss: 0.1277\n",
      "Epoch 26/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00026: val_loss did not improve from 0.12751\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1236 - val_loss: 0.1279\n",
      " ###4 fold : val acc1 0.592, acc3 0.965, mae 0.225###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/154 [===========================>..] - ETA: 0s - loss: 11.6895\n",
      "Epoch 00001: val_loss improved from inf to 2.74482, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.1650 - val_loss: 2.7448\n",
      "Epoch 2/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 1.3445\n",
      "Epoch 00002: val_loss improved from 2.74482 to 0.63452, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.3435 - val_loss: 0.6345\n",
      "Epoch 3/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.4078\n",
      "Epoch 00003: val_loss improved from 0.63452 to 0.28355, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3932 - val_loss: 0.2836\n",
      "Epoch 4/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2179\n",
      "Epoch 00004: val_loss improved from 0.28355 to 0.21049, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2175 - val_loss: 0.2105\n",
      "Epoch 5/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1769\n",
      "Epoch 00005: val_loss improved from 0.21049 to 0.18465, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1769 - val_loss: 0.1846\n",
      "Epoch 6/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1606\n",
      "Epoch 00006: val_loss improved from 0.18465 to 0.16985, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1607 - val_loss: 0.1699\n",
      "Epoch 7/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1501\n",
      "Epoch 00007: val_loss improved from 0.16985 to 0.15915, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1507 - val_loss: 0.1592\n",
      "Epoch 8/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1435\n",
      "Epoch 00008: val_loss improved from 0.15915 to 0.15214, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1439 - val_loss: 0.1521\n",
      "Epoch 9/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1404\n",
      "Epoch 00009: val_loss improved from 0.15214 to 0.14718, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1395 - val_loss: 0.1472\n",
      "Epoch 10/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1367\n",
      "Epoch 00010: val_loss improved from 0.14718 to 0.14344, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1366 - val_loss: 0.1434\n",
      "Epoch 11/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1344\n",
      "Epoch 00011: val_loss improved from 0.14344 to 0.14085, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1344 - val_loss: 0.1409\n",
      "Epoch 12/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1324\n",
      "Epoch 00012: val_loss improved from 0.14085 to 0.13876, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1326 - val_loss: 0.1388\n",
      "Epoch 13/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1313\n",
      "Epoch 00013: val_loss improved from 0.13876 to 0.13669, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1312 - val_loss: 0.1367\n",
      "Epoch 14/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1302\n",
      "Epoch 00014: val_loss improved from 0.13669 to 0.13639, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1302 - val_loss: 0.1364\n",
      "Epoch 15/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1286\n",
      "Epoch 00015: val_loss improved from 0.13639 to 0.13424, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1290 - val_loss: 0.1342\n",
      "Epoch 16/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1286\n",
      "Epoch 00016: val_loss improved from 0.13424 to 0.13377, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1280 - val_loss: 0.1338\n",
      "Epoch 17/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1275\n",
      "Epoch 00017: val_loss improved from 0.13377 to 0.13209, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1275 - val_loss: 0.1321\n",
      "Epoch 18/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1270\n",
      "Epoch 00018: val_loss improved from 0.13209 to 0.13119, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1267 - val_loss: 0.1312\n",
      "Epoch 19/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00019: val_loss improved from 0.13119 to 0.13043, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1263 - val_loss: 0.1304\n",
      "Epoch 20/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00020: val_loss did not improve from 0.13043\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1255 - val_loss: 0.1310\n",
      "Epoch 21/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00021: val_loss improved from 0.13043 to 0.12898, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1251 - val_loss: 0.1290\n",
      "Epoch 22/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00022: val_loss did not improve from 0.12898\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1248 - val_loss: 0.1309\n",
      "Epoch 23/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00023: val_loss improved from 0.12898 to 0.12861, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1246 - val_loss: 0.1286\n",
      "Epoch 24/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00024: val_loss improved from 0.12861 to 0.12778, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1240 - val_loss: 0.1278\n",
      "Epoch 25/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00025: val_loss did not improve from 0.12778\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1238 - val_loss: 0.1280\n",
      "Epoch 26/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00026: val_loss did not improve from 0.12778\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1239 - val_loss: 0.1284\n",
      " ###5 fold : val acc1 0.589, acc3 0.967, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/154 [============================>.] - ETA: 0s - loss: 11.4712\n",
      "Epoch 00001: val_loss improved from inf to 2.76609, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 11.2917 - val_loss: 2.7661\n",
      "Epoch 2/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 1.4003\n",
      "Epoch 00002: val_loss improved from 2.76609 to 0.63887, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.5863 - val_loss: 0.6389\n",
      "Epoch 3/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.5693\n",
      "Epoch 00003: val_loss improved from 0.63887 to 0.30019, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5523 - val_loss: 0.3002\n",
      "Epoch 4/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.3184\n",
      "Epoch 00004: val_loss improved from 0.30019 to 0.22176, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3127 - val_loss: 0.2218\n",
      "Epoch 5/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.2362\n",
      "Epoch 00005: val_loss improved from 0.22176 to 0.19284, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2319 - val_loss: 0.1928\n",
      "Epoch 6/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1918\n",
      "Epoch 00006: val_loss improved from 0.19284 to 0.17590, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1913 - val_loss: 0.1759\n",
      "Epoch 7/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1741\n",
      "Epoch 00007: val_loss improved from 0.17590 to 0.16446, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1736 - val_loss: 0.1645\n",
      "Epoch 8/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1549\n",
      "Epoch 00008: val_loss improved from 0.16446 to 0.15440, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1536 - val_loss: 0.1544\n",
      "Epoch 9/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1462\n",
      "Epoch 00009: val_loss improved from 0.15440 to 0.14810, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1453 - val_loss: 0.1481\n",
      "Epoch 10/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1375\n",
      "Epoch 00010: val_loss improved from 0.14810 to 0.14234, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1382 - val_loss: 0.1423\n",
      "Epoch 11/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1332\n",
      "Epoch 00011: val_loss improved from 0.14234 to 0.13937, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1346 - val_loss: 0.1394\n",
      "Epoch 12/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1320\n",
      "Epoch 00012: val_loss improved from 0.13937 to 0.13598, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1316 - val_loss: 0.1360\n",
      "Epoch 13/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1312\n",
      "Epoch 00013: val_loss improved from 0.13598 to 0.13457, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1304 - val_loss: 0.1346\n",
      "Epoch 14/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00014: val_loss improved from 0.13457 to 0.13331, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1287 - val_loss: 0.1333\n",
      "Epoch 15/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1278\n",
      "Epoch 00015: val_loss improved from 0.13331 to 0.13286, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1277 - val_loss: 0.1329\n",
      "Epoch 16/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1271\n",
      "Epoch 00016: val_loss improved from 0.13286 to 0.13190, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1268 - val_loss: 0.1319\n",
      "Epoch 17/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00017: val_loss improved from 0.13190 to 0.13102, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1263 - val_loss: 0.1310\n",
      "Epoch 18/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00018: val_loss improved from 0.13102 to 0.13029, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1255 - val_loss: 0.1303\n",
      "Epoch 19/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00019: val_loss improved from 0.13029 to 0.12952, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 0.1295\n",
      "Epoch 20/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00020: val_loss did not improve from 0.12952\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1245 - val_loss: 0.1298\n",
      "Epoch 21/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00021: val_loss improved from 0.12952 to 0.12834, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1242 - val_loss: 0.1283\n",
      "Epoch 22/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00022: val_loss did not improve from 0.12834\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1238 - val_loss: 0.1306\n",
      "Epoch 23/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00023: val_loss did not improve from 0.12834\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1287\n",
      " ###6 fold : val acc1 0.577, acc3 0.962, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/154 [============================>.] - ETA: 0s - loss: 11.4255\n",
      "Epoch 00001: val_loss improved from inf to 2.75367, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.3072 - val_loss: 2.7537\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 1.5957\n",
      "Epoch 00002: val_loss improved from 2.75367 to 0.64698, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.5957 - val_loss: 0.6470\n",
      "Epoch 3/100\n",
      "134/154 [=========================>....] - ETA: 0s - loss: 0.6108\n",
      "Epoch 00003: val_loss improved from 0.64698 to 0.30390, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5711 - val_loss: 0.3039\n",
      "Epoch 4/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3305\n",
      "Epoch 00004: val_loss improved from 0.30390 to 0.22215, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3263 - val_loss: 0.2222\n",
      "Epoch 5/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.2449\n",
      "Epoch 00005: val_loss improved from 0.22215 to 0.19173, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2402 - val_loss: 0.1917\n",
      "Epoch 6/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1963\n",
      "Epoch 00006: val_loss improved from 0.19173 to 0.17444, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1963 - val_loss: 0.1744\n",
      "Epoch 7/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.1769\n",
      "Epoch 00007: val_loss improved from 0.17444 to 0.16280, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1766 - val_loss: 0.1628\n",
      "Epoch 8/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1546\n",
      "Epoch 00008: val_loss improved from 0.16280 to 0.15335, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1547 - val_loss: 0.1533\n",
      "Epoch 9/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1475\n",
      "Epoch 00009: val_loss improved from 0.15335 to 0.14783, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1462 - val_loss: 0.1478\n",
      "Epoch 10/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1380\n",
      "Epoch 00010: val_loss improved from 0.14783 to 0.14239, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1384 - val_loss: 0.1424\n",
      "Epoch 11/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1339\n",
      "Epoch 00011: val_loss improved from 0.14239 to 0.13937, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1348 - val_loss: 0.1394\n",
      "Epoch 12/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1319\n",
      "Epoch 00012: val_loss improved from 0.13937 to 0.13582, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1318 - val_loss: 0.1358\n",
      "Epoch 13/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1317\n",
      "Epoch 00013: val_loss improved from 0.13582 to 0.13453, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1305 - val_loss: 0.1345\n",
      "Epoch 14/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1290\n",
      "Epoch 00014: val_loss improved from 0.13453 to 0.13335, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1288 - val_loss: 0.1333\n",
      "Epoch 15/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1280\n",
      "Epoch 00015: val_loss improved from 0.13335 to 0.13280, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1280 - val_loss: 0.1328\n",
      "Epoch 16/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00016: val_loss improved from 0.13280 to 0.13169, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1270 - val_loss: 0.1317\n",
      "Epoch 17/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00017: val_loss improved from 0.13169 to 0.13066, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1265 - val_loss: 0.1307\n",
      "Epoch 18/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00018: val_loss improved from 0.13066 to 0.13056, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1257 - val_loss: 0.1306\n",
      "Epoch 19/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00019: val_loss improved from 0.13056 to 0.12969, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1255 - val_loss: 0.1297\n",
      "Epoch 20/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00020: val_loss did not improve from 0.12969\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1249 - val_loss: 0.1301\n",
      "Epoch 21/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00021: val_loss improved from 0.12969 to 0.12841, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1245 - val_loss: 0.1284\n",
      "Epoch 22/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00022: val_loss did not improve from 0.12841\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1245 - val_loss: 0.1300\n",
      "Epoch 23/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00023: val_loss improved from 0.12841 to 0.12837, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1241 - val_loss: 0.1284\n",
      "Epoch 24/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00024: val_loss did not improve from 0.12837\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1247 - val_loss: 0.1291\n",
      "Epoch 25/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1251\n",
      "Epoch 00025: val_loss improved from 0.12837 to 0.12778, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1262 - val_loss: 0.1278\n",
      "Epoch 26/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00026: val_loss did not improve from 0.12778\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1258 - val_loss: 0.1302\n",
      "Epoch 27/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00027: val_loss improved from 0.12778 to 0.12715, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1248 - val_loss: 0.1272\n",
      "Epoch 28/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00028: val_loss did not improve from 0.12715\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 0.1280\n",
      "Epoch 29/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00029: val_loss improved from 0.12715 to 0.12691, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1254 - val_loss: 0.1269\n",
      "Epoch 30/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00030: val_loss did not improve from 0.12691\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1251 - val_loss: 0.1272\n",
      "Epoch 31/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00031: val_loss did not improve from 0.12691\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 0.1273\n",
      " ###7 fold : val acc1 0.585, acc3 0.964, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/154 [==========================>...] - ETA: 0s - loss: 12.0341\n",
      "Epoch 00001: val_loss improved from inf to 2.79849, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.3182 - val_loss: 2.7985\n",
      "Epoch 2/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 1.3825\n",
      "Epoch 00002: val_loss improved from 2.79849 to 0.65950, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.5927 - val_loss: 0.6595\n",
      "Epoch 3/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.5743\n",
      "Epoch 00003: val_loss improved from 0.65950 to 0.30077, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5669 - val_loss: 0.3008\n",
      "Epoch 4/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.3241\n",
      "Epoch 00004: val_loss improved from 0.30077 to 0.21380, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3234 - val_loss: 0.2138\n",
      "Epoch 5/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.2455\n",
      "Epoch 00005: val_loss improved from 0.21380 to 0.18310, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2381 - val_loss: 0.1831\n",
      "Epoch 6/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1973\n",
      "Epoch 00006: val_loss improved from 0.18310 to 0.16694, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1950 - val_loss: 0.1669\n",
      "Epoch 7/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1756\n",
      "Epoch 00007: val_loss improved from 0.16694 to 0.15661, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1756 - val_loss: 0.1566\n",
      "Epoch 8/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1547\n",
      "Epoch 00008: val_loss improved from 0.15661 to 0.14852, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1545 - val_loss: 0.1485\n",
      "Epoch 9/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1467\n",
      "Epoch 00009: val_loss improved from 0.14852 to 0.14369, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1456 - val_loss: 0.1437\n",
      "Epoch 10/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1383\n",
      "Epoch 00010: val_loss improved from 0.14369 to 0.13947, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1382 - val_loss: 0.1395\n",
      "Epoch 11/100\n",
      "135/154 [=========================>....] - ETA: 0s - loss: 0.1342\n",
      "Epoch 00011: val_loss improved from 0.13947 to 0.13632, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1345 - val_loss: 0.1363\n",
      "Epoch 12/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1318\n",
      "Epoch 00012: val_loss improved from 0.13632 to 0.13391, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1315 - val_loss: 0.1339\n",
      "Epoch 13/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1301\n",
      "Epoch 00013: val_loss improved from 0.13391 to 0.13236, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1301 - val_loss: 0.1324\n",
      "Epoch 14/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00014: val_loss improved from 0.13236 to 0.13155, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1284 - val_loss: 0.1316\n",
      "Epoch 15/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1274\n",
      "Epoch 00015: val_loss improved from 0.13155 to 0.13086, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1276 - val_loss: 0.1309\n",
      "Epoch 16/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1268\n",
      "Epoch 00016: val_loss improved from 0.13086 to 0.12939, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1266 - val_loss: 0.1294\n",
      "Epoch 17/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00017: val_loss improved from 0.12939 to 0.12864, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1261 - val_loss: 0.1286\n",
      "Epoch 18/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00018: val_loss improved from 0.12864 to 0.12851, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1253 - val_loss: 0.1285\n",
      "Epoch 19/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00019: val_loss improved from 0.12851 to 0.12772, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1250 - val_loss: 0.1277\n",
      "Epoch 20/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00020: val_loss did not improve from 0.12772\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1245 - val_loss: 0.1279\n",
      "Epoch 21/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00021: val_loss improved from 0.12772 to 0.12720, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1241 - val_loss: 0.1272\n",
      "Epoch 22/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00022: val_loss did not improve from 0.12720\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1242 - val_loss: 0.1274\n",
      "Epoch 23/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00023: val_loss improved from 0.12720 to 0.12666, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1237 - val_loss: 0.1267\n",
      "Epoch 24/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00024: val_loss improved from 0.12666 to 0.12656, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1238 - val_loss: 0.1266\n",
      "Epoch 25/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00025: val_loss improved from 0.12656 to 0.12565, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1249 - val_loss: 0.1257\n",
      "Epoch 26/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00026: val_loss did not improve from 0.12565\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 0.1277\n",
      "Epoch 27/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1246\n",
      "Epoch 00027: val_loss did not improve from 0.12565\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1246 - val_loss: 0.1262\n",
      " ###8 fold : val acc1 0.580, acc3 0.961, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/154 [==========================>...] - ETA: 0s - loss: 12.0341\n",
      "Epoch 00001: val_loss improved from inf to 2.80512, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 11.3182 - val_loss: 2.8051\n",
      "Epoch 2/100\n",
      "135/154 [=========================>....] - ETA: 0s - loss: 1.3933\n",
      "Epoch 00002: val_loss improved from 2.80512 to 0.65927, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.5927 - val_loss: 0.6593\n",
      "Epoch 3/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.5762\n",
      "Epoch 00003: val_loss improved from 0.65927 to 0.29438, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5669 - val_loss: 0.2944\n",
      "Epoch 4/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.3272\n",
      "Epoch 00004: val_loss improved from 0.29438 to 0.20898, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3234 - val_loss: 0.2090\n",
      "Epoch 5/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.2381\n",
      "Epoch 00005: val_loss improved from 0.20898 to 0.18004, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2381 - val_loss: 0.1800\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1965\n",
      "Epoch 00006: val_loss improved from 0.18004 to 0.16514, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1950 - val_loss: 0.1651\n",
      "Epoch 7/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1758\n",
      "Epoch 00007: val_loss improved from 0.16514 to 0.15583, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1756 - val_loss: 0.1558\n",
      "Epoch 8/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1549\n",
      "Epoch 00008: val_loss improved from 0.15583 to 0.14825, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1545 - val_loss: 0.1483\n",
      "Epoch 9/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1457\n",
      "Epoch 00009: val_loss improved from 0.14825 to 0.14400, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1456 - val_loss: 0.1440\n",
      "Epoch 10/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1377\n",
      "Epoch 00010: val_loss improved from 0.14400 to 0.13964, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1382 - val_loss: 0.1396\n",
      "Epoch 11/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1334\n",
      "Epoch 00011: val_loss improved from 0.13964 to 0.13690, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1345 - val_loss: 0.1369\n",
      "Epoch 12/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1315\n",
      "Epoch 00012: val_loss improved from 0.13690 to 0.13418, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1315 - val_loss: 0.1342\n",
      "Epoch 13/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1309\n",
      "Epoch 00013: val_loss improved from 0.13418 to 0.13297, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1301 - val_loss: 0.1330\n",
      "Epoch 14/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00014: val_loss improved from 0.13297 to 0.13198, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1284 - val_loss: 0.1320\n",
      "Epoch 15/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1277\n",
      "Epoch 00015: val_loss improved from 0.13198 to 0.13134, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1276 - val_loss: 0.1313\n",
      "Epoch 16/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00016: val_loss improved from 0.13134 to 0.13051, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1266 - val_loss: 0.1305\n",
      "Epoch 17/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00017: val_loss improved from 0.13051 to 0.12945, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1261 - val_loss: 0.1295\n",
      "Epoch 18/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00018: val_loss improved from 0.12945 to 0.12941, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 0.1294\n",
      "Epoch 19/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00019: val_loss improved from 0.12941 to 0.12868, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1250 - val_loss: 0.1287\n",
      "Epoch 20/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00020: val_loss did not improve from 0.12868\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1245 - val_loss: 0.1293\n",
      "Epoch 21/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00021: val_loss improved from 0.12868 to 0.12759, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 0.1276\n",
      "Epoch 22/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00022: val_loss did not improve from 0.12759\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1242 - val_loss: 0.1286\n",
      "Epoch 23/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00023: val_loss improved from 0.12759 to 0.12732, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1273\n",
      "Epoch 24/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00024: val_loss did not improve from 0.12732\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1238 - val_loss: 0.1276\n",
      "Epoch 25/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00025: val_loss improved from 0.12732 to 0.12659, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1249 - val_loss: 0.1266\n",
      "Epoch 26/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00026: val_loss did not improve from 0.12659\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 0.1286\n",
      "Epoch 27/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00027: val_loss improved from 0.12659 to 0.12644, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1246 - val_loss: 0.1264\n",
      "Epoch 28/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1271\n",
      "Epoch 00028: val_loss did not improve from 0.12644\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1261 - val_loss: 0.1274\n",
      "Epoch 29/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1275\n",
      "Epoch 00029: val_loss improved from 0.12644 to 0.12613, saving model to result/size/DNN_size_both_y/batch128,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1275 - val_loss: 0.1261\n",
      "Epoch 30/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00030: val_loss did not improve from 0.12613\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1273 - val_loss: 0.1267\n",
      "Epoch 31/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00031: val_loss did not improve from 0.12613\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1257 - val_loss: 0.1265\n",
      " ###9 fold : val acc1 0.581, acc3 0.964, mae 0.231###\n",
      "acc10.582_acc30.964\n",
      "random search 6/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/154 [==========================>...] - ETA: 0s - loss: 14.5634\n",
      "Epoch 00001: val_loss improved from inf to 5.77980, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 13.8157 - val_loss: 5.7798\n",
      "Epoch 2/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 3.6397\n",
      "Epoch 00002: val_loss improved from 5.77980 to 1.26724, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 3.5429 - val_loss: 1.2672\n",
      "Epoch 3/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 1.1079\n",
      "Epoch 00003: val_loss improved from 1.26724 to 0.59965, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.5213 - val_loss: 0.5996\n",
      "Epoch 4/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 1.0746\n",
      "Epoch 00004: val_loss improved from 0.59965 to 0.36768, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.0715 - val_loss: 0.3677\n",
      "Epoch 5/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.6434\n",
      "Epoch 00005: val_loss improved from 0.36768 to 0.27471, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6393 - val_loss: 0.2747\n",
      "Epoch 6/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.7594\n",
      "Epoch 00006: val_loss improved from 0.27471 to 0.23320, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7452 - val_loss: 0.2332\n",
      "Epoch 7/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.5729\n",
      "Epoch 00007: val_loss improved from 0.23320 to 0.21021, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5697 - val_loss: 0.2102\n",
      "Epoch 8/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.6754\n",
      "Epoch 00008: val_loss improved from 0.21021 to 0.20184, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6648 - val_loss: 0.2018\n",
      "Epoch 9/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.4249\n",
      "Epoch 00009: val_loss improved from 0.20184 to 0.18620, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4239 - val_loss: 0.1862\n",
      "Epoch 10/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.4072\n",
      "Epoch 00010: val_loss improved from 0.18620 to 0.17942, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5654 - val_loss: 0.1794\n",
      "Epoch 11/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.4471\n",
      "Epoch 00011: val_loss improved from 0.17942 to 0.17255, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4435 - val_loss: 0.1725\n",
      "Epoch 12/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.4667\n",
      "Epoch 00012: val_loss improved from 0.17255 to 0.16666, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4649 - val_loss: 0.1667\n",
      "Epoch 13/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4278\n",
      "Epoch 00013: val_loss improved from 0.16666 to 0.16347, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4257 - val_loss: 0.1635\n",
      "Epoch 14/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.3778\n",
      "Epoch 00014: val_loss improved from 0.16347 to 0.15888, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3759 - val_loss: 0.1589\n",
      "Epoch 15/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.4103\n",
      "Epoch 00015: val_loss improved from 0.15888 to 0.15411, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4096 - val_loss: 0.1541\n",
      "Epoch 16/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.4109\n",
      "Epoch 00016: val_loss improved from 0.15411 to 0.15190, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4092 - val_loss: 0.1519\n",
      "Epoch 17/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.3540\n",
      "Epoch 00017: val_loss improved from 0.15190 to 0.15060, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3527 - val_loss: 0.1506\n",
      "Epoch 18/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.3487\n",
      "Epoch 00018: val_loss improved from 0.15060 to 0.14639, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3472 - val_loss: 0.1464\n",
      "Epoch 19/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.3336\n",
      "Epoch 00019: val_loss did not improve from 0.14639\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3332 - val_loss: 0.1480\n",
      "Epoch 20/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.3357\n",
      "Epoch 00020: val_loss improved from 0.14639 to 0.14428, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3343 - val_loss: 0.1443\n",
      "Epoch 21/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3192\n",
      "Epoch 00021: val_loss did not improve from 0.14428\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3192 - val_loss: 0.1449\n",
      "Epoch 22/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.3278\n",
      "Epoch 00022: val_loss improved from 0.14428 to 0.14082, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3278 - val_loss: 0.1408\n",
      "Epoch 23/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.3441\n",
      "Epoch 00023: val_loss improved from 0.14082 to 0.13930, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3430 - val_loss: 0.1393\n",
      "Epoch 24/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.3125\n",
      "Epoch 00024: val_loss improved from 0.13930 to 0.13793, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3125 - val_loss: 0.1379\n",
      "Epoch 25/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3088\n",
      "Epoch 00025: val_loss improved from 0.13793 to 0.13727, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3082 - val_loss: 0.1373\n",
      "Epoch 26/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.3042\n",
      "Epoch 00026: val_loss improved from 0.13727 to 0.13618, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3056 - val_loss: 0.1362\n",
      "Epoch 27/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.2943\n",
      "Epoch 00027: val_loss did not improve from 0.13618\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2946 - val_loss: 0.1366\n",
      "Epoch 28/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.3452\n",
      "Epoch 00028: val_loss improved from 0.13618 to 0.13541, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3440 - val_loss: 0.1354\n",
      "Epoch 29/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.2940\n",
      "Epoch 00029: val_loss improved from 0.13541 to 0.13336, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2949 - val_loss: 0.1334\n",
      "Epoch 30/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.2834\n",
      "Epoch 00030: val_loss improved from 0.13336 to 0.13311, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2842 - val_loss: 0.1331\n",
      "Epoch 31/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.2826\n",
      "Epoch 00031: val_loss did not improve from 0.13311\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2831 - val_loss: 0.1336\n",
      "Epoch 32/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.2866\n",
      "Epoch 00032: val_loss improved from 0.13311 to 0.13278, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2885 - val_loss: 0.1328\n",
      "Epoch 33/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.2809\n",
      "Epoch 00033: val_loss did not improve from 0.13278\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2808 - val_loss: 0.1339\n",
      "Epoch 34/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.2845\n",
      "Epoch 00034: val_loss did not improve from 0.13278\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2839 - val_loss: 0.1331\n",
      " ###0 fold : val acc1 0.586, acc3 0.960, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/154 [=========================>....] - ETA: 0s - loss: 14.7270\n",
      "Epoch 00001: val_loss improved from inf to 5.76695, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 13.8072 - val_loss: 5.7669\n",
      "Epoch 2/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 3.5805\n",
      "Epoch 00002: val_loss improved from 5.76695 to 1.25934, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 3.5517 - val_loss: 1.2593\n",
      "Epoch 3/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.1124\n",
      "Epoch 00003: val_loss improved from 1.25934 to 0.59736, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.5190 - val_loss: 0.5974\n",
      "Epoch 4/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.1068\n",
      "Epoch 00004: val_loss improved from 0.59736 to 0.36817, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0714 - val_loss: 0.3682\n",
      "Epoch 5/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.6450\n",
      "Epoch 00005: val_loss improved from 0.36817 to 0.27463, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6427 - val_loss: 0.2746\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.7669\n",
      "Epoch 00006: val_loss improved from 0.27463 to 0.23363, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7535 - val_loss: 0.2336\n",
      "Epoch 7/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.5800\n",
      "Epoch 00007: val_loss improved from 0.23363 to 0.20945, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5757 - val_loss: 0.2094\n",
      "Epoch 8/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.6704\n",
      "Epoch 00008: val_loss improved from 0.20945 to 0.19905, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6684 - val_loss: 0.1991\n",
      "Epoch 9/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.4275\n",
      "Epoch 00009: val_loss improved from 0.19905 to 0.18388, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4255 - val_loss: 0.1839\n",
      "Epoch 10/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.5668\n",
      "Epoch 00010: val_loss improved from 0.18388 to 0.17565, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5637 - val_loss: 0.1757\n",
      "Epoch 11/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.4575\n",
      "Epoch 00011: val_loss improved from 0.17565 to 0.17011, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4491 - val_loss: 0.1701\n",
      "Epoch 12/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.4640\n",
      "Epoch 00012: val_loss improved from 0.17011 to 0.16394, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4614 - val_loss: 0.1639\n",
      "Epoch 13/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.4283\n",
      "Epoch 00013: val_loss improved from 0.16394 to 0.16030, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4270 - val_loss: 0.1603\n",
      "Epoch 14/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.3742\n",
      "Epoch 00014: val_loss improved from 0.16030 to 0.15618, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3738 - val_loss: 0.1562\n",
      "Epoch 15/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.4129\n",
      "Epoch 00015: val_loss improved from 0.15618 to 0.15131, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4111 - val_loss: 0.1513\n",
      "Epoch 16/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4183\n",
      "Epoch 00016: val_loss improved from 0.15131 to 0.14848, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4134 - val_loss: 0.1485\n",
      "Epoch 17/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3530\n",
      "Epoch 00017: val_loss improved from 0.14848 to 0.14794, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3523 - val_loss: 0.1479\n",
      "Epoch 18/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.3515\n",
      "Epoch 00018: val_loss improved from 0.14794 to 0.14419, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3501 - val_loss: 0.1442\n",
      "Epoch 19/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.3333\n",
      "Epoch 00019: val_loss did not improve from 0.14419\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3316 - val_loss: 0.1450\n",
      "Epoch 20/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.3357\n",
      "Epoch 00020: val_loss improved from 0.14419 to 0.14310, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3352 - val_loss: 0.1431\n",
      "Epoch 21/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3223\n",
      "Epoch 00021: val_loss did not improve from 0.14310\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3223 - val_loss: 0.1437\n",
      "Epoch 22/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3312\n",
      "Epoch 00022: val_loss improved from 0.14310 to 0.13921, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3309 - val_loss: 0.1392\n",
      "Epoch 23/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.3452\n",
      "Epoch 00023: val_loss improved from 0.13921 to 0.13848, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3439 - val_loss: 0.1385\n",
      "Epoch 24/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3123\n",
      "Epoch 00024: val_loss improved from 0.13848 to 0.13687, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3124 - val_loss: 0.1369\n",
      "Epoch 25/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3117\n",
      "Epoch 00025: val_loss improved from 0.13687 to 0.13623, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3110 - val_loss: 0.1362\n",
      "Epoch 26/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3087\n",
      "Epoch 00026: val_loss did not improve from 0.13623\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3092 - val_loss: 0.1365\n",
      "Epoch 27/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.2976\n",
      "Epoch 00027: val_loss improved from 0.13623 to 0.13558, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2977 - val_loss: 0.1356\n",
      "Epoch 28/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.3485\n",
      "Epoch 00028: val_loss improved from 0.13558 to 0.13418, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3447 - val_loss: 0.1342\n",
      "Epoch 29/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.2945\n",
      "Epoch 00029: val_loss improved from 0.13418 to 0.13298, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2937 - val_loss: 0.1330\n",
      "Epoch 30/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.2822\n",
      "Epoch 00030: val_loss improved from 0.13298 to 0.13171, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2823 - val_loss: 0.1317\n",
      "Epoch 31/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.2885\n",
      "Epoch 00031: val_loss did not improve from 0.13171\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2881 - val_loss: 0.1321\n",
      "Epoch 32/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.2875\n",
      "Epoch 00032: val_loss improved from 0.13171 to 0.13126, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2891 - val_loss: 0.1313\n",
      "Epoch 33/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.2807\n",
      "Epoch 00033: val_loss did not improve from 0.13126\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2797 - val_loss: 0.1327\n",
      "Epoch 34/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.2861\n",
      "Epoch 00034: val_loss did not improve from 0.13126\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2855 - val_loss: 0.1321\n",
      " ###1 fold : val acc1 0.591, acc3 0.961, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/154 [============================>.] - ETA: 0s - loss: 13.9890\n",
      "Epoch 00001: val_loss improved from inf to 5.76621, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 13.8219 - val_loss: 5.7662\n",
      "Epoch 2/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 3.6875\n",
      "Epoch 00002: val_loss improved from 5.76621 to 1.27506, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 3.5617 - val_loss: 1.2751\n",
      "Epoch 3/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 1.1113\n",
      "Epoch 00003: val_loss improved from 1.27506 to 0.60007, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.5361 - val_loss: 0.6001\n",
      "Epoch 4/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 1.0928\n",
      "Epoch 00004: val_loss improved from 0.60007 to 0.36791, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.0782 - val_loss: 0.3679\n",
      "Epoch 5/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.6537\n",
      "Epoch 00005: val_loss improved from 0.36791 to 0.27440, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.6442 - val_loss: 0.2744\n",
      "Epoch 6/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.7748\n",
      "Epoch 00006: val_loss improved from 0.27440 to 0.23408, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.7482 - val_loss: 0.2341\n",
      "Epoch 7/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.5893\n",
      "Epoch 00007: val_loss improved from 0.23408 to 0.21012, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5772 - val_loss: 0.2101\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.6689\n",
      "Epoch 00008: val_loss improved from 0.21012 to 0.20033, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6689 - val_loss: 0.2003\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.4280\n",
      "Epoch 00009: val_loss improved from 0.20033 to 0.18565, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4280 - val_loss: 0.1856\n",
      "Epoch 10/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.5604\n",
      "Epoch 00010: val_loss improved from 0.18565 to 0.17807, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5604 - val_loss: 0.1781\n",
      "Epoch 11/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.4556\n",
      "Epoch 00011: val_loss improved from 0.17807 to 0.17218, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4479 - val_loss: 0.1722\n",
      "Epoch 12/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.4714\n",
      "Epoch 00012: val_loss improved from 0.17218 to 0.16546, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4631 - val_loss: 0.1655\n",
      "Epoch 13/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.4343\n",
      "Epoch 00013: val_loss improved from 0.16546 to 0.16193, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.4302 - val_loss: 0.1619\n",
      "Epoch 14/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.3773\n",
      "Epoch 00014: val_loss improved from 0.16193 to 0.15865, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3745 - val_loss: 0.1586\n",
      "Epoch 15/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.4164\n",
      "Epoch 00015: val_loss improved from 0.15865 to 0.15264, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4117 - val_loss: 0.1526\n",
      "Epoch 16/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.4224\n",
      "Epoch 00016: val_loss improved from 0.15264 to 0.14920, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.4138 - val_loss: 0.1492\n",
      "Epoch 17/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.3512\n",
      "Epoch 00017: val_loss improved from 0.14920 to 0.14892, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3502 - val_loss: 0.1489\n",
      "Epoch 18/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3495\n",
      "Epoch 00018: val_loss improved from 0.14892 to 0.14534, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3482 - val_loss: 0.1453\n",
      "Epoch 19/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.3311\n",
      "Epoch 00019: val_loss improved from 0.14534 to 0.14473, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3302 - val_loss: 0.1447\n",
      "Epoch 20/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.3351\n",
      "Epoch 00020: val_loss improved from 0.14473 to 0.14341, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3335 - val_loss: 0.1434\n",
      "Epoch 21/100\n",
      "134/154 [=========================>....] - ETA: 0s - loss: 0.3199\n",
      "Epoch 00021: val_loss improved from 0.14341 to 0.14323, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3198 - val_loss: 0.1432\n",
      "Epoch 22/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.3282\n",
      "Epoch 00022: val_loss improved from 0.14323 to 0.14022, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3281 - val_loss: 0.1402\n",
      "Epoch 23/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.3463\n",
      "Epoch 00023: val_loss improved from 0.14022 to 0.13901, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3429 - val_loss: 0.1390\n",
      "Epoch 24/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.3134\n",
      "Epoch 00024: val_loss improved from 0.13901 to 0.13719, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3124 - val_loss: 0.1372\n",
      "Epoch 25/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.3087\n",
      "Epoch 00025: val_loss improved from 0.13719 to 0.13637, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3102 - val_loss: 0.1364\n",
      "Epoch 26/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.3071\n",
      "Epoch 00026: val_loss did not improve from 0.13637\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3084 - val_loss: 0.1373\n",
      "Epoch 27/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.2972\n",
      "Epoch 00027: val_loss improved from 0.13637 to 0.13546, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2985 - val_loss: 0.1355\n",
      "Epoch 28/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.3478\n",
      "Epoch 00028: val_loss improved from 0.13546 to 0.13441, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3450 - val_loss: 0.1344\n",
      "Epoch 29/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.2947\n",
      "Epoch 00029: val_loss improved from 0.13441 to 0.13372, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2941 - val_loss: 0.1337\n",
      "Epoch 30/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.2849\n",
      "Epoch 00030: val_loss improved from 0.13372 to 0.13185, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2851 - val_loss: 0.1318\n",
      "Epoch 31/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.2874\n",
      "Epoch 00031: val_loss did not improve from 0.13185\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2871 - val_loss: 0.1328\n",
      "Epoch 32/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2873\n",
      "Epoch 00032: val_loss did not improve from 0.13185\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2877 - val_loss: 0.1336\n",
      " ###2 fold : val acc1 0.583, acc3 0.964, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/154 [==========================>...] - ETA: 0s - loss: 14.3606\n",
      "Epoch 00001: val_loss improved from inf to 5.78986, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 13.8278 - val_loss: 5.7899\n",
      "Epoch 2/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 3.6070\n",
      "Epoch 00002: val_loss improved from 5.78986 to 1.27671, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 3.5640 - val_loss: 1.2767\n",
      "Epoch 3/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 1.5363\n",
      "Epoch 00003: val_loss improved from 1.27671 to 0.59709, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.5314 - val_loss: 0.5971\n",
      "Epoch 4/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 1.0794\n",
      "Epoch 00004: val_loss improved from 0.59709 to 0.36780, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.0698 - val_loss: 0.3678\n",
      "Epoch 5/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.6456\n",
      "Epoch 00005: val_loss improved from 0.36780 to 0.27558, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6406 - val_loss: 0.2756\n",
      "Epoch 6/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.7549\n",
      "Epoch 00006: val_loss improved from 0.27558 to 0.23481, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.7479 - val_loss: 0.2348\n",
      "Epoch 7/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.5853\n",
      "Epoch 00007: val_loss improved from 0.23481 to 0.21068, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5784 - val_loss: 0.2107\n",
      "Epoch 8/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.6710\n",
      "Epoch 00008: val_loss improved from 0.21068 to 0.20186, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6706 - val_loss: 0.2019\n",
      "Epoch 9/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.4272\n",
      "Epoch 00009: val_loss improved from 0.20186 to 0.18512, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4278 - val_loss: 0.1851\n",
      "Epoch 10/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.5623\n",
      "Epoch 00010: val_loss improved from 0.18512 to 0.17886, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5621 - val_loss: 0.1789\n",
      "Epoch 11/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.4497\n",
      "Epoch 00011: val_loss improved from 0.17886 to 0.17127, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4487 - val_loss: 0.1713\n",
      "Epoch 12/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.4664\n",
      "Epoch 00012: val_loss improved from 0.17127 to 0.16511, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4647 - val_loss: 0.1651\n",
      "Epoch 13/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4358\n",
      "Epoch 00013: val_loss improved from 0.16511 to 0.16230, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4324 - val_loss: 0.1623\n",
      "Epoch 14/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.3774\n",
      "Epoch 00014: val_loss improved from 0.16230 to 0.15879, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3774 - val_loss: 0.1588\n",
      "Epoch 15/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.4124\n",
      "Epoch 00015: val_loss improved from 0.15879 to 0.15301, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4121 - val_loss: 0.1530\n",
      "Epoch 16/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.4177\n",
      "Epoch 00016: val_loss improved from 0.15301 to 0.14893, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4153 - val_loss: 0.1489\n",
      "Epoch 17/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3517\n",
      "Epoch 00017: val_loss did not improve from 0.14893\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3509 - val_loss: 0.1490\n",
      "Epoch 18/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.3485\n",
      "Epoch 00018: val_loss improved from 0.14893 to 0.14513, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3484 - val_loss: 0.1451\n",
      "Epoch 19/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.3293\n",
      "Epoch 00019: val_loss improved from 0.14513 to 0.14382, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3296 - val_loss: 0.1438\n",
      "Epoch 20/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3366\n",
      "Epoch 00020: val_loss improved from 0.14382 to 0.14310, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3342 - val_loss: 0.1431\n",
      "Epoch 21/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.3228\n",
      "Epoch 00021: val_loss improved from 0.14310 to 0.14251, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3224 - val_loss: 0.1425\n",
      "Epoch 22/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3302\n",
      "Epoch 00022: val_loss improved from 0.14251 to 0.14127, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3302 - val_loss: 0.1413\n",
      "Epoch 23/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.3506\n",
      "Epoch 00023: val_loss improved from 0.14127 to 0.13963, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3467 - val_loss: 0.1396\n",
      "Epoch 24/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.3148\n",
      "Epoch 00024: val_loss improved from 0.13963 to 0.13723, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3147 - val_loss: 0.1372\n",
      "Epoch 25/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.3095\n",
      "Epoch 00025: val_loss improved from 0.13723 to 0.13638, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3097 - val_loss: 0.1364\n",
      "Epoch 26/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3092\n",
      "Epoch 00026: val_loss did not improve from 0.13638\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3096 - val_loss: 0.1381\n",
      "Epoch 27/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.2989\n",
      "Epoch 00027: val_loss improved from 0.13638 to 0.13492, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3004 - val_loss: 0.1349\n",
      "Epoch 28/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.3459\n",
      "Epoch 00028: val_loss improved from 0.13492 to 0.13404, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3458 - val_loss: 0.1340\n",
      "Epoch 29/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.2945\n",
      "Epoch 00029: val_loss improved from 0.13404 to 0.13383, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2937 - val_loss: 0.1338\n",
      "Epoch 30/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2854\n",
      "Epoch 00030: val_loss improved from 0.13383 to 0.13210, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2856 - val_loss: 0.1321\n",
      "Epoch 31/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.2894\n",
      "Epoch 00031: val_loss did not improve from 0.13210\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2896 - val_loss: 0.1333\n",
      "Epoch 32/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.2884\n",
      "Epoch 00032: val_loss did not improve from 0.13210\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2887 - val_loss: 0.1329\n",
      " ###3 fold : val acc1 0.575, acc3 0.965, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/154 [=========================>....] - ETA: 0s - loss: 14.7932\n",
      "Epoch 00001: val_loss improved from inf to 5.97838, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 13.9441 - val_loss: 5.9784\n",
      "Epoch 2/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 3.5351\n",
      "Epoch 00002: val_loss improved from 5.97838 to 1.27437, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 3.4529 - val_loss: 1.2744\n",
      "Epoch 3/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.4300\n",
      "Epoch 00003: val_loss improved from 1.27437 to 0.57980, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4011 - val_loss: 0.5798\n",
      "Epoch 4/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.9996\n",
      "Epoch 00004: val_loss improved from 0.57980 to 0.36279, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.9935 - val_loss: 0.3628\n",
      "Epoch 5/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.8239\n",
      "Epoch 00005: val_loss improved from 0.36279 to 0.27245, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8091 - val_loss: 0.2724\n",
      "Epoch 6/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.6575\n",
      "Epoch 00006: val_loss improved from 0.27245 to 0.23338, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6576 - val_loss: 0.2334\n",
      "Epoch 7/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.5386\n",
      "Epoch 00007: val_loss improved from 0.23338 to 0.21225, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5385 - val_loss: 0.2123\n",
      "Epoch 8/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.6379\n",
      "Epoch 00008: val_loss improved from 0.21225 to 0.19879, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6269 - val_loss: 0.1988\n",
      "Epoch 9/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.5672\n",
      "Epoch 00009: val_loss improved from 0.19879 to 0.18990, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5614 - val_loss: 0.1899\n",
      "Epoch 10/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.4104\n",
      "Epoch 00010: val_loss improved from 0.18990 to 0.17954, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4110 - val_loss: 0.1795\n",
      "Epoch 11/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4390\n",
      "Epoch 00011: val_loss improved from 0.17954 to 0.17381, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4352 - val_loss: 0.1738\n",
      "Epoch 12/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.4258\n",
      "Epoch 00012: val_loss improved from 0.17381 to 0.16636, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4257 - val_loss: 0.1664\n",
      "Epoch 13/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4144\n",
      "Epoch 00013: val_loss improved from 0.16636 to 0.16399, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4120 - val_loss: 0.1640\n",
      "Epoch 14/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3825\n",
      "Epoch 00014: val_loss improved from 0.16399 to 0.15831, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3796 - val_loss: 0.1583\n",
      "Epoch 15/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3733\n",
      "Epoch 00015: val_loss improved from 0.15831 to 0.15474, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3720 - val_loss: 0.1547\n",
      "Epoch 16/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3840\n",
      "Epoch 00016: val_loss improved from 0.15474 to 0.14910, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3812 - val_loss: 0.1491\n",
      "Epoch 17/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.3442\n",
      "Epoch 00017: val_loss did not improve from 0.14910\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3433 - val_loss: 0.1492\n",
      "Epoch 18/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.3334\n",
      "Epoch 00018: val_loss improved from 0.14910 to 0.14491, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3339 - val_loss: 0.1449\n",
      "Epoch 19/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3257\n",
      "Epoch 00019: val_loss improved from 0.14491 to 0.14423, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3270 - val_loss: 0.1442\n",
      "Epoch 20/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3301\n",
      "Epoch 00020: val_loss improved from 0.14423 to 0.14198, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3286 - val_loss: 0.1420\n",
      "Epoch 21/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.3307\n",
      "Epoch 00021: val_loss improved from 0.14198 to 0.14185, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3303 - val_loss: 0.1418\n",
      "Epoch 22/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.3140\n",
      "Epoch 00022: val_loss improved from 0.14185 to 0.14175, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3139 - val_loss: 0.1417\n",
      "Epoch 23/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.3113\n",
      "Epoch 00023: val_loss improved from 0.14175 to 0.13658, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3115 - val_loss: 0.1366\n",
      "Epoch 24/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.3275\n",
      "Epoch 00024: val_loss improved from 0.13658 to 0.13600, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3263 - val_loss: 0.1360\n",
      "Epoch 25/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3017\n",
      "Epoch 00025: val_loss improved from 0.13600 to 0.13542, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3016 - val_loss: 0.1354\n",
      "Epoch 26/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.2972\n",
      "Epoch 00026: val_loss did not improve from 0.13542\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2959 - val_loss: 0.1363\n",
      "Epoch 27/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3030\n",
      "Epoch 00027: val_loss improved from 0.13542 to 0.13374, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3028 - val_loss: 0.1337\n",
      "Epoch 28/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2959\n",
      "Epoch 00028: val_loss improved from 0.13374 to 0.13332, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2954 - val_loss: 0.1333\n",
      "Epoch 29/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.2844\n",
      "Epoch 00029: val_loss did not improve from 0.13332\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2845 - val_loss: 0.1352\n",
      "Epoch 30/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.2876\n",
      "Epoch 00030: val_loss improved from 0.13332 to 0.13184, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2874 - val_loss: 0.1318\n",
      "Epoch 31/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.2921\n",
      "Epoch 00031: val_loss did not improve from 0.13184\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2921 - val_loss: 0.1319\n",
      "Epoch 32/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.2766\n",
      "Epoch 00032: val_loss did not improve from 0.13184\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2768 - val_loss: 0.1340\n",
      " ###4 fold : val acc1 0.576, acc3 0.968, mae 0.232###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/154 [============================>.] - ETA: 0s - loss: 13.7756\n",
      "Epoch 00001: val_loss improved from inf to 5.52847, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 13.6062 - val_loss: 5.5285\n",
      "Epoch 2/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 2.9239\n",
      "Epoch 00002: val_loss improved from 5.52847 to 1.20421, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 2.8276 - val_loss: 1.2042\n",
      "Epoch 3/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 1.0447\n",
      "Epoch 00003: val_loss improved from 1.20421 to 0.52340, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.0281 - val_loss: 0.5234\n",
      "Epoch 4/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.6529\n",
      "Epoch 00004: val_loss improved from 0.52340 to 0.31946, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6457 - val_loss: 0.3195\n",
      "Epoch 5/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.5159\n",
      "Epoch 00005: val_loss improved from 0.31946 to 0.24425, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5135 - val_loss: 0.2443\n",
      "Epoch 6/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.4636\n",
      "Epoch 00006: val_loss improved from 0.24425 to 0.21548, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4634 - val_loss: 0.2155\n",
      "Epoch 7/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.4463\n",
      "Epoch 00007: val_loss improved from 0.21548 to 0.19891, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.4466 - val_loss: 0.1989\n",
      "Epoch 8/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4276\n",
      "Epoch 00008: val_loss improved from 0.19891 to 0.18943, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.4255 - val_loss: 0.1894\n",
      "Epoch 9/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.4073\n",
      "Epoch 00009: val_loss improved from 0.18943 to 0.17924, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4072 - val_loss: 0.1792\n",
      "Epoch 10/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.3875\n",
      "Epoch 00010: val_loss improved from 0.17924 to 0.17196, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3900 - val_loss: 0.1720\n",
      "Epoch 11/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.3856\n",
      "Epoch 00011: val_loss improved from 0.17196 to 0.16654, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3850 - val_loss: 0.1665\n",
      "Epoch 12/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.3739\n",
      "Epoch 00012: val_loss improved from 0.16654 to 0.15961, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3722 - val_loss: 0.1596\n",
      "Epoch 13/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.3707\n",
      "Epoch 00013: val_loss improved from 0.15961 to 0.15720, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3694 - val_loss: 0.1572\n",
      "Epoch 14/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.3578\n",
      "Epoch 00014: val_loss improved from 0.15720 to 0.15510, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3568 - val_loss: 0.1551\n",
      "Epoch 15/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.3563\n",
      "Epoch 00015: val_loss improved from 0.15510 to 0.15133, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3554 - val_loss: 0.1513\n",
      "Epoch 16/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3468\n",
      "Epoch 00016: val_loss improved from 0.15133 to 0.14771, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3457 - val_loss: 0.1477\n",
      "Epoch 17/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.3353\n",
      "Epoch 00017: val_loss did not improve from 0.14771\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3336 - val_loss: 0.1490\n",
      "Epoch 18/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.3296\n",
      "Epoch 00018: val_loss improved from 0.14771 to 0.14438, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3320 - val_loss: 0.1444\n",
      "Epoch 19/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.3214\n",
      "Epoch 00019: val_loss improved from 0.14438 to 0.14417, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3223 - val_loss: 0.1442\n",
      "Epoch 20/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.3247\n",
      "Epoch 00020: val_loss improved from 0.14417 to 0.14248, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3235 - val_loss: 0.1425\n",
      "Epoch 21/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.3121\n",
      "Epoch 00021: val_loss did not improve from 0.14248\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3125 - val_loss: 0.1430\n",
      "Epoch 22/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.3068\n",
      "Epoch 00022: val_loss did not improve from 0.14248\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3065 - val_loss: 0.1436\n",
      " ###5 fold : val acc1 0.585, acc3 0.961, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/154 [============================>.] - ETA: 0s - loss: 13.9247\n",
      "Epoch 00001: val_loss improved from inf to 5.81530, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 13.8180 - val_loss: 5.8153\n",
      "Epoch 2/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 2.9579\n",
      "Epoch 00002: val_loss improved from 5.81530 to 1.19031, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 3.1228 - val_loss: 1.1903\n",
      "Epoch 3/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 1.4307\n",
      "Epoch 00003: val_loss improved from 1.19031 to 0.54477, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4047 - val_loss: 0.5448\n",
      "Epoch 4/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.9160\n",
      "Epoch 00004: val_loss improved from 0.54477 to 0.34204, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8990 - val_loss: 0.3420\n",
      "Epoch 5/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.6778\n",
      "Epoch 00005: val_loss improved from 0.34204 to 0.26089, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6668 - val_loss: 0.2609\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.6354\n",
      "Epoch 00006: val_loss improved from 0.26089 to 0.22591, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6279 - val_loss: 0.2259\n",
      "Epoch 7/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.6271\n",
      "Epoch 00007: val_loss improved from 0.22591 to 0.21140, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6203 - val_loss: 0.2114\n",
      "Epoch 8/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.4876\n",
      "Epoch 00008: val_loss improved from 0.21140 to 0.19573, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4834 - val_loss: 0.1957\n",
      "Epoch 9/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.4488\n",
      "Epoch 00009: val_loss improved from 0.19573 to 0.18592, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4479 - val_loss: 0.1859\n",
      "Epoch 10/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.5434\n",
      "Epoch 00010: val_loss improved from 0.18592 to 0.17751, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5384 - val_loss: 0.1775\n",
      "Epoch 11/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.3992\n",
      "Epoch 00011: val_loss improved from 0.17751 to 0.17159, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4014 - val_loss: 0.1716\n",
      "Epoch 12/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4003\n",
      "Epoch 00012: val_loss improved from 0.17159 to 0.16296, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3988 - val_loss: 0.1630\n",
      "Epoch 13/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.4504\n",
      "Epoch 00013: val_loss improved from 0.16296 to 0.16130, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.4494 - val_loss: 0.1613\n",
      "Epoch 14/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.3681\n",
      "Epoch 00014: val_loss improved from 0.16130 to 0.15722, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3673 - val_loss: 0.1572\n",
      "Epoch 15/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.3665\n",
      "Epoch 00015: val_loss improved from 0.15722 to 0.15204, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3663 - val_loss: 0.1520\n",
      "Epoch 16/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3576\n",
      "Epoch 00016: val_loss improved from 0.15204 to 0.14696, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3563 - val_loss: 0.1470\n",
      "Epoch 17/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.3776\n",
      "Epoch 00017: val_loss did not improve from 0.14696\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3745 - val_loss: 0.1499\n",
      "Epoch 18/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.3393\n",
      "Epoch 00018: val_loss improved from 0.14696 to 0.14513, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3396 - val_loss: 0.1451\n",
      "Epoch 19/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.3288\n",
      "Epoch 00019: val_loss improved from 0.14513 to 0.14364, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3289 - val_loss: 0.1436\n",
      "Epoch 20/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.3310\n",
      "Epoch 00020: val_loss improved from 0.14364 to 0.14103, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3290 - val_loss: 0.1410\n",
      "Epoch 21/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.3195\n",
      "Epoch 00021: val_loss did not improve from 0.14103\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3195 - val_loss: 0.1418\n",
      "Epoch 22/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3491\n",
      "Epoch 00022: val_loss did not improve from 0.14103\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3464 - val_loss: 0.1425\n",
      " ###6 fold : val acc1 0.574, acc3 0.955, mae 0.240###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 13.9888\n",
      "Epoch 00001: val_loss improved from inf to 5.83404, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 13.8253 - val_loss: 5.8340\n",
      "Epoch 2/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 3.1308\n",
      "Epoch 00002: val_loss improved from 5.83404 to 1.19114, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 3.1286 - val_loss: 1.1911\n",
      "Epoch 3/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 1.4988\n",
      "Epoch 00003: val_loss improved from 1.19114 to 0.54988, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.4183 - val_loss: 0.5499\n",
      "Epoch 4/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.9135\n",
      "Epoch 00004: val_loss improved from 0.54988 to 0.34812, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.9136 - val_loss: 0.3481\n",
      "Epoch 5/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.6833\n",
      "Epoch 00005: val_loss improved from 0.34812 to 0.26308, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6829 - val_loss: 0.2631\n",
      "Epoch 6/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.6582\n",
      "Epoch 00006: val_loss improved from 0.26308 to 0.22608, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6473 - val_loss: 0.2261\n",
      "Epoch 7/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.6508\n",
      "Epoch 00007: val_loss improved from 0.22608 to 0.20930, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6311 - val_loss: 0.2093\n",
      "Epoch 8/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.4991\n",
      "Epoch 00008: val_loss improved from 0.20930 to 0.19535, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4916 - val_loss: 0.1953\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.4516\n",
      "Epoch 00009: val_loss improved from 0.19535 to 0.18592, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4516 - val_loss: 0.1859\n",
      "Epoch 10/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.5671\n",
      "Epoch 00010: val_loss improved from 0.18592 to 0.17663, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5503 - val_loss: 0.1766\n",
      "Epoch 11/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.4004\n",
      "Epoch 00011: val_loss improved from 0.17663 to 0.17001, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4004 - val_loss: 0.1700\n",
      "Epoch 12/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.4104\n",
      "Epoch 00012: val_loss improved from 0.17001 to 0.16176, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4053 - val_loss: 0.1618\n",
      "Epoch 13/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.4581\n",
      "Epoch 00013: val_loss improved from 0.16176 to 0.16094, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4581 - val_loss: 0.1609\n",
      "Epoch 14/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.3679\n",
      "Epoch 00014: val_loss improved from 0.16094 to 0.15652, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3679 - val_loss: 0.1565\n",
      "Epoch 15/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.3679\n",
      "Epoch 00015: val_loss improved from 0.15652 to 0.15169, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3669 - val_loss: 0.1517\n",
      "Epoch 16/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.3579\n",
      "Epoch 00016: val_loss improved from 0.15169 to 0.14633, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3559 - val_loss: 0.1463\n",
      "Epoch 17/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.3737\n",
      "Epoch 00017: val_loss did not improve from 0.14633\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3728 - val_loss: 0.1498\n",
      "Epoch 18/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.3402\n",
      "Epoch 00018: val_loss improved from 0.14633 to 0.14556, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3402 - val_loss: 0.1456\n",
      "Epoch 19/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.3294\n",
      "Epoch 00019: val_loss improved from 0.14556 to 0.14266, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3294 - val_loss: 0.1427\n",
      "Epoch 20/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.3295\n",
      "Epoch 00020: val_loss improved from 0.14266 to 0.14054, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3295 - val_loss: 0.1405\n",
      "Epoch 21/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.3204\n",
      "Epoch 00021: val_loss did not improve from 0.14054\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3198 - val_loss: 0.1413\n",
      "Epoch 22/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.3578\n",
      "Epoch 00022: val_loss did not improve from 0.14054\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3512 - val_loss: 0.1420\n",
      " ###7 fold : val acc1 0.583, acc3 0.958, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/154 [=========================>....] - ETA: 0s - loss: 14.6386\n",
      "Epoch 00001: val_loss improved from inf to 5.89038, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 13.8323 - val_loss: 5.8904\n",
      "Epoch 2/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 2.9336\n",
      "Epoch 00002: val_loss improved from 5.89038 to 1.22614, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 3.1248 - val_loss: 1.2261\n",
      "Epoch 3/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 1.4659\n",
      "Epoch 00003: val_loss improved from 1.22614 to 0.56534, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4117 - val_loss: 0.5653\n",
      "Epoch 4/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.9264\n",
      "Epoch 00004: val_loss improved from 0.56534 to 0.35210, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9078 - val_loss: 0.3521\n",
      "Epoch 5/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.6846\n",
      "Epoch 00005: val_loss improved from 0.35210 to 0.26220, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6791 - val_loss: 0.2622\n",
      "Epoch 6/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.6479\n",
      "Epoch 00006: val_loss improved from 0.26220 to 0.22220, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6422 - val_loss: 0.2222\n",
      "Epoch 7/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.6411\n",
      "Epoch 00007: val_loss improved from 0.22220 to 0.20383, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6292 - val_loss: 0.2038\n",
      "Epoch 8/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.4952\n",
      "Epoch 00008: val_loss improved from 0.20383 to 0.18921, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4908 - val_loss: 0.1892\n",
      "Epoch 9/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4511\n",
      "Epoch 00009: val_loss improved from 0.18921 to 0.17987, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4489 - val_loss: 0.1799\n",
      "Epoch 10/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.5529\n",
      "Epoch 00010: val_loss improved from 0.17987 to 0.17064, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5458 - val_loss: 0.1706\n",
      "Epoch 11/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.3962\n",
      "Epoch 00011: val_loss improved from 0.17064 to 0.16470, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3998 - val_loss: 0.1647\n",
      "Epoch 12/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.4035\n",
      "Epoch 00012: val_loss improved from 0.16470 to 0.15738, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4030 - val_loss: 0.1574\n",
      "Epoch 13/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4607\n",
      "Epoch 00013: val_loss improved from 0.15738 to 0.15557, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4551 - val_loss: 0.1556\n",
      "Epoch 14/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3676\n",
      "Epoch 00014: val_loss improved from 0.15557 to 0.15187, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3657 - val_loss: 0.1519\n",
      "Epoch 15/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.3652\n",
      "Epoch 00015: val_loss improved from 0.15187 to 0.14754, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3653 - val_loss: 0.1475\n",
      "Epoch 16/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.3545\n",
      "Epoch 00016: val_loss improved from 0.14754 to 0.14296, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3544 - val_loss: 0.1430\n",
      "Epoch 17/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.3717\n",
      "Epoch 00017: val_loss did not improve from 0.14296\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3715 - val_loss: 0.1459\n",
      "Epoch 18/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.3374\n",
      "Epoch 00018: val_loss improved from 0.14296 to 0.14136, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3396 - val_loss: 0.1414\n",
      "Epoch 19/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3257\n",
      "Epoch 00019: val_loss improved from 0.14136 to 0.13962, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3272 - val_loss: 0.1396\n",
      "Epoch 20/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.3306\n",
      "Epoch 00020: val_loss improved from 0.13962 to 0.13812, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3280 - val_loss: 0.1381\n",
      "Epoch 21/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.3194\n",
      "Epoch 00021: val_loss did not improve from 0.13812\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3199 - val_loss: 0.1385\n",
      "Epoch 22/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.3494\n",
      "Epoch 00022: val_loss did not improve from 0.13812\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3494 - val_loss: 0.1393\n",
      " ###8 fold : val acc1 0.566, acc3 0.949, mae 0.247###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/154 [============================>.] - ETA: 0s - loss: 13.9974\n",
      "Epoch 00001: val_loss improved from inf to 5.90273, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 13.8323 - val_loss: 5.9027\n",
      "Epoch 2/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 2.9637\n",
      "Epoch 00002: val_loss improved from 5.90273 to 1.22768, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 3.1248 - val_loss: 1.2277\n",
      "Epoch 3/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 1.4201\n",
      "Epoch 00003: val_loss improved from 1.22768 to 0.56054, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.4117 - val_loss: 0.5605\n",
      "Epoch 4/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.9203\n",
      "Epoch 00004: val_loss improved from 0.56054 to 0.34424, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9078 - val_loss: 0.3442\n",
      "Epoch 5/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.6846\n",
      "Epoch 00005: val_loss improved from 0.34424 to 0.25465, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6791 - val_loss: 0.2546\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.6508\n",
      "Epoch 00006: val_loss improved from 0.25465 to 0.21578, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6422 - val_loss: 0.2158\n",
      "Epoch 7/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.6358\n",
      "Epoch 00007: val_loss improved from 0.21578 to 0.19869, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6292 - val_loss: 0.1987\n",
      "Epoch 8/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.4957\n",
      "Epoch 00008: val_loss improved from 0.19869 to 0.18525, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4908 - val_loss: 0.1852\n",
      "Epoch 9/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.4515\n",
      "Epoch 00009: val_loss improved from 0.18525 to 0.17644, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4489 - val_loss: 0.1764\n",
      "Epoch 10/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.5532\n",
      "Epoch 00010: val_loss improved from 0.17644 to 0.16790, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5458 - val_loss: 0.1679\n",
      "Epoch 11/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.3985\n",
      "Epoch 00011: val_loss improved from 0.16790 to 0.16227, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3998 - val_loss: 0.1623\n",
      "Epoch 12/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.4037\n",
      "Epoch 00012: val_loss improved from 0.16227 to 0.15494, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4030 - val_loss: 0.1549\n",
      "Epoch 13/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4607\n",
      "Epoch 00013: val_loss improved from 0.15494 to 0.15409, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4551 - val_loss: 0.1541\n",
      "Epoch 14/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3676\n",
      "Epoch 00014: val_loss improved from 0.15409 to 0.15023, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3657 - val_loss: 0.1502\n",
      "Epoch 15/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3660\n",
      "Epoch 00015: val_loss improved from 0.15023 to 0.14617, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3653 - val_loss: 0.1462\n",
      "Epoch 16/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.3550\n",
      "Epoch 00016: val_loss improved from 0.14617 to 0.14153, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3544 - val_loss: 0.1415\n",
      "Epoch 17/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.3773\n",
      "Epoch 00017: val_loss did not improve from 0.14153\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3715 - val_loss: 0.1452\n",
      "Epoch 18/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.3366\n",
      "Epoch 00018: val_loss improved from 0.14153 to 0.14143, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3396 - val_loss: 0.1414\n",
      "Epoch 19/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.3271\n",
      "Epoch 00019: val_loss improved from 0.14143 to 0.13912, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3272 - val_loss: 0.1391\n",
      "Epoch 20/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.3299\n",
      "Epoch 00020: val_loss improved from 0.13912 to 0.13765, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3280 - val_loss: 0.1376\n",
      "Epoch 21/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.3196\n",
      "Epoch 00021: val_loss did not improve from 0.13765\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3199 - val_loss: 0.1378\n",
      "Epoch 22/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.3532\n",
      "Epoch 00022: val_loss did not improve from 0.13765\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.3495 - val_loss: 0.1394\n",
      " ###9 fold : val acc1 0.575, acc3 0.956, mae 0.240###\n",
      "acc10.579_acc30.960\n",
      "random search 7/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/307 [============================>.] - ETA: 0s - loss: 6.9434\n",
      "Epoch 00001: val_loss improved from inf to 1.09260, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 6.9359 - val_loss: 1.0926\n",
      "Epoch 2/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.7173\n",
      "Epoch 00002: val_loss improved from 1.09260 to 0.21076, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.7173 - val_loss: 0.2108\n",
      "Epoch 3/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.1645\n",
      "Epoch 00003: val_loss improved from 0.21076 to 0.17964, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3131 - val_loss: 0.1796\n",
      "Epoch 4/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.2059\n",
      "Epoch 00004: val_loss improved from 0.17964 to 0.15967, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2028 - val_loss: 0.1597\n",
      "Epoch 5/100\n",
      "286/307 [==========================>...] - ETA: 0s - loss: 0.1613\n",
      "Epoch 00005: val_loss improved from 0.15967 to 0.15242, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1610 - val_loss: 0.1524\n",
      "Epoch 6/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.1417\n",
      "Epoch 00006: val_loss improved from 0.15242 to 0.14578, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1416 - val_loss: 0.1458\n",
      "Epoch 7/100\n",
      "287/307 [===========================>..] - ETA: 0s - loss: 0.1339\n",
      "Epoch 00007: val_loss improved from 0.14578 to 0.14030, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1334 - val_loss: 0.1403\n",
      "Epoch 8/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.1300\n",
      "Epoch 00008: val_loss improved from 0.14030 to 0.13661, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1297 - val_loss: 0.1366\n",
      "Epoch 9/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.1276\n",
      "Epoch 00009: val_loss improved from 0.13661 to 0.13449, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1277 - val_loss: 0.1345\n",
      "Epoch 10/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.1271\n",
      "Epoch 00010: val_loss improved from 0.13449 to 0.13339, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1268 - val_loss: 0.1334\n",
      "Epoch 11/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00011: val_loss improved from 0.13339 to 0.13207, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1261 - val_loss: 0.1321\n",
      "Epoch 12/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00012: val_loss improved from 0.13207 to 0.13195, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1255 - val_loss: 0.1319\n",
      "Epoch 13/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00013: val_loss did not improve from 0.13195\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1247 - val_loss: 0.1324\n",
      "Epoch 14/100\n",
      "287/307 [===========================>..] - ETA: 0s - loss: 0.1246\n",
      "Epoch 00014: val_loss improved from 0.13195 to 0.13043, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1244 - val_loss: 0.1304\n",
      "Epoch 15/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00015: val_loss did not improve from 0.13043\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1239 - val_loss: 0.1305\n",
      "Epoch 16/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00016: val_loss improved from 0.13043 to 0.12923, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1237 - val_loss: 0.1292\n",
      "Epoch 17/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00017: val_loss did not improve from 0.12923\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1234 - val_loss: 0.1302\n",
      "Epoch 18/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.1228\n",
      "Epoch 00018: val_loss improved from 0.12923 to 0.12866, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1228 - val_loss: 0.1287\n",
      "Epoch 19/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.1228\n",
      "Epoch 00019: val_loss did not improve from 0.12866\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1230 - val_loss: 0.1288\n",
      "Epoch 20/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00020: val_loss did not improve from 0.12866\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1237 - val_loss: 0.1327\n",
      " ###0 fold : val acc1 0.579, acc3 0.965, mae 0.232###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289/307 [===========================>..] - ETA: 0s - loss: 7.2756\n",
      "Epoch 00001: val_loss improved from inf to 1.11120, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 6.9333 - val_loss: 1.1112\n",
      "Epoch 2/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.7457\n",
      "Epoch 00002: val_loss improved from 1.11120 to 0.21448, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.7319 - val_loss: 0.2145\n",
      "Epoch 3/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.1661\n",
      "Epoch 00003: val_loss improved from 0.21448 to 0.17854, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3177 - val_loss: 0.1785\n",
      "Epoch 4/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.2083\n",
      "Epoch 00004: val_loss improved from 0.17854 to 0.15842, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2069 - val_loss: 0.1584\n",
      "Epoch 5/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.1615\n",
      "Epoch 00005: val_loss improved from 0.15842 to 0.15081, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1615 - val_loss: 0.1508\n",
      "Epoch 6/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.1424\n",
      "Epoch 00006: val_loss improved from 0.15081 to 0.14404, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1422 - val_loss: 0.1440\n",
      "Epoch 7/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.1334\n",
      "Epoch 00007: val_loss improved from 0.14404 to 0.13925, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1334 - val_loss: 0.1393\n",
      "Epoch 8/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.1297\n",
      "Epoch 00008: val_loss improved from 0.13925 to 0.13617, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1297 - val_loss: 0.1362\n",
      "Epoch 9/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.1278\n",
      "Epoch 00009: val_loss improved from 0.13617 to 0.13430, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1278 - val_loss: 0.1343\n",
      "Epoch 10/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.1268\n",
      "Epoch 00010: val_loss improved from 0.13430 to 0.13346, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1268 - val_loss: 0.1335\n",
      "Epoch 11/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.1266\n",
      "Epoch 00011: val_loss improved from 0.13346 to 0.13253, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1262 - val_loss: 0.1325\n",
      "Epoch 12/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00012: val_loss improved from 0.13253 to 0.13213, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1256 - val_loss: 0.1321\n",
      "Epoch 13/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00013: val_loss did not improve from 0.13213\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1250 - val_loss: 0.1330\n",
      "Epoch 14/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00014: val_loss improved from 0.13213 to 0.13033, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1245 - val_loss: 0.1303\n",
      "Epoch 15/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00015: val_loss did not improve from 0.13033\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1241 - val_loss: 0.1307\n",
      "Epoch 16/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00016: val_loss improved from 0.13033 to 0.12937, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1239 - val_loss: 0.1294\n",
      "Epoch 17/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00017: val_loss did not improve from 0.12937\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1239 - val_loss: 0.1303\n",
      "Epoch 18/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00018: val_loss did not improve from 0.12937\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1248 - val_loss: 0.1305\n",
      " ###1 fold : val acc1 0.580, acc3 0.959, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293/307 [===========================>..] - ETA: 0s - loss: 7.2072\n",
      "Epoch 00001: val_loss improved from inf to 1.11659, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 6.9431 - val_loss: 1.1166\n",
      "Epoch 2/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.7369\n",
      "Epoch 00002: val_loss improved from 1.11659 to 0.21447, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.7275 - val_loss: 0.2145\n",
      "Epoch 3/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.3181\n",
      "Epoch 00003: val_loss improved from 0.21447 to 0.17906, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3181 - val_loss: 0.1791\n",
      "Epoch 4/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.2032\n",
      "Epoch 00004: val_loss improved from 0.17906 to 0.15808, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1998 - val_loss: 0.1581\n",
      "Epoch 5/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.1616\n",
      "Epoch 00005: val_loss improved from 0.15808 to 0.15040, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1603 - val_loss: 0.1504\n",
      "Epoch 6/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.1410\n",
      "Epoch 00006: val_loss improved from 0.15040 to 0.14387, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1416 - val_loss: 0.1439\n",
      "Epoch 7/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.1331\n",
      "Epoch 00007: val_loss improved from 0.14387 to 0.13950, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1332 - val_loss: 0.1395\n",
      "Epoch 8/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.1296\n",
      "Epoch 00008: val_loss improved from 0.13950 to 0.13606, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1296 - val_loss: 0.1361\n",
      "Epoch 9/100\n",
      "289/307 [===========================>..] - ETA: 0s - loss: 0.1276\n",
      "Epoch 00009: val_loss improved from 0.13606 to 0.13438, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1279 - val_loss: 0.1344\n",
      "Epoch 10/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.1270\n",
      "Epoch 00010: val_loss improved from 0.13438 to 0.13348, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1270 - val_loss: 0.1335\n",
      "Epoch 11/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.1261\n",
      "Epoch 00011: val_loss did not improve from 0.13348\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1263 - val_loss: 0.1335\n",
      "Epoch 12/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00012: val_loss improved from 0.13348 to 0.13240, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1259 - val_loss: 0.1324\n",
      "Epoch 13/100\n",
      "289/307 [===========================>..] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00013: val_loss improved from 0.13240 to 0.13224, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1254 - val_loss: 0.1322\n",
      "Epoch 14/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00014: val_loss improved from 0.13224 to 0.13082, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1248 - val_loss: 0.1308\n",
      "Epoch 15/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00015: val_loss improved from 0.13082 to 0.13034, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1245 - val_loss: 0.1303\n",
      "Epoch 16/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00016: val_loss improved from 0.13034 to 0.12939, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1244 - val_loss: 0.1294\n",
      "Epoch 17/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00017: val_loss did not improve from 0.12939\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1246 - val_loss: 0.1305\n",
      "Epoch 18/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00018: val_loss did not improve from 0.12939\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1269 - val_loss: 0.1305\n",
      " ###2 fold : val acc1 0.580, acc3 0.965, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/307 [============================>.] - ETA: 0s - loss: 6.9949\n",
      "Epoch 00001: val_loss improved from inf to 1.10287, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 6.9498 - val_loss: 1.1029\n",
      "Epoch 2/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.7183\n",
      "Epoch 00002: val_loss improved from 1.10287 to 0.21369, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.7176 - val_loss: 0.2137\n",
      "Epoch 3/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.1656\n",
      "Epoch 00003: val_loss improved from 0.21369 to 0.17879, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3179 - val_loss: 0.1788\n",
      "Epoch 4/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00004: val_loss improved from 0.17879 to 0.15836, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1999 - val_loss: 0.1584\n",
      "Epoch 5/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1614\n",
      "Epoch 00005: val_loss improved from 0.15836 to 0.15080, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1609 - val_loss: 0.1508\n",
      "Epoch 6/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.1425\n",
      "Epoch 00006: val_loss improved from 0.15080 to 0.14355, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1425 - val_loss: 0.1436\n",
      "Epoch 7/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.1330\n",
      "Epoch 00007: val_loss improved from 0.14355 to 0.13881, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1341 - val_loss: 0.1388\n",
      "Epoch 8/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.1311\n",
      "Epoch 00008: val_loss improved from 0.13881 to 0.13565, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1306 - val_loss: 0.1357\n",
      "Epoch 9/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.1284\n",
      "Epoch 00009: val_loss improved from 0.13565 to 0.13412, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1287 - val_loss: 0.1341\n",
      "Epoch 10/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.1277\n",
      "Epoch 00010: val_loss improved from 0.13412 to 0.13313, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1279 - val_loss: 0.1331\n",
      "Epoch 11/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.1270\n",
      "Epoch 00011: val_loss improved from 0.13313 to 0.13284, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1272 - val_loss: 0.1328\n",
      "Epoch 12/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00012: val_loss improved from 0.13284 to 0.13160, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1266 - val_loss: 0.1316\n",
      "Epoch 13/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.1268\n",
      "Epoch 00013: val_loss improved from 0.13160 to 0.13116, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1263 - val_loss: 0.1312\n",
      "Epoch 14/100\n",
      "289/307 [===========================>..] - ETA: 0s - loss: 0.1267\n",
      "Epoch 00014: val_loss improved from 0.13116 to 0.13012, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1258 - val_loss: 0.1301\n",
      "Epoch 15/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00015: val_loss improved from 0.13012 to 0.12939, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1254 - val_loss: 0.1294\n",
      "Epoch 16/100\n",
      "291/307 [===========================>..] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00016: val_loss did not improve from 0.12939\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1253 - val_loss: 0.1294\n",
      "Epoch 17/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00017: val_loss did not improve from 0.12939\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1252 - val_loss: 0.1295\n",
      " ###3 fold : val acc1 0.572, acc3 0.964, mae 0.235###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/307 [============================>.] - ETA: 0s - loss: 7.1719\n",
      "Epoch 00001: val_loss improved from inf to 1.18980, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 7.0667 - val_loss: 1.1898\n",
      "Epoch 2/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.8791\n",
      "Epoch 00002: val_loss improved from 1.18980 to 0.22236, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.8430 - val_loss: 0.2224\n",
      "Epoch 3/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.3629\n",
      "Epoch 00003: val_loss improved from 0.22236 to 0.17084, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3512 - val_loss: 0.1708\n",
      "Epoch 4/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.2405\n",
      "Epoch 00004: val_loss improved from 0.17084 to 0.15715, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2366 - val_loss: 0.1572\n",
      "Epoch 5/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.1813\n",
      "Epoch 00005: val_loss improved from 0.15715 to 0.14927, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1804 - val_loss: 0.1493\n",
      "Epoch 6/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.1504\n",
      "Epoch 00006: val_loss improved from 0.14927 to 0.14307, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1499 - val_loss: 0.1431\n",
      "Epoch 7/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.1364\n",
      "Epoch 00007: val_loss improved from 0.14307 to 0.13808, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1370 - val_loss: 0.1381\n",
      "Epoch 8/100\n",
      "289/307 [===========================>..] - ETA: 0s - loss: 0.1309\n",
      "Epoch 00008: val_loss improved from 0.13808 to 0.13634, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1309 - val_loss: 0.1363\n",
      "Epoch 9/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.1282\n",
      "Epoch 00009: val_loss improved from 0.13634 to 0.13471, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1284 - val_loss: 0.1347\n",
      "Epoch 10/100\n",
      "291/307 [===========================>..] - ETA: 0s - loss: 0.1271\n",
      "Epoch 00010: val_loss improved from 0.13471 to 0.13241, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1276 - val_loss: 0.1324\n",
      "Epoch 11/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.1267\n",
      "Epoch 00011: val_loss did not improve from 0.13241\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1267 - val_loss: 0.1329\n",
      "Epoch 12/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.1264\n",
      "Epoch 00012: val_loss improved from 0.13241 to 0.13120, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1264 - val_loss: 0.1312\n",
      "Epoch 13/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00013: val_loss improved from 0.13120 to 0.13007, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1259 - val_loss: 0.1301\n",
      "Epoch 14/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00014: val_loss improved from 0.13007 to 0.12995, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1253 - val_loss: 0.1300\n",
      "Epoch 15/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00015: val_loss improved from 0.12995 to 0.12958, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1252 - val_loss: 0.1296\n",
      "Epoch 16/100\n",
      "289/307 [===========================>..] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00016: val_loss improved from 0.12958 to 0.12940, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1248 - val_loss: 0.1294\n",
      "Epoch 17/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00017: val_loss improved from 0.12940 to 0.12844, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1245 - val_loss: 0.1284\n",
      "Epoch 18/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00018: val_loss did not improve from 0.12844\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1247 - val_loss: 0.1284\n",
      "Epoch 19/100\n",
      "291/307 [===========================>..] - ETA: 0s - loss: 0.1226\n",
      "Epoch 00019: val_loss did not improve from 0.12844\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1245 - val_loss: 0.1293\n",
      " ###4 fold : val acc1 0.579, acc3 0.965, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/307 [============================>.] - ETA: 0s - loss: 6.5529\n",
      "Epoch 00001: val_loss improved from inf to 1.00793, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 6.5096 - val_loss: 1.0079\n",
      "Epoch 2/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.4021\n",
      "Epoch 00002: val_loss improved from 1.00793 to 0.19280, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.4010 - val_loss: 0.1928\n",
      "Epoch 3/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.1583\n",
      "Epoch 00003: val_loss improved from 0.19280 to 0.16332, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1577 - val_loss: 0.1633\n",
      "Epoch 4/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1452\n",
      "Epoch 00004: val_loss improved from 0.16332 to 0.15428, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1450 - val_loss: 0.1543\n",
      "Epoch 5/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.1401\n",
      "Epoch 00005: val_loss improved from 0.15428 to 0.14748, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1401 - val_loss: 0.1475\n",
      "Epoch 6/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.1366\n",
      "Epoch 00006: val_loss improved from 0.14748 to 0.14370, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1363 - val_loss: 0.1437\n",
      "Epoch 7/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1338\n",
      "Epoch 00007: val_loss improved from 0.14370 to 0.14002, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1337 - val_loss: 0.1400\n",
      "Epoch 8/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.1314\n",
      "Epoch 00008: val_loss improved from 0.14002 to 0.13789, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1318 - val_loss: 0.1379\n",
      "Epoch 9/100\n",
      "291/307 [===========================>..] - ETA: 0s - loss: 0.1303\n",
      "Epoch 00009: val_loss improved from 0.13789 to 0.13588, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1303 - val_loss: 0.1359\n",
      "Epoch 10/100\n",
      "289/307 [===========================>..] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00010: val_loss improved from 0.13588 to 0.13342, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1292 - val_loss: 0.1334\n",
      "Epoch 11/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.1278\n",
      "Epoch 00011: val_loss did not improve from 0.13342\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1281 - val_loss: 0.1347\n",
      "Epoch 12/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1282\n",
      "Epoch 00012: val_loss improved from 0.13342 to 0.13125, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1280 - val_loss: 0.1313\n",
      "Epoch 13/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1274\n",
      "Epoch 00013: val_loss improved from 0.13125 to 0.13043, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1272 - val_loss: 0.1304\n",
      "Epoch 14/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00014: val_loss improved from 0.13043 to 0.12945, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1269 - val_loss: 0.1294\n",
      "Epoch 15/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1264\n",
      "Epoch 00015: val_loss did not improve from 0.12945\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1265 - val_loss: 0.1300\n",
      "Epoch 16/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00016: val_loss improved from 0.12945 to 0.12934, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1260 - val_loss: 0.1293\n",
      "Epoch 17/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00017: val_loss improved from 0.12934 to 0.12924, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1260 - val_loss: 0.1292\n",
      "Epoch 18/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00018: val_loss did not improve from 0.12924\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1254 - val_loss: 0.1314\n",
      "Epoch 19/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00019: val_loss did not improve from 0.12924\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1254 - val_loss: 0.1310\n",
      " ###5 fold : val acc1 0.569, acc3 0.972, mae 0.237###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/307 [============================>.] - ETA: 0s - loss: 7.0237\n",
      "Epoch 00001: val_loss improved from inf to 1.24797, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 6.9223 - val_loss: 1.2480\n",
      "Epoch 2/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.6885\n",
      "Epoch 00002: val_loss improved from 1.24797 to 0.23637, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.6880 - val_loss: 0.2364\n",
      "Epoch 3/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.2210\n",
      "Epoch 00003: val_loss improved from 0.23637 to 0.17125, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2188 - val_loss: 0.1713\n",
      "Epoch 4/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1659\n",
      "Epoch 00004: val_loss improved from 0.17125 to 0.15763, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1652 - val_loss: 0.1576\n",
      "Epoch 5/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.1471\n",
      "Epoch 00005: val_loss improved from 0.15763 to 0.14868, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1468 - val_loss: 0.1487\n",
      "Epoch 6/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.1354\n",
      "Epoch 00006: val_loss improved from 0.14868 to 0.14216, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1356 - val_loss: 0.1422\n",
      "Epoch 7/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.1311\n",
      "Epoch 00007: val_loss improved from 0.14216 to 0.13814, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1309 - val_loss: 0.1381\n",
      "Epoch 8/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.1289\n",
      "Epoch 00008: val_loss improved from 0.13814 to 0.13539, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1288 - val_loss: 0.1354\n",
      "Epoch 9/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00009: val_loss improved from 0.13539 to 0.13429, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1275 - val_loss: 0.1343\n",
      "Epoch 10/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00010: val_loss improved from 0.13429 to 0.13252, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1266 - val_loss: 0.1325\n",
      "Epoch 11/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00011: val_loss did not improve from 0.13252\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1259 - val_loss: 0.1337\n",
      "Epoch 12/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00012: val_loss improved from 0.13252 to 0.13094, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1256 - val_loss: 0.1309\n",
      "Epoch 13/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00013: val_loss improved from 0.13094 to 0.13047, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1252 - val_loss: 0.1305\n",
      "Epoch 14/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00014: val_loss improved from 0.13047 to 0.12975, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1246 - val_loss: 0.1298\n",
      "Epoch 15/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00015: val_loss did not improve from 0.12975\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1245 - val_loss: 0.1303\n",
      "Epoch 16/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.1264\n",
      "Epoch 00016: val_loss did not improve from 0.12975\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1263 - val_loss: 0.1315\n",
      " ###6 fold : val acc1 0.577, acc3 0.962, mae 0.235###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/307 [============================>.] - ETA: 0s - loss: 7.0435\n",
      "Epoch 00001: val_loss improved from inf to 1.24352, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 6.9046 - val_loss: 1.2435\n",
      "Epoch 2/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.6983\n",
      "Epoch 00002: val_loss improved from 1.24352 to 0.23784, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.6929 - val_loss: 0.2378\n",
      "Epoch 3/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.2279\n",
      "Epoch 00003: val_loss improved from 0.23784 to 0.17069, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2265 - val_loss: 0.1707\n",
      "Epoch 4/100\n",
      "287/307 [===========================>..] - ETA: 0s - loss: 0.1696\n",
      "Epoch 00004: val_loss improved from 0.17069 to 0.15706, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1690 - val_loss: 0.1571\n",
      "Epoch 5/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.1491\n",
      "Epoch 00005: val_loss improved from 0.15706 to 0.14881, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1490 - val_loss: 0.1488\n",
      "Epoch 6/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1365\n",
      "Epoch 00006: val_loss improved from 0.14881 to 0.14231, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1367 - val_loss: 0.1423\n",
      "Epoch 7/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.1309\n",
      "Epoch 00007: val_loss improved from 0.14231 to 0.13817, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1319 - val_loss: 0.1382\n",
      "Epoch 8/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.1295\n",
      "Epoch 00008: val_loss improved from 0.13817 to 0.13573, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1295 - val_loss: 0.1357\n",
      "Epoch 9/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.1282\n",
      "Epoch 00009: val_loss improved from 0.13573 to 0.13492, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1281 - val_loss: 0.1349\n",
      "Epoch 10/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.1270\n",
      "Epoch 00010: val_loss improved from 0.13492 to 0.13334, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1274 - val_loss: 0.1333\n",
      "Epoch 11/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1264\n",
      "Epoch 00011: val_loss did not improve from 0.13334\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1266 - val_loss: 0.1336\n",
      "Epoch 12/100\n",
      "291/307 [===========================>..] - ETA: 0s - loss: 0.1261\n",
      "Epoch 00012: val_loss improved from 0.13334 to 0.13174, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1262 - val_loss: 0.1317\n",
      "Epoch 13/100\n",
      "291/307 [===========================>..] - ETA: 0s - loss: 0.1266\n",
      "Epoch 00013: val_loss improved from 0.13174 to 0.13108, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1257 - val_loss: 0.1311\n",
      "Epoch 14/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00014: val_loss improved from 0.13108 to 0.13027, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1255 - val_loss: 0.1303\n",
      "Epoch 15/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1279\n",
      "Epoch 00015: val_loss did not improve from 0.13027\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1279 - val_loss: 0.1317\n",
      "Epoch 16/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.1351\n",
      "Epoch 00016: val_loss did not improve from 0.13027\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1348 - val_loss: 0.1318\n",
      " ###7 fold : val acc1 0.587, acc3 0.963, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/307 [===========================>..] - ETA: 0s - loss: 7.2728\n",
      "Epoch 00001: val_loss improved from inf to 1.25019, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 6.9161 - val_loss: 1.2502\n",
      "Epoch 2/100\n",
      "289/307 [===========================>..] - ETA: 0s - loss: 0.5269\n",
      "Epoch 00002: val_loss improved from 1.25019 to 0.23207, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.6893 - val_loss: 0.2321\n",
      "Epoch 3/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.2269\n",
      "Epoch 00003: val_loss improved from 0.23207 to 0.16192, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2240 - val_loss: 0.1619\n",
      "Epoch 4/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.1684\n",
      "Epoch 00004: val_loss improved from 0.16192 to 0.14960, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1678 - val_loss: 0.1496\n",
      "Epoch 5/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.1488\n",
      "Epoch 00005: val_loss improved from 0.14960 to 0.14321, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1487 - val_loss: 0.1432\n",
      "Epoch 6/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.1364\n",
      "Epoch 00006: val_loss improved from 0.14321 to 0.13750, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1364 - val_loss: 0.1375\n",
      "Epoch 7/100\n",
      "289/307 [===========================>..] - ETA: 0s - loss: 0.1304\n",
      "Epoch 00007: val_loss improved from 0.13750 to 0.13446, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1318 - val_loss: 0.1345\n",
      "Epoch 8/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.1290\n",
      "Epoch 00008: val_loss improved from 0.13446 to 0.13217, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1292 - val_loss: 0.1322\n",
      "Epoch 9/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.1278\n",
      "Epoch 00009: val_loss improved from 0.13217 to 0.13125, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1278 - val_loss: 0.1313\n",
      "Epoch 10/100\n",
      "291/307 [===========================>..] - ETA: 0s - loss: 0.1267\n",
      "Epoch 00010: val_loss improved from 0.13125 to 0.13047, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1270 - val_loss: 0.1305\n",
      "Epoch 11/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00011: val_loss improved from 0.13047 to 0.13027, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1262 - val_loss: 0.1303\n",
      "Epoch 12/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00012: val_loss improved from 0.13027 to 0.12920, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1258 - val_loss: 0.1292\n",
      "Epoch 13/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00013: val_loss improved from 0.12920 to 0.12872, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1252 - val_loss: 0.1287\n",
      "Epoch 14/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00014: val_loss improved from 0.12872 to 0.12765, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1250 - val_loss: 0.1276\n",
      "Epoch 15/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.1259\n",
      "Epoch 00015: val_loss did not improve from 0.12765\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1262 - val_loss: 0.1291\n",
      "Epoch 16/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00016: val_loss did not improve from 0.12765\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1281 - val_loss: 0.1303\n",
      " ###8 fold : val acc1 0.569, acc3 0.960, mae 0.240###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/307 [===========================>..] - ETA: 0s - loss: 7.1122\n",
      "Epoch 00001: val_loss improved from inf to 1.28353, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 6.9161 - val_loss: 1.2835\n",
      "Epoch 2/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.5234\n",
      "Epoch 00002: val_loss improved from 1.28353 to 0.23382, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.6893 - val_loss: 0.2338\n",
      "Epoch 3/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.2267\n",
      "Epoch 00003: val_loss improved from 0.23382 to 0.16155, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2240 - val_loss: 0.1616\n",
      "Epoch 4/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.1688\n",
      "Epoch 00004: val_loss improved from 0.16155 to 0.15003, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1678 - val_loss: 0.1500\n",
      "Epoch 5/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1487\n",
      "Epoch 00005: val_loss improved from 0.15003 to 0.14356, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1487 - val_loss: 0.1436\n",
      "Epoch 6/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.1361\n",
      "Epoch 00006: val_loss improved from 0.14356 to 0.13799, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1364 - val_loss: 0.1380\n",
      "Epoch 7/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.1310\n",
      "Epoch 00007: val_loss improved from 0.13799 to 0.13420, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1318 - val_loss: 0.1342\n",
      "Epoch 8/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.1291\n",
      "Epoch 00008: val_loss improved from 0.13420 to 0.13241, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1292 - val_loss: 0.1324\n",
      "Epoch 9/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.1275\n",
      "Epoch 00009: val_loss improved from 0.13241 to 0.13188, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1278 - val_loss: 0.1319\n",
      "Epoch 10/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00010: val_loss improved from 0.13188 to 0.13041, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1270 - val_loss: 0.1304\n",
      "Epoch 11/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.1261\n",
      "Epoch 00011: val_loss improved from 0.13041 to 0.13035, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1262 - val_loss: 0.1303\n",
      "Epoch 12/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00012: val_loss improved from 0.13035 to 0.12881, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1258 - val_loss: 0.1288\n",
      "Epoch 13/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.1264\n",
      "Epoch 00013: val_loss improved from 0.12881 to 0.12818, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1252 - val_loss: 0.1282\n",
      "Epoch 14/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.1251\n",
      "Epoch 00014: val_loss improved from 0.12818 to 0.12780, saving model to result/size/DNN_size_both_y/batch64,dnodes16_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1250 - val_loss: 0.1278\n",
      "Epoch 15/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00015: val_loss did not improve from 0.12780\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1262 - val_loss: 0.1289\n",
      "Epoch 16/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.1283\n",
      "Epoch 00016: val_loss did not improve from 0.12780\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1281 - val_loss: 0.1303\n",
      " ###9 fold : val acc1 0.577, acc3 0.962, mae 0.235###\n",
      "acc10.577_acc30.964\n",
      "random search 8/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/154 [==========================>...] - ETA: 0s - loss: 5.3475\n",
      "Epoch 00001: val_loss improved from inf to 0.30780, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 5.0099 - val_loss: 0.3078\n",
      "Epoch 2/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 2.3238\n",
      "Epoch 00002: val_loss improved from 0.30780 to 0.23908, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.2447 - val_loss: 0.2391\n",
      "Epoch 3/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.9160\n",
      "Epoch 00003: val_loss did not improve from 0.23908\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.2420 - val_loss: 0.2794\n",
      "Epoch 4/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 1.2272\n",
      "Epoch 00004: val_loss improved from 0.23908 to 0.16712, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.2194 - val_loss: 0.1671\n",
      "Epoch 5/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.8637\n",
      "Epoch 00005: val_loss did not improve from 0.16712\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8637 - val_loss: 0.1864\n",
      "Epoch 6/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.8536\n",
      "Epoch 00006: val_loss did not improve from 0.16712\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8515 - val_loss: 0.1685\n",
      " ###0 fold : val acc1 0.532, acc3 0.935, mae 0.273###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/154 [============================>.] - ETA: 0s - loss: 4.9915\n",
      "Epoch 00001: val_loss improved from inf to 0.31039, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 4.9862 - val_loss: 0.3104\n",
      "Epoch 2/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 2.1841\n",
      "Epoch 00002: val_loss improved from 0.31039 to 0.23571, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.1546 - val_loss: 0.2357\n",
      "Epoch 3/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.9201\n",
      "Epoch 00003: val_loss did not improve from 0.23571\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.2232 - val_loss: 0.2809\n",
      "Epoch 4/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 1.2112\n",
      "Epoch 00004: val_loss improved from 0.23571 to 0.16973, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.2095 - val_loss: 0.1697\n",
      "Epoch 5/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.8803\n",
      "Epoch 00005: val_loss did not improve from 0.16973\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.8760 - val_loss: 0.1850\n",
      "Epoch 6/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.8584\n",
      "Epoch 00006: val_loss improved from 0.16973 to 0.16678, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8573 - val_loss: 0.1668\n",
      "Epoch 7/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.8209\n",
      "Epoch 00007: val_loss improved from 0.16678 to 0.16573, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8209 - val_loss: 0.1657\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.8012\n",
      "Epoch 00008: val_loss did not improve from 0.16573\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8012 - val_loss: 0.1714\n",
      "Epoch 9/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.8025\n",
      "Epoch 00009: val_loss improved from 0.16573 to 0.16162, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8025 - val_loss: 0.1616\n",
      "Epoch 10/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.7623\n",
      "Epoch 00010: val_loss improved from 0.16162 to 0.16133, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7584 - val_loss: 0.1613\n",
      "Epoch 11/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.7685\n",
      "Epoch 00011: val_loss improved from 0.16133 to 0.15828, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7683 - val_loss: 0.1583\n",
      "Epoch 12/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.7177\n",
      "Epoch 00012: val_loss improved from 0.15828 to 0.15020, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7177 - val_loss: 0.1502\n",
      "Epoch 13/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.7195\n",
      "Epoch 00013: val_loss did not improve from 0.15020\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7187 - val_loss: 0.1800\n",
      "Epoch 14/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.7044\n",
      "Epoch 00014: val_loss did not improve from 0.15020\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7043 - val_loss: 0.1548\n",
      " ###1 fold : val acc1 0.528, acc3 0.948, mae 0.266###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/154 [===========================>..] - ETA: 0s - loss: 5.2176\n",
      "Epoch 00001: val_loss improved from inf to 0.30886, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 4.9690 - val_loss: 0.3089\n",
      "Epoch 2/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 2.2107\n",
      "Epoch 00002: val_loss improved from 0.30886 to 0.23844, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.1556 - val_loss: 0.2384\n",
      "Epoch 3/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.9183\n",
      "Epoch 00003: val_loss did not improve from 0.23844\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.2177 - val_loss: 0.2708\n",
      "Epoch 4/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 1.1534\n",
      "Epoch 00004: val_loss improved from 0.23844 to 0.17231, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1530 - val_loss: 0.1723\n",
      "Epoch 5/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.8839\n",
      "Epoch 00005: val_loss did not improve from 0.17231\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8805 - val_loss: 0.1807\n",
      "Epoch 6/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.8475\n",
      "Epoch 00006: val_loss improved from 0.17231 to 0.16195, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8470 - val_loss: 0.1620\n",
      "Epoch 7/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.8022\n",
      "Epoch 00007: val_loss did not improve from 0.16195\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8047 - val_loss: 0.1654\n",
      "Epoch 8/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.8032\n",
      "Epoch 00008: val_loss did not improve from 0.16195\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.7971 - val_loss: 0.1740\n",
      " ###2 fold : val acc1 0.515, acc3 0.946, mae 0.274###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/154 [===========================>..] - ETA: 0s - loss: 5.1152\n",
      "Epoch 00001: val_loss improved from inf to 0.30299, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 4.9011 - val_loss: 0.3030\n",
      "Epoch 2/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 2.1294\n",
      "Epoch 00002: val_loss improved from 0.30299 to 0.23414, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.1129 - val_loss: 0.2341\n",
      "Epoch 3/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.9196\n",
      "Epoch 00003: val_loss did not improve from 0.23414\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.2249 - val_loss: 0.2711\n",
      "Epoch 4/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 1.1467\n",
      "Epoch 00004: val_loss improved from 0.23414 to 0.17659, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1371 - val_loss: 0.1766\n",
      "Epoch 5/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.8779\n",
      "Epoch 00005: val_loss did not improve from 0.17659\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8741 - val_loss: 0.1816\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.8468\n",
      "Epoch 00006: val_loss improved from 0.17659 to 0.16799, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8466 - val_loss: 0.1680\n",
      "Epoch 7/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.8009\n",
      "Epoch 00007: val_loss improved from 0.16799 to 0.16465, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8024 - val_loss: 0.1647\n",
      "Epoch 8/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.8004\n",
      "Epoch 00008: val_loss did not improve from 0.16465\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7970 - val_loss: 0.1777\n",
      "Epoch 9/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.7981\n",
      "Epoch 00009: val_loss improved from 0.16465 to 0.15964, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7988 - val_loss: 0.1596\n",
      "Epoch 10/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.7552\n",
      "Epoch 00010: val_loss did not improve from 0.15964\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7629 - val_loss: 0.1777\n",
      "Epoch 11/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.7689\n",
      "Epoch 00011: val_loss improved from 0.15964 to 0.15441, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7668 - val_loss: 0.1544\n",
      "Epoch 12/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.7210\n",
      "Epoch 00012: val_loss improved from 0.15441 to 0.15311, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7247 - val_loss: 0.1531\n",
      "Epoch 13/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.7144\n",
      "Epoch 00013: val_loss did not improve from 0.15311\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7123 - val_loss: 0.1761\n",
      "Epoch 14/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.7271\n",
      "Epoch 00014: val_loss did not improve from 0.15311\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7257 - val_loss: 0.1540\n",
      " ###3 fold : val acc1 0.529, acc3 0.954, mae 0.262###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/154 [============================>.] - ETA: 0s - loss: 5.4930\n",
      "Epoch 00001: val_loss improved from inf to 0.33152, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 5.4877 - val_loss: 0.3315\n",
      "Epoch 2/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 1.3533\n",
      "Epoch 00002: val_loss improved from 0.33152 to 0.22782, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.3425 - val_loss: 0.2278\n",
      "Epoch 3/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.3634\n",
      "Epoch 00003: val_loss improved from 0.22782 to 0.20326, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.3310 - val_loss: 0.2033\n",
      "Epoch 4/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.9913\n",
      "Epoch 00004: val_loss improved from 0.20326 to 0.18097, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9911 - val_loss: 0.1810\n",
      "Epoch 5/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.8531\n",
      "Epoch 00005: val_loss improved from 0.18097 to 0.16938, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8508 - val_loss: 0.1694\n",
      "Epoch 6/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.8830\n",
      "Epoch 00006: val_loss did not improve from 0.16938\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8763 - val_loss: 0.1830\n",
      "Epoch 7/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.8938\n",
      "Epoch 00007: val_loss improved from 0.16938 to 0.15784, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8912 - val_loss: 0.1578\n",
      "Epoch 8/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.8650\n",
      "Epoch 00008: val_loss did not improve from 0.15784\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8652 - val_loss: 0.1715\n",
      "Epoch 9/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.7941\n",
      "Epoch 00009: val_loss did not improve from 0.15784\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7940 - val_loss: 0.1614\n",
      " ###4 fold : val acc1 0.538, acc3 0.948, mae 0.261###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/154 [===========================>..] - ETA: 0s - loss: 4.3405\n",
      "Epoch 00001: val_loss improved from inf to 0.31728, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 4.1688 - val_loss: 0.3173\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 1.0057\n",
      "Epoch 00002: val_loss improved from 0.31728 to 0.23887, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0057 - val_loss: 0.2389\n",
      "Epoch 3/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.9314\n",
      "Epoch 00003: val_loss improved from 0.23887 to 0.20231, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9291 - val_loss: 0.2023\n",
      "Epoch 4/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.8800\n",
      "Epoch 00004: val_loss did not improve from 0.20231\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8791 - val_loss: 0.2049\n",
      "Epoch 5/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.8420\n",
      "Epoch 00005: val_loss improved from 0.20231 to 0.17648, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8393 - val_loss: 0.1765\n",
      "Epoch 6/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.8431\n",
      "Epoch 00006: val_loss did not improve from 0.17648\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8417 - val_loss: 0.1792\n",
      "Epoch 7/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.7952\n",
      "Epoch 00007: val_loss did not improve from 0.17648\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7963 - val_loss: 0.1883\n",
      " ###5 fold : val acc1 0.509, acc3 0.945, mae 0.297###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/154 [==========================>...] - ETA: 0s - loss: 7.2282\n",
      "Epoch 00001: val_loss improved from inf to 0.38208, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 6.8363 - val_loss: 0.3821\n",
      "Epoch 2/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.1178\n",
      "Epoch 00002: val_loss improved from 0.38208 to 0.31796, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.7019 - val_loss: 0.3180\n",
      "Epoch 3/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.1439\n",
      "Epoch 00003: val_loss improved from 0.31796 to 0.21867, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1337 - val_loss: 0.2187\n",
      "Epoch 4/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 1.4695\n",
      "Epoch 00004: val_loss improved from 0.21867 to 0.18819, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4686 - val_loss: 0.1882\n",
      "Epoch 5/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.9041\n",
      "Epoch 00005: val_loss improved from 0.18819 to 0.17758, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9041 - val_loss: 0.1776\n",
      "Epoch 6/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.9569\n",
      "Epoch 00006: val_loss improved from 0.17758 to 0.17467, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9558 - val_loss: 0.1747\n",
      "Epoch 7/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.9522\n",
      "Epoch 00007: val_loss improved from 0.17467 to 0.14777, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9518 - val_loss: 0.1478\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.8027\n",
      "Epoch 00008: val_loss did not improve from 0.14777\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8027 - val_loss: 0.1704\n",
      "Epoch 9/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.8426\n",
      "Epoch 00009: val_loss did not improve from 0.14777\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8408 - val_loss: 0.1706\n",
      " ###6 fold : val acc1 0.560, acc3 0.951, mae 0.249###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/154 [===========================>..] - ETA: 0s - loss: 7.1826\n",
      "Epoch 00001: val_loss improved from inf to 0.38844, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 6.8633 - val_loss: 0.3884\n",
      "Epoch 2/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 1.7948\n",
      "Epoch 00002: val_loss improved from 0.38844 to 0.34870, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.7938 - val_loss: 0.3487\n",
      "Epoch 3/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 1.1612\n",
      "Epoch 00003: val_loss improved from 0.34870 to 0.21746, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1591 - val_loss: 0.2175\n",
      "Epoch 4/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 1.5316\n",
      "Epoch 00004: val_loss improved from 0.21746 to 0.18923, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.5091 - val_loss: 0.1892\n",
      "Epoch 5/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.9271\n",
      "Epoch 00005: val_loss did not improve from 0.18923\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9212 - val_loss: 0.1902\n",
      "Epoch 6/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 1.0093\n",
      "Epoch 00006: val_loss improved from 0.18923 to 0.17616, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0022 - val_loss: 0.1762\n",
      "Epoch 7/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.9954\n",
      "Epoch 00007: val_loss improved from 0.17616 to 0.15204, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9872 - val_loss: 0.1520\n",
      "Epoch 8/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.8043\n",
      "Epoch 00008: val_loss did not improve from 0.15204\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8016 - val_loss: 0.1679\n",
      "Epoch 9/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.8479\n",
      "Epoch 00009: val_loss did not improve from 0.15204\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8466 - val_loss: 0.1712\n",
      " ###7 fold : val acc1 0.548, acc3 0.950, mae 0.254###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/154 [==========================>...] - ETA: 0s - loss: 7.4346\n",
      "Epoch 00001: val_loss improved from inf to 0.43827, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 6.8584 - val_loss: 0.4383\n",
      "Epoch 2/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 1.8203\n",
      "Epoch 00002: val_loss improved from 0.43827 to 0.38015, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.7898 - val_loss: 0.3802\n",
      "Epoch 3/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 1.1628\n",
      "Epoch 00003: val_loss improved from 0.38015 to 0.23049, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1518 - val_loss: 0.2305\n",
      "Epoch 4/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 1.4871\n",
      "Epoch 00004: val_loss improved from 0.23049 to 0.18761, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4709 - val_loss: 0.1876\n",
      "Epoch 5/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.8987\n",
      "Epoch 00005: val_loss did not improve from 0.18761\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8943 - val_loss: 0.1887\n",
      "Epoch 6/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 1.0152\n",
      "Epoch 00006: val_loss improved from 0.18761 to 0.17203, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0076 - val_loss: 0.1720\n",
      "Epoch 7/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.9337\n",
      "Epoch 00007: val_loss improved from 0.17203 to 0.14678, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9300 - val_loss: 0.1468\n",
      "Epoch 8/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.8102\n",
      "Epoch 00008: val_loss did not improve from 0.14678\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8061 - val_loss: 0.1648\n",
      "Epoch 9/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.8831\n",
      "Epoch 00009: val_loss did not improve from 0.14678\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8789 - val_loss: 0.1647\n",
      " ###8 fold : val acc1 0.524, acc3 0.941, mae 0.271###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/154 [=========================>....] - ETA: 0s - loss: 7.5729\n",
      "Epoch 00001: val_loss improved from inf to 0.41377, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 6.8584 - val_loss: 0.4138\n",
      "Epoch 2/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 1.7908\n",
      "Epoch 00002: val_loss improved from 0.41377 to 0.36460, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.7898 - val_loss: 0.3646\n",
      "Epoch 3/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.1761\n",
      "Epoch 00003: val_loss improved from 0.36460 to 0.22004, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1518 - val_loss: 0.2200\n",
      "Epoch 4/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 1.5220\n",
      "Epoch 00004: val_loss improved from 0.22004 to 0.18551, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4709 - val_loss: 0.1855\n",
      "Epoch 5/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.9009\n",
      "Epoch 00005: val_loss did not improve from 0.18551\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8943 - val_loss: 0.1886\n",
      "Epoch 6/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.0220\n",
      "Epoch 00006: val_loss improved from 0.18551 to 0.17449, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0076 - val_loss: 0.1745\n",
      "Epoch 7/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.9341\n",
      "Epoch 00007: val_loss improved from 0.17449 to 0.14791, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0,dnodes64_dropout0.5,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9300 - val_loss: 0.1479\n",
      "Epoch 8/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.8115\n",
      "Epoch 00008: val_loss did not improve from 0.14791\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8061 - val_loss: 0.1667\n",
      "Epoch 9/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.8860\n",
      "Epoch 00009: val_loss did not improve from 0.14791\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.8789 - val_loss: 0.1671\n",
      " ###9 fold : val acc1 0.542, acc3 0.950, mae 0.258###\n",
      "acc10.533_acc30.947\n",
      "random search 9/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/39 [==========================>...] - ETA: 0s - loss: 10.4979\n",
      "Epoch 00001: val_loss improved from inf to 1.63418, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_0.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 10.0140 - val_loss: 1.6342\n",
      "Epoch 2/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 3.7496\n",
      "Epoch 00002: val_loss improved from 1.63418 to 0.48193, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.7313 - val_loss: 0.4819\n",
      "Epoch 3/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.9397\n",
      "Epoch 00003: val_loss improved from 0.48193 to 0.31949, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3543 - val_loss: 0.3195\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.3994\n",
      "Epoch 00004: val_loss did not improve from 0.31949\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3550 - val_loss: 0.3279\n",
      "Epoch 5/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 0.9113\n",
      "Epoch 00005: val_loss improved from 0.31949 to 0.21578, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.8701 - val_loss: 0.2158\n",
      "Epoch 6/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 0.5765\n",
      "Epoch 00006: val_loss improved from 0.21578 to 0.20390, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.5589 - val_loss: 0.2039\n",
      "Epoch 7/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.6923\n",
      "Epoch 00007: val_loss improved from 0.20390 to 0.17918, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.6747 - val_loss: 0.1792\n",
      "Epoch 8/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 0.4005\n",
      "Epoch 00008: val_loss did not improve from 0.17918\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.5707 - val_loss: 0.2146\n",
      "Epoch 9/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.4624\n",
      "Epoch 00009: val_loss did not improve from 0.17918\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4585 - val_loss: 0.1986\n",
      " ###0 fold : val acc1 0.479, acc3 0.928, mae 0.304###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/39 [============================>.] - ETA: 0s - loss: 10.0944\n",
      "Epoch 00001: val_loss improved from inf to 1.63740, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_1.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 10.0351 - val_loss: 1.6374\n",
      "Epoch 2/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 3.7093\n",
      "Epoch 00002: val_loss improved from 1.63740 to 0.48213, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.6902 - val_loss: 0.4821\n",
      "Epoch 3/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.9389\n",
      "Epoch 00003: val_loss improved from 0.48213 to 0.32446, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3607 - val_loss: 0.3245\n",
      "Epoch 4/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.4135\n",
      "Epoch 00004: val_loss improved from 0.32446 to 0.31415, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3496 - val_loss: 0.3141\n",
      "Epoch 5/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 0.8936\n",
      "Epoch 00005: val_loss improved from 0.31415 to 0.21196, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.8542 - val_loss: 0.2120\n",
      "Epoch 6/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 0.5773\n",
      "Epoch 00006: val_loss improved from 0.21196 to 0.20830, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.5604 - val_loss: 0.2083\n",
      "Epoch 7/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 0.7301\n",
      "Epoch 00007: val_loss improved from 0.20830 to 0.18324, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.6767 - val_loss: 0.1832\n",
      "Epoch 8/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.5743\n",
      "Epoch 00008: val_loss did not improve from 0.18324\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.5607 - val_loss: 0.2112\n",
      "Epoch 9/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 0.4523\n",
      "Epoch 00009: val_loss did not improve from 0.18324\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4437 - val_loss: 0.1992\n",
      " ###1 fold : val acc1 0.472, acc3 0.938, mae 0.300###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/39 [===========================>..] - ETA: 0s - loss: 10.3085\n",
      "Epoch 00001: val_loss improved from inf to 1.63485, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_2.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 10.0313 - val_loss: 1.6349\n",
      "Epoch 2/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 3.8420\n",
      "Epoch 00002: val_loss improved from 1.63485 to 0.48422, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.6836 - val_loss: 0.4842\n",
      "Epoch 3/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.9566\n",
      "Epoch 00003: val_loss improved from 0.48422 to 0.32602, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3685 - val_loss: 0.3260\n",
      "Epoch 4/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.3658\n",
      "Epoch 00004: val_loss improved from 0.32602 to 0.31623, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3408 - val_loss: 0.3162\n",
      "Epoch 5/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.8610\n",
      "Epoch 00005: val_loss improved from 0.31623 to 0.21116, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.8414 - val_loss: 0.2112\n",
      "Epoch 6/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.5714\n",
      "Epoch 00006: val_loss improved from 0.21116 to 0.21053, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.5590 - val_loss: 0.2105\n",
      "Epoch 7/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.6874\n",
      "Epoch 00007: val_loss improved from 0.21053 to 0.17781, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.6630 - val_loss: 0.1778\n",
      "Epoch 8/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 0.4018\n",
      "Epoch 00008: val_loss did not improve from 0.17781\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.5613 - val_loss: 0.2130\n",
      "Epoch 9/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 0.4516\n",
      "Epoch 00009: val_loss did not improve from 0.17781\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.4394 - val_loss: 0.2023\n",
      " ###2 fold : val acc1 0.482, acc3 0.938, mae 0.294###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/39 [============================>.] - ETA: 0s - loss: 10.0591\n",
      "Epoch 00001: val_loss improved from inf to 1.62865, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 9.9990 - val_loss: 1.6287\n",
      "Epoch 2/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 3.6792\n",
      "Epoch 00002: val_loss improved from 1.62865 to 0.48301, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.6608 - val_loss: 0.4830\n",
      "Epoch 3/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.3592\n",
      "Epoch 00003: val_loss improved from 0.48301 to 0.32583, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3544 - val_loss: 0.3258\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.3732\n",
      "Epoch 00004: val_loss improved from 0.32583 to 0.31219, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3316 - val_loss: 0.3122\n",
      "Epoch 5/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.8857\n",
      "Epoch 00005: val_loss improved from 0.31219 to 0.21165, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.8542 - val_loss: 0.2117\n",
      "Epoch 6/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.5758\n",
      "Epoch 00006: val_loss improved from 0.21165 to 0.20446, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.5626 - val_loss: 0.2045\n",
      "Epoch 7/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.6732\n",
      "Epoch 00007: val_loss improved from 0.20446 to 0.17976, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.6569 - val_loss: 0.1798\n",
      "Epoch 8/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.3960\n",
      "Epoch 00008: val_loss did not improve from 0.17976\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.5623 - val_loss: 0.2168\n",
      "Epoch 9/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 0.4534\n",
      "Epoch 00009: val_loss did not improve from 0.17976\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4473 - val_loss: 0.2041\n",
      " ###3 fold : val acc1 0.491, acc3 0.943, mae 0.287###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 9.2731 \n",
      "Epoch 00001: val_loss improved from inf to 1.49234, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_4.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 8.8607 - val_loss: 1.4923\n",
      "Epoch 2/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.9882\n",
      "Epoch 00002: val_loss improved from 1.49234 to 0.53591, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.9079 - val_loss: 0.5359\n",
      "Epoch 3/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.0164\n",
      "Epoch 00003: val_loss improved from 0.53591 to 0.27504, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.0133 - val_loss: 0.2750\n",
      "Epoch 4/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.2441\n",
      "Epoch 00004: val_loss improved from 0.27504 to 0.21549, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.1933 - val_loss: 0.2155\n",
      "Epoch 5/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 0.9297\n",
      "Epoch 00005: val_loss improved from 0.21549 to 0.19569, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.8805 - val_loss: 0.1957\n",
      "Epoch 6/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.6465\n",
      "Epoch 00006: val_loss did not improve from 0.19569\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.6394 - val_loss: 0.2130\n",
      "Epoch 7/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.7143\n",
      "Epoch 00007: val_loss did not improve from 0.19569\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.6877 - val_loss: 0.2165\n",
      " ###4 fold : val acc1 0.483, acc3 0.929, mae 0.301###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/39 [==========================>...] - ETA: 0s - loss: 8.2531 \n",
      "Epoch 00001: val_loss improved from inf to 1.57324, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_5.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 7.9074 - val_loss: 1.5732\n",
      "Epoch 2/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.5695\n",
      "Epoch 00002: val_loss improved from 1.57324 to 0.48559, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.5379 - val_loss: 0.4856\n",
      "Epoch 3/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 0.8953\n",
      "Epoch 00003: val_loss improved from 0.48559 to 0.28092, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.8787 - val_loss: 0.2809\n",
      "Epoch 4/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.6270\n",
      "Epoch 00004: val_loss improved from 0.28092 to 0.24621, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.6238 - val_loss: 0.2462\n",
      "Epoch 5/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.4988\n",
      "Epoch 00005: val_loss improved from 0.24621 to 0.24583, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.4923 - val_loss: 0.2458\n",
      "Epoch 6/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.4351\n",
      "Epoch 00006: val_loss improved from 0.24583 to 0.22992, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.4327 - val_loss: 0.2299\n",
      "Epoch 7/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 0.3831\n",
      "Epoch 00007: val_loss improved from 0.22992 to 0.20350, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.3812 - val_loss: 0.2035\n",
      "Epoch 8/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 0.3643\n",
      "Epoch 00008: val_loss did not improve from 0.20350\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3614 - val_loss: 0.2070\n",
      "Epoch 9/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 0.3484\n",
      "Epoch 00009: val_loss did not improve from 0.20350\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.3458 - val_loss: 0.2053\n",
      " ###5 fold : val acc1 0.488, acc3 0.931, mae 0.317###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/39 [===========================>..] - ETA: 0s - loss: 8.3501 \n",
      "Epoch 00001: val_loss improved from inf to 1.47594, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_6.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 8.1482 - val_loss: 1.4759\n",
      "Epoch 2/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 2.3929\n",
      "Epoch 00002: val_loss improved from 1.47594 to 0.62340, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.3929 - val_loss: 0.6234\n",
      "Epoch 3/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.4314\n",
      "Epoch 00003: val_loss improved from 0.62340 to 0.25939, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4257 - val_loss: 0.2594\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.9240\n",
      "Epoch 00004: val_loss improved from 0.25939 to 0.25146, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.9077 - val_loss: 0.2515\n",
      "Epoch 5/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.8092\n",
      "Epoch 00005: val_loss improved from 0.25146 to 0.19475, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.7813 - val_loss: 0.1948\n",
      "Epoch 6/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.5353\n",
      "Epoch 00006: val_loss did not improve from 0.19475\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.5320 - val_loss: 0.2086\n",
      "Epoch 7/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.4364\n",
      "Epoch 00007: val_loss did not improve from 0.19475\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4355 - val_loss: 0.1992\n",
      " ###6 fold : val acc1 0.492, acc3 0.931, mae 0.295###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/39 [============================>.] - ETA: 0s - loss: 8.1528 \n",
      "Epoch 00001: val_loss improved from inf to 1.48401, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_7.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 8.1078 - val_loss: 1.4840\n",
      "Epoch 2/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 2.4625\n",
      "Epoch 00002: val_loss improved from 1.48401 to 0.63706, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.4625 - val_loss: 0.6371\n",
      "Epoch 3/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.5766\n",
      "Epoch 00003: val_loss improved from 0.63706 to 0.26483, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.5493 - val_loss: 0.2648\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.9809\n",
      "Epoch 00004: val_loss did not improve from 0.26483\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.9613 - val_loss: 0.2663\n",
      "Epoch 5/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.8514\n",
      "Epoch 00005: val_loss improved from 0.26483 to 0.18713, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.8198 - val_loss: 0.1871\n",
      "Epoch 6/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 0.5465\n",
      "Epoch 00006: val_loss did not improve from 0.18713\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.5357 - val_loss: 0.2049\n",
      "Epoch 7/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 0.4426\n",
      "Epoch 00007: val_loss did not improve from 0.18713\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4363 - val_loss: 0.1949\n",
      " ###7 fold : val acc1 0.500, acc3 0.929, mae 0.291###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/39 [===========================>..] - ETA: 0s - loss: 8.3287 \n",
      "Epoch 00001: val_loss improved from inf to 1.56489, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_8.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 8.1259 - val_loss: 1.5649\n",
      "Epoch 2/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 2.4616\n",
      "Epoch 00002: val_loss improved from 1.56489 to 0.66774, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.4520 - val_loss: 0.6677\n",
      "Epoch 3/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.5550\n",
      "Epoch 00003: val_loss improved from 0.66774 to 0.27192, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.5281 - val_loss: 0.2719\n",
      "Epoch 4/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.9743\n",
      "Epoch 00004: val_loss improved from 0.27192 to 0.26436, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.9461 - val_loss: 0.2644\n",
      "Epoch 5/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.8555\n",
      "Epoch 00005: val_loss improved from 0.26436 to 0.18290, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.8237 - val_loss: 0.1829\n",
      "Epoch 6/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.5919\n",
      "Epoch 00006: val_loss did not improve from 0.18290\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.5273 - val_loss: 0.1965\n",
      "Epoch 7/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 0.4160\n",
      "Epoch 00007: val_loss did not improve from 0.18290\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4375 - val_loss: 0.1888\n",
      " ###8 fold : val acc1 0.489, acc3 0.923, mae 0.304###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/39 [===========================>..] - ETA: 0s - loss: 8.3287 \n",
      "Epoch 00001: val_loss improved from inf to 1.54601, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_9.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 8.1259 - val_loss: 1.5460\n",
      "Epoch 2/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.5670\n",
      "Epoch 00002: val_loss improved from 1.54601 to 0.67003, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.4520 - val_loss: 0.6700\n",
      "Epoch 3/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.5778\n",
      "Epoch 00003: val_loss improved from 0.67003 to 0.27441, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.5281 - val_loss: 0.2744\n",
      "Epoch 4/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.9743\n",
      "Epoch 00004: val_loss improved from 0.27441 to 0.26780, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.9461 - val_loss: 0.2678\n",
      "Epoch 5/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 0.8648\n",
      "Epoch 00005: val_loss improved from 0.26780 to 0.18511, saving model to result/size/DNN_size_both_y/batch512,dnodes16_dropout0.2,dnodes256_dropout0.4,lr0.002/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.8237 - val_loss: 0.1851\n",
      "Epoch 6/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 0.5426\n",
      "Epoch 00006: val_loss did not improve from 0.18511\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.5273 - val_loss: 0.2000\n",
      "Epoch 7/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 0.4196\n",
      "Epoch 00007: val_loss did not improve from 0.18511\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.4375 - val_loss: 0.1914\n",
      " ###9 fold : val acc1 0.492, acc3 0.927, mae 0.299###\n",
      "acc10.487_acc30.932\n",
      "random search 10/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/154 [============================>.] - ETA: 0s - loss: 8.1390\n",
      "Epoch 00001: val_loss improved from inf to 1.16350, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 8.0929 - val_loss: 1.1635\n",
      "Epoch 2/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 2.8454\n",
      "Epoch 00002: val_loss improved from 1.16350 to 0.49434, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.7515 - val_loss: 0.4943\n",
      "Epoch 3/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 1.1607\n",
      "Epoch 00003: val_loss improved from 0.49434 to 0.44383, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.2242 - val_loss: 0.4438\n",
      "Epoch 4/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 1.3032\n",
      "Epoch 00004: val_loss improved from 0.44383 to 0.28646, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.2514 - val_loss: 0.2865\n",
      "Epoch 5/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.9075\n",
      "Epoch 00005: val_loss improved from 0.28646 to 0.25240, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8713 - val_loss: 0.2524\n",
      "Epoch 6/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 1.0304\n",
      "Epoch 00006: val_loss improved from 0.25240 to 0.22706, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0304 - val_loss: 0.2271\n",
      "Epoch 7/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.5214\n",
      "Epoch 00007: val_loss improved from 0.22706 to 0.20886, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5210 - val_loss: 0.2089\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.4175\n",
      "Epoch 00008: val_loss did not improve from 0.20886\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4175 - val_loss: 0.2728\n",
      "Epoch 9/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.4659\n",
      "Epoch 00009: val_loss did not improve from 0.20886\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4476 - val_loss: 0.2269\n",
      " ###0 fold : val acc1 0.456, acc3 0.919, mae 0.323###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/154 [==========================>...] - ETA: 0s - loss: 8.5193\n",
      "Epoch 00001: val_loss improved from inf to 1.16540, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 8.0627 - val_loss: 1.1654\n",
      "Epoch 2/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 2.7404\n",
      "Epoch 00002: val_loss improved from 1.16540 to 0.48781, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.7031 - val_loss: 0.4878\n",
      "Epoch 3/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 1.1470\n",
      "Epoch 00003: val_loss improved from 0.48781 to 0.44723, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.1925 - val_loss: 0.4472\n",
      "Epoch 4/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.2675\n",
      "Epoch 00004: val_loss improved from 0.44723 to 0.28884, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.2407 - val_loss: 0.2888\n",
      "Epoch 5/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.8858\n",
      "Epoch 00005: val_loss improved from 0.28884 to 0.25199, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8748 - val_loss: 0.2520\n",
      "Epoch 6/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 1.0670\n",
      "Epoch 00006: val_loss improved from 0.25199 to 0.23549, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0263 - val_loss: 0.2355\n",
      "Epoch 7/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.5252\n",
      "Epoch 00007: val_loss improved from 0.23549 to 0.20889, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5202 - val_loss: 0.2089\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.4230\n",
      "Epoch 00008: val_loss did not improve from 0.20889\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4230 - val_loss: 0.2748\n",
      "Epoch 9/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.4560\n",
      "Epoch 00009: val_loss did not improve from 0.20889\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4532 - val_loss: 0.2323\n",
      " ###1 fold : val acc1 0.456, acc3 0.924, mae 0.315###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/154 [==========================>...] - ETA: 0s - loss: 8.5110\n",
      "Epoch 00001: val_loss improved from inf to 1.14843, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 8.0531 - val_loss: 1.1484\n",
      "Epoch 2/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 2.7192\n",
      "Epoch 00002: val_loss improved from 1.14843 to 0.48822, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.7012 - val_loss: 0.4882\n",
      "Epoch 3/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 1.1568\n",
      "Epoch 00003: val_loss improved from 0.48822 to 0.45360, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.2056 - val_loss: 0.4536\n",
      "Epoch 4/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 1.2727\n",
      "Epoch 00004: val_loss improved from 0.45360 to 0.29242, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.2562 - val_loss: 0.2924\n",
      "Epoch 5/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.8836\n",
      "Epoch 00005: val_loss improved from 0.29242 to 0.25122, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8697 - val_loss: 0.2512\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.0640\n",
      "Epoch 00006: val_loss improved from 0.25122 to 0.23843, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0338 - val_loss: 0.2384\n",
      "Epoch 7/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.5232\n",
      "Epoch 00007: val_loss improved from 0.23843 to 0.20595, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5176 - val_loss: 0.2059\n",
      "Epoch 8/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.4261\n",
      "Epoch 00008: val_loss did not improve from 0.20595\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4201 - val_loss: 0.2681\n",
      "Epoch 9/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.4557\n",
      "Epoch 00009: val_loss did not improve from 0.20595\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4513 - val_loss: 0.2267\n",
      " ###2 fold : val acc1 0.451, acc3 0.925, mae 0.318###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/154 [==========================>...] - ETA: 0s - loss: 8.6223\n",
      "Epoch 00001: val_loss improved from inf to 1.13961, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 8.0367 - val_loss: 1.1396\n",
      "Epoch 2/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 2.7393\n",
      "Epoch 00002: val_loss improved from 1.13961 to 0.49614, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.7025 - val_loss: 0.4961\n",
      "Epoch 3/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 1.1365\n",
      "Epoch 00003: val_loss improved from 0.49614 to 0.45159, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.2041 - val_loss: 0.4516\n",
      "Epoch 4/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 1.2670\n",
      "Epoch 00004: val_loss improved from 0.45159 to 0.28558, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.2535 - val_loss: 0.2856\n",
      "Epoch 5/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.8838\n",
      "Epoch 00005: val_loss improved from 0.28558 to 0.25154, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8712 - val_loss: 0.2515\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.0776\n",
      "Epoch 00006: val_loss improved from 0.25154 to 0.24211, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0467 - val_loss: 0.2421\n",
      "Epoch 7/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.5139\n",
      "Epoch 00007: val_loss improved from 0.24211 to 0.20770, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5135 - val_loss: 0.2077\n",
      "Epoch 8/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.4321\n",
      "Epoch 00008: val_loss did not improve from 0.20770\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4259 - val_loss: 0.2750\n",
      "Epoch 9/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.4609\n",
      "Epoch 00009: val_loss did not improve from 0.20770\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4525 - val_loss: 0.2311\n",
      " ###3 fold : val acc1 0.465, acc3 0.929, mae 0.309###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/154 [============================>.] - ETA: 0s - loss: 9.3265 \n",
      "Epoch 00001: val_loss improved from inf to 1.13439, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 9.2210 - val_loss: 1.1344\n",
      "Epoch 2/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 2.3703\n",
      "Epoch 00002: val_loss improved from 1.13439 to 0.54086, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.3334 - val_loss: 0.5409\n",
      "Epoch 3/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 2.5327\n",
      "Epoch 00003: val_loss improved from 0.54086 to 0.32133, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.4602 - val_loss: 0.3213\n",
      "Epoch 4/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 1.5298\n",
      "Epoch 00004: val_loss improved from 0.32133 to 0.26830, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4731 - val_loss: 0.2683\n",
      "Epoch 5/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 1.4357\n",
      "Epoch 00005: val_loss improved from 0.26830 to 0.23836, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.3890 - val_loss: 0.2384\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.9228\n",
      "Epoch 00006: val_loss improved from 0.23836 to 0.23040, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9015 - val_loss: 0.2304\n",
      "Epoch 7/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.5565\n",
      "Epoch 00007: val_loss improved from 0.23040 to 0.20607, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5470 - val_loss: 0.2061\n",
      "Epoch 8/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.4547\n",
      "Epoch 00008: val_loss did not improve from 0.20607\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4453 - val_loss: 0.2199\n",
      "Epoch 9/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.5111\n",
      "Epoch 00009: val_loss did not improve from 0.20607\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4945 - val_loss: 0.2116\n",
      " ###4 fold : val acc1 0.472, acc3 0.932, mae 0.304###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/154 [============================>.] - ETA: 0s - loss: 6.9707\n",
      "Epoch 00001: val_loss improved from inf to 1.03702, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 6.8989 - val_loss: 1.0370\n",
      "Epoch 2/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 1.6191\n",
      "Epoch 00002: val_loss improved from 1.03702 to 0.46948, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.6094 - val_loss: 0.4695\n",
      "Epoch 3/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.0171\n",
      "Epoch 00003: val_loss improved from 0.46948 to 0.33378, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0073 - val_loss: 0.3338\n",
      "Epoch 4/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.6588\n",
      "Epoch 00004: val_loss improved from 0.33378 to 0.32288, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6504 - val_loss: 0.3229\n",
      "Epoch 5/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.4324\n",
      "Epoch 00005: val_loss improved from 0.32288 to 0.30844, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4324 - val_loss: 0.3084\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3036\n",
      "Epoch 00006: val_loss did not improve from 0.30844\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3029 - val_loss: 0.3458\n",
      "Epoch 7/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.2664\n",
      "Epoch 00007: val_loss did not improve from 0.30844\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2665 - val_loss: 0.3218\n",
      " ###5 fold : val acc1 0.365, acc3 0.852, mae 0.424###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/154 [==========================>...] - ETA: 0s - loss: 8.1071\n",
      "Epoch 00001: val_loss improved from inf to 1.11453, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 7.6681 - val_loss: 1.1145\n",
      "Epoch 2/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 1.7440\n",
      "Epoch 00002: val_loss improved from 1.11453 to 0.60411, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.9457 - val_loss: 0.6041\n",
      "Epoch 3/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 1.9856\n",
      "Epoch 00003: val_loss improved from 0.60411 to 0.31495, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.9048 - val_loss: 0.3150\n",
      "Epoch 4/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 1.1568\n",
      "Epoch 00004: val_loss improved from 0.31495 to 0.26131, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1522 - val_loss: 0.2613\n",
      "Epoch 5/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.4308\n",
      "Epoch 00005: val_loss improved from 0.26131 to 0.23397, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.3885 - val_loss: 0.2340\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.6045\n",
      "Epoch 00006: val_loss improved from 0.23397 to 0.22212, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5947 - val_loss: 0.2221\n",
      "Epoch 7/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.6174\n",
      "Epoch 00007: val_loss did not improve from 0.22212\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6053 - val_loss: 0.2479\n",
      "Epoch 8/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3960\n",
      "Epoch 00008: val_loss improved from 0.22212 to 0.22174, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3922 - val_loss: 0.2217\n",
      "Epoch 9/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.4511\n",
      "Epoch 00009: val_loss improved from 0.22174 to 0.21599, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4451 - val_loss: 0.2160\n",
      "Epoch 10/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3354\n",
      "Epoch 00010: val_loss improved from 0.21599 to 0.21576, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3320 - val_loss: 0.2158\n",
      "Epoch 11/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.2990\n",
      "Epoch 00011: val_loss did not improve from 0.21576\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2990 - val_loss: 0.2352\n",
      "Epoch 12/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.3184\n",
      "Epoch 00012: val_loss did not improve from 0.21576\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3113 - val_loss: 0.2237\n",
      " ###6 fold : val acc1 0.459, acc3 0.924, mae 0.316###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/154 [===========================>..] - ETA: 0s - loss: 7.8396\n",
      "Epoch 00001: val_loss improved from inf to 1.09854, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 7.6470 - val_loss: 1.0985\n",
      "Epoch 2/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 1.9618\n",
      "Epoch 00002: val_loss improved from 1.09854 to 0.59945, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.9334 - val_loss: 0.5995\n",
      "Epoch 3/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 1.9825\n",
      "Epoch 00003: val_loss improved from 0.59945 to 0.31530, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.9810 - val_loss: 0.3153\n",
      "Epoch 4/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 1.1779\n",
      "Epoch 00004: val_loss improved from 0.31530 to 0.26380, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1522 - val_loss: 0.2638\n",
      "Epoch 5/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.5290\n",
      "Epoch 00005: val_loss improved from 0.26380 to 0.23871, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4821 - val_loss: 0.2387\n",
      "Epoch 6/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.6127\n",
      "Epoch 00006: val_loss improved from 0.23871 to 0.22509, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5969 - val_loss: 0.2251\n",
      "Epoch 7/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.6567\n",
      "Epoch 00007: val_loss did not improve from 0.22509\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6344 - val_loss: 0.2389\n",
      "Epoch 8/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4026\n",
      "Epoch 00008: val_loss improved from 0.22509 to 0.21712, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3976 - val_loss: 0.2171\n",
      "Epoch 9/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.4713\n",
      "Epoch 00009: val_loss improved from 0.21712 to 0.21392, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4709 - val_loss: 0.2139\n",
      "Epoch 10/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.3524\n",
      "Epoch 00010: val_loss did not improve from 0.21392\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3524 - val_loss: 0.2144\n",
      "Epoch 11/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.3185\n",
      "Epoch 00011: val_loss did not improve from 0.21392\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3138 - val_loss: 0.2305\n",
      " ###7 fold : val acc1 0.470, acc3 0.924, mae 0.308###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/154 [============================>.] - ETA: 0s - loss: 7.7705\n",
      "Epoch 00001: val_loss improved from inf to 1.21281, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 7.6564 - val_loss: 1.2128\n",
      "Epoch 2/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 1.7017\n",
      "Epoch 00002: val_loss improved from 1.21281 to 0.65142, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.9324 - val_loss: 0.6514\n",
      "Epoch 3/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 2.0483\n",
      "Epoch 00003: val_loss improved from 0.65142 to 0.33856, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.9821 - val_loss: 0.3386\n",
      "Epoch 4/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 1.1648\n",
      "Epoch 00004: val_loss improved from 0.33856 to 0.27884, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1460 - val_loss: 0.2788\n",
      "Epoch 5/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 1.5741\n",
      "Epoch 00005: val_loss improved from 0.27884 to 0.23679, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4754 - val_loss: 0.2368\n",
      "Epoch 6/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.6070\n",
      "Epoch 00006: val_loss improved from 0.23679 to 0.22296, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5941 - val_loss: 0.2230\n",
      "Epoch 7/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.6540\n",
      "Epoch 00007: val_loss did not improve from 0.22296\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6325 - val_loss: 0.2344\n",
      "Epoch 8/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.4000\n",
      "Epoch 00008: val_loss improved from 0.22296 to 0.21014, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3955 - val_loss: 0.2101\n",
      "Epoch 9/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.4829\n",
      "Epoch 00009: val_loss improved from 0.21014 to 0.20527, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4660 - val_loss: 0.2053\n",
      "Epoch 10/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.3575\n",
      "Epoch 00010: val_loss improved from 0.20527 to 0.20329, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3507 - val_loss: 0.2033\n",
      "Epoch 11/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.3170\n",
      "Epoch 00011: val_loss did not improve from 0.20329\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3128 - val_loss: 0.2195\n",
      "Epoch 12/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.3365\n",
      "Epoch 00012: val_loss did not improve from 0.20329\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3291 - val_loss: 0.2083\n",
      " ###8 fold : val acc1 0.451, acc3 0.906, mae 0.331###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/154 [============================>.] - ETA: 0s - loss: 7.7705\n",
      "Epoch 00001: val_loss improved from inf to 1.16429, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 7.6564 - val_loss: 1.1643\n",
      "Epoch 2/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.6967\n",
      "Epoch 00002: val_loss improved from 1.16429 to 0.63131, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.9324 - val_loss: 0.6313\n",
      "Epoch 3/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 2.0327\n",
      "Epoch 00003: val_loss improved from 0.63131 to 0.32599, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.9821 - val_loss: 0.3260\n",
      "Epoch 4/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 1.1781\n",
      "Epoch 00004: val_loss improved from 0.32599 to 0.27050, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1460 - val_loss: 0.2705\n",
      "Epoch 5/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 1.4767\n",
      "Epoch 00005: val_loss improved from 0.27050 to 0.23637, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4754 - val_loss: 0.2364\n",
      "Epoch 6/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.6104\n",
      "Epoch 00006: val_loss improved from 0.23637 to 0.22254, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5941 - val_loss: 0.2225\n",
      "Epoch 7/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.6438\n",
      "Epoch 00007: val_loss did not improve from 0.22254\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6325 - val_loss: 0.2369\n",
      "Epoch 8/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.4007\n",
      "Epoch 00008: val_loss improved from 0.22254 to 0.21291, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3955 - val_loss: 0.2129\n",
      "Epoch 9/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.4829\n",
      "Epoch 00009: val_loss improved from 0.21291 to 0.20909, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4660 - val_loss: 0.2091\n",
      "Epoch 10/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3542\n",
      "Epoch 00010: val_loss improved from 0.20909 to 0.20763, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,dnodes256_dropout0,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3507 - val_loss: 0.2076\n",
      "Epoch 11/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3135\n",
      "Epoch 00011: val_loss did not improve from 0.20763\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3128 - val_loss: 0.2248\n",
      "Epoch 12/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3330\n",
      "Epoch 00012: val_loss did not improve from 0.20763\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3291 - val_loss: 0.2132\n",
      " ###9 fold : val acc1 0.461, acc3 0.918, mae 0.320###\n",
      "acc10.451_acc30.915\n",
      "random search 11/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610/613 [============================>.] - ETA: 0s - loss: 1.1246\n",
      "Epoch 00001: val_loss improved from inf to 0.14191, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 1.1203 - val_loss: 0.1419\n",
      "Epoch 2/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.1458\n",
      "Epoch 00002: val_loss improved from 0.14191 to 0.13260, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1457 - val_loss: 0.1326\n",
      "Epoch 3/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.1270\n",
      "Epoch 00003: val_loss did not improve from 0.13260\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1271 - val_loss: 0.1369\n",
      "Epoch 4/100\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.1364\n",
      "Epoch 00004: val_loss did not improve from 0.13260\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1364 - val_loss: 0.1634\n",
      " ###0 fold : val acc1 0.559, acc3 0.963, mae 0.244###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611/613 [============================>.] - ETA: 0s - loss: 1.1123\n",
      "Epoch 00001: val_loss improved from inf to 0.14110, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 1.1094 - val_loss: 0.1411\n",
      "Epoch 2/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.1511\n",
      "Epoch 00002: val_loss improved from 0.14110 to 0.13133, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1504 - val_loss: 0.1313\n",
      "Epoch 3/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.1277\n",
      "Epoch 00003: val_loss improved from 0.13133 to 0.13130, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1277 - val_loss: 0.1313\n",
      "Epoch 4/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.1322\n",
      "Epoch 00004: val_loss did not improve from 0.13130\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1321 - val_loss: 0.1573\n",
      "Epoch 5/100\n",
      "594/613 [============================>.] - ETA: 0s - loss: 0.1573\n",
      "Epoch 00005: val_loss did not improve from 0.13130\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1570 - val_loss: 0.1547\n",
      " ###1 fold : val acc1 0.564, acc3 0.965, mae 0.240###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/613 [============================>.] - ETA: 0s - loss: 1.1301\n",
      "Epoch 00001: val_loss improved from inf to 0.14516, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_2.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 1.1165 - val_loss: 0.1452\n",
      "Epoch 2/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.1475\n",
      "Epoch 00002: val_loss improved from 0.14516 to 0.13424, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_2.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1473 - val_loss: 0.1342\n",
      "Epoch 3/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.1280\n",
      "Epoch 00003: val_loss did not improve from 0.13424\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1281 - val_loss: 0.1364\n",
      "Epoch 4/100\n",
      "597/613 [============================>.] - ETA: 0s - loss: 0.1342\n",
      "Epoch 00004: val_loss did not improve from 0.13424\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1347 - val_loss: 0.1637\n",
      " ###2 fold : val acc1 0.553, acc3 0.965, mae 0.245###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607/613 [============================>.] - ETA: 0s - loss: 1.1409\n",
      "Epoch 00001: val_loss improved from inf to 0.14456, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 1.1322 - val_loss: 0.1446\n",
      "Epoch 2/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.1452\n",
      "Epoch 00002: val_loss improved from 0.14456 to 0.13504, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1452 - val_loss: 0.1350\n",
      "Epoch 3/100\n",
      "597/613 [============================>.] - ETA: 0s - loss: 0.1282\n",
      "Epoch 00003: val_loss did not improve from 0.13504\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1286 - val_loss: 0.1388\n",
      "Epoch 4/100\n",
      "596/613 [============================>.] - ETA: 0s - loss: 0.1422\n",
      "Epoch 00004: val_loss did not improve from 0.13504\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1435 - val_loss: 0.1822\n",
      " ###3 fold : val acc1 0.546, acc3 0.967, mae 0.247###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609/613 [============================>.] - ETA: 0s - loss: 1.1270\n",
      "Epoch 00001: val_loss improved from inf to 0.13696, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 1.1209 - val_loss: 0.1370\n",
      "Epoch 2/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.1585\n",
      "Epoch 00002: val_loss did not improve from 0.13696\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1579 - val_loss: 0.1537\n",
      "Epoch 3/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.1350\n",
      "Epoch 00003: val_loss improved from 0.13696 to 0.13395, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1346 - val_loss: 0.1339\n",
      "Epoch 4/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.1541\n",
      "Epoch 00004: val_loss did not improve from 0.13395\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1536 - val_loss: 0.1786\n",
      "Epoch 5/100\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.3702\n",
      "Epoch 00005: val_loss did not improve from 0.13395\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.3702 - val_loss: 0.1501\n",
      " ###4 fold : val acc1 0.580, acc3 0.961, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602/613 [============================>.] - ETA: 0s - loss: 1.0078\n",
      "Epoch 00001: val_loss improved from inf to 0.14305, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.9923 - val_loss: 0.1430\n",
      "Epoch 2/100\n",
      "597/613 [============================>.] - ETA: 0s - loss: 0.1319\n",
      "Epoch 00002: val_loss improved from 0.14305 to 0.13795, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1325 - val_loss: 0.1379\n",
      "Epoch 3/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.1298\n",
      "Epoch 00003: val_loss improved from 0.13795 to 0.12991, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1294 - val_loss: 0.1299\n",
      "Epoch 4/100\n",
      "595/613 [============================>.] - ETA: 0s - loss: 0.1299\n",
      "Epoch 00004: val_loss did not improve from 0.12991\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1299 - val_loss: 0.1305\n",
      "Epoch 5/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.1289\n",
      "Epoch 00005: val_loss did not improve from 0.12991\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1290 - val_loss: 0.1502\n",
      " ###5 fold : val acc1 0.585, acc3 0.965, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609/613 [============================>.] - ETA: 0s - loss: 1.2362\n",
      "Epoch 00001: val_loss improved from inf to 0.13878, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 1.2293 - val_loss: 0.1388\n",
      "Epoch 2/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.1533\n",
      "Epoch 00002: val_loss did not improve from 0.13878\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1531 - val_loss: 0.1643\n",
      "Epoch 3/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.1399\n",
      "Epoch 00003: val_loss improved from 0.13878 to 0.13508, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1399 - val_loss: 0.1351\n",
      "Epoch 4/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.1354\n",
      "Epoch 00004: val_loss improved from 0.13508 to 0.13234, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1351 - val_loss: 0.1323\n",
      "Epoch 5/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.1722\n",
      "Epoch 00005: val_loss did not improve from 0.13234\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1730 - val_loss: 0.1564\n",
      "Epoch 6/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.3572\n",
      "Epoch 00006: val_loss did not improve from 0.13234\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.3547 - val_loss: 0.1364\n",
      " ###6 fold : val acc1 0.564, acc3 0.959, mae 0.243###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601/613 [============================>.] - ETA: 0s - loss: 1.2255\n",
      "Epoch 00001: val_loss improved from inf to 0.13832, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 1.2044 - val_loss: 0.1383\n",
      "Epoch 2/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.1738\n",
      "Epoch 00002: val_loss did not improve from 0.13832\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1738 - val_loss: 0.1836\n",
      "Epoch 3/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.1340\n",
      "Epoch 00003: val_loss improved from 0.13832 to 0.13427, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1337 - val_loss: 0.1343\n",
      "Epoch 4/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.1736\n",
      "Epoch 00004: val_loss did not improve from 0.13427\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1733 - val_loss: 0.1445\n",
      "Epoch 5/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.2259\n",
      "Epoch 00005: val_loss did not improve from 0.13427\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2250 - val_loss: 0.1526\n",
      " ###7 fold : val acc1 0.559, acc3 0.958, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599/613 [============================>.] - ETA: 0s - loss: 1.2347\n",
      "Epoch 00001: val_loss improved from inf to 0.13698, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 1.2097 - val_loss: 0.1370\n",
      "Epoch 2/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.1731\n",
      "Epoch 00002: val_loss did not improve from 0.13698\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1732 - val_loss: 0.1878\n",
      "Epoch 3/100\n",
      "597/613 [============================>.] - ETA: 0s - loss: 0.1308\n",
      "Epoch 00003: val_loss improved from 0.13698 to 0.13197, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1306 - val_loss: 0.1320\n",
      "Epoch 4/100\n",
      "597/613 [============================>.] - ETA: 0s - loss: 0.1479\n",
      "Epoch 00004: val_loss did not improve from 0.13197\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1472 - val_loss: 0.1427\n",
      "Epoch 5/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.1810\n",
      "Epoch 00005: val_loss did not improve from 0.13197\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1802 - val_loss: 0.1505\n",
      " ###8 fold : val acc1 0.567, acc3 0.959, mae 0.241###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602/613 [============================>.] - ETA: 0s - loss: 1.2293\n",
      "Epoch 00001: val_loss improved from inf to 0.13706, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 1.2097 - val_loss: 0.1371\n",
      "Epoch 2/100\n",
      "595/613 [============================>.] - ETA: 0s - loss: 0.1733\n",
      "Epoch 00002: val_loss did not improve from 0.13706\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1732 - val_loss: 0.1814\n",
      "Epoch 3/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.1309\n",
      "Epoch 00003: val_loss improved from 0.13706 to 0.13141, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0,lr0.002/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1306 - val_loss: 0.1314\n",
      "Epoch 4/100\n",
      "597/613 [============================>.] - ETA: 0s - loss: 0.1479\n",
      "Epoch 00004: val_loss did not improve from 0.13141\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1472 - val_loss: 0.1400\n",
      "Epoch 5/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.1808\n",
      "Epoch 00005: val_loss did not improve from 0.13141\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1802 - val_loss: 0.1518\n",
      " ###9 fold : val acc1 0.566, acc3 0.963, mae 0.240###\n",
      "acc10.564_acc30.963\n",
      "random search 12/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/307 [============================>.] - ETA: 0s - loss: 1.5813\n",
      "Epoch 00001: val_loss improved from inf to 0.16828, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 1.5541 - val_loss: 0.1683\n",
      "Epoch 2/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.2827\n",
      "Epoch 00002: val_loss improved from 0.16828 to 0.13788, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2811 - val_loss: 0.1379\n",
      "Epoch 3/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.2597\n",
      "Epoch 00003: val_loss did not improve from 0.13788\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2604 - val_loss: 0.2154\n",
      "Epoch 4/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.2298\n",
      "Epoch 00004: val_loss did not improve from 0.13788\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2294 - val_loss: 0.1391\n",
      " ###0 fold : val acc1 0.580, acc3 0.956, mae 0.237###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/307 [============================>.] - ETA: 0s - loss: 1.5524\n",
      "Epoch 00001: val_loss improved from inf to 0.16912, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.5467 - val_loss: 0.1691\n",
      "Epoch 2/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.2758\n",
      "Epoch 00002: val_loss improved from 0.16912 to 0.14137, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2754 - val_loss: 0.1414\n",
      "Epoch 3/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.2253\n",
      "Epoch 00003: val_loss did not improve from 0.14137\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2568 - val_loss: 0.2312\n",
      "Epoch 4/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.2310\n",
      "Epoch 00004: val_loss improved from 0.14137 to 0.14088, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2299 - val_loss: 0.1409\n",
      "Epoch 5/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.2119\n",
      "Epoch 00005: val_loss improved from 0.14088 to 0.13519, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2125 - val_loss: 0.1352\n",
      "Epoch 6/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.2522\n",
      "Epoch 00006: val_loss did not improve from 0.13519\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2511 - val_loss: 0.1621\n",
      "Epoch 7/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.3068\n",
      "Epoch 00007: val_loss did not improve from 0.13519\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3058 - val_loss: 0.1406\n",
      " ###1 fold : val acc1 0.562, acc3 0.956, mae 0.244###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291/307 [===========================>..] - ETA: 0s - loss: 1.6193\n",
      "Epoch 00001: val_loss improved from inf to 0.17017, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 1.5519 - val_loss: 0.1702\n",
      "Epoch 2/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.2800\n",
      "Epoch 00002: val_loss improved from 0.17017 to 0.14294, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2797 - val_loss: 0.1429\n",
      "Epoch 3/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.2551\n",
      "Epoch 00003: val_loss did not improve from 0.14294\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2558 - val_loss: 0.2235\n",
      "Epoch 4/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.2302\n",
      "Epoch 00004: val_loss did not improve from 0.14294\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2301 - val_loss: 0.1502\n",
      " ###2 fold : val acc1 0.563, acc3 0.956, mae 0.244###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297/307 [============================>.] - ETA: 0s - loss: 1.6051\n",
      "Epoch 00001: val_loss improved from inf to 0.17240, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 1.5642 - val_loss: 0.1724\n",
      "Epoch 2/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.2821\n",
      "Epoch 00002: val_loss improved from 0.17240 to 0.14357, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2818 - val_loss: 0.1436\n",
      "Epoch 3/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.2279\n",
      "Epoch 00003: val_loss did not improve from 0.14357\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2596 - val_loss: 0.2124\n",
      "Epoch 4/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.2332\n",
      "Epoch 00004: val_loss did not improve from 0.14357\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2339 - val_loss: 0.1614\n",
      " ###3 fold : val acc1 0.570, acc3 0.959, mae 0.239###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293/307 [===========================>..] - ETA: 0s - loss: 1.6418\n",
      "Epoch 00001: val_loss improved from inf to 0.15757, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 1.5815 - val_loss: 0.1576\n",
      "Epoch 2/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.2573\n",
      "Epoch 00002: val_loss improved from 0.15757 to 0.14746, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2564 - val_loss: 0.1475\n",
      "Epoch 3/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.2282\n",
      "Epoch 00003: val_loss improved from 0.14746 to 0.13267, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2279 - val_loss: 0.1327\n",
      "Epoch 4/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.2217\n",
      "Epoch 00004: val_loss did not improve from 0.13267\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2219 - val_loss: 0.1453\n",
      "Epoch 5/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.2719\n",
      "Epoch 00005: val_loss did not improve from 0.13267\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2702 - val_loss: 0.1329\n",
      " ###4 fold : val acc1 0.577, acc3 0.963, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/307 [============================>.] - ETA: 0s - loss: 1.4032\n",
      "Epoch 00001: val_loss improved from inf to 0.15595, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.3904 - val_loss: 0.1560\n",
      "Epoch 2/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.2441\n",
      "Epoch 00002: val_loss improved from 0.15595 to 0.14475, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2436 - val_loss: 0.1448\n",
      "Epoch 3/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.2244\n",
      "Epoch 00003: val_loss improved from 0.14475 to 0.13768, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2241 - val_loss: 0.1377\n",
      "Epoch 4/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.2148\n",
      "Epoch 00004: val_loss did not improve from 0.13768\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2148 - val_loss: 0.1413\n",
      "Epoch 5/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.2071\n",
      "Epoch 00005: val_loss improved from 0.13768 to 0.13036, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2069 - val_loss: 0.1304\n",
      "Epoch 6/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.2039\n",
      "Epoch 00006: val_loss did not improve from 0.13036\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2039 - val_loss: 0.1351\n",
      "Epoch 7/100\n",
      "291/307 [===========================>..] - ETA: 0s - loss: 0.2010\n",
      "Epoch 00007: val_loss did not improve from 0.13036\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2031 - val_loss: 0.1460\n",
      " ###5 fold : val acc1 0.589, acc3 0.966, mae 0.227###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/307 [============================>.] - ETA: 0s - loss: 1.8203\n",
      "Epoch 00001: val_loss improved from inf to 0.15922, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 1.8184 - val_loss: 0.1592\n",
      "Epoch 2/100\n",
      "291/307 [===========================>..] - ETA: 0s - loss: 0.2456\n",
      "Epoch 00002: val_loss improved from 0.15922 to 0.14824, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3460 - val_loss: 0.1482\n",
      "Epoch 3/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.2307\n",
      "Epoch 00003: val_loss improved from 0.14824 to 0.13624, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2303 - val_loss: 0.1362\n",
      "Epoch 4/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.2323\n",
      "Epoch 00004: val_loss did not improve from 0.13624\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2315 - val_loss: 0.1437\n",
      "Epoch 5/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.2083\n",
      "Epoch 00005: val_loss improved from 0.13624 to 0.12937, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2084 - val_loss: 0.1294\n",
      "Epoch 6/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.2082\n",
      "Epoch 00006: val_loss did not improve from 0.12937\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2081 - val_loss: 0.1363\n",
      "Epoch 7/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.2044\n",
      "Epoch 00007: val_loss did not improve from 0.12937\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2042 - val_loss: 0.1535\n",
      " ###6 fold : val acc1 0.577, acc3 0.963, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/307 [============================>.] - ETA: 0s - loss: 1.7858\n",
      "Epoch 00001: val_loss improved from inf to 0.15964, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 1.7689 - val_loss: 0.1596\n",
      "Epoch 2/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.4375\n",
      "Epoch 00002: val_loss improved from 0.15964 to 0.14633, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.4307 - val_loss: 0.1463\n",
      "Epoch 3/100\n",
      "291/307 [===========================>..] - ETA: 0s - loss: 0.2392\n",
      "Epoch 00003: val_loss improved from 0.14633 to 0.13809, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2383 - val_loss: 0.1381\n",
      "Epoch 4/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.2157\n",
      "Epoch 00004: val_loss did not improve from 0.13809\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2160 - val_loss: 0.1388\n",
      "Epoch 5/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.2254\n",
      "Epoch 00005: val_loss improved from 0.13809 to 0.13092, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2254 - val_loss: 0.1309\n",
      "Epoch 6/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.2168\n",
      "Epoch 00006: val_loss did not improve from 0.13092\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2164 - val_loss: 0.1406\n",
      "Epoch 7/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.2308\n",
      "Epoch 00007: val_loss did not improve from 0.13092\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2342 - val_loss: 0.2167\n",
      " ###7 fold : val acc1 0.592, acc3 0.965, mae 0.225###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291/307 [===========================>..] - ETA: 0s - loss: 1.8792\n",
      "Epoch 00001: val_loss improved from inf to 0.16152, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 1.7984 - val_loss: 0.1615\n",
      "Epoch 2/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.4297\n",
      "Epoch 00002: val_loss improved from 0.16152 to 0.14318, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.4250 - val_loss: 0.1432\n",
      "Epoch 3/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.2382\n",
      "Epoch 00003: val_loss improved from 0.14318 to 0.13825, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2372 - val_loss: 0.1382\n",
      "Epoch 4/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.2152\n",
      "Epoch 00004: val_loss improved from 0.13825 to 0.13734, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2149 - val_loss: 0.1373\n",
      "Epoch 5/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.2204\n",
      "Epoch 00005: val_loss improved from 0.13734 to 0.12930, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2200 - val_loss: 0.1293\n",
      "Epoch 6/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.2208\n",
      "Epoch 00006: val_loss did not improve from 0.12930\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2208 - val_loss: 0.1389\n",
      "Epoch 7/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.2259\n",
      "Epoch 00007: val_loss did not improve from 0.12930\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2285 - val_loss: 0.2088\n",
      " ###8 fold : val acc1 0.577, acc3 0.956, mae 0.238###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/307 [============================>.] - ETA: 0s - loss: 1.8262\n",
      "Epoch 00001: val_loss improved from inf to 0.15810, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 1.7984 - val_loss: 0.1581\n",
      "Epoch 2/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.4316\n",
      "Epoch 00002: val_loss improved from 0.15810 to 0.14255, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.4250 - val_loss: 0.1426\n",
      "Epoch 3/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.2377\n",
      "Epoch 00003: val_loss improved from 0.14255 to 0.13695, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2372 - val_loss: 0.1369\n",
      "Epoch 4/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.2149\n",
      "Epoch 00004: val_loss did not improve from 0.13695\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2149 - val_loss: 0.1389\n",
      "Epoch 5/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.2204\n",
      "Epoch 00005: val_loss improved from 0.13695 to 0.13012, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.4,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2200 - val_loss: 0.1301\n",
      "Epoch 6/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.2209\n",
      "Epoch 00006: val_loss did not improve from 0.13012\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2208 - val_loss: 0.1390\n",
      "Epoch 7/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.2285\n",
      "Epoch 00007: val_loss did not improve from 0.13012\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2285 - val_loss: 0.2059\n",
      " ###9 fold : val acc1 0.584, acc3 0.963, mae 0.232###\n",
      "acc10.577_acc30.960\n",
      "random search 13/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 18.1346\n",
      "Epoch 00001: val_loss improved from inf to 10.99797, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 17.3639 - val_loss: 10.9980\n",
      "Epoch 2/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 8.3365\n",
      "Epoch 00002: val_loss improved from 10.99797 to 3.98259, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 7.8530 - val_loss: 3.9826\n",
      "Epoch 3/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 3.4120\n",
      "Epoch 00003: val_loss improved from 3.98259 to 1.69785, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.6161 - val_loss: 1.6978\n",
      "Epoch 4/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 2.0460\n",
      "Epoch 00004: val_loss improved from 1.69785 to 0.91410, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.4620 - val_loss: 0.9141\n",
      "Epoch 5/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 2.0416\n",
      "Epoch 00005: val_loss improved from 0.91410 to 0.58042, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.8961 - val_loss: 0.5804\n",
      "Epoch 6/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.3873\n",
      "Epoch 00006: val_loss improved from 0.58042 to 0.41682, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3348 - val_loss: 0.4168\n",
      "Epoch 7/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.2584\n",
      "Epoch 00007: val_loss improved from 0.41682 to 0.33465, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2117 - val_loss: 0.3346\n",
      "Epoch 8/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.9983\n",
      "Epoch 00008: val_loss improved from 0.33465 to 0.29549, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0502 - val_loss: 0.2955\n",
      "Epoch 9/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.2506\n",
      "Epoch 00009: val_loss improved from 0.29549 to 0.26495, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1763 - val_loss: 0.2649\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.0674\n",
      "Epoch 00010: val_loss improved from 0.26495 to 0.25165, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0674 - val_loss: 0.2516\n",
      "Epoch 11/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.9688\n",
      "Epoch 00011: val_loss improved from 0.25165 to 0.23082, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9501 - val_loss: 0.2308\n",
      "Epoch 12/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.9110\n",
      "Epoch 00012: val_loss improved from 0.23082 to 0.22448, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8903 - val_loss: 0.2245\n",
      "Epoch 13/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8194\n",
      "Epoch 00013: val_loss improved from 0.22448 to 0.21111, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8166 - val_loss: 0.2111\n",
      "Epoch 14/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.7853\n",
      "Epoch 00014: val_loss improved from 0.21111 to 0.20754, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7826 - val_loss: 0.2075\n",
      "Epoch 15/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8146\n",
      "Epoch 00015: val_loss improved from 0.20754 to 0.19558, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8019 - val_loss: 0.1956\n",
      "Epoch 16/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8736\n",
      "Epoch 00016: val_loss improved from 0.19558 to 0.19371, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8730 - val_loss: 0.1937\n",
      "Epoch 17/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7247\n",
      "Epoch 00017: val_loss improved from 0.19371 to 0.18899, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7297 - val_loss: 0.1890\n",
      "Epoch 18/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7264\n",
      "Epoch 00018: val_loss improved from 0.18899 to 0.18491, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7234 - val_loss: 0.1849\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.7040\n",
      "Epoch 00019: val_loss improved from 0.18491 to 0.18084, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7040 - val_loss: 0.1808\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6887\n",
      "Epoch 00020: val_loss improved from 0.18084 to 0.17366, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6887 - val_loss: 0.1737\n",
      "Epoch 21/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8127\n",
      "Epoch 00021: val_loss did not improve from 0.17366\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8110 - val_loss: 0.1931\n",
      "Epoch 22/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6488\n",
      "Epoch 00022: val_loss improved from 0.17366 to 0.17144, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6573 - val_loss: 0.1714\n",
      "Epoch 23/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6472\n",
      "Epoch 00023: val_loss did not improve from 0.17144\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6480 - val_loss: 0.1732\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6675\n",
      "Epoch 00024: val_loss improved from 0.17144 to 0.16682, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6675 - val_loss: 0.1668\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6365\n",
      "Epoch 00025: val_loss improved from 0.16682 to 0.16637, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6365 - val_loss: 0.1664\n",
      "Epoch 26/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6087\n",
      "Epoch 00026: val_loss improved from 0.16637 to 0.15931, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6073 - val_loss: 0.1593\n",
      "Epoch 27/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6063\n",
      "Epoch 00027: val_loss did not improve from 0.15931\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6081 - val_loss: 0.1635\n",
      "Epoch 28/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5877\n",
      "Epoch 00028: val_loss improved from 0.15931 to 0.15927, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5891 - val_loss: 0.1593\n",
      "Epoch 29/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5916\n",
      "Epoch 00029: val_loss did not improve from 0.15927\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5919 - val_loss: 0.1650\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6088\n",
      "Epoch 00030: val_loss did not improve from 0.15927\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6088 - val_loss: 0.1619\n",
      " ###0 fold : val acc1 0.545, acc3 0.939, mae 0.265###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/77 [===========================>..] - ETA: 0s - loss: 17.5664\n",
      "Epoch 00001: val_loss improved from inf to 10.99114, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 17.3608 - val_loss: 10.9911\n",
      "Epoch 2/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 8.1035\n",
      "Epoch 00002: val_loss improved from 10.99114 to 3.98153, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 7.8645 - val_loss: 3.9815\n",
      "Epoch 3/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 3.2873\n",
      "Epoch 00003: val_loss improved from 3.98153 to 1.69587, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.6153 - val_loss: 1.6959\n",
      "Epoch 4/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 2.5081\n",
      "Epoch 00004: val_loss improved from 1.69587 to 0.91944, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.4689 - val_loss: 0.9194\n",
      "Epoch 5/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 1.9987\n",
      "Epoch 00005: val_loss improved from 0.91944 to 0.58719, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.9026 - val_loss: 0.5872\n",
      "Epoch 6/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.3915\n",
      "Epoch 00006: val_loss improved from 0.58719 to 0.41880, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3429 - val_loss: 0.4188\n",
      "Epoch 7/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.2542\n",
      "Epoch 00007: val_loss improved from 0.41880 to 0.33562, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2144 - val_loss: 0.3356\n",
      "Epoch 8/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.0141\n",
      "Epoch 00008: val_loss improved from 0.33562 to 0.29452, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0633 - val_loss: 0.2945\n",
      "Epoch 9/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.2312\n",
      "Epoch 00009: val_loss improved from 0.29452 to 0.26629, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1809 - val_loss: 0.2663\n",
      "Epoch 10/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.9054\n",
      "Epoch 00010: val_loss improved from 0.26629 to 0.24777, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0795 - val_loss: 0.2478\n",
      "Epoch 11/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.9715\n",
      "Epoch 00011: val_loss improved from 0.24777 to 0.23010, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.9583 - val_loss: 0.2301\n",
      "Epoch 12/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.9190\n",
      "Epoch 00012: val_loss improved from 0.23010 to 0.22275, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8961 - val_loss: 0.2227\n",
      "Epoch 13/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8262\n",
      "Epoch 00013: val_loss improved from 0.22275 to 0.20937, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8219 - val_loss: 0.2094\n",
      "Epoch 14/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7924\n",
      "Epoch 00014: val_loss improved from 0.20937 to 0.20617, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7852 - val_loss: 0.2062\n",
      "Epoch 15/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.8164\n",
      "Epoch 00015: val_loss improved from 0.20617 to 0.19322, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8042 - val_loss: 0.1932\n",
      "Epoch 16/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.9055\n",
      "Epoch 00016: val_loss improved from 0.19322 to 0.19111, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8750 - val_loss: 0.1911\n",
      "Epoch 17/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.7336\n",
      "Epoch 00017: val_loss improved from 0.19111 to 0.18694, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7333 - val_loss: 0.1869\n",
      "Epoch 18/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.7266\n",
      "Epoch 00018: val_loss improved from 0.18694 to 0.18163, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7203 - val_loss: 0.1816\n",
      "Epoch 19/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.7146\n",
      "Epoch 00019: val_loss improved from 0.18163 to 0.17925, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7104 - val_loss: 0.1793\n",
      "Epoch 20/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.6799\n",
      "Epoch 00020: val_loss improved from 0.17925 to 0.17209, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6859 - val_loss: 0.1721\n",
      "Epoch 21/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.8335\n",
      "Epoch 00021: val_loss did not improve from 0.17209\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8093 - val_loss: 0.1888\n",
      "Epoch 22/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.6488\n",
      "Epoch 00022: val_loss improved from 0.17209 to 0.16814, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6596 - val_loss: 0.1681\n",
      "Epoch 23/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6443\n",
      "Epoch 00023: val_loss did not improve from 0.16814\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6475 - val_loss: 0.1714\n",
      "Epoch 24/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6773\n",
      "Epoch 00024: val_loss improved from 0.16814 to 0.16584, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6707 - val_loss: 0.1658\n",
      "Epoch 25/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6220\n",
      "Epoch 00025: val_loss improved from 0.16584 to 0.16387, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6360 - val_loss: 0.1639\n",
      "Epoch 26/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6178\n",
      "Epoch 00026: val_loss improved from 0.16387 to 0.15785, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6183 - val_loss: 0.1578\n",
      "Epoch 27/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.6092\n",
      "Epoch 00027: val_loss did not improve from 0.15785\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6108 - val_loss: 0.1621\n",
      "Epoch 28/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.5932\n",
      "Epoch 00028: val_loss did not improve from 0.15785\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5915 - val_loss: 0.1592\n",
      " ###1 fold : val acc1 0.551, acc3 0.948, mae 0.255###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/77 [===========================>..] - ETA: 0s - loss: 17.7193\n",
      "Epoch 00001: val_loss improved from inf to 10.97316, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 17.3543 - val_loss: 10.9732\n",
      "Epoch 2/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 8.1210\n",
      "Epoch 00002: val_loss improved from 10.97316 to 3.97731, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 7.8418 - val_loss: 3.9773\n",
      "Epoch 3/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 3.3998\n",
      "Epoch 00003: val_loss improved from 3.97731 to 1.69672, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.6291 - val_loss: 1.6967\n",
      "Epoch 4/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 2.6025\n",
      "Epoch 00004: val_loss improved from 1.69672 to 0.91735, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.4629 - val_loss: 0.9174\n",
      "Epoch 5/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 1.9708\n",
      "Epoch 00005: val_loss improved from 0.91735 to 0.58605, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.8975 - val_loss: 0.5861\n",
      "Epoch 6/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 1.3573\n",
      "Epoch 00006: val_loss improved from 0.58605 to 0.41683, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.3338 - val_loss: 0.4168\n",
      "Epoch 7/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 1.2229\n",
      "Epoch 00007: val_loss improved from 0.41683 to 0.33469, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.2076 - val_loss: 0.3347\n",
      "Epoch 8/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 1.0101\n",
      "Epoch 00008: val_loss improved from 0.33469 to 0.29394, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0596 - val_loss: 0.2939\n",
      "Epoch 9/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 1.2160\n",
      "Epoch 00009: val_loss improved from 0.29394 to 0.26493, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1797 - val_loss: 0.2649\n",
      "Epoch 10/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.9082\n",
      "Epoch 00010: val_loss improved from 0.26493 to 0.24875, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0799 - val_loss: 0.2487\n",
      "Epoch 11/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.9661\n",
      "Epoch 00011: val_loss improved from 0.24875 to 0.23026, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.9529 - val_loss: 0.2303\n",
      "Epoch 12/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.9090\n",
      "Epoch 00012: val_loss improved from 0.23026 to 0.22356, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8892 - val_loss: 0.2236\n",
      "Epoch 13/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.8172\n",
      "Epoch 00013: val_loss improved from 0.22356 to 0.20971, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8177 - val_loss: 0.2097\n",
      "Epoch 14/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.7912\n",
      "Epoch 00014: val_loss improved from 0.20971 to 0.20627, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7862 - val_loss: 0.2063\n",
      "Epoch 15/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8215\n",
      "Epoch 00015: val_loss improved from 0.20627 to 0.19414, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8044 - val_loss: 0.1941\n",
      "Epoch 16/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.9061\n",
      "Epoch 00016: val_loss improved from 0.19414 to 0.19322, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8738 - val_loss: 0.1932\n",
      "Epoch 17/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7295\n",
      "Epoch 00017: val_loss improved from 0.19322 to 0.19055, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7364 - val_loss: 0.1906\n",
      "Epoch 18/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7286\n",
      "Epoch 00018: val_loss improved from 0.19055 to 0.18351, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7211 - val_loss: 0.1835\n",
      "Epoch 19/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.7154\n",
      "Epoch 00019: val_loss improved from 0.18351 to 0.18092, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7111 - val_loss: 0.1809\n",
      "Epoch 20/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6830\n",
      "Epoch 00020: val_loss improved from 0.18092 to 0.17366, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6876 - val_loss: 0.1737\n",
      "Epoch 21/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6582\n",
      "Epoch 00021: val_loss did not improve from 0.17366\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8167 - val_loss: 0.1935\n",
      "Epoch 22/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6553\n",
      "Epoch 00022: val_loss improved from 0.17366 to 0.16937, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6561 - val_loss: 0.1694\n",
      "Epoch 23/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6505\n",
      "Epoch 00023: val_loss did not improve from 0.16937\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6513 - val_loss: 0.1716\n",
      "Epoch 24/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6786\n",
      "Epoch 00024: val_loss improved from 0.16937 to 0.16820, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6747 - val_loss: 0.1682\n",
      "Epoch 25/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6276\n",
      "Epoch 00025: val_loss improved from 0.16820 to 0.16472, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6433 - val_loss: 0.1647\n",
      "Epoch 26/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6203\n",
      "Epoch 00026: val_loss improved from 0.16472 to 0.15903, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6200 - val_loss: 0.1590\n",
      "Epoch 27/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6083\n",
      "Epoch 00027: val_loss did not improve from 0.15903\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6100 - val_loss: 0.1614\n",
      "Epoch 28/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.5925\n",
      "Epoch 00028: val_loss did not improve from 0.15903\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5932 - val_loss: 0.1610\n",
      " ###2 fold : val acc1 0.536, acc3 0.948, mae 0.264###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/77 [=======================>......] - ETA: 0s - loss: 18.3809\n",
      "Epoch 00001: val_loss improved from inf to 11.00776, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 17.3473 - val_loss: 11.0078\n",
      "Epoch 2/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 8.4945\n",
      "Epoch 00002: val_loss improved from 11.00776 to 3.99813, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 7.8683 - val_loss: 3.9981\n",
      "Epoch 3/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 3.4422\n",
      "Epoch 00003: val_loss improved from 3.99813 to 1.69253, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 3.6358 - val_loss: 1.6925\n",
      "Epoch 4/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 2.4722\n",
      "Epoch 00004: val_loss improved from 1.69253 to 0.91336, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.4665 - val_loss: 0.9134\n",
      "Epoch 5/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 2.0264\n",
      "Epoch 00005: val_loss improved from 0.91336 to 0.58328, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.8844 - val_loss: 0.5833\n",
      "Epoch 6/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.3682\n",
      "Epoch 00006: val_loss improved from 0.58328 to 0.41902, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.3335 - val_loss: 0.4190\n",
      "Epoch 7/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.2328\n",
      "Epoch 00007: val_loss improved from 0.41902 to 0.33604, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2024 - val_loss: 0.3360\n",
      "Epoch 8/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0633\n",
      "Epoch 00008: val_loss improved from 0.33604 to 0.29523, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0619 - val_loss: 0.2952\n",
      "Epoch 9/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.1927\n",
      "Epoch 00009: val_loss improved from 0.29523 to 0.26521, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1832 - val_loss: 0.2652\n",
      "Epoch 10/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.0797\n",
      "Epoch 00010: val_loss improved from 0.26521 to 0.24919, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0778 - val_loss: 0.2492\n",
      "Epoch 11/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.9535\n",
      "Epoch 00011: val_loss improved from 0.24919 to 0.23065, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9510 - val_loss: 0.2307\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8902\n",
      "Epoch 00012: val_loss improved from 0.23065 to 0.22305, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8902 - val_loss: 0.2231\n",
      "Epoch 13/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8178\n",
      "Epoch 00013: val_loss improved from 0.22305 to 0.21164, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8172 - val_loss: 0.2116\n",
      "Epoch 14/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7913\n",
      "Epoch 00014: val_loss improved from 0.21164 to 0.20547, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7864 - val_loss: 0.2055\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8027\n",
      "Epoch 00015: val_loss improved from 0.20547 to 0.19496, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8027 - val_loss: 0.1950\n",
      "Epoch 16/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8699\n",
      "Epoch 00016: val_loss improved from 0.19496 to 0.19374, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8697 - val_loss: 0.1937\n",
      "Epoch 17/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7410\n",
      "Epoch 00017: val_loss improved from 0.19374 to 0.19327, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7417 - val_loss: 0.1933\n",
      "Epoch 18/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.7259\n",
      "Epoch 00018: val_loss improved from 0.19327 to 0.18632, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7214 - val_loss: 0.1863\n",
      "Epoch 19/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7148\n",
      "Epoch 00019: val_loss improved from 0.18632 to 0.17975, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7150 - val_loss: 0.1798\n",
      "Epoch 20/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6906\n",
      "Epoch 00020: val_loss improved from 0.17975 to 0.17402, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6873 - val_loss: 0.1740\n",
      "Epoch 21/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8173\n",
      "Epoch 00021: val_loss did not improve from 0.17402\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8153 - val_loss: 0.1891\n",
      "Epoch 22/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6550\n",
      "Epoch 00022: val_loss improved from 0.17402 to 0.17015, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6555 - val_loss: 0.1701\n",
      "Epoch 23/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6458\n",
      "Epoch 00023: val_loss did not improve from 0.17015\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6473 - val_loss: 0.1703\n",
      "Epoch 24/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6783\n",
      "Epoch 00024: val_loss improved from 0.17015 to 0.16603, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6767 - val_loss: 0.1660\n",
      "Epoch 25/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6437\n",
      "Epoch 00025: val_loss improved from 0.16603 to 0.16522, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6456 - val_loss: 0.1652\n",
      "Epoch 26/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6160\n",
      "Epoch 00026: val_loss improved from 0.16522 to 0.15905, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6172 - val_loss: 0.1591\n",
      "Epoch 27/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6079\n",
      "Epoch 00027: val_loss did not improve from 0.15905\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6079 - val_loss: 0.1608\n",
      "Epoch 28/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.5886\n",
      "Epoch 00028: val_loss did not improve from 0.15905\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5897 - val_loss: 0.1612\n",
      " ###3 fold : val acc1 0.548, acc3 0.948, mae 0.257###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/77 [=======================>......] - ETA: 0s - loss: 18.2932\n",
      "Epoch 00001: val_loss improved from inf to 10.95627, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 17.2803 - val_loss: 10.9563\n",
      "Epoch 2/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 8.0616\n",
      "Epoch 00002: val_loss improved from 10.95627 to 3.72518, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 7.4895 - val_loss: 3.7252\n",
      "Epoch 3/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 3.9596\n",
      "Epoch 00003: val_loss improved from 3.72518 to 1.67276, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 3.6628 - val_loss: 1.6728\n",
      "Epoch 4/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 2.2827\n",
      "Epoch 00004: val_loss improved from 1.67276 to 0.86510, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.1749 - val_loss: 0.8651\n",
      "Epoch 5/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 2.1481\n",
      "Epoch 00005: val_loss improved from 0.86510 to 0.55009, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.9734 - val_loss: 0.5501\n",
      "Epoch 6/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.7168\n",
      "Epoch 00006: val_loss improved from 0.55009 to 0.40042, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7078 - val_loss: 0.4004\n",
      "Epoch 7/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 1.4336\n",
      "Epoch 00007: val_loss improved from 0.40042 to 0.32629, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4071 - val_loss: 0.3263\n",
      "Epoch 8/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.1189\n",
      "Epoch 00008: val_loss improved from 0.32629 to 0.28404, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1145 - val_loss: 0.2840\n",
      "Epoch 9/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.2289\n",
      "Epoch 00009: val_loss improved from 0.28404 to 0.27465, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2173 - val_loss: 0.2747\n",
      "Epoch 10/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.1893\n",
      "Epoch 00010: val_loss improved from 0.27465 to 0.24954, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1715 - val_loss: 0.2495\n",
      "Epoch 11/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.9637\n",
      "Epoch 00011: val_loss improved from 0.24954 to 0.23034, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9576 - val_loss: 0.2303\n",
      "Epoch 12/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.9225\n",
      "Epoch 00012: val_loss improved from 0.23034 to 0.22665, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9184 - val_loss: 0.2266\n",
      "Epoch 13/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.8344\n",
      "Epoch 00013: val_loss improved from 0.22665 to 0.21367, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8353 - val_loss: 0.2137\n",
      "Epoch 14/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8707\n",
      "Epoch 00014: val_loss improved from 0.21367 to 0.20398, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8689 - val_loss: 0.2040\n",
      "Epoch 15/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8313\n",
      "Epoch 00015: val_loss improved from 0.20398 to 0.20041, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8299 - val_loss: 0.2004\n",
      "Epoch 16/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.7903\n",
      "Epoch 00016: val_loss improved from 0.20041 to 0.19282, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7885 - val_loss: 0.1928\n",
      "Epoch 17/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7354\n",
      "Epoch 00017: val_loss improved from 0.19282 to 0.18734, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7371 - val_loss: 0.1873\n",
      "Epoch 18/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7354\n",
      "Epoch 00018: val_loss did not improve from 0.18734\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7344 - val_loss: 0.1878\n",
      "Epoch 19/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.7304\n",
      "Epoch 00019: val_loss improved from 0.18734 to 0.17905, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7328 - val_loss: 0.1790\n",
      "Epoch 20/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.7254\n",
      "Epoch 00020: val_loss improved from 0.17905 to 0.17683, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7209 - val_loss: 0.1768\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6923\n",
      "Epoch 00021: val_loss improved from 0.17683 to 0.17565, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6923 - val_loss: 0.1756\n",
      "Epoch 22/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6624\n",
      "Epoch 00022: val_loss improved from 0.17565 to 0.16960, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6608 - val_loss: 0.1696\n",
      "Epoch 23/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6562\n",
      "Epoch 00023: val_loss improved from 0.16960 to 0.16859, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6565 - val_loss: 0.1686\n",
      "Epoch 24/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6701\n",
      "Epoch 00024: val_loss improved from 0.16859 to 0.16690, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6691 - val_loss: 0.1669\n",
      "Epoch 25/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6426\n",
      "Epoch 00025: val_loss improved from 0.16690 to 0.16199, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6419 - val_loss: 0.1620\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6280\n",
      "Epoch 00026: val_loss did not improve from 0.16199\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6280 - val_loss: 0.1634\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6659\n",
      "Epoch 00027: val_loss improved from 0.16199 to 0.16047, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6659 - val_loss: 0.1605\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.5933\n",
      "Epoch 00028: val_loss did not improve from 0.16047\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5933 - val_loss: 0.1619\n",
      "Epoch 29/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.5924\n",
      "Epoch 00029: val_loss did not improve from 0.16047\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5916 - val_loss: 0.1683\n",
      " ###4 fold : val acc1 0.551, acc3 0.947, mae 0.257###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/77 [==========================>...] - ETA: 0s - loss: 17.7223\n",
      "Epoch 00001: val_loss improved from inf to 10.76682, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 17.1905 - val_loss: 10.7668\n",
      "Epoch 2/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 7.3951\n",
      "Epoch 00002: val_loss improved from 10.76682 to 3.61398, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 7.0250 - val_loss: 3.6140\n",
      "Epoch 3/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 3.2284\n",
      "Epoch 00003: val_loss improved from 3.61398 to 1.55508, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.1097 - val_loss: 1.5551\n",
      "Epoch 4/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 1.8911\n",
      "Epoch 00004: val_loss improved from 1.55508 to 0.74437, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.8432 - val_loss: 0.7444\n",
      "Epoch 5/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 1.3559\n",
      "Epoch 00005: val_loss improved from 0.74437 to 0.45761, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.3371 - val_loss: 0.4576\n",
      "Epoch 6/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 1.0968\n",
      "Epoch 00006: val_loss improved from 0.45761 to 0.34353, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0950 - val_loss: 0.3435\n",
      "Epoch 7/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 1.0107\n",
      "Epoch 00007: val_loss improved from 0.34353 to 0.28583, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0068 - val_loss: 0.2858\n",
      "Epoch 8/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.9653\n",
      "Epoch 00008: val_loss improved from 0.28583 to 0.25403, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.9564 - val_loss: 0.2540\n",
      "Epoch 9/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.8955\n",
      "Epoch 00009: val_loss improved from 0.25403 to 0.24125, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8939 - val_loss: 0.2413\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8649\n",
      "Epoch 00010: val_loss improved from 0.24125 to 0.22676, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8664 - val_loss: 0.2268\n",
      "Epoch 11/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.8271\n",
      "Epoch 00011: val_loss improved from 0.22676 to 0.21468, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8216 - val_loss: 0.2147\n",
      "Epoch 12/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8052\n",
      "Epoch 00012: val_loss improved from 0.21468 to 0.21050, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7968 - val_loss: 0.2105\n",
      "Epoch 13/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7740\n",
      "Epoch 00013: val_loss improved from 0.21050 to 0.20077, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7701 - val_loss: 0.2008\n",
      "Epoch 14/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7567\n",
      "Epoch 00014: val_loss improved from 0.20077 to 0.19325, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7485 - val_loss: 0.1932\n",
      "Epoch 15/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.7388\n",
      "Epoch 00015: val_loss improved from 0.19325 to 0.18762, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7353 - val_loss: 0.1876\n",
      "Epoch 16/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6969\n",
      "Epoch 00016: val_loss improved from 0.18762 to 0.18234, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6976 - val_loss: 0.1823\n",
      "Epoch 17/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.7003\n",
      "Epoch 00017: val_loss improved from 0.18234 to 0.18075, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6941 - val_loss: 0.1808\n",
      "Epoch 18/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6773\n",
      "Epoch 00018: val_loss did not improve from 0.18075\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6743 - val_loss: 0.1826\n",
      "Epoch 19/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6624\n",
      "Epoch 00019: val_loss improved from 0.18075 to 0.17327, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6652 - val_loss: 0.1733\n",
      "Epoch 20/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6492\n",
      "Epoch 00020: val_loss did not improve from 0.17327\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6495 - val_loss: 0.1754\n",
      "Epoch 21/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6437\n",
      "Epoch 00021: val_loss improved from 0.17327 to 0.17229, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6491 - val_loss: 0.1723\n",
      "Epoch 22/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6247\n",
      "Epoch 00022: val_loss improved from 0.17229 to 0.16453, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6281 - val_loss: 0.1645\n",
      "Epoch 23/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6189\n",
      "Epoch 00023: val_loss improved from 0.16453 to 0.16215, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6154 - val_loss: 0.1621\n",
      "Epoch 24/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6065\n",
      "Epoch 00024: val_loss improved from 0.16215 to 0.16202, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6161 - val_loss: 0.1620\n",
      "Epoch 25/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5993\n",
      "Epoch 00025: val_loss improved from 0.16202 to 0.15886, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6016 - val_loss: 0.1589\n",
      "Epoch 26/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5849\n",
      "Epoch 00026: val_loss did not improve from 0.15886\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5862 - val_loss: 0.1591\n",
      "Epoch 27/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5910\n",
      "Epoch 00027: val_loss did not improve from 0.15886\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5918 - val_loss: 0.1626\n",
      " ###5 fold : val acc1 0.549, acc3 0.955, mae 0.268###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/77 [=======================>......] - ETA: 0s - loss: 18.4066\n",
      "Epoch 00001: val_loss improved from inf to 10.95376, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 17.3060 - val_loss: 10.9538\n",
      "Epoch 2/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 7.4901\n",
      "Epoch 00002: val_loss improved from 10.95376 to 3.70146, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 7.4601 - val_loss: 3.7015\n",
      "Epoch 3/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 4.3975\n",
      "Epoch 00003: val_loss improved from 3.70146 to 1.70728, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.0947 - val_loss: 1.7073\n",
      "Epoch 4/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 2.5691\n",
      "Epoch 00004: val_loss improved from 1.70728 to 0.89126, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.3926 - val_loss: 0.8913\n",
      "Epoch 5/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 2.6328\n",
      "Epoch 00005: val_loss improved from 0.89126 to 0.57389, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.3593 - val_loss: 0.5739\n",
      "Epoch 6/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.5749\n",
      "Epoch 00006: val_loss improved from 0.57389 to 0.41192, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4961 - val_loss: 0.4119\n",
      "Epoch 7/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.1156\n",
      "Epoch 00007: val_loss improved from 0.41192 to 0.33737, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1873 - val_loss: 0.3374\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.0608\n",
      "Epoch 00008: val_loss improved from 0.33737 to 0.29003, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0608 - val_loss: 0.2900\n",
      "Epoch 9/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.9662\n",
      "Epoch 00009: val_loss improved from 0.29003 to 0.27075, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0043 - val_loss: 0.2708\n",
      "Epoch 10/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.4632\n",
      "Epoch 00010: val_loss improved from 0.27075 to 0.24768, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3391 - val_loss: 0.2477\n",
      "Epoch 11/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9925\n",
      "Epoch 00011: val_loss improved from 0.24768 to 0.23231, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9907 - val_loss: 0.2323\n",
      "Epoch 12/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.9194\n",
      "Epoch 00012: val_loss improved from 0.23231 to 0.22113, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9143 - val_loss: 0.2211\n",
      "Epoch 13/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.9451\n",
      "Epoch 00013: val_loss improved from 0.22113 to 0.21118, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9425 - val_loss: 0.2112\n",
      "Epoch 14/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.8707\n",
      "Epoch 00014: val_loss improved from 0.21118 to 0.20617, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8505 - val_loss: 0.2062\n",
      "Epoch 15/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8365\n",
      "Epoch 00015: val_loss improved from 0.20617 to 0.20068, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8356 - val_loss: 0.2007\n",
      "Epoch 16/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7633\n",
      "Epoch 00016: val_loss improved from 0.20068 to 0.19356, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7675 - val_loss: 0.1936\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.7419\n",
      "Epoch 00017: val_loss improved from 0.19356 to 0.18899, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7419 - val_loss: 0.1890\n",
      "Epoch 18/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7347\n",
      "Epoch 00018: val_loss improved from 0.18899 to 0.18724, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7339 - val_loss: 0.1872\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.7168\n",
      "Epoch 00019: val_loss improved from 0.18724 to 0.17742, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7168 - val_loss: 0.1774\n",
      "Epoch 20/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6976\n",
      "Epoch 00020: val_loss improved from 0.17742 to 0.17509, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6955 - val_loss: 0.1751\n",
      "Epoch 21/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6861\n",
      "Epoch 00021: val_loss improved from 0.17509 to 0.17264, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6867 - val_loss: 0.1726\n",
      "Epoch 22/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7067\n",
      "Epoch 00022: val_loss improved from 0.17264 to 0.16621, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7072 - val_loss: 0.1662\n",
      "Epoch 23/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7062\n",
      "Epoch 00023: val_loss did not improve from 0.16621\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7060 - val_loss: 0.1675\n",
      "Epoch 24/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.6536\n",
      "Epoch 00024: val_loss improved from 0.16621 to 0.16218, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6563 - val_loss: 0.1622\n",
      "Epoch 25/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6730\n",
      "Epoch 00025: val_loss improved from 0.16218 to 0.16205, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6718 - val_loss: 0.1621\n",
      "Epoch 26/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6473\n",
      "Epoch 00026: val_loss improved from 0.16205 to 0.16113, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6474 - val_loss: 0.1611\n",
      "Epoch 27/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6434\n",
      "Epoch 00027: val_loss did not improve from 0.16113\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6433 - val_loss: 0.1652\n",
      "Epoch 28/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.5992\n",
      "Epoch 00028: val_loss improved from 0.16113 to 0.16048, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6001 - val_loss: 0.1605\n",
      "Epoch 29/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6021\n",
      "Epoch 00029: val_loss did not improve from 0.16048\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6016 - val_loss: 0.1665\n",
      "Epoch 30/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6175\n",
      "Epoch 00030: val_loss improved from 0.16048 to 0.15552, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6147 - val_loss: 0.1555\n",
      "Epoch 31/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7041\n",
      "Epoch 00031: val_loss did not improve from 0.15552\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6769 - val_loss: 0.1612\n",
      "Epoch 32/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6098\n",
      "Epoch 00032: val_loss did not improve from 0.15552\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6061 - val_loss: 0.1593\n",
      " ###6 fold : val acc1 0.554, acc3 0.949, mae 0.254###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/77 [========================>.....] - ETA: 0s - loss: 18.1758\n",
      "Epoch 00001: val_loss improved from inf to 10.98397, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 17.3227 - val_loss: 10.9840\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 7.4501\n",
      "Epoch 00002: val_loss improved from 10.98397 to 3.71238, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 7.4671 - val_loss: 3.7124\n",
      "Epoch 3/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 4.3243\n",
      "Epoch 00003: val_loss improved from 3.71238 to 1.71665, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.0942 - val_loss: 1.7167\n",
      "Epoch 4/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 2.5157\n",
      "Epoch 00004: val_loss improved from 1.71665 to 0.90900, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.4061 - val_loss: 0.9090\n",
      "Epoch 5/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 2.5701\n",
      "Epoch 00005: val_loss improved from 0.90900 to 0.59047, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.3867 - val_loss: 0.5905\n",
      "Epoch 6/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 1.5447\n",
      "Epoch 00006: val_loss improved from 0.59047 to 0.41987, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.5201 - val_loss: 0.4199\n",
      "Epoch 7/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 1.1049\n",
      "Epoch 00007: val_loss improved from 0.41987 to 0.34376, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1978 - val_loss: 0.3438\n",
      "Epoch 8/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.0802\n",
      "Epoch 00008: val_loss improved from 0.34376 to 0.29486, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0671 - val_loss: 0.2949\n",
      "Epoch 9/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 1.0262\n",
      "Epoch 00009: val_loss improved from 0.29486 to 0.27313, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0160 - val_loss: 0.2731\n",
      "Epoch 10/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 1.4542\n",
      "Epoch 00010: val_loss improved from 0.27313 to 0.25020, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.3778 - val_loss: 0.2502\n",
      "Epoch 11/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.8916\n",
      "Epoch 00011: val_loss improved from 0.25020 to 0.23500, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0020 - val_loss: 0.2350\n",
      "Epoch 12/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.9552\n",
      "Epoch 00012: val_loss improved from 0.23500 to 0.22112, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9248 - val_loss: 0.2211\n",
      "Epoch 13/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.0110\n",
      "Epoch 00013: val_loss improved from 0.22112 to 0.21125, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9568 - val_loss: 0.2112\n",
      "Epoch 14/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.8851\n",
      "Epoch 00014: val_loss improved from 0.21125 to 0.20623, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8651 - val_loss: 0.2062\n",
      "Epoch 15/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.8710\n",
      "Epoch 00015: val_loss improved from 0.20623 to 0.20099, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8455 - val_loss: 0.2010\n",
      "Epoch 16/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7689\n",
      "Epoch 00016: val_loss improved from 0.20099 to 0.19443, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7732 - val_loss: 0.1944\n",
      "Epoch 17/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7453\n",
      "Epoch 00017: val_loss improved from 0.19443 to 0.19099, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7390 - val_loss: 0.1910\n",
      "Epoch 18/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7357\n",
      "Epoch 00018: val_loss improved from 0.19099 to 0.18552, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7332 - val_loss: 0.1855\n",
      "Epoch 19/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.7209\n",
      "Epoch 00019: val_loss improved from 0.18552 to 0.17596, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7176 - val_loss: 0.1760\n",
      "Epoch 20/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6963\n",
      "Epoch 00020: val_loss improved from 0.17596 to 0.17436, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6937 - val_loss: 0.1744\n",
      "Epoch 21/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6839\n",
      "Epoch 00021: val_loss improved from 0.17436 to 0.17149, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6858 - val_loss: 0.1715\n",
      "Epoch 22/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7124\n",
      "Epoch 00022: val_loss improved from 0.17149 to 0.16523, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7036 - val_loss: 0.1652\n",
      "Epoch 23/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7185\n",
      "Epoch 00023: val_loss did not improve from 0.16523\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7032 - val_loss: 0.1671\n",
      "Epoch 24/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6488\n",
      "Epoch 00024: val_loss improved from 0.16523 to 0.16120, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6533 - val_loss: 0.1612\n",
      "Epoch 25/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6844\n",
      "Epoch 00025: val_loss improved from 0.16120 to 0.16092, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6766 - val_loss: 0.1609\n",
      "Epoch 26/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6472\n",
      "Epoch 00026: val_loss improved from 0.16092 to 0.16015, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6440 - val_loss: 0.1601\n",
      "Epoch 27/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.6492\n",
      "Epoch 00027: val_loss did not improve from 0.16015\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6428 - val_loss: 0.1636\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6023\n",
      "Epoch 00028: val_loss improved from 0.16015 to 0.15993, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6023 - val_loss: 0.1599\n",
      "Epoch 29/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6075\n",
      "Epoch 00029: val_loss did not improve from 0.15993\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6008 - val_loss: 0.1613\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6102\n",
      "Epoch 00030: val_loss improved from 0.15993 to 0.15602, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6102 - val_loss: 0.1560\n",
      "Epoch 31/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7046\n",
      "Epoch 00031: val_loss did not improve from 0.15602\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6808 - val_loss: 0.1598\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6023\n",
      "Epoch 00032: val_loss did not improve from 0.15602\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6023 - val_loss: 0.1578\n",
      " ###7 fold : val acc1 0.563, acc3 0.950, mae 0.248###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 18.1013\n",
      "Epoch 00001: val_loss improved from inf to 11.06205, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 17.3225 - val_loss: 11.0620\n",
      "Epoch 2/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 7.5345\n",
      "Epoch 00002: val_loss improved from 11.06205 to 3.78372, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 7.4701 - val_loss: 3.7837\n",
      "Epoch 3/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 4.4094\n",
      "Epoch 00003: val_loss improved from 3.78372 to 1.76216, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 4.0806 - val_loss: 1.7622\n",
      "Epoch 4/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 2.5686\n",
      "Epoch 00004: val_loss improved from 1.76216 to 0.94501, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.4014 - val_loss: 0.9450\n",
      "Epoch 5/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 2.6322\n",
      "Epoch 00005: val_loss improved from 0.94501 to 0.60993, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.3738 - val_loss: 0.6099\n",
      "Epoch 6/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.5758\n",
      "Epoch 00006: val_loss improved from 0.60993 to 0.43136, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5104 - val_loss: 0.4314\n",
      "Epoch 7/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.1009\n",
      "Epoch 00007: val_loss improved from 0.43136 to 0.35152, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1942 - val_loss: 0.3515\n",
      "Epoch 8/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.0640\n",
      "Epoch 00008: val_loss improved from 0.35152 to 0.29910, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0637 - val_loss: 0.2991\n",
      "Epoch 9/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.0230\n",
      "Epoch 00009: val_loss improved from 0.29910 to 0.27569, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0184 - val_loss: 0.2757\n",
      "Epoch 10/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.3782\n",
      "Epoch 00010: val_loss improved from 0.27569 to 0.24961, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3735 - val_loss: 0.2496\n",
      "Epoch 11/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.9986\n",
      "Epoch 00011: val_loss improved from 0.24961 to 0.23254, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9992 - val_loss: 0.2325\n",
      "Epoch 12/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.9259\n",
      "Epoch 00012: val_loss improved from 0.23254 to 0.21829, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9233 - val_loss: 0.2183\n",
      "Epoch 13/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.9572\n",
      "Epoch 00013: val_loss improved from 0.21829 to 0.20546, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9528 - val_loss: 0.2055\n",
      "Epoch 14/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.8670\n",
      "Epoch 00014: val_loss improved from 0.20546 to 0.20057, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8636 - val_loss: 0.2006\n",
      "Epoch 15/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.8460\n",
      "Epoch 00015: val_loss improved from 0.20057 to 0.19598, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8439 - val_loss: 0.1960\n",
      "Epoch 16/100\n",
      "58/77 [=====================>........] - ETA: 0s - loss: 0.7667\n",
      "Epoch 00016: val_loss improved from 0.19598 to 0.18901, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7685 - val_loss: 0.1890\n",
      "Epoch 17/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7435\n",
      "Epoch 00017: val_loss improved from 0.18901 to 0.18532, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7374 - val_loss: 0.1853\n",
      "Epoch 18/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.7356\n",
      "Epoch 00018: val_loss improved from 0.18532 to 0.18066, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7332 - val_loss: 0.1807\n",
      "Epoch 19/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.7238\n",
      "Epoch 00019: val_loss improved from 0.18066 to 0.17194, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7197 - val_loss: 0.1719\n",
      "Epoch 20/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6936\n",
      "Epoch 00020: val_loss improved from 0.17194 to 0.17001, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6921 - val_loss: 0.1700\n",
      "Epoch 21/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6841\n",
      "Epoch 00021: val_loss improved from 0.17001 to 0.16733, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6874 - val_loss: 0.1673\n",
      "Epoch 22/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.7085\n",
      "Epoch 00022: val_loss improved from 0.16733 to 0.16134, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7031 - val_loss: 0.1613\n",
      "Epoch 23/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7182\n",
      "Epoch 00023: val_loss did not improve from 0.16134\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7031 - val_loss: 0.1624\n",
      "Epoch 24/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6562\n",
      "Epoch 00024: val_loss improved from 0.16134 to 0.15648, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6542 - val_loss: 0.1565\n",
      "Epoch 25/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6745\n",
      "Epoch 00025: val_loss did not improve from 0.15648\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6733 - val_loss: 0.1580\n",
      "Epoch 26/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6480\n",
      "Epoch 00026: val_loss improved from 0.15648 to 0.15627, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6474 - val_loss: 0.1563\n",
      "Epoch 27/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6428\n",
      "Epoch 00027: val_loss did not improve from 0.15627\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6426 - val_loss: 0.1595\n",
      "Epoch 28/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6007\n",
      "Epoch 00028: val_loss improved from 0.15627 to 0.15604, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6025 - val_loss: 0.1560\n",
      "Epoch 29/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6011\n",
      "Epoch 00029: val_loss did not improve from 0.15604\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6006 - val_loss: 0.1567\n",
      "Epoch 30/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6093\n",
      "Epoch 00030: val_loss improved from 0.15604 to 0.15326, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6068 - val_loss: 0.1533\n",
      "Epoch 31/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6816\n",
      "Epoch 00031: val_loss did not improve from 0.15326\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6779 - val_loss: 0.1560\n",
      "Epoch 32/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6018\n",
      "Epoch 00032: val_loss did not improve from 0.15326\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6015 - val_loss: 0.1542\n",
      " ###8 fold : val acc1 0.530, acc3 0.937, mae 0.272###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/77 [=========================>....] - ETA: 0s - loss: 18.0173\n",
      "Epoch 00001: val_loss improved from inf to 11.05424, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 17.3225 - val_loss: 11.0542\n",
      "Epoch 2/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 7.4898\n",
      "Epoch 00002: val_loss improved from 11.05424 to 3.76293, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 7.4701 - val_loss: 3.7629\n",
      "Epoch 3/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 4.3605\n",
      "Epoch 00003: val_loss improved from 3.76293 to 1.75733, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 4.0806 - val_loss: 1.7573\n",
      "Epoch 4/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 2.5826\n",
      "Epoch 00004: val_loss improved from 1.75733 to 0.93501, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.4014 - val_loss: 0.9350\n",
      "Epoch 5/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 2.5164\n",
      "Epoch 00005: val_loss improved from 0.93501 to 0.60128, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.3738 - val_loss: 0.6013\n",
      "Epoch 6/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 1.5568\n",
      "Epoch 00006: val_loss improved from 0.60128 to 0.42131, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.5104 - val_loss: 0.4213\n",
      "Epoch 7/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.1051\n",
      "Epoch 00007: val_loss improved from 0.42131 to 0.34139, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1942 - val_loss: 0.3414\n",
      "Epoch 8/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.0863\n",
      "Epoch 00008: val_loss improved from 0.34139 to 0.28861, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0637 - val_loss: 0.2886\n",
      "Epoch 9/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.9728\n",
      "Epoch 00009: val_loss improved from 0.28861 to 0.26674, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0184 - val_loss: 0.2667\n",
      "Epoch 10/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.5133\n",
      "Epoch 00010: val_loss improved from 0.26674 to 0.24181, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3735 - val_loss: 0.2418\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.9992\n",
      "Epoch 00011: val_loss improved from 0.24181 to 0.22599, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9992 - val_loss: 0.2260\n",
      "Epoch 12/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.9504\n",
      "Epoch 00012: val_loss improved from 0.22599 to 0.21188, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9233 - val_loss: 0.2119\n",
      "Epoch 13/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.0056\n",
      "Epoch 00013: val_loss improved from 0.21188 to 0.20075, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9528 - val_loss: 0.2007\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8636\n",
      "Epoch 00014: val_loss improved from 0.20075 to 0.19610, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8636 - val_loss: 0.1961\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8439\n",
      "Epoch 00015: val_loss improved from 0.19610 to 0.19122, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8439 - val_loss: 0.1912\n",
      "Epoch 16/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7660\n",
      "Epoch 00016: val_loss improved from 0.19122 to 0.18562, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7685 - val_loss: 0.1856\n",
      "Epoch 17/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7438\n",
      "Epoch 00017: val_loss improved from 0.18562 to 0.18256, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7374 - val_loss: 0.1826\n",
      "Epoch 18/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7357\n",
      "Epoch 00018: val_loss improved from 0.18256 to 0.17725, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7332 - val_loss: 0.1772\n",
      "Epoch 19/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7256\n",
      "Epoch 00019: val_loss improved from 0.17725 to 0.16906, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7197 - val_loss: 0.1691\n",
      "Epoch 20/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.6928\n",
      "Epoch 00020: val_loss improved from 0.16906 to 0.16812, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6921 - val_loss: 0.1681\n",
      "Epoch 21/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6841\n",
      "Epoch 00021: val_loss improved from 0.16812 to 0.16541, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6874 - val_loss: 0.1654\n",
      "Epoch 22/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7099\n",
      "Epoch 00022: val_loss improved from 0.16541 to 0.15969, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7031 - val_loss: 0.1597\n",
      "Epoch 23/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7178\n",
      "Epoch 00023: val_loss did not improve from 0.15969\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7031 - val_loss: 0.1617\n",
      "Epoch 24/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6484\n",
      "Epoch 00024: val_loss improved from 0.15969 to 0.15577, saving model to result/size/DNN_size_both_y/batch256,dnodes64_dropout0.4,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6542 - val_loss: 0.1558\n",
      "Epoch 25/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6830\n",
      "Epoch 00025: val_loss did not improve from 0.15577\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6733 - val_loss: 0.1566\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6474\n",
      "Epoch 00026: val_loss did not improve from 0.15577\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6474 - val_loss: 0.1559\n",
      " ###9 fold : val acc1 0.530, acc3 0.944, mae 0.270###\n",
      "acc10.546_acc30.946\n",
      "random search 14/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603/613 [============================>.] - ETA: 0s - loss: 2.0491\n",
      "Epoch 00001: val_loss improved from inf to 0.17032, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.0197 - val_loss: 0.1703\n",
      "Epoch 2/100\n",
      "595/613 [============================>.] - ETA: 0s - loss: 0.1699\n",
      "Epoch 00002: val_loss improved from 0.17032 to 0.13618, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1686 - val_loss: 0.1362\n",
      "Epoch 3/100\n",
      "608/613 [============================>.] - ETA: 0s - loss: 0.1321\n",
      "Epoch 00003: val_loss did not improve from 0.13618\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1322 - val_loss: 0.1522\n",
      "Epoch 4/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.1348\n",
      "Epoch 00004: val_loss did not improve from 0.13618\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1347 - val_loss: 0.1415\n",
      " ###0 fold : val acc1 0.561, acc3 0.959, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607/613 [============================>.] - ETA: 0s - loss: 2.0398\n",
      "Epoch 00001: val_loss improved from inf to 0.16963, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.0230 - val_loss: 0.1696\n",
      "Epoch 2/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.1779\n",
      "Epoch 00002: val_loss improved from 0.16963 to 0.13545, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1772 - val_loss: 0.1355\n",
      "Epoch 3/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.1324\n",
      "Epoch 00003: val_loss did not improve from 0.13545\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1325 - val_loss: 0.1563\n",
      "Epoch 4/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.1323\n",
      "Epoch 00004: val_loss did not improve from 0.13545\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1318 - val_loss: 0.1376\n",
      " ###1 fold : val acc1 0.574, acc3 0.962, mae 0.236###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599/613 [============================>.] - ETA: 0s - loss: 2.0748\n",
      "Epoch 00001: val_loss improved from inf to 0.17259, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_2.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.0323 - val_loss: 0.1726\n",
      "Epoch 2/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.1757\n",
      "Epoch 00002: val_loss improved from 0.17259 to 0.13557, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_2.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1755 - val_loss: 0.1356\n",
      "Epoch 3/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.1330\n",
      "Epoch 00003: val_loss did not improve from 0.13557\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1333 - val_loss: 0.1561\n",
      "Epoch 4/100\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.1331\n",
      "Epoch 00004: val_loss did not improve from 0.13557\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1331 - val_loss: 0.1368\n",
      " ###2 fold : val acc1 0.559, acc3 0.962, mae 0.243###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603/613 [============================>.] - ETA: 0s - loss: 2.0784\n",
      "Epoch 00001: val_loss improved from inf to 0.17194, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.0485 - val_loss: 0.1719\n",
      "Epoch 2/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.1807\n",
      "Epoch 00002: val_loss improved from 0.17194 to 0.13693, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1800 - val_loss: 0.1369\n",
      "Epoch 3/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.1362\n",
      "Epoch 00003: val_loss did not improve from 0.13693\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1363 - val_loss: 0.1607\n",
      "Epoch 4/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.1330\n",
      "Epoch 00004: val_loss did not improve from 0.13693\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1328 - val_loss: 0.1379\n",
      " ###3 fold : val acc1 0.555, acc3 0.968, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608/613 [============================>.] - ETA: 0s - loss: 2.0348\n",
      "Epoch 00001: val_loss improved from inf to 0.16603, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.0202 - val_loss: 0.1660\n",
      "Epoch 2/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.1931\n",
      "Epoch 00002: val_loss improved from 0.16603 to 0.14146, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1928 - val_loss: 0.1415\n",
      "Epoch 3/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.1353\n",
      "Epoch 00003: val_loss improved from 0.14146 to 0.13588, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1348 - val_loss: 0.1359\n",
      "Epoch 4/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.1299\n",
      "Epoch 00004: val_loss improved from 0.13588 to 0.13403, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1299 - val_loss: 0.1340\n",
      "Epoch 5/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.1501\n",
      "Epoch 00005: val_loss did not improve from 0.13403\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1506 - val_loss: 0.1364\n",
      "Epoch 6/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.1899\n",
      "Epoch 00006: val_loss did not improve from 0.13403\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1883 - val_loss: 0.1378\n",
      " ###4 fold : val acc1 0.585, acc3 0.961, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607/613 [============================>.] - ETA: 0s - loss: 1.8916\n",
      "Epoch 00001: val_loss improved from inf to 0.16306, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 1.8752 - val_loss: 0.1631\n",
      "Epoch 2/100\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.1374\n",
      "Epoch 00002: val_loss improved from 0.16306 to 0.13921, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1374 - val_loss: 0.1392\n",
      "Epoch 3/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.1304\n",
      "Epoch 00003: val_loss improved from 0.13921 to 0.13587, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1300 - val_loss: 0.1359\n",
      "Epoch 4/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.1283\n",
      "Epoch 00004: val_loss improved from 0.13587 to 0.13087, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1283 - val_loss: 0.1309\n",
      "Epoch 5/100\n",
      "596/613 [============================>.] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00005: val_loss did not improve from 0.13087\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1265 - val_loss: 0.1348\n",
      "Epoch 6/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00006: val_loss improved from 0.13087 to 0.13046, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1270 - val_loss: 0.1305\n",
      "Epoch 7/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.1270\n",
      "Epoch 00007: val_loss improved from 0.13046 to 0.12886, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1269 - val_loss: 0.1289\n",
      "Epoch 8/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00008: val_loss improved from 0.12886 to 0.12842, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1262 - val_loss: 0.1284\n",
      "Epoch 9/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00009: val_loss did not improve from 0.12842\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1254 - val_loss: 0.1294\n",
      "Epoch 10/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00010: val_loss did not improve from 0.12842\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1263 - val_loss: 0.1311\n",
      " ###5 fold : val acc1 0.567, acc3 0.972, mae 0.240###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608/613 [============================>.] - ETA: 0s - loss: 2.2401\n",
      "Epoch 00001: val_loss improved from inf to 0.16829, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.2238 - val_loss: 0.1683\n",
      "Epoch 2/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.2819\n",
      "Epoch 00002: val_loss improved from 0.16829 to 0.14827, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2797 - val_loss: 0.1483\n",
      "Epoch 3/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.1467\n",
      "Epoch 00003: val_loss improved from 0.14827 to 0.13450, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1461 - val_loss: 0.1345\n",
      "Epoch 4/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.1290\n",
      "Epoch 00004: val_loss improved from 0.13450 to 0.12991, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1287 - val_loss: 0.1299\n",
      "Epoch 5/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.1261\n",
      "Epoch 00005: val_loss did not improve from 0.12991\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1262 - val_loss: 0.1312\n",
      "Epoch 6/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.1359\n",
      "Epoch 00006: val_loss did not improve from 0.12991\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1359 - val_loss: 0.1317\n",
      " ###6 fold : val acc1 0.571, acc3 0.961, mae 0.238###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611/613 [============================>.] - ETA: 0s - loss: 2.1803\n",
      "Epoch 00001: val_loss improved from inf to 0.16567, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.1744 - val_loss: 0.1657\n",
      "Epoch 2/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.3142\n",
      "Epoch 00002: val_loss improved from 0.16567 to 0.15129, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.3107 - val_loss: 0.1513\n",
      "Epoch 3/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.1415\n",
      "Epoch 00003: val_loss improved from 0.15129 to 0.13432, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1410 - val_loss: 0.1343\n",
      "Epoch 4/100\n",
      "596/613 [============================>.] - ETA: 0s - loss: 0.1286\n",
      "Epoch 00004: val_loss improved from 0.13432 to 0.12894, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1285 - val_loss: 0.1289\n",
      "Epoch 5/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.1269\n",
      "Epoch 00005: val_loss did not improve from 0.12894\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1271 - val_loss: 0.1321\n",
      "Epoch 6/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.1389\n",
      "Epoch 00006: val_loss did not improve from 0.12894\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1390 - val_loss: 0.1335\n",
      " ###7 fold : val acc1 0.576, acc3 0.965, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/613 [============================>.] - ETA: 0s - loss: 2.2355\n",
      "Epoch 00001: val_loss improved from inf to 0.16643, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.1784 - val_loss: 0.1664\n",
      "Epoch 2/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.3063\n",
      "Epoch 00002: val_loss improved from 0.16643 to 0.14960, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.3028 - val_loss: 0.1496\n",
      "Epoch 3/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.1355\n",
      "Epoch 00003: val_loss improved from 0.14960 to 0.13257, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1351 - val_loss: 0.1326\n",
      "Epoch 4/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.1288\n",
      "Epoch 00004: val_loss improved from 0.13257 to 0.12685, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1286 - val_loss: 0.1268\n",
      "Epoch 5/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.1267\n",
      "Epoch 00005: val_loss did not improve from 0.12685\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1267 - val_loss: 0.1291\n",
      "Epoch 6/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.1357\n",
      "Epoch 00006: val_loss did not improve from 0.12685\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1359 - val_loss: 0.1321\n",
      " ###8 fold : val acc1 0.569, acc3 0.958, mae 0.241###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606/613 [============================>.] - ETA: 0s - loss: 2.2013\n",
      "Epoch 00001: val_loss improved from inf to 0.16117, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.1784 - val_loss: 0.1612\n",
      "Epoch 2/100\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.3028\n",
      "Epoch 00002: val_loss improved from 0.16117 to 0.14904, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.3028 - val_loss: 0.1490\n",
      "Epoch 3/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.1351\n",
      "Epoch 00003: val_loss improved from 0.14904 to 0.13268, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1351 - val_loss: 0.1327\n",
      "Epoch 4/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.1288\n",
      "Epoch 00004: val_loss improved from 0.13268 to 0.12780, saving model to result/size/DNN_size_both_y/batch32,dnodes64_dropout0,lr0.002/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1286 - val_loss: 0.1278\n",
      "Epoch 5/100\n",
      "593/613 [============================>.] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00005: val_loss did not improve from 0.12780\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1267 - val_loss: 0.1306\n",
      "Epoch 6/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.1357\n",
      "Epoch 00006: val_loss did not improve from 0.12780\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1359 - val_loss: 0.1313\n",
      " ###9 fold : val acc1 0.584, acc3 0.963, mae 0.232###\n",
      "acc10.570_acc30.963\n",
      "random search 15/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "289/307 [===========================>..] - ETA: 0s - loss: 1.7991\n",
      "Epoch 00001: val_loss improved from inf to 0.18663, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 1.7071 - val_loss: 0.1866\n",
      "Epoch 2/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.1561\n",
      "Epoch 00002: val_loss improved from 0.18663 to 0.13878, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1554 - val_loss: 0.1388\n",
      "Epoch 3/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.1283\n",
      "Epoch 00003: val_loss did not improve from 0.13878\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1316 - val_loss: 0.1656\n",
      "Epoch 4/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.1443\n",
      "Epoch 00004: val_loss did not improve from 0.13878\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1447 - val_loss: 0.1666\n",
      " ###0 fold : val acc1 0.556, acc3 0.957, mae 0.249###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/307 [============================>.] - ETA: 0s - loss: 1.7290\n",
      "Epoch 00001: val_loss improved from inf to 0.18432, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.7221 - val_loss: 0.1843\n",
      "Epoch 2/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.1634\n",
      "Epoch 00002: val_loss improved from 0.18432 to 0.13828, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1627 - val_loss: 0.1383\n",
      "Epoch 3/100\n",
      "291/307 [===========================>..] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00003: val_loss did not improve from 0.13828\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1325 - val_loss: 0.1736\n",
      "Epoch 4/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.1443\n",
      "Epoch 00004: val_loss did not improve from 0.13828\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1442 - val_loss: 0.1600\n",
      " ###1 fold : val acc1 0.566, acc3 0.959, mae 0.241###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291/307 [===========================>..] - ETA: 0s - loss: 1.8150\n",
      "Epoch 00001: val_loss improved from inf to 0.18835, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.7335 - val_loss: 0.1883\n",
      "Epoch 2/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.1663\n",
      "Epoch 00002: val_loss improved from 0.18835 to 0.13959, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1653 - val_loss: 0.1396\n",
      "Epoch 3/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.1291\n",
      "Epoch 00003: val_loss did not improve from 0.13959\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1325 - val_loss: 0.1664\n",
      "Epoch 4/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.1454\n",
      "Epoch 00004: val_loss did not improve from 0.13959\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1455 - val_loss: 0.1628\n",
      " ###2 fold : val acc1 0.558, acc3 0.959, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/307 [===========================>..] - ETA: 0s - loss: 1.7941\n",
      "Epoch 00001: val_loss improved from inf to 0.19030, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 1.7394 - val_loss: 0.1903\n",
      "Epoch 2/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.1686\n",
      "Epoch 00002: val_loss improved from 0.19030 to 0.14239, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1684 - val_loss: 0.1424\n",
      "Epoch 3/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.1338\n",
      "Epoch 00003: val_loss did not improve from 0.14239\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1338 - val_loss: 0.1593\n",
      "Epoch 4/100\n",
      "290/307 [===========================>..] - ETA: 0s - loss: 0.1457\n",
      "Epoch 00004: val_loss did not improve from 0.14239\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1474 - val_loss: 0.1702\n",
      " ###3 fold : val acc1 0.543, acc3 0.967, mae 0.248###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/307 [===========================>..] - ETA: 0s - loss: 1.7846\n",
      "Epoch 00001: val_loss improved from inf to 0.17022, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 1.7193 - val_loss: 0.1702\n",
      "Epoch 2/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1985\n",
      "Epoch 00002: val_loss improved from 0.17022 to 0.16355, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1973 - val_loss: 0.1635\n",
      "Epoch 3/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.1354\n",
      "Epoch 00003: val_loss improved from 0.16355 to 0.13486, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1352 - val_loss: 0.1349\n",
      "Epoch 4/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.1283\n",
      "Epoch 00004: val_loss improved from 0.13486 to 0.12862, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1279 - val_loss: 0.1286\n",
      "Epoch 5/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.1321\n",
      "Epoch 00005: val_loss did not improve from 0.12862\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1324 - val_loss: 0.1320\n",
      "Epoch 6/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.1659\n",
      "Epoch 00006: val_loss did not improve from 0.12862\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1653 - val_loss: 0.1357\n",
      " ###4 fold : val acc1 0.581, acc3 0.964, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/307 [============================>.] - ETA: 0s - loss: 1.5934\n",
      "Epoch 00001: val_loss improved from inf to 0.16560, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 1.5916 - val_loss: 0.1656\n",
      "Epoch 2/100\n",
      "288/307 [===========================>..] - ETA: 0s - loss: 0.1390\n",
      "Epoch 00002: val_loss improved from 0.16560 to 0.13973, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1396 - val_loss: 0.1397\n",
      "Epoch 3/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.1310\n",
      "Epoch 00003: val_loss improved from 0.13973 to 0.13829, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1308 - val_loss: 0.1383\n",
      "Epoch 4/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00004: val_loss improved from 0.13829 to 0.13116, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1284 - val_loss: 0.1312\n",
      "Epoch 5/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00005: val_loss improved from 0.13116 to 0.13005, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1268 - val_loss: 0.1300\n",
      "Epoch 6/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.1267\n",
      "Epoch 00006: val_loss did not improve from 0.13005\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1267 - val_loss: 0.1317\n",
      "Epoch 7/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.1259\n",
      "Epoch 00007: val_loss did not improve from 0.13005\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1265 - val_loss: 0.1374\n",
      " ###5 fold : val acc1 0.575, acc3 0.968, mae 0.236###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/307 [===========================>..] - ETA: 0s - loss: 2.1368\n",
      "Epoch 00001: val_loss improved from inf to 0.17387, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 2.0693 - val_loss: 0.1739\n",
      "Epoch 2/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.2684\n",
      "Epoch 00002: val_loss improved from 0.17387 to 0.15745, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2672 - val_loss: 0.1574\n",
      "Epoch 3/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.1491\n",
      "Epoch 00003: val_loss improved from 0.15745 to 0.13653, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1484 - val_loss: 0.1365\n",
      "Epoch 4/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.1275\n",
      "Epoch 00004: val_loss improved from 0.13653 to 0.13023, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1270 - val_loss: 0.1302\n",
      "Epoch 5/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.1267\n",
      "Epoch 00005: val_loss improved from 0.13023 to 0.12913, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1269 - val_loss: 0.1291\n",
      "Epoch 6/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.1271\n",
      "Epoch 00006: val_loss did not improve from 0.12913\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1272 - val_loss: 0.1303\n",
      "Epoch 7/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.1364\n",
      "Epoch 00007: val_loss did not improve from 0.12913\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1363 - val_loss: 0.1751\n",
      " ###6 fold : val acc1 0.567, acc3 0.963, mae 0.239###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/307 [===========================>..] - ETA: 0s - loss: 2.0877\n",
      "Epoch 00001: val_loss improved from inf to 0.17013, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 2.0222 - val_loss: 0.1701\n",
      "Epoch 2/100\n",
      "291/307 [===========================>..] - ETA: 0s - loss: 0.1416\n",
      "Epoch 00002: val_loss improved from 0.17013 to 0.16455, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3002 - val_loss: 0.1646\n",
      "Epoch 3/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.1393\n",
      "Epoch 00003: val_loss improved from 0.16455 to 0.13784, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1389 - val_loss: 0.1378\n",
      "Epoch 4/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.1270\n",
      "Epoch 00004: val_loss improved from 0.13784 to 0.12885, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1270 - val_loss: 0.1289\n",
      "Epoch 5/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.1295\n",
      "Epoch 00005: val_loss did not improve from 0.12885\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1297 - val_loss: 0.1335\n",
      "Epoch 6/100\n",
      "289/307 [===========================>..] - ETA: 0s - loss: 0.1401\n",
      "Epoch 00006: val_loss did not improve from 0.12885\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1399 - val_loss: 0.1351\n",
      " ###7 fold : val acc1 0.586, acc3 0.965, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/307 [============================>.] - ETA: 0s - loss: 2.0685\n",
      "Epoch 00001: val_loss improved from inf to 0.17102, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 2.0347 - val_loss: 0.1710\n",
      "Epoch 2/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.2998\n",
      "Epoch 00002: val_loss improved from 0.17102 to 0.16127, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2958 - val_loss: 0.1613\n",
      "Epoch 3/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1389\n",
      "Epoch 00003: val_loss improved from 0.16127 to 0.13714, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1385 - val_loss: 0.1371\n",
      "Epoch 4/100\n",
      "289/307 [===========================>..] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00004: val_loss improved from 0.13714 to 0.12815, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1274 - val_loss: 0.1282\n",
      "Epoch 5/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00005: val_loss improved from 0.12815 to 0.12812, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1253 - val_loss: 0.1281\n",
      "Epoch 6/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00006: val_loss did not improve from 0.12812\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1292 - val_loss: 0.1310\n",
      "Epoch 7/100\n",
      "289/307 [===========================>..] - ETA: 0s - loss: 0.1293\n",
      "Epoch 00007: val_loss did not improve from 0.12812\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1319 - val_loss: 0.1700\n",
      " ###8 fold : val acc1 0.566, acc3 0.959, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283/307 [==========================>...] - ETA: 0s - loss: 2.1889\n",
      "Epoch 00001: val_loss improved from inf to 0.16568, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 2.0347 - val_loss: 0.1657\n",
      "Epoch 2/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.2958\n",
      "Epoch 00002: val_loss improved from 0.16568 to 0.16014, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.2958 - val_loss: 0.1601\n",
      "Epoch 3/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.1389\n",
      "Epoch 00003: val_loss improved from 0.16014 to 0.13591, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1385 - val_loss: 0.1359\n",
      "Epoch 4/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.1275\n",
      "Epoch 00004: val_loss improved from 0.13591 to 0.12850, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0,lr0.002/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1274 - val_loss: 0.1285\n",
      "Epoch 5/100\n",
      "288/307 [===========================>..] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00005: val_loss did not improve from 0.12850\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1253 - val_loss: 0.1293\n",
      "Epoch 6/100\n",
      "289/307 [===========================>..] - ETA: 0s - loss: 0.1288\n",
      "Epoch 00006: val_loss did not improve from 0.12850\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.1292 - val_loss: 0.1306\n",
      " ###9 fold : val acc1 0.585, acc3 0.961, mae 0.231###\n",
      "acc10.568_acc30.962\n",
      "random search 16/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608/613 [============================>.] - ETA: 0s - loss: 0.8941\n",
      "Epoch 00001: val_loss improved from inf to 0.13499, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.8883 - val_loss: 0.1350\n",
      "Epoch 2/100\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.1704\n",
      "Epoch 00002: val_loss improved from 0.13499 to 0.13116, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1704 - val_loss: 0.1312\n",
      "Epoch 3/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.1460\n",
      "Epoch 00003: val_loss did not improve from 0.13116\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1550 - val_loss: 0.2014\n",
      "Epoch 4/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.2174\n",
      "Epoch 00004: val_loss did not improve from 0.13116\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2179 - val_loss: 0.1812\n",
      " ###0 fold : val acc1 0.565, acc3 0.961, mae 0.241###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/613 [============================>.] - ETA: 0s - loss: 0.8966\n",
      "Epoch 00001: val_loss improved from inf to 0.13769, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.8813 - val_loss: 0.1377\n",
      "Epoch 2/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.1717\n",
      "Epoch 00002: val_loss improved from 0.13769 to 0.13291, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1709 - val_loss: 0.1329\n",
      "Epoch 3/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.1527\n",
      "Epoch 00003: val_loss did not improve from 0.13291\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1537 - val_loss: 0.1861\n",
      "Epoch 4/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.1837\n",
      "Epoch 00004: val_loss did not improve from 0.13291\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1844 - val_loss: 0.1811\n",
      " ###1 fold : val acc1 0.552, acc3 0.961, mae 0.247###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610/613 [============================>.] - ETA: 0s - loss: 0.8876\n",
      "Epoch 00001: val_loss improved from inf to 0.13857, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_2.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.8844 - val_loss: 0.1386\n",
      "Epoch 2/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.1747\n",
      "Epoch 00002: val_loss did not improve from 0.13857\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1742 - val_loss: 0.1397\n",
      "Epoch 3/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.1506\n",
      "Epoch 00003: val_loss did not improve from 0.13857\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1511 - val_loss: 0.1865\n",
      " ###2 fold : val acc1 0.546, acc3 0.960, mae 0.251###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612/613 [============================>.] - ETA: 0s - loss: 0.9034\n",
      "Epoch 00001: val_loss improved from inf to 0.13958, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.9024 - val_loss: 0.1396\n",
      "Epoch 2/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.1633\n",
      "Epoch 00002: val_loss did not improve from 0.13958\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1631 - val_loss: 0.1424\n",
      "Epoch 3/100\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.1551\n",
      "Epoch 00003: val_loss did not improve from 0.13958\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1551 - val_loss: 0.1922\n",
      " ###3 fold : val acc1 0.553, acc3 0.962, mae 0.245###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/613 [============================>.] - ETA: 0s - loss: 0.9282\n",
      "Epoch 00001: val_loss improved from inf to 0.13862, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.9171 - val_loss: 0.1386\n",
      "Epoch 2/100\n",
      "596/613 [============================>.] - ETA: 0s - loss: 0.1855\n",
      "Epoch 00002: val_loss did not improve from 0.13862\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1850 - val_loss: 0.1424\n",
      "Epoch 3/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.1611\n",
      "Epoch 00003: val_loss improved from 0.13862 to 0.13555, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1602 - val_loss: 0.1355\n",
      "Epoch 4/100\n",
      "596/613 [============================>.] - ETA: 0s - loss: 0.1616\n",
      "Epoch 00004: val_loss did not improve from 0.13555\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1614 - val_loss: 0.1480\n",
      "Epoch 5/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00005: val_loss did not improve from 0.13555\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2060 - val_loss: 0.1601\n",
      " ###4 fold : val acc1 0.569, acc3 0.963, mae 0.237###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602/613 [============================>.] - ETA: 0s - loss: 0.7646\n",
      "Epoch 00001: val_loss improved from inf to 0.14359, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_5.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 0.7538 - val_loss: 0.1436\n",
      "Epoch 2/100\n",
      "597/613 [============================>.] - ETA: 0s - loss: 0.1519\n",
      "Epoch 00002: val_loss improved from 0.14359 to 0.13679, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1525 - val_loss: 0.1368\n",
      "Epoch 3/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.1492\n",
      "Epoch 00003: val_loss improved from 0.13679 to 0.12937, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1489 - val_loss: 0.1294\n",
      "Epoch 4/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.1497\n",
      "Epoch 00004: val_loss did not improve from 0.12937\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1498 - val_loss: 0.1376\n",
      "Epoch 5/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.1491\n",
      "Epoch 00005: val_loss did not improve from 0.12937\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1492 - val_loss: 0.1614\n",
      " ###5 fold : val acc1 0.581, acc3 0.965, mae 0.232###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610/613 [============================>.] - ETA: 0s - loss: 1.0102\n",
      "Epoch 00001: val_loss improved from inf to 0.14008, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 1.0063 - val_loss: 0.1401\n",
      "Epoch 2/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.1683\n",
      "Epoch 00002: val_loss did not improve from 0.14008\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1682 - val_loss: 0.1650\n",
      "Epoch 3/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.1605\n",
      "Epoch 00003: val_loss improved from 0.14008 to 0.13018, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1601 - val_loss: 0.1302\n",
      "Epoch 4/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.1532\n",
      "Epoch 00004: val_loss did not improve from 0.13018\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1527 - val_loss: 0.1418\n",
      "Epoch 5/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.2225\n",
      "Epoch 00005: val_loss did not improve from 0.13018\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2224 - val_loss: 0.1454\n",
      " ###6 fold : val acc1 0.565, acc3 0.960, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/613 [============================>.] - ETA: 0s - loss: 1.0137\n",
      "Epoch 00001: val_loss improved from inf to 0.14101, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.9902 - val_loss: 0.1410\n",
      "Epoch 2/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.1867\n",
      "Epoch 00002: val_loss did not improve from 0.14101\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1866 - val_loss: 0.1838\n",
      "Epoch 3/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.1589\n",
      "Epoch 00003: val_loss improved from 0.14101 to 0.13169, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1582 - val_loss: 0.1317\n",
      "Epoch 4/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.1465\n",
      "Epoch 00004: val_loss did not improve from 0.13169\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1464 - val_loss: 0.1384\n",
      "Epoch 5/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.1554\n",
      "Epoch 00005: val_loss did not improve from 0.13169\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1557 - val_loss: 0.1462\n",
      " ###7 fold : val acc1 0.565, acc3 0.963, mae 0.240###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/613 [============================>.] - ETA: 0s - loss: 1.0162\n",
      "Epoch 00001: val_loss improved from inf to 0.13751, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.9926 - val_loss: 0.1375\n",
      "Epoch 2/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.1848\n",
      "Epoch 00002: val_loss did not improve from 0.13751\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1845 - val_loss: 0.1792\n",
      "Epoch 3/100\n",
      "596/613 [============================>.] - ETA: 0s - loss: 0.1556\n",
      "Epoch 00003: val_loss improved from 0.13751 to 0.13167, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1550 - val_loss: 0.1317\n",
      "Epoch 4/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.1496\n",
      "Epoch 00004: val_loss did not improve from 0.13167\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1493 - val_loss: 0.1384\n",
      "Epoch 5/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.1996\n",
      "Epoch 00005: val_loss did not improve from 0.13167\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1994 - val_loss: 0.1565\n",
      " ###8 fold : val acc1 0.574, acc3 0.960, mae 0.237###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597/613 [============================>.] - ETA: 0s - loss: 1.0151\n",
      "Epoch 00001: val_loss improved from inf to 0.13850, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.9926 - val_loss: 0.1385\n",
      "Epoch 2/100\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.1845\n",
      "Epoch 00002: val_loss did not improve from 0.13850\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1845 - val_loss: 0.1741\n",
      "Epoch 3/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.1556\n",
      "Epoch 00003: val_loss improved from 0.13850 to 0.13079, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.1,lr0.002/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1550 - val_loss: 0.1308\n",
      "Epoch 4/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.1493\n",
      "Epoch 00004: val_loss did not improve from 0.13079\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1493 - val_loss: 0.1370\n",
      "Epoch 5/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.1996\n",
      "Epoch 00005: val_loss did not improve from 0.13079\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1994 - val_loss: 0.1557\n",
      " ###9 fold : val acc1 0.573, acc3 0.963, mae 0.236###\n",
      "acc10.564_acc30.962\n",
      "random search 17/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/77 [========================>.....] - ETA: 0s - loss: 21.4016\n",
      "Epoch 00001: val_loss improved from inf to 15.36923, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 20.6811 - val_loss: 15.3692\n",
      "Epoch 2/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 12.0409\n",
      "Epoch 00002: val_loss improved from 15.36923 to 7.57218, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 11.4996 - val_loss: 7.5722\n",
      "Epoch 3/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 5.8519\n",
      "Epoch 00003: val_loss improved from 7.57218 to 3.17779, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 5.6420 - val_loss: 3.1778\n",
      "Epoch 4/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 3.0299\n",
      "Epoch 00004: val_loss improved from 3.17779 to 1.67528, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 3.1876 - val_loss: 1.6753\n",
      "Epoch 5/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 2.2567\n",
      "Epoch 00005: val_loss improved from 1.67528 to 1.08651, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.1749 - val_loss: 1.0865\n",
      "Epoch 6/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 2.5529\n",
      "Epoch 00006: val_loss improved from 1.08651 to 0.77845, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.3340 - val_loss: 0.7785\n",
      "Epoch 7/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.3933\n",
      "Epoch 00007: val_loss improved from 0.77845 to 0.56372, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3640 - val_loss: 0.5637\n",
      "Epoch 8/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.2051\n",
      "Epoch 00008: val_loss improved from 0.56372 to 0.44605, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.2570 - val_loss: 0.4460\n",
      "Epoch 9/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.2486\n",
      "Epoch 00009: val_loss improved from 0.44605 to 0.36516, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1972 - val_loss: 0.3652\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.0287\n",
      "Epoch 00010: val_loss improved from 0.36516 to 0.31372, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0287 - val_loss: 0.3137\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.9750\n",
      "Epoch 00011: val_loss improved from 0.31372 to 0.28241, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9750 - val_loss: 0.2824\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.9898\n",
      "Epoch 00012: val_loss improved from 0.28241 to 0.26258, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9898 - val_loss: 0.2626\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8959\n",
      "Epoch 00013: val_loss improved from 0.26258 to 0.24459, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8959 - val_loss: 0.2446\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8845\n",
      "Epoch 00014: val_loss improved from 0.24459 to 0.23206, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8845 - val_loss: 0.2321\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8890\n",
      "Epoch 00015: val_loss improved from 0.23206 to 0.22503, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8890 - val_loss: 0.2250\n",
      "Epoch 16/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8858\n",
      "Epoch 00016: val_loss improved from 0.22503 to 0.21511, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8850 - val_loss: 0.2151\n",
      "Epoch 17/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8187\n",
      "Epoch 00017: val_loss improved from 0.21511 to 0.21144, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8175 - val_loss: 0.2114\n",
      "Epoch 18/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8453\n",
      "Epoch 00018: val_loss improved from 0.21144 to 0.20429, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8324 - val_loss: 0.2043\n",
      "Epoch 19/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8195\n",
      "Epoch 00019: val_loss improved from 0.20429 to 0.20405, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8076 - val_loss: 0.2041\n",
      "Epoch 20/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7759\n",
      "Epoch 00020: val_loss improved from 0.20405 to 0.19713, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7744 - val_loss: 0.1971\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8658\n",
      "Epoch 00021: val_loss did not improve from 0.19713\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8658 - val_loss: 0.1975\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.7934\n",
      "Epoch 00022: val_loss improved from 0.19713 to 0.19324, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7934 - val_loss: 0.1932\n",
      "Epoch 23/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8134\n",
      "Epoch 00023: val_loss improved from 0.19324 to 0.18981, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7969 - val_loss: 0.1898\n",
      "Epoch 24/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7466\n",
      "Epoch 00024: val_loss improved from 0.18981 to 0.18768, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7436 - val_loss: 0.1877\n",
      "Epoch 25/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7007\n",
      "Epoch 00025: val_loss improved from 0.18768 to 0.18246, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7041 - val_loss: 0.1825\n",
      "Epoch 26/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6970\n",
      "Epoch 00026: val_loss did not improve from 0.18246\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7502 - val_loss: 0.1880\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7249\n",
      "Epoch 00027: val_loss improved from 0.18246 to 0.17959, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7131 - val_loss: 0.1796\n",
      "Epoch 28/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6579\n",
      "Epoch 00028: val_loss improved from 0.17959 to 0.17853, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6572 - val_loss: 0.1785\n",
      "Epoch 29/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6574\n",
      "Epoch 00029: val_loss improved from 0.17853 to 0.17447, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6560 - val_loss: 0.1745\n",
      "Epoch 30/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6493\n",
      "Epoch 00030: val_loss improved from 0.17447 to 0.17292, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6456 - val_loss: 0.1729\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6388\n",
      "Epoch 00031: val_loss improved from 0.17292 to 0.16623, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6388 - val_loss: 0.1662\n",
      "Epoch 32/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6332\n",
      "Epoch 00032: val_loss did not improve from 0.16623\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6307 - val_loss: 0.1707\n",
      "Epoch 33/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6151\n",
      "Epoch 00033: val_loss improved from 0.16623 to 0.16585, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6152 - val_loss: 0.1659\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6088\n",
      "Epoch 00034: val_loss did not improve from 0.16585\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6088 - val_loss: 0.1667\n",
      "Epoch 35/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6215\n",
      "Epoch 00035: val_loss improved from 0.16585 to 0.16185, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6141 - val_loss: 0.1618\n",
      "Epoch 36/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6464\n",
      "Epoch 00036: val_loss improved from 0.16185 to 0.16058, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6373 - val_loss: 0.1606\n",
      "Epoch 37/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6403\n",
      "Epoch 00037: val_loss did not improve from 0.16058\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6275 - val_loss: 0.1620\n",
      "Epoch 38/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.5690\n",
      "Epoch 00038: val_loss improved from 0.16058 to 0.15812, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5700 - val_loss: 0.1581\n",
      "Epoch 39/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5652\n",
      "Epoch 00039: val_loss improved from 0.15812 to 0.15788, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5621 - val_loss: 0.1579\n",
      "Epoch 40/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6144\n",
      "Epoch 00040: val_loss improved from 0.15788 to 0.15463, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5946 - val_loss: 0.1546\n",
      "Epoch 41/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5730\n",
      "Epoch 00041: val_loss did not improve from 0.15463\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5728 - val_loss: 0.1596\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.5623\n",
      "Epoch 00042: val_loss did not improve from 0.15463\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5623 - val_loss: 0.1562\n",
      " ###0 fold : val acc1 0.552, acc3 0.943, mae 0.258###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/77 [=========================>....] - ETA: 0s - loss: 21.2446\n",
      "Epoch 00001: val_loss improved from inf to 15.34684, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 20.6520 - val_loss: 15.3468\n",
      "Epoch 2/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 12.0444\n",
      "Epoch 00002: val_loss improved from 15.34684 to 7.55837, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 11.4998 - val_loss: 7.5584\n",
      "Epoch 3/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 5.7834\n",
      "Epoch 00003: val_loss improved from 7.55837 to 3.16372, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 5.6455 - val_loss: 3.1637\n",
      "Epoch 4/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 3.1959\n",
      "Epoch 00004: val_loss improved from 3.16372 to 1.67111, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 3.1882 - val_loss: 1.6711\n",
      "Epoch 5/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 2.2495\n",
      "Epoch 00005: val_loss improved from 1.67111 to 1.08954, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.1686 - val_loss: 1.0895\n",
      "Epoch 6/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 2.5908\n",
      "Epoch 00006: val_loss improved from 1.08954 to 0.78114, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.3394 - val_loss: 0.7811\n",
      "Epoch 7/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.4017\n",
      "Epoch 00007: val_loss improved from 0.78114 to 0.56612, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3724 - val_loss: 0.5661\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.2608\n",
      "Epoch 00008: val_loss improved from 0.56612 to 0.44618, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2608 - val_loss: 0.4462\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.2037\n",
      "Epoch 00009: val_loss improved from 0.44618 to 0.36590, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2037 - val_loss: 0.3659\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.0293\n",
      "Epoch 00010: val_loss improved from 0.36590 to 0.31251, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0293 - val_loss: 0.3125\n",
      "Epoch 11/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.9783\n",
      "Epoch 00011: val_loss improved from 0.31251 to 0.28074, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9788 - val_loss: 0.2807\n",
      "Epoch 12/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.0206\n",
      "Epoch 00012: val_loss improved from 0.28074 to 0.26068, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0008 - val_loss: 0.2607\n",
      "Epoch 13/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.9090\n",
      "Epoch 00013: val_loss improved from 0.26068 to 0.24417, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9039 - val_loss: 0.2442\n",
      "Epoch 14/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.9020\n",
      "Epoch 00014: val_loss improved from 0.24417 to 0.23105, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8914 - val_loss: 0.2310\n",
      "Epoch 15/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.9094\n",
      "Epoch 00015: val_loss improved from 0.23105 to 0.22277, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8968 - val_loss: 0.2228\n",
      "Epoch 16/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.9067\n",
      "Epoch 00016: val_loss improved from 0.22277 to 0.21265, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8865 - val_loss: 0.2126\n",
      "Epoch 17/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.8155\n",
      "Epoch 00017: val_loss improved from 0.21265 to 0.20943, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8174 - val_loss: 0.2094\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8338\n",
      "Epoch 00018: val_loss improved from 0.20943 to 0.20057, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8338 - val_loss: 0.2006\n",
      "Epoch 19/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8180\n",
      "Epoch 00019: val_loss did not improve from 0.20057\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8157 - val_loss: 0.2042\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.7730\n",
      "Epoch 00020: val_loss improved from 0.20057 to 0.19451, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7730 - val_loss: 0.1945\n",
      "Epoch 21/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7659\n",
      "Epoch 00021: val_loss did not improve from 0.19451\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8744 - val_loss: 0.1957\n",
      "Epoch 22/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7362\n",
      "Epoch 00022: val_loss improved from 0.19451 to 0.19145, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7978 - val_loss: 0.1915\n",
      "Epoch 23/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.8035\n",
      "Epoch 00023: val_loss improved from 0.19145 to 0.18725, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7950 - val_loss: 0.1873\n",
      "Epoch 24/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7442\n",
      "Epoch 00024: val_loss improved from 0.18725 to 0.18570, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7407 - val_loss: 0.1857\n",
      "Epoch 25/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7000\n",
      "Epoch 00025: val_loss improved from 0.18570 to 0.18050, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7022 - val_loss: 0.1805\n",
      "Epoch 26/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6998\n",
      "Epoch 00026: val_loss did not improve from 0.18050\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7540 - val_loss: 0.1845\n",
      "Epoch 27/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7259\n",
      "Epoch 00027: val_loss improved from 0.18050 to 0.17563, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7128 - val_loss: 0.1756\n",
      "Epoch 28/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6571\n",
      "Epoch 00028: val_loss improved from 0.17563 to 0.17444, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6568 - val_loss: 0.1744\n",
      "Epoch 29/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.6576\n",
      "Epoch 00029: val_loss improved from 0.17444 to 0.17168, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6568 - val_loss: 0.1717\n",
      "Epoch 30/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6468\n",
      "Epoch 00030: val_loss improved from 0.17168 to 0.17049, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6444 - val_loss: 0.1705\n",
      "Epoch 31/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6421\n",
      "Epoch 00031: val_loss improved from 0.17049 to 0.16224, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6444 - val_loss: 0.1622\n",
      "Epoch 32/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6319\n",
      "Epoch 00032: val_loss did not improve from 0.16224\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6302 - val_loss: 0.1668\n",
      "Epoch 33/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6136\n",
      "Epoch 00033: val_loss did not improve from 0.16224\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6132 - val_loss: 0.1632\n",
      " ###1 fold : val acc1 0.545, acc3 0.948, mae 0.258###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/77 [=======================>......] - ETA: 0s - loss: 21.7379\n",
      "Epoch 00001: val_loss improved from inf to 15.34578, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 20.6868 - val_loss: 15.3458\n",
      "Epoch 2/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 12.1929\n",
      "Epoch 00002: val_loss improved from 15.34578 to 7.56560, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 11.4931 - val_loss: 7.5656\n",
      "Epoch 3/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 5.8937\n",
      "Epoch 00003: val_loss improved from 7.56560 to 3.17769, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 5.6497 - val_loss: 3.1777\n",
      "Epoch 4/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 3.1879\n",
      "Epoch 00004: val_loss improved from 3.17769 to 1.68084, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 3.1801 - val_loss: 1.6808\n",
      "Epoch 5/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 2.2470\n",
      "Epoch 00005: val_loss improved from 1.68084 to 1.09587, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.1807 - val_loss: 1.0959\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 2.3421\n",
      "Epoch 00006: val_loss improved from 1.09587 to 0.78143, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.3421 - val_loss: 0.7814\n",
      "Epoch 7/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.3881\n",
      "Epoch 00007: val_loss improved from 0.78143 to 0.56454, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3651 - val_loss: 0.5645\n",
      "Epoch 8/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.2642\n",
      "Epoch 00008: val_loss improved from 0.56454 to 0.44543, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2596 - val_loss: 0.4454\n",
      "Epoch 9/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.2062\n",
      "Epoch 00009: val_loss improved from 0.44543 to 0.36380, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2031 - val_loss: 0.3638\n",
      "Epoch 10/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.0301\n",
      "Epoch 00010: val_loss improved from 0.36380 to 0.31193, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0279 - val_loss: 0.3119\n",
      "Epoch 11/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9795\n",
      "Epoch 00011: val_loss improved from 0.31193 to 0.28075, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9804 - val_loss: 0.2807\n",
      "Epoch 12/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.9979\n",
      "Epoch 00012: val_loss improved from 0.28075 to 0.26178, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9950 - val_loss: 0.2618\n",
      "Epoch 13/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9051\n",
      "Epoch 00013: val_loss improved from 0.26178 to 0.24342, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9045 - val_loss: 0.2434\n",
      "Epoch 14/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.8903\n",
      "Epoch 00014: val_loss improved from 0.24342 to 0.23107, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8887 - val_loss: 0.2311\n",
      "Epoch 15/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.8941\n",
      "Epoch 00015: val_loss improved from 0.23107 to 0.22312, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8927 - val_loss: 0.2231\n",
      "Epoch 16/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.8934\n",
      "Epoch 00016: val_loss improved from 0.22312 to 0.21477, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8880 - val_loss: 0.2148\n",
      "Epoch 17/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.8232\n",
      "Epoch 00017: val_loss improved from 0.21477 to 0.20996, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8236 - val_loss: 0.2100\n",
      "Epoch 18/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8352\n",
      "Epoch 00018: val_loss improved from 0.20996 to 0.20141, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8329 - val_loss: 0.2014\n",
      "Epoch 19/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8121\n",
      "Epoch 00019: val_loss did not improve from 0.20141\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8097 - val_loss: 0.2049\n",
      "Epoch 20/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7710\n",
      "Epoch 00020: val_loss improved from 0.20141 to 0.19394, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7698 - val_loss: 0.1939\n",
      "Epoch 21/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.8808\n",
      "Epoch 00021: val_loss did not improve from 0.19394\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8735 - val_loss: 0.1991\n",
      "Epoch 22/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.8012\n",
      "Epoch 00022: val_loss improved from 0.19394 to 0.19127, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7981 - val_loss: 0.1913\n",
      "Epoch 23/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7922\n",
      "Epoch 00023: val_loss improved from 0.19127 to 0.18892, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7895 - val_loss: 0.1889\n",
      "Epoch 24/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7379\n",
      "Epoch 00024: val_loss improved from 0.18892 to 0.18645, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7380 - val_loss: 0.1865\n",
      "Epoch 25/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7010\n",
      "Epoch 00025: val_loss improved from 0.18645 to 0.18198, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7021 - val_loss: 0.1820\n",
      "Epoch 26/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7553\n",
      "Epoch 00026: val_loss did not improve from 0.18198\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7538 - val_loss: 0.1854\n",
      "Epoch 27/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7223\n",
      "Epoch 00027: val_loss improved from 0.18198 to 0.17591, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7207 - val_loss: 0.1759\n",
      "Epoch 28/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6581\n",
      "Epoch 00028: val_loss improved from 0.17591 to 0.17470, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6574 - val_loss: 0.1747\n",
      "Epoch 29/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6528\n",
      "Epoch 00029: val_loss improved from 0.17470 to 0.17248, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6529 - val_loss: 0.1725\n",
      "Epoch 30/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6423\n",
      "Epoch 00030: val_loss improved from 0.17248 to 0.17060, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6418 - val_loss: 0.1706\n",
      "Epoch 31/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6428\n",
      "Epoch 00031: val_loss improved from 0.17060 to 0.16343, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6426 - val_loss: 0.1634\n",
      "Epoch 32/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6284\n",
      "Epoch 00032: val_loss did not improve from 0.16343\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6293 - val_loss: 0.1676\n",
      "Epoch 33/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6126\n",
      "Epoch 00033: val_loss did not improve from 0.16343\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6113 - val_loss: 0.1638\n",
      " ###2 fold : val acc1 0.535, acc3 0.947, mae 0.266###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/77 [=======================>......] - ETA: 0s - loss: 21.5860\n",
      "Epoch 00001: val_loss improved from inf to 15.36541, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 20.6938 - val_loss: 15.3654\n",
      "Epoch 2/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 12.0018\n",
      "Epoch 00002: val_loss improved from 15.36541 to 7.58435, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 11.5038 - val_loss: 7.5843\n",
      "Epoch 3/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 5.8180\n",
      "Epoch 00003: val_loss improved from 7.58435 to 3.18397, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 5.6708 - val_loss: 3.1840\n",
      "Epoch 4/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 2.9758\n",
      "Epoch 00004: val_loss improved from 3.18397 to 1.67792, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.1770 - val_loss: 1.6779\n",
      "Epoch 5/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 2.2340\n",
      "Epoch 00005: val_loss improved from 1.67792 to 1.09066, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.1811 - val_loss: 1.0907\n",
      "Epoch 6/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 2.4695\n",
      "Epoch 00006: val_loss improved from 1.09066 to 0.77760, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.3329 - val_loss: 0.7776\n",
      "Epoch 7/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.3795\n",
      "Epoch 00007: val_loss improved from 0.77760 to 0.56385, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.3604 - val_loss: 0.5639\n",
      "Epoch 8/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.2013\n",
      "Epoch 00008: val_loss improved from 0.56385 to 0.44371, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.2588 - val_loss: 0.4437\n",
      "Epoch 9/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 1.2354\n",
      "Epoch 00009: val_loss improved from 0.44371 to 0.36237, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.2052 - val_loss: 0.3624\n",
      "Epoch 10/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.0189\n",
      "Epoch 00010: val_loss improved from 0.36237 to 0.31101, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0314 - val_loss: 0.3110\n",
      "Epoch 11/100\n",
      "58/77 [=====================>........] - ETA: 0s - loss: 0.9833\n",
      "Epoch 00011: val_loss improved from 0.31101 to 0.28149, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9801 - val_loss: 0.2815\n",
      "Epoch 12/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.0161\n",
      "Epoch 00012: val_loss improved from 0.28149 to 0.26163, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9945 - val_loss: 0.2616\n",
      "Epoch 13/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.9071\n",
      "Epoch 00013: val_loss improved from 0.26163 to 0.24464, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9005 - val_loss: 0.2446\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8874\n",
      "Epoch 00014: val_loss improved from 0.24464 to 0.23117, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8874 - val_loss: 0.2312\n",
      "Epoch 15/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8983\n",
      "Epoch 00015: val_loss improved from 0.23117 to 0.22203, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8876 - val_loss: 0.2220\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8928\n",
      "Epoch 00016: val_loss improved from 0.22203 to 0.21490, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8928 - val_loss: 0.2149\n",
      "Epoch 17/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8236\n",
      "Epoch 00017: val_loss improved from 0.21490 to 0.20924, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8222 - val_loss: 0.2092\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8328\n",
      "Epoch 00018: val_loss improved from 0.20924 to 0.20207, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8328 - val_loss: 0.2021\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8059\n",
      "Epoch 00019: val_loss did not improve from 0.20207\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8059 - val_loss: 0.2060\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.7697\n",
      "Epoch 00020: val_loss improved from 0.20207 to 0.19404, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7697 - val_loss: 0.1940\n",
      "Epoch 21/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7653\n",
      "Epoch 00021: val_loss did not improve from 0.19404\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8756 - val_loss: 0.1995\n",
      "Epoch 22/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7417\n",
      "Epoch 00022: val_loss improved from 0.19404 to 0.19253, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8012 - val_loss: 0.1925\n",
      "Epoch 23/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7890\n",
      "Epoch 00023: val_loss improved from 0.19253 to 0.18954, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7866 - val_loss: 0.1895\n",
      "Epoch 24/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7406\n",
      "Epoch 00024: val_loss improved from 0.18954 to 0.18860, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7411 - val_loss: 0.1886\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6999\n",
      "Epoch 00025: val_loss improved from 0.18860 to 0.18345, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6999 - val_loss: 0.1835\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.7585\n",
      "Epoch 00026: val_loss did not improve from 0.18345\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7585 - val_loss: 0.1858\n",
      "Epoch 27/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.7297\n",
      "Epoch 00027: val_loss improved from 0.18345 to 0.17792, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7269 - val_loss: 0.1779\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6626\n",
      "Epoch 00028: val_loss improved from 0.17792 to 0.17515, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6626 - val_loss: 0.1751\n",
      "Epoch 29/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6615\n",
      "Epoch 00029: val_loss improved from 0.17515 to 0.17397, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6551 - val_loss: 0.1740\n",
      "Epoch 30/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6498\n",
      "Epoch 00030: val_loss improved from 0.17397 to 0.17150, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6462 - val_loss: 0.1715\n",
      "Epoch 31/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6449\n",
      "Epoch 00031: val_loss improved from 0.17150 to 0.16358, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6452 - val_loss: 0.1636\n",
      "Epoch 32/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6322\n",
      "Epoch 00032: val_loss did not improve from 0.16358\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6307 - val_loss: 0.1675\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6161\n",
      "Epoch 00033: val_loss did not improve from 0.16358\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6161 - val_loss: 0.1639\n",
      " ###3 fold : val acc1 0.549, acc3 0.946, mae 0.258###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/77 [=======================>......] - ETA: 0s - loss: 21.7337\n",
      "Epoch 00001: val_loss improved from inf to 15.33082, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 20.7099 - val_loss: 15.3308\n",
      "Epoch 2/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 12.2203\n",
      "Epoch 00002: val_loss improved from 15.33082 to 7.51845, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 11.5271 - val_loss: 7.5184\n",
      "Epoch 3/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 6.2593\n",
      "Epoch 00003: val_loss improved from 7.51845 to 3.29542, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 5.8114 - val_loss: 3.2954\n",
      "Epoch 4/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 3.2098\n",
      "Epoch 00004: val_loss improved from 3.29542 to 1.72980, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 3.1819 - val_loss: 1.7298\n",
      "Epoch 5/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 2.2720\n",
      "Epoch 00005: val_loss improved from 1.72980 to 1.12536, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.2539 - val_loss: 1.1254\n",
      "Epoch 6/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.8633\n",
      "Epoch 00006: val_loss improved from 1.12536 to 0.79055, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.8568 - val_loss: 0.7905\n",
      "Epoch 7/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.5123\n",
      "Epoch 00007: val_loss improved from 0.79055 to 0.58057, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4967 - val_loss: 0.5806\n",
      "Epoch 8/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 1.4426\n",
      "Epoch 00008: val_loss improved from 0.58057 to 0.45501, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4103 - val_loss: 0.4550\n",
      "Epoch 9/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.2360\n",
      "Epoch 00009: val_loss improved from 0.45501 to 0.37595, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2311 - val_loss: 0.3759\n",
      "Epoch 10/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.0902\n",
      "Epoch 00010: val_loss improved from 0.37595 to 0.31911, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0854 - val_loss: 0.3191\n",
      "Epoch 11/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.0315\n",
      "Epoch 00011: val_loss improved from 0.31911 to 0.28823, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0271 - val_loss: 0.2882\n",
      "Epoch 12/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.9268\n",
      "Epoch 00012: val_loss improved from 0.28823 to 0.26478, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2088 - val_loss: 0.2648\n",
      "Epoch 13/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.9393\n",
      "Epoch 00013: val_loss improved from 0.26478 to 0.24675, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9384 - val_loss: 0.2468\n",
      "Epoch 14/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.9194\n",
      "Epoch 00014: val_loss improved from 0.24675 to 0.23340, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9163 - val_loss: 0.2334\n",
      "Epoch 15/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.8638\n",
      "Epoch 00015: val_loss improved from 0.23340 to 0.22548, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8632 - val_loss: 0.2255\n",
      "Epoch 16/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.8796\n",
      "Epoch 00016: val_loss improved from 0.22548 to 0.22040, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8733 - val_loss: 0.2204\n",
      "Epoch 17/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8878\n",
      "Epoch 00017: val_loss improved from 0.22040 to 0.21372, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8863 - val_loss: 0.2137\n",
      "Epoch 18/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8268\n",
      "Epoch 00018: val_loss improved from 0.21372 to 0.20543, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8254 - val_loss: 0.2054\n",
      "Epoch 19/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.7971\n",
      "Epoch 00019: val_loss did not improve from 0.20543\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7929 - val_loss: 0.2065\n",
      "Epoch 20/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7660\n",
      "Epoch 00020: val_loss improved from 0.20543 to 0.19631, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7659 - val_loss: 0.1963\n",
      "Epoch 21/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.7595\n",
      "Epoch 00021: val_loss improved from 0.19631 to 0.19410, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7582 - val_loss: 0.1941\n",
      "Epoch 22/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7424\n",
      "Epoch 00022: val_loss improved from 0.19410 to 0.19071, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7402 - val_loss: 0.1907\n",
      "Epoch 23/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7369\n",
      "Epoch 00023: val_loss improved from 0.19071 to 0.18440, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7361 - val_loss: 0.1844\n",
      "Epoch 24/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.7174\n",
      "Epoch 00024: val_loss did not improve from 0.18440\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7216 - val_loss: 0.1845\n",
      "Epoch 25/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.8376\n",
      "Epoch 00025: val_loss did not improve from 0.18440\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8307 - val_loss: 0.1895\n",
      " ###4 fold : val acc1 0.531, acc3 0.935, mae 0.275###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/77 [========================>.....] - ETA: 0s - loss: 21.4228\n",
      "Epoch 00001: val_loss improved from inf to 15.29002, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 20.6987 - val_loss: 15.2900\n",
      "Epoch 2/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 11.8015\n",
      "Epoch 00002: val_loss improved from 15.29002 to 7.36286, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 11.3247 - val_loss: 7.3629\n",
      "Epoch 3/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 5.7467\n",
      "Epoch 00003: val_loss improved from 7.36286 to 3.11290, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 5.3916 - val_loss: 3.1129\n",
      "Epoch 4/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 2.9554\n",
      "Epoch 00004: val_loss improved from 3.11290 to 1.64379, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.8375 - val_loss: 1.6438\n",
      "Epoch 5/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 2.0441\n",
      "Epoch 00005: val_loss improved from 1.64379 to 1.04475, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.9950 - val_loss: 1.0447\n",
      "Epoch 6/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.5717\n",
      "Epoch 00006: val_loss improved from 1.04475 to 0.71252, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5424 - val_loss: 0.7125\n",
      "Epoch 7/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 1.2956\n",
      "Epoch 00007: val_loss improved from 0.71252 to 0.50829, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.2858 - val_loss: 0.5083\n",
      "Epoch 8/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 1.1363\n",
      "Epoch 00008: val_loss improved from 0.50829 to 0.39381, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1284 - val_loss: 0.3938\n",
      "Epoch 9/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.0483\n",
      "Epoch 00009: val_loss improved from 0.39381 to 0.32438, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0364 - val_loss: 0.3244\n",
      "Epoch 10/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.9664\n",
      "Epoch 00010: val_loss improved from 0.32438 to 0.28127, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.9599 - val_loss: 0.2813\n",
      "Epoch 11/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.9291\n",
      "Epoch 00011: val_loss improved from 0.28127 to 0.25688, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.9261 - val_loss: 0.2569\n",
      "Epoch 12/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.8942\n",
      "Epoch 00012: val_loss improved from 0.25688 to 0.23600, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8849 - val_loss: 0.2360\n",
      "Epoch 13/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8771\n",
      "Epoch 00013: val_loss improved from 0.23600 to 0.22664, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8700 - val_loss: 0.2266\n",
      "Epoch 14/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.8355\n",
      "Epoch 00014: val_loss improved from 0.22664 to 0.21592, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8400 - val_loss: 0.2159\n",
      "Epoch 15/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7945\n",
      "Epoch 00015: val_loss improved from 0.21592 to 0.20782, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7972 - val_loss: 0.2078\n",
      "Epoch 16/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7994\n",
      "Epoch 00016: val_loss improved from 0.20782 to 0.20515, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7972 - val_loss: 0.2051\n",
      "Epoch 17/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7856\n",
      "Epoch 00017: val_loss improved from 0.20515 to 0.19658, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7816 - val_loss: 0.1966\n",
      "Epoch 18/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7506\n",
      "Epoch 00018: val_loss improved from 0.19658 to 0.19349, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7553 - val_loss: 0.1935\n",
      "Epoch 19/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7455\n",
      "Epoch 00019: val_loss did not improve from 0.19349\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7479 - val_loss: 0.1971\n",
      "Epoch 20/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.7344\n",
      "Epoch 00020: val_loss improved from 0.19349 to 0.18830, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7327 - val_loss: 0.1883\n",
      "Epoch 21/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7217\n",
      "Epoch 00021: val_loss improved from 0.18830 to 0.18665, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7197 - val_loss: 0.1866\n",
      "Epoch 22/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.7155\n",
      "Epoch 00022: val_loss improved from 0.18665 to 0.18428, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7140 - val_loss: 0.1843\n",
      "Epoch 23/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7072\n",
      "Epoch 00023: val_loss improved from 0.18428 to 0.17670, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7012 - val_loss: 0.1767\n",
      "Epoch 24/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.6905\n",
      "Epoch 00024: val_loss did not improve from 0.17670\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6917 - val_loss: 0.1812\n",
      "Epoch 25/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6759\n",
      "Epoch 00025: val_loss improved from 0.17670 to 0.17644, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6773 - val_loss: 0.1764\n",
      "Epoch 26/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6820\n",
      "Epoch 00026: val_loss improved from 0.17644 to 0.17524, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6798 - val_loss: 0.1752\n",
      "Epoch 27/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6603\n",
      "Epoch 00027: val_loss improved from 0.17524 to 0.17413, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6556 - val_loss: 0.1741\n",
      "Epoch 28/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6437\n",
      "Epoch 00028: val_loss improved from 0.17413 to 0.17205, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6431 - val_loss: 0.1720\n",
      "Epoch 29/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6338\n",
      "Epoch 00029: val_loss improved from 0.17205 to 0.17139, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6288 - val_loss: 0.1714\n",
      "Epoch 30/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6371\n",
      "Epoch 00030: val_loss improved from 0.17139 to 0.16782, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6394 - val_loss: 0.1678\n",
      "Epoch 31/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6219\n",
      "Epoch 00031: val_loss improved from 0.16782 to 0.16494, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6210 - val_loss: 0.1649\n",
      "Epoch 32/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.5943\n",
      "Epoch 00032: val_loss did not improve from 0.16494\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5914 - val_loss: 0.1675\n",
      "Epoch 33/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.5940\n",
      "Epoch 00033: val_loss did not improve from 0.16494\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5903 - val_loss: 0.1669\n",
      " ###5 fold : val acc1 0.541, acc3 0.952, mae 0.275###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/77 [=========================>....] - ETA: 0s - loss: 21.3025\n",
      "Epoch 00001: val_loss improved from inf to 15.38970, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 20.7385 - val_loss: 15.3897\n",
      "Epoch 2/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 11.8713\n",
      "Epoch 00002: val_loss improved from 15.38970 to 7.49200, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 11.4818 - val_loss: 7.4920\n",
      "Epoch 3/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 5.9547\n",
      "Epoch 00003: val_loss improved from 7.49200 to 3.25927, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 5.6947 - val_loss: 3.2593\n",
      "Epoch 4/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 3.4846\n",
      "Epoch 00004: val_loss improved from 3.25927 to 1.72338, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.3269 - val_loss: 1.7234\n",
      "Epoch 5/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 2.4640\n",
      "Epoch 00005: val_loss improved from 1.72338 to 1.11172, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.3741 - val_loss: 1.1117\n",
      "Epoch 6/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 1.8445\n",
      "Epoch 00006: val_loss improved from 1.11172 to 0.77441, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.7862 - val_loss: 0.7744\n",
      "Epoch 7/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.3866\n",
      "Epoch 00007: val_loss improved from 0.77441 to 0.56996, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5338 - val_loss: 0.5700\n",
      "Epoch 8/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.4360\n",
      "Epoch 00008: val_loss improved from 0.56996 to 0.44730, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3765 - val_loss: 0.4473\n",
      "Epoch 9/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.1172\n",
      "Epoch 00009: val_loss improved from 0.44730 to 0.36597, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2142 - val_loss: 0.3660\n",
      "Epoch 10/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 1.1948\n",
      "Epoch 00010: val_loss improved from 0.36597 to 0.31371, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1650 - val_loss: 0.3137\n",
      "Epoch 11/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.9632\n",
      "Epoch 00011: val_loss improved from 0.31371 to 0.28401, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0497 - val_loss: 0.2840\n",
      "Epoch 12/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.2592\n",
      "Epoch 00012: val_loss improved from 0.28401 to 0.25908, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1869 - val_loss: 0.2591\n",
      "Epoch 13/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.9881\n",
      "Epoch 00013: val_loss improved from 0.25908 to 0.24499, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9697 - val_loss: 0.2450\n",
      "Epoch 14/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.8815\n",
      "Epoch 00014: val_loss improved from 0.24499 to 0.23161, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8799 - val_loss: 0.2316\n",
      "Epoch 15/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.8773\n",
      "Epoch 00015: val_loss improved from 0.23161 to 0.22550, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8721 - val_loss: 0.2255\n",
      "Epoch 16/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.8363\n",
      "Epoch 00016: val_loss improved from 0.22550 to 0.21864, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8386 - val_loss: 0.2186\n",
      "Epoch 17/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0154\n",
      "Epoch 00017: val_loss improved from 0.21864 to 0.21337, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0128 - val_loss: 0.2134\n",
      "Epoch 18/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8475\n",
      "Epoch 00018: val_loss improved from 0.21337 to 0.20715, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8331 - val_loss: 0.2071\n",
      "Epoch 19/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.8172\n",
      "Epoch 00019: val_loss did not improve from 0.20715\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8121 - val_loss: 0.2083\n",
      "Epoch 20/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7740\n",
      "Epoch 00020: val_loss improved from 0.20715 to 0.19853, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7722 - val_loss: 0.1985\n",
      "Epoch 21/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7530\n",
      "Epoch 00021: val_loss improved from 0.19853 to 0.19535, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7529 - val_loss: 0.1953\n",
      "Epoch 22/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7422\n",
      "Epoch 00022: val_loss improved from 0.19535 to 0.19185, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7431 - val_loss: 0.1919\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.7461\n",
      "Epoch 00023: val_loss improved from 0.19185 to 0.18488, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7461 - val_loss: 0.1849\n",
      "Epoch 24/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7239\n",
      "Epoch 00024: val_loss did not improve from 0.18488\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7244 - val_loss: 0.1871\n",
      "Epoch 25/100\n",
      "58/77 [=====================>........] - ETA: 0s - loss: 0.7124\n",
      "Epoch 00025: val_loss improved from 0.18488 to 0.18214, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7133 - val_loss: 0.1821\n",
      "Epoch 26/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7260\n",
      "Epoch 00026: val_loss improved from 0.18214 to 0.18083, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7209 - val_loss: 0.1808\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7683\n",
      "Epoch 00027: val_loss improved from 0.18083 to 0.18074, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7448 - val_loss: 0.1807\n",
      "Epoch 28/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6717\n",
      "Epoch 00028: val_loss improved from 0.18074 to 0.17535, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6724 - val_loss: 0.1753\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6528\n",
      "Epoch 00029: val_loss did not improve from 0.17535\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6528 - val_loss: 0.1755\n",
      "Epoch 30/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6609\n",
      "Epoch 00030: val_loss improved from 0.17535 to 0.17247, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6607 - val_loss: 0.1725\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6438\n",
      "Epoch 00031: val_loss improved from 0.17247 to 0.16669, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6438 - val_loss: 0.1667\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.6203\n",
      "Epoch 00032: val_loss did not improve from 0.16669\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6203 - val_loss: 0.1683\n",
      "Epoch 33/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6178\n",
      "Epoch 00033: val_loss did not improve from 0.16669\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6166 - val_loss: 0.1674\n",
      " ###6 fold : val acc1 0.551, acc3 0.942, mae 0.261###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 21.3806\n",
      "Epoch 00001: val_loss improved from inf to 15.40799, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 20.7416 - val_loss: 15.4080\n",
      "Epoch 2/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 11.6906\n",
      "Epoch 00002: val_loss improved from 15.40799 to 7.49853, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 11.4960 - val_loss: 7.4985\n",
      "Epoch 3/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 5.8130\n",
      "Epoch 00003: val_loss improved from 7.49853 to 3.25571, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 5.6884 - val_loss: 3.2557\n",
      "Epoch 4/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 3.4053\n",
      "Epoch 00004: val_loss improved from 3.25571 to 1.72036, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.3103 - val_loss: 1.7204\n",
      "Epoch 5/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 2.4063\n",
      "Epoch 00005: val_loss improved from 1.72036 to 1.11416, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.3606 - val_loss: 1.1142\n",
      "Epoch 6/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 1.8408\n",
      "Epoch 00006: val_loss improved from 1.11416 to 0.78165, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.7903 - val_loss: 0.7817\n",
      "Epoch 7/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 1.3736\n",
      "Epoch 00007: val_loss improved from 0.78165 to 0.57636, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.5404 - val_loss: 0.5764\n",
      "Epoch 8/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.4025\n",
      "Epoch 00008: val_loss improved from 0.57636 to 0.45472, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.3902 - val_loss: 0.4547\n",
      "Epoch 9/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.2249\n",
      "Epoch 00009: val_loss improved from 0.45472 to 0.36995, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.2176 - val_loss: 0.3699\n",
      "Epoch 10/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 1.2018\n",
      "Epoch 00010: val_loss improved from 0.36995 to 0.31745, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1794 - val_loss: 0.3174\n",
      "Epoch 11/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.9566\n",
      "Epoch 00011: val_loss improved from 0.31745 to 0.28476, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0524 - val_loss: 0.2848\n",
      "Epoch 12/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.2773\n",
      "Epoch 00012: val_loss improved from 0.28476 to 0.25962, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.2072 - val_loss: 0.2596\n",
      "Epoch 13/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.0095\n",
      "Epoch 00013: val_loss improved from 0.25962 to 0.24648, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9812 - val_loss: 0.2465\n",
      "Epoch 14/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.8793\n",
      "Epoch 00014: val_loss improved from 0.24648 to 0.23113, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8787 - val_loss: 0.2311\n",
      "Epoch 15/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8899\n",
      "Epoch 00015: val_loss improved from 0.23113 to 0.22519, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8767 - val_loss: 0.2252\n",
      "Epoch 16/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.8375\n",
      "Epoch 00016: val_loss improved from 0.22519 to 0.21949, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8382 - val_loss: 0.2195\n",
      "Epoch 17/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.0748\n",
      "Epoch 00017: val_loss improved from 0.21949 to 0.21395, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0142 - val_loss: 0.2140\n",
      "Epoch 18/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.8518\n",
      "Epoch 00018: val_loss improved from 0.21395 to 0.20736, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8399 - val_loss: 0.2074\n",
      "Epoch 19/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8192\n",
      "Epoch 00019: val_loss improved from 0.20736 to 0.20733, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8101 - val_loss: 0.2073\n",
      "Epoch 20/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7845\n",
      "Epoch 00020: val_loss improved from 0.20733 to 0.19761, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7790 - val_loss: 0.1976\n",
      "Epoch 21/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7499\n",
      "Epoch 00021: val_loss improved from 0.19761 to 0.19524, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7494 - val_loss: 0.1952\n",
      "Epoch 22/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.7376\n",
      "Epoch 00022: val_loss improved from 0.19524 to 0.19019, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7371 - val_loss: 0.1902\n",
      "Epoch 23/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.7399\n",
      "Epoch 00023: val_loss improved from 0.19019 to 0.18493, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7375 - val_loss: 0.1849\n",
      "Epoch 24/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7251\n",
      "Epoch 00024: val_loss did not improve from 0.18493\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7253 - val_loss: 0.1868\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.7062\n",
      "Epoch 00025: val_loss improved from 0.18493 to 0.18094, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7062 - val_loss: 0.1809\n",
      "Epoch 26/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.7307\n",
      "Epoch 00026: val_loss improved from 0.18094 to 0.18000, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7229 - val_loss: 0.1800\n",
      "Epoch 27/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.7582\n",
      "Epoch 00027: val_loss improved from 0.18000 to 0.17826, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7409 - val_loss: 0.1783\n",
      "Epoch 28/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6721\n",
      "Epoch 00028: val_loss improved from 0.17826 to 0.17259, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6730 - val_loss: 0.1726\n",
      "Epoch 29/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.6571\n",
      "Epoch 00029: val_loss did not improve from 0.17259\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6511 - val_loss: 0.1736\n",
      "Epoch 30/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.6598\n",
      "Epoch 00030: val_loss improved from 0.17259 to 0.17067, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6616 - val_loss: 0.1707\n",
      "Epoch 31/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.6374\n",
      "Epoch 00031: val_loss improved from 0.17067 to 0.16529, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6377 - val_loss: 0.1653\n",
      "Epoch 32/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.6167\n",
      "Epoch 00032: val_loss did not improve from 0.16529\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6159 - val_loss: 0.1664\n",
      "Epoch 33/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.6155\n",
      "Epoch 00033: val_loss did not improve from 0.16529\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6184 - val_loss: 0.1660\n",
      " ###7 fold : val acc1 0.558, acc3 0.945, mae 0.253###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/77 [=======================>......] - ETA: 0s - loss: 21.7482\n",
      "Epoch 00001: val_loss improved from inf to 15.47729, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 20.7494 - val_loss: 15.4773\n",
      "Epoch 2/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 11.8692\n",
      "Epoch 00002: val_loss improved from 15.47729 to 7.55522, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 11.4913 - val_loss: 7.5552\n",
      "Epoch 3/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 6.0823\n",
      "Epoch 00003: val_loss improved from 7.55522 to 3.29670, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 5.6863 - val_loss: 3.2967\n",
      "Epoch 4/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 3.3070\n",
      "Epoch 00004: val_loss improved from 3.29670 to 1.75168, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 3.3008 - val_loss: 1.7517\n",
      "Epoch 5/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 2.4926\n",
      "Epoch 00005: val_loss improved from 1.75168 to 1.13856, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.3562 - val_loss: 1.1386\n",
      "Epoch 6/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.7898\n",
      "Epoch 00006: val_loss improved from 1.13856 to 0.79932, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7869 - val_loss: 0.7993\n",
      "Epoch 7/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.3733\n",
      "Epoch 00007: val_loss improved from 0.79932 to 0.58809, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5356 - val_loss: 0.5881\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.3894\n",
      "Epoch 00008: val_loss improved from 0.58809 to 0.45850, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3894 - val_loss: 0.4585\n",
      "Epoch 9/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.1114\n",
      "Epoch 00009: val_loss improved from 0.45850 to 0.36952, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.2119 - val_loss: 0.3695\n",
      "Epoch 10/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.2349\n",
      "Epoch 00010: val_loss improved from 0.36952 to 0.31449, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1755 - val_loss: 0.3145\n",
      "Epoch 11/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.0500\n",
      "Epoch 00011: val_loss improved from 0.31449 to 0.27944, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0493 - val_loss: 0.2794\n",
      "Epoch 12/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.2173\n",
      "Epoch 00012: val_loss improved from 0.27944 to 0.25249, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2032 - val_loss: 0.2525\n",
      "Epoch 13/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.9864\n",
      "Epoch 00013: val_loss improved from 0.25249 to 0.23811, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9805 - val_loss: 0.2381\n",
      "Epoch 14/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.8763\n",
      "Epoch 00014: val_loss improved from 0.23811 to 0.22344, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8774 - val_loss: 0.2234\n",
      "Epoch 15/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8863\n",
      "Epoch 00015: val_loss improved from 0.22344 to 0.21686, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8793 - val_loss: 0.2169\n",
      "Epoch 16/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8366\n",
      "Epoch 00016: val_loss improved from 0.21686 to 0.21050, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8353 - val_loss: 0.2105\n",
      "Epoch 17/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.0707\n",
      "Epoch 00017: val_loss improved from 0.21050 to 0.20458, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0143 - val_loss: 0.2046\n",
      "Epoch 18/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8531\n",
      "Epoch 00018: val_loss improved from 0.20458 to 0.19870, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8384 - val_loss: 0.1987\n",
      "Epoch 19/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8100\n",
      "Epoch 00019: val_loss improved from 0.19870 to 0.19769, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8096 - val_loss: 0.1977\n",
      "Epoch 20/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7757\n",
      "Epoch 00020: val_loss improved from 0.19769 to 0.18951, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7743 - val_loss: 0.1895\n",
      "Epoch 21/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7437\n",
      "Epoch 00021: val_loss improved from 0.18951 to 0.18728, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7444 - val_loss: 0.1873\n",
      "Epoch 22/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.7378\n",
      "Epoch 00022: val_loss improved from 0.18728 to 0.18219, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7367 - val_loss: 0.1822\n",
      "Epoch 23/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7398\n",
      "Epoch 00023: val_loss improved from 0.18219 to 0.17780, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7382 - val_loss: 0.1778\n",
      "Epoch 24/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7233\n",
      "Epoch 00024: val_loss did not improve from 0.17780\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7236 - val_loss: 0.1797\n",
      "Epoch 25/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7064\n",
      "Epoch 00025: val_loss improved from 0.17780 to 0.17577, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7063 - val_loss: 0.1758\n",
      "Epoch 26/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7199\n",
      "Epoch 00026: val_loss improved from 0.17577 to 0.17472, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7211 - val_loss: 0.1747\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7671\n",
      "Epoch 00027: val_loss improved from 0.17472 to 0.17206, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7415 - val_loss: 0.1721\n",
      "Epoch 28/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6703\n",
      "Epoch 00028: val_loss improved from 0.17206 to 0.16752, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6704 - val_loss: 0.1675\n",
      "Epoch 29/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6505\n",
      "Epoch 00029: val_loss did not improve from 0.16752\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6495 - val_loss: 0.1685\n",
      "Epoch 30/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6603\n",
      "Epoch 00030: val_loss improved from 0.16752 to 0.16636, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6587 - val_loss: 0.1664\n",
      "Epoch 31/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.6355\n",
      "Epoch 00031: val_loss improved from 0.16636 to 0.16067, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6359 - val_loss: 0.1607\n",
      "Epoch 32/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6168\n",
      "Epoch 00032: val_loss did not improve from 0.16067\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6171 - val_loss: 0.1619\n",
      "Epoch 33/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6151\n",
      "Epoch 00033: val_loss did not improve from 0.16067\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6160 - val_loss: 0.1614\n",
      " ###8 fold : val acc1 0.524, acc3 0.936, mae 0.277###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/77 [=======================>......] - ETA: 0s - loss: 21.7482\n",
      "Epoch 00001: val_loss improved from inf to 15.49761, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 20.7494 - val_loss: 15.4976\n",
      "Epoch 2/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 12.0252\n",
      "Epoch 00002: val_loss improved from 15.49761 to 7.55721, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 11.4913 - val_loss: 7.5572\n",
      "Epoch 3/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 6.1487\n",
      "Epoch 00003: val_loss improved from 7.55721 to 3.30295, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 5.6863 - val_loss: 3.3030\n",
      "Epoch 4/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 3.3342\n",
      "Epoch 00004: val_loss improved from 3.30295 to 1.76386, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 3.3008 - val_loss: 1.7639\n",
      "Epoch 5/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 2.3759\n",
      "Epoch 00005: val_loss improved from 1.76386 to 1.14943, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.3562 - val_loss: 1.1494\n",
      "Epoch 6/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.8077\n",
      "Epoch 00006: val_loss improved from 1.14943 to 0.80529, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7869 - val_loss: 0.8053\n",
      "Epoch 7/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.5459\n",
      "Epoch 00007: val_loss improved from 0.80529 to 0.58992, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5356 - val_loss: 0.5899\n",
      "Epoch 8/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.4025\n",
      "Epoch 00008: val_loss improved from 0.58992 to 0.45935, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3894 - val_loss: 0.4594\n",
      "Epoch 9/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.2196\n",
      "Epoch 00009: val_loss improved from 0.45935 to 0.36822, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2119 - val_loss: 0.3682\n",
      "Epoch 10/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.1811\n",
      "Epoch 00010: val_loss improved from 0.36822 to 0.31260, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1755 - val_loss: 0.3126\n",
      "Epoch 11/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.0485\n",
      "Epoch 00011: val_loss improved from 0.31260 to 0.27786, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0493 - val_loss: 0.2779\n",
      "Epoch 12/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.2173\n",
      "Epoch 00012: val_loss improved from 0.27786 to 0.25041, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2032 - val_loss: 0.2504\n",
      "Epoch 13/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.9840\n",
      "Epoch 00013: val_loss improved from 0.25041 to 0.23702, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9805 - val_loss: 0.2370\n",
      "Epoch 14/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.8763\n",
      "Epoch 00014: val_loss improved from 0.23702 to 0.22105, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8774 - val_loss: 0.2210\n",
      "Epoch 15/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.8825\n",
      "Epoch 00015: val_loss improved from 0.22105 to 0.21474, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8793 - val_loss: 0.2147\n",
      "Epoch 16/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.8375\n",
      "Epoch 00016: val_loss improved from 0.21474 to 0.20921, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8353 - val_loss: 0.2092\n",
      "Epoch 17/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.0248\n",
      "Epoch 00017: val_loss improved from 0.20921 to 0.20347, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0143 - val_loss: 0.2035\n",
      "Epoch 18/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.8413\n",
      "Epoch 00018: val_loss improved from 0.20347 to 0.19732, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8384 - val_loss: 0.1973\n",
      "Epoch 19/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8100\n",
      "Epoch 00019: val_loss improved from 0.19732 to 0.19640, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8096 - val_loss: 0.1964\n",
      "Epoch 20/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7757\n",
      "Epoch 00020: val_loss improved from 0.19640 to 0.18784, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7743 - val_loss: 0.1878\n",
      "Epoch 21/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7444\n",
      "Epoch 00021: val_loss improved from 0.18784 to 0.18620, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7444 - val_loss: 0.1862\n",
      "Epoch 22/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.7378\n",
      "Epoch 00022: val_loss improved from 0.18620 to 0.18142, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7367 - val_loss: 0.1814\n",
      "Epoch 23/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.7395\n",
      "Epoch 00023: val_loss improved from 0.18142 to 0.17684, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7382 - val_loss: 0.1768\n",
      "Epoch 24/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7239\n",
      "Epoch 00024: val_loss did not improve from 0.17684\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7236 - val_loss: 0.1791\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.7063\n",
      "Epoch 00025: val_loss improved from 0.17684 to 0.17487, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7063 - val_loss: 0.1749\n",
      "Epoch 26/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7212\n",
      "Epoch 00026: val_loss improved from 0.17487 to 0.17397, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7211 - val_loss: 0.1740\n",
      "Epoch 27/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.7459\n",
      "Epoch 00027: val_loss improved from 0.17397 to 0.17159, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7415 - val_loss: 0.1716\n",
      "Epoch 28/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6697\n",
      "Epoch 00028: val_loss improved from 0.17159 to 0.16657, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6704 - val_loss: 0.1666\n",
      "Epoch 29/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6505\n",
      "Epoch 00029: val_loss did not improve from 0.16657\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6495 - val_loss: 0.1683\n",
      "Epoch 30/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6603\n",
      "Epoch 00030: val_loss improved from 0.16657 to 0.16567, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6587 - val_loss: 0.1657\n",
      "Epoch 31/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.6349\n",
      "Epoch 00031: val_loss improved from 0.16567 to 0.16033, saving model to result/size/DNN_size_both_y/batch256,dnodes128_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6359 - val_loss: 0.1603\n",
      "Epoch 32/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.6170\n",
      "Epoch 00032: val_loss did not improve from 0.16033\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6171 - val_loss: 0.1615\n",
      "Epoch 33/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6151\n",
      "Epoch 00033: val_loss did not improve from 0.16033\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6160 - val_loss: 0.1612\n",
      " ###9 fold : val acc1 0.527, acc3 0.942, mae 0.274###\n",
      "acc10.541_acc30.944\n",
      "random search 18/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/39 [===============>..............] - ETA: 0s - loss: 18.5828 \n",
      "Epoch 00001: val_loss improved from inf to 11.65489, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 1s 12ms/step - loss: 16.5506 - val_loss: 11.6549\n",
      "Epoch 2/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 9.5514 \n",
      "Epoch 00002: val_loss improved from 11.65489 to 4.74171, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.0896 - val_loss: 4.7417\n",
      "Epoch 3/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 3.7095\n",
      "Epoch 00003: val_loss improved from 4.74171 to 2.00836, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.3564 - val_loss: 2.0084\n",
      "Epoch 4/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 1.7886\n",
      "Epoch 00004: val_loss improved from 2.00836 to 1.10972, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7143 - val_loss: 1.1097\n",
      "Epoch 5/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 1.0028\n",
      "Epoch 00005: val_loss improved from 1.10972 to 0.67626, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.0550 - val_loss: 0.6763\n",
      "Epoch 6/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.6551\n",
      "Epoch 00006: val_loss improved from 0.67626 to 0.45895, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.6671 - val_loss: 0.4589\n",
      "Epoch 7/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.4746\n",
      "Epoch 00007: val_loss improved from 0.45895 to 0.34714, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4773 - val_loss: 0.3471\n",
      "Epoch 8/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3755\n",
      "Epoch 00008: val_loss improved from 0.34714 to 0.28621, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3716 - val_loss: 0.2862\n",
      "Epoch 9/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3295\n",
      "Epoch 00009: val_loss improved from 0.28621 to 0.25098, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3269 - val_loss: 0.2510\n",
      "Epoch 10/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2912\n",
      "Epoch 00010: val_loss improved from 0.25098 to 0.22886, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3103 - val_loss: 0.2289\n",
      "Epoch 11/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2865\n",
      "Epoch 00011: val_loss improved from 0.22886 to 0.21505, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2869 - val_loss: 0.2151\n",
      "Epoch 12/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2670\n",
      "Epoch 00012: val_loss improved from 0.21505 to 0.20433, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2689 - val_loss: 0.2043\n",
      "Epoch 13/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2524\n",
      "Epoch 00013: val_loss improved from 0.20433 to 0.19598, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2642 - val_loss: 0.1960\n",
      "Epoch 14/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2553\n",
      "Epoch 00014: val_loss improved from 0.19598 to 0.18785, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2548 - val_loss: 0.1879\n",
      "Epoch 15/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2536\n",
      "Epoch 00015: val_loss improved from 0.18785 to 0.18188, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2520 - val_loss: 0.1819\n",
      "Epoch 16/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2503\n",
      "Epoch 00016: val_loss improved from 0.18188 to 0.17712, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2517 - val_loss: 0.1771\n",
      "Epoch 17/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2427\n",
      "Epoch 00017: val_loss improved from 0.17712 to 0.17262, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2393 - val_loss: 0.1726\n",
      "Epoch 18/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 0.2360\n",
      "Epoch 00018: val_loss improved from 0.17262 to 0.16862, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2362 - val_loss: 0.1686\n",
      "Epoch 19/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2331\n",
      "Epoch 00019: val_loss improved from 0.16862 to 0.16216, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2330 - val_loss: 0.1622\n",
      "Epoch 20/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2253\n",
      "Epoch 00020: val_loss improved from 0.16216 to 0.15811, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2253 - val_loss: 0.1581\n",
      "Epoch 21/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2266\n",
      "Epoch 00021: val_loss improved from 0.15811 to 0.15748, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2264 - val_loss: 0.1575\n",
      "Epoch 22/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2252\n",
      "Epoch 00022: val_loss improved from 0.15748 to 0.15328, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2254 - val_loss: 0.1533\n",
      "Epoch 23/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2255\n",
      "Epoch 00023: val_loss did not improve from 0.15328\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2255 - val_loss: 0.1538\n",
      "Epoch 24/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2192\n",
      "Epoch 00024: val_loss improved from 0.15328 to 0.15142, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2193 - val_loss: 0.1514\n",
      "Epoch 25/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2125\n",
      "Epoch 00025: val_loss improved from 0.15142 to 0.14852, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2127 - val_loss: 0.1485\n",
      "Epoch 26/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2250\n",
      "Epoch 00026: val_loss did not improve from 0.14852\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2245 - val_loss: 0.1504\n",
      "Epoch 27/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.2097\n",
      "Epoch 00027: val_loss improved from 0.14852 to 0.14687, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2107 - val_loss: 0.1469\n",
      "Epoch 28/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2110\n",
      "Epoch 00028: val_loss improved from 0.14687 to 0.14387, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2110 - val_loss: 0.1439\n",
      "Epoch 29/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00029: val_loss improved from 0.14387 to 0.14233, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2065 - val_loss: 0.1423\n",
      "Epoch 30/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2075\n",
      "Epoch 00030: val_loss improved from 0.14233 to 0.14166, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2075 - val_loss: 0.1417\n",
      "Epoch 31/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.2051\n",
      "Epoch 00031: val_loss improved from 0.14166 to 0.14081, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2050 - val_loss: 0.1408\n",
      "Epoch 32/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2110\n",
      "Epoch 00032: val_loss did not improve from 0.14081\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2111 - val_loss: 0.1425\n",
      "Epoch 33/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.2032\n",
      "Epoch 00033: val_loss improved from 0.14081 to 0.13953, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2051 - val_loss: 0.1395\n",
      "Epoch 34/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2026\n",
      "Epoch 00034: val_loss did not improve from 0.13953\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2023 - val_loss: 0.1399\n",
      "Epoch 35/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 0.2128\n",
      "Epoch 00035: val_loss did not improve from 0.13953\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2103 - val_loss: 0.1411\n",
      " ###0 fold : val acc1 0.564, acc3 0.956, mae 0.245###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/39 [==================>...........] - ETA: 0s - loss: 18.2032 \n",
      "Epoch 00001: val_loss improved from inf to 11.64542, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 16.5616 - val_loss: 11.6454\n",
      "Epoch 2/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 9.4632 \n",
      "Epoch 00002: val_loss improved from 11.64542 to 4.73304, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.1035 - val_loss: 4.7330\n",
      "Epoch 3/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 3.6983\n",
      "Epoch 00003: val_loss improved from 4.73304 to 2.01420, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.3709 - val_loss: 2.0142\n",
      "Epoch 4/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 1.7968\n",
      "Epoch 00004: val_loss improved from 2.01420 to 1.12518, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7301 - val_loss: 1.1252\n",
      "Epoch 5/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 1.2298\n",
      "Epoch 00005: val_loss improved from 1.12518 to 0.68716, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.0775 - val_loss: 0.6872\n",
      "Epoch 6/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.6631\n",
      "Epoch 00006: val_loss improved from 0.68716 to 0.46183, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.6817 - val_loss: 0.4618\n",
      "Epoch 7/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.4804\n",
      "Epoch 00007: val_loss improved from 0.46183 to 0.34593, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4869 - val_loss: 0.3459\n",
      "Epoch 8/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3796\n",
      "Epoch 00008: val_loss improved from 0.34593 to 0.28393, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3770 - val_loss: 0.2839\n",
      "Epoch 9/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.3417\n",
      "Epoch 00009: val_loss improved from 0.28393 to 0.24847, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3342 - val_loss: 0.2485\n",
      "Epoch 10/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2962\n",
      "Epoch 00010: val_loss improved from 0.24847 to 0.22583, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3154 - val_loss: 0.2258\n",
      "Epoch 11/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2936\n",
      "Epoch 00011: val_loss improved from 0.22583 to 0.21158, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2936 - val_loss: 0.2116\n",
      "Epoch 12/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2735\n",
      "Epoch 00012: val_loss improved from 0.21158 to 0.20084, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2735 - val_loss: 0.2008\n",
      "Epoch 13/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2549\n",
      "Epoch 00013: val_loss improved from 0.20084 to 0.19296, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2649 - val_loss: 0.1930\n",
      "Epoch 14/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2582\n",
      "Epoch 00014: val_loss improved from 0.19296 to 0.18449, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2582 - val_loss: 0.1845\n",
      "Epoch 15/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2501\n",
      "Epoch 00015: val_loss improved from 0.18449 to 0.17862, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2532 - val_loss: 0.1786\n",
      "Epoch 16/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2556\n",
      "Epoch 00016: val_loss improved from 0.17862 to 0.17299, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2535 - val_loss: 0.1730\n",
      "Epoch 17/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2457\n",
      "Epoch 00017: val_loss improved from 0.17299 to 0.16871, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2405 - val_loss: 0.1687\n",
      "Epoch 18/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2409\n",
      "Epoch 00018: val_loss improved from 0.16871 to 0.16439, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2375 - val_loss: 0.1644\n",
      "Epoch 19/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2326\n",
      "Epoch 00019: val_loss improved from 0.16439 to 0.15867, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2341 - val_loss: 0.1587\n",
      "Epoch 20/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2263\n",
      "Epoch 00020: val_loss improved from 0.15867 to 0.15509, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2263 - val_loss: 0.1551\n",
      "Epoch 21/100\n",
      "20/39 [==============>...............] - ETA: 0s - loss: 0.2278\n",
      "Epoch 00021: val_loss improved from 0.15509 to 0.15430, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2277 - val_loss: 0.1543\n",
      "Epoch 22/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2256\n",
      "Epoch 00022: val_loss improved from 0.15430 to 0.15074, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2257 - val_loss: 0.1507\n",
      "Epoch 23/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2177\n",
      "Epoch 00023: val_loss did not improve from 0.15074\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2283 - val_loss: 0.1513\n",
      "Epoch 24/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2178\n",
      "Epoch 00024: val_loss improved from 0.15074 to 0.14895, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2200 - val_loss: 0.1490\n",
      "Epoch 25/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2116\n",
      "Epoch 00025: val_loss improved from 0.14895 to 0.14610, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2153 - val_loss: 0.1461\n",
      "Epoch 26/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2176\n",
      "Epoch 00026: val_loss did not improve from 0.14610\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2270 - val_loss: 0.1474\n",
      "Epoch 27/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2120\n",
      "Epoch 00027: val_loss improved from 0.14610 to 0.14463, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2120 - val_loss: 0.1446\n",
      "Epoch 28/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2117\n",
      "Epoch 00028: val_loss improved from 0.14463 to 0.14177, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2113 - val_loss: 0.1418\n",
      "Epoch 29/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2046\n",
      "Epoch 00029: val_loss improved from 0.14177 to 0.14057, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2073 - val_loss: 0.1406\n",
      "Epoch 30/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2094\n",
      "Epoch 00030: val_loss improved from 0.14057 to 0.14037, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2093 - val_loss: 0.1404\n",
      "Epoch 31/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2059\n",
      "Epoch 00031: val_loss improved from 0.14037 to 0.13927, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2059 - val_loss: 0.1393\n",
      "Epoch 32/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2128\n",
      "Epoch 00032: val_loss did not improve from 0.13927\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2128 - val_loss: 0.1407\n",
      "Epoch 33/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00033: val_loss improved from 0.13927 to 0.13854, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2044 - val_loss: 0.1385\n",
      "Epoch 34/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2023\n",
      "Epoch 00034: val_loss did not improve from 0.13854\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2025 - val_loss: 0.1391\n",
      "Epoch 35/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2116\n",
      "Epoch 00035: val_loss did not improve from 0.13854\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2116 - val_loss: 0.1386\n",
      " ###1 fold : val acc1 0.571, acc3 0.960, mae 0.238###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/39 [===============>..............] - ETA: 0s - loss: 18.5921 \n",
      "Epoch 00001: val_loss improved from inf to 11.64829, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 16.5544 - val_loss: 11.6483\n",
      "Epoch 2/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 9.6434 \n",
      "Epoch 00002: val_loss improved from 11.64829 to 4.74199, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.0935 - val_loss: 4.7420\n",
      "Epoch 3/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 3.7854\n",
      "Epoch 00003: val_loss improved from 4.74199 to 2.01175, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.3600 - val_loss: 2.0118\n",
      "Epoch 4/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.7185\n",
      "Epoch 00004: val_loss improved from 2.01175 to 1.11855, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7185 - val_loss: 1.1185\n",
      "Epoch 5/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.0725\n",
      "Epoch 00005: val_loss improved from 1.11855 to 0.68135, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.0700 - val_loss: 0.6813\n",
      "Epoch 6/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.6761\n",
      "Epoch 00006: val_loss improved from 0.68135 to 0.45997, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.6749 - val_loss: 0.4600\n",
      "Epoch 7/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.4892\n",
      "Epoch 00007: val_loss improved from 0.45997 to 0.34666, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.4826 - val_loss: 0.3467\n",
      "Epoch 8/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.3728\n",
      "Epoch 00008: val_loss improved from 0.34666 to 0.28495, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.3729 - val_loss: 0.2850\n",
      "Epoch 9/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.3330\n",
      "Epoch 00009: val_loss improved from 0.28495 to 0.24960, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.3319 - val_loss: 0.2496\n",
      "Epoch 10/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.3133\n",
      "Epoch 00010: val_loss improved from 0.24960 to 0.22735, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.3132 - val_loss: 0.2274\n",
      "Epoch 11/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.2945\n",
      "Epoch 00011: val_loss improved from 0.22735 to 0.21350, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2909 - val_loss: 0.2135\n",
      "Epoch 12/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2741\n",
      "Epoch 00012: val_loss improved from 0.21350 to 0.20230, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2736 - val_loss: 0.2023\n",
      "Epoch 13/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2647\n",
      "Epoch 00013: val_loss improved from 0.20230 to 0.19468, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2643 - val_loss: 0.1947\n",
      "Epoch 14/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2568\n",
      "Epoch 00014: val_loss improved from 0.19468 to 0.18566, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2562 - val_loss: 0.1857\n",
      "Epoch 15/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2514\n",
      "Epoch 00015: val_loss improved from 0.18566 to 0.17983, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2520 - val_loss: 0.1798\n",
      "Epoch 16/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2521\n",
      "Epoch 00016: val_loss improved from 0.17983 to 0.17440, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2524 - val_loss: 0.1744\n",
      "Epoch 17/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.2396\n",
      "Epoch 00017: val_loss improved from 0.17440 to 0.17015, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2395 - val_loss: 0.1702\n",
      "Epoch 18/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2388\n",
      "Epoch 00018: val_loss improved from 0.17015 to 0.16590, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2375 - val_loss: 0.1659\n",
      "Epoch 19/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2314\n",
      "Epoch 00019: val_loss improved from 0.16590 to 0.16000, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2314 - val_loss: 0.1600\n",
      "Epoch 20/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2260\n",
      "Epoch 00020: val_loss improved from 0.16000 to 0.15637, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2254 - val_loss: 0.1564\n",
      "Epoch 21/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2278\n",
      "Epoch 00021: val_loss improved from 0.15637 to 0.15581, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2279 - val_loss: 0.1558\n",
      "Epoch 22/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2262\n",
      "Epoch 00022: val_loss improved from 0.15581 to 0.15180, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2260 - val_loss: 0.1518\n",
      "Epoch 23/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2284\n",
      "Epoch 00023: val_loss did not improve from 0.15180\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2285 - val_loss: 0.1521\n",
      "Epoch 24/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2208\n",
      "Epoch 00024: val_loss improved from 0.15180 to 0.14997, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2205 - val_loss: 0.1500\n",
      "Epoch 25/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2145\n",
      "Epoch 00025: val_loss improved from 0.14997 to 0.14687, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2142 - val_loss: 0.1469\n",
      "Epoch 26/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2277\n",
      "Epoch 00026: val_loss did not improve from 0.14687\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2272 - val_loss: 0.1485\n",
      "Epoch 27/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2126\n",
      "Epoch 00027: val_loss improved from 0.14687 to 0.14471, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2127 - val_loss: 0.1447\n",
      "Epoch 28/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2107\n",
      "Epoch 00028: val_loss improved from 0.14471 to 0.14220, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2107 - val_loss: 0.1422\n",
      "Epoch 29/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2073\n",
      "Epoch 00029: val_loss improved from 0.14220 to 0.14158, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2073 - val_loss: 0.1416\n",
      "Epoch 30/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2098\n",
      "Epoch 00030: val_loss improved from 0.14158 to 0.14077, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2096 - val_loss: 0.1408\n",
      "Epoch 31/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2040\n",
      "Epoch 00031: val_loss improved from 0.14077 to 0.13964, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2042 - val_loss: 0.1396\n",
      "Epoch 32/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2140\n",
      "Epoch 00032: val_loss did not improve from 0.13964\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2140 - val_loss: 0.1416\n",
      "Epoch 33/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.2050\n",
      "Epoch 00033: val_loss improved from 0.13964 to 0.13910, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2065 - val_loss: 0.1391\n",
      "Epoch 34/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2032\n",
      "Epoch 00034: val_loss did not improve from 0.13910\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2026 - val_loss: 0.1393\n",
      "Epoch 35/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.2120\n",
      "Epoch 00035: val_loss did not improve from 0.13910\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2118 - val_loss: 0.1392\n",
      " ###2 fold : val acc1 0.562, acc3 0.960, mae 0.244###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/39 [==================>...........] - ETA: 0s - loss: 18.1892 \n",
      "Epoch 00001: val_loss improved from inf to 11.66290, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 16.5420 - val_loss: 11.6629\n",
      "Epoch 2/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 9.4118 \n",
      "Epoch 00002: val_loss improved from 11.66290 to 4.75505, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.1035 - val_loss: 4.7551\n",
      "Epoch 3/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 3.6612\n",
      "Epoch 00003: val_loss improved from 4.75505 to 2.01976, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.3740 - val_loss: 2.0198\n",
      "Epoch 4/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 1.7675\n",
      "Epoch 00004: val_loss improved from 2.01976 to 1.12720, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7294 - val_loss: 1.1272\n",
      "Epoch 5/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 1.2385\n",
      "Epoch 00005: val_loss improved from 1.12720 to 0.68914, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.0796 - val_loss: 0.6891\n",
      "Epoch 6/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.6636\n",
      "Epoch 00006: val_loss improved from 0.68914 to 0.46559, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.6826 - val_loss: 0.4656\n",
      "Epoch 7/100\n",
      "25/39 [==================>...........] - ETA: 0s - loss: 0.5347\n",
      "Epoch 00007: val_loss improved from 0.46559 to 0.34909, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4899 - val_loss: 0.3491\n",
      "Epoch 8/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.3751\n",
      "Epoch 00008: val_loss improved from 0.34909 to 0.28539, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3750 - val_loss: 0.2854\n",
      "Epoch 9/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.3439\n",
      "Epoch 00009: val_loss improved from 0.28539 to 0.24995, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3341 - val_loss: 0.2499\n",
      "Epoch 10/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2976\n",
      "Epoch 00010: val_loss improved from 0.24995 to 0.22757, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3150 - val_loss: 0.2276\n",
      "Epoch 11/100\n",
      "25/39 [==================>...........] - ETA: 0s - loss: 0.2928\n",
      "Epoch 00011: val_loss improved from 0.22757 to 0.21394, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2910 - val_loss: 0.2139\n",
      "Epoch 12/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.2710\n",
      "Epoch 00012: val_loss improved from 0.21394 to 0.20294, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2728 - val_loss: 0.2029\n",
      "Epoch 13/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2635\n",
      "Epoch 00013: val_loss improved from 0.20294 to 0.19511, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2688 - val_loss: 0.1951\n",
      "Epoch 14/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2592\n",
      "Epoch 00014: val_loss improved from 0.19511 to 0.18641, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2592 - val_loss: 0.1864\n",
      "Epoch 15/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2517\n",
      "Epoch 00015: val_loss improved from 0.18641 to 0.18013, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2534 - val_loss: 0.1801\n",
      "Epoch 16/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2465\n",
      "Epoch 00016: val_loss improved from 0.18013 to 0.17479, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2556 - val_loss: 0.1748\n",
      "Epoch 17/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2411\n",
      "Epoch 00017: val_loss improved from 0.17479 to 0.17082, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2411 - val_loss: 0.1708\n",
      "Epoch 18/100\n",
      "20/39 [==============>...............] - ETA: 0s - loss: 0.2452\n",
      "Epoch 00018: val_loss improved from 0.17082 to 0.16657, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2387 - val_loss: 0.1666\n",
      "Epoch 19/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2290\n",
      "Epoch 00019: val_loss improved from 0.16657 to 0.16060, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2334 - val_loss: 0.1606\n",
      "Epoch 20/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2293\n",
      "Epoch 00020: val_loss improved from 0.16060 to 0.15704, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2278 - val_loss: 0.1570\n",
      "Epoch 21/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2327\n",
      "Epoch 00021: val_loss improved from 0.15704 to 0.15573, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2290 - val_loss: 0.1557\n",
      "Epoch 22/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2246\n",
      "Epoch 00022: val_loss improved from 0.15573 to 0.15253, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2267 - val_loss: 0.1525\n",
      "Epoch 23/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2177\n",
      "Epoch 00023: val_loss did not improve from 0.15253\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2287 - val_loss: 0.1527\n",
      "Epoch 24/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2202\n",
      "Epoch 00024: val_loss improved from 0.15253 to 0.15017, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2224 - val_loss: 0.1502\n",
      "Epoch 25/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2133\n",
      "Epoch 00025: val_loss improved from 0.15017 to 0.14696, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2159 - val_loss: 0.1470\n",
      "Epoch 26/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2169\n",
      "Epoch 00026: val_loss did not improve from 0.14696\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2281 - val_loss: 0.1487\n",
      "Epoch 27/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2131\n",
      "Epoch 00027: val_loss improved from 0.14696 to 0.14546, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2131 - val_loss: 0.1455\n",
      "Epoch 28/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2098\n",
      "Epoch 00028: val_loss improved from 0.14546 to 0.14264, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2105 - val_loss: 0.1426\n",
      "Epoch 29/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2074\n",
      "Epoch 00029: val_loss improved from 0.14264 to 0.14141, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2084 - val_loss: 0.1414\n",
      "Epoch 30/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2117\n",
      "Epoch 00030: val_loss improved from 0.14141 to 0.14072, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2099 - val_loss: 0.1407\n",
      "Epoch 31/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00031: val_loss improved from 0.14072 to 0.13962, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2043 - val_loss: 0.1396\n",
      "Epoch 32/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2146\n",
      "Epoch 00032: val_loss did not improve from 0.13962\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2119 - val_loss: 0.1409\n",
      "Epoch 33/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2024\n",
      "Epoch 00033: val_loss improved from 0.13962 to 0.13866, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2050 - val_loss: 0.1387\n",
      "Epoch 34/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2009\n",
      "Epoch 00034: val_loss did not improve from 0.13866\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2042 - val_loss: 0.1391\n",
      "Epoch 35/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2128\n",
      "Epoch 00035: val_loss did not improve from 0.13866\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2128 - val_loss: 0.1395\n",
      " ###3 fold : val acc1 0.563, acc3 0.960, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/39 [==================>...........] - ETA: 0s - loss: 18.2469 \n",
      "Epoch 00001: val_loss improved from inf to 11.61036, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 16.5575 - val_loss: 11.6104\n",
      "Epoch 2/100\n",
      "25/39 [==================>...........] - ETA: 0s - loss: 9.1922 \n",
      "Epoch 00002: val_loss improved from 11.61036 to 4.71423, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 8.1139 - val_loss: 4.7142\n",
      "Epoch 3/100\n",
      "25/39 [==================>...........] - ETA: 0s - loss: 3.9806\n",
      "Epoch 00003: val_loss improved from 4.71423 to 2.07777, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.4398 - val_loss: 2.0778\n",
      "Epoch 4/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 2.0777\n",
      "Epoch 00004: val_loss improved from 2.07777 to 1.15593, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7958 - val_loss: 1.1559\n",
      "Epoch 5/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 1.2311\n",
      "Epoch 00005: val_loss improved from 1.15593 to 0.70504, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.0719 - val_loss: 0.7050\n",
      "Epoch 6/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.8486\n",
      "Epoch 00006: val_loss improved from 0.70504 to 0.47330, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.7383 - val_loss: 0.4733\n",
      "Epoch 7/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.5431\n",
      "Epoch 00007: val_loss improved from 0.47330 to 0.35239, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.5006 - val_loss: 0.3524\n",
      "Epoch 8/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.4017\n",
      "Epoch 00008: val_loss improved from 0.35239 to 0.28716, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3846 - val_loss: 0.2872\n",
      "Epoch 9/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.3408\n",
      "Epoch 00009: val_loss improved from 0.28716 to 0.25085, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3482 - val_loss: 0.2509\n",
      "Epoch 10/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.3057\n",
      "Epoch 00010: val_loss improved from 0.25085 to 0.22977, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3144 - val_loss: 0.2298\n",
      "Epoch 11/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2942\n",
      "Epoch 00011: val_loss improved from 0.22977 to 0.21569, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2978 - val_loss: 0.2157\n",
      "Epoch 12/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2671\n",
      "Epoch 00012: val_loss improved from 0.21569 to 0.20413, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2714 - val_loss: 0.2041\n",
      "Epoch 13/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2909\n",
      "Epoch 00013: val_loss improved from 0.20413 to 0.19761, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2806 - val_loss: 0.1976\n",
      "Epoch 14/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2618\n",
      "Epoch 00014: val_loss improved from 0.19761 to 0.18923, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2607 - val_loss: 0.1892\n",
      "Epoch 15/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2462\n",
      "Epoch 00015: val_loss improved from 0.18923 to 0.18271, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2502 - val_loss: 0.1827\n",
      "Epoch 16/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2477\n",
      "Epoch 00016: val_loss improved from 0.18271 to 0.17600, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2465 - val_loss: 0.1760\n",
      "Epoch 17/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2440\n",
      "Epoch 00017: val_loss improved from 0.17600 to 0.17109, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2405 - val_loss: 0.1711\n",
      "Epoch 18/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2458\n",
      "Epoch 00018: val_loss improved from 0.17109 to 0.16486, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2380 - val_loss: 0.1649\n",
      "Epoch 19/100\n",
      "20/39 [==============>...............] - ETA: 0s - loss: 0.2515\n",
      "Epoch 00019: val_loss improved from 0.16486 to 0.16302, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2466 - val_loss: 0.1630\n",
      "Epoch 20/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2271\n",
      "Epoch 00020: val_loss improved from 0.16302 to 0.15906, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2311 - val_loss: 0.1591\n",
      "Epoch 21/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2720\n",
      "Epoch 00021: val_loss improved from 0.15906 to 0.15900, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2520 - val_loss: 0.1590\n",
      "Epoch 22/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2229\n",
      "Epoch 00022: val_loss improved from 0.15900 to 0.15461, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2253 - val_loss: 0.1546\n",
      "Epoch 23/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2225\n",
      "Epoch 00023: val_loss improved from 0.15461 to 0.15338, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2399 - val_loss: 0.1534\n",
      "Epoch 24/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2192\n",
      "Epoch 00024: val_loss improved from 0.15338 to 0.15151, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2221 - val_loss: 0.1515\n",
      "Epoch 25/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2158\n",
      "Epoch 00025: val_loss improved from 0.15151 to 0.14833, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2209 - val_loss: 0.1483\n",
      "Epoch 26/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2183\n",
      "Epoch 00026: val_loss improved from 0.14833 to 0.14627, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2204 - val_loss: 0.1463\n",
      "Epoch 27/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2117\n",
      "Epoch 00027: val_loss improved from 0.14627 to 0.14562, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2149 - val_loss: 0.1456\n",
      "Epoch 28/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2118\n",
      "Epoch 00028: val_loss improved from 0.14562 to 0.14272, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2117 - val_loss: 0.1427\n",
      "Epoch 29/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2056\n",
      "Epoch 00029: val_loss improved from 0.14272 to 0.14215, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2082 - val_loss: 0.1421\n",
      "Epoch 30/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2053\n",
      "Epoch 00030: val_loss improved from 0.14215 to 0.14087, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2089 - val_loss: 0.1409\n",
      "Epoch 31/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2087\n",
      "Epoch 00031: val_loss improved from 0.14087 to 0.13916, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2052 - val_loss: 0.1392\n",
      "Epoch 32/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2035\n",
      "Epoch 00032: val_loss improved from 0.13916 to 0.13833, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2028 - val_loss: 0.1383\n",
      "Epoch 33/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2042\n",
      "Epoch 00033: val_loss improved from 0.13833 to 0.13734, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2047 - val_loss: 0.1373\n",
      "Epoch 34/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1993\n",
      "Epoch 00034: val_loss improved from 0.13734 to 0.13642, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2036 - val_loss: 0.1364\n",
      "Epoch 35/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00035: val_loss improved from 0.13642 to 0.13632, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2013 - val_loss: 0.1363\n",
      "Epoch 36/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2013\n",
      "Epoch 00036: val_loss improved from 0.13632 to 0.13621, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2012 - val_loss: 0.1362\n",
      "Epoch 37/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1980\n",
      "Epoch 00037: val_loss improved from 0.13621 to 0.13564, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1992 - val_loss: 0.1356\n",
      "Epoch 38/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1989\n",
      "Epoch 00038: val_loss improved from 0.13564 to 0.13512, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1977 - val_loss: 0.1351\n",
      "Epoch 39/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1983\n",
      "Epoch 00039: val_loss improved from 0.13512 to 0.13395, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1967 - val_loss: 0.1339\n",
      "Epoch 40/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1887\n",
      "Epoch 00040: val_loss did not improve from 0.13395\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1959 - val_loss: 0.1361\n",
      "Epoch 41/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2028\n",
      "Epoch 00041: val_loss did not improve from 0.13395\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1995 - val_loss: 0.1341\n",
      " ###4 fold : val acc1 0.582, acc3 0.963, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/39 [==================>...........] - ETA: 0s - loss: 18.2301 \n",
      "Epoch 00001: val_loss improved from inf to 11.60754, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 16.5419 - val_loss: 11.6075\n",
      "Epoch 2/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 9.3610 \n",
      "Epoch 00002: val_loss improved from 11.60754 to 4.64777, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 7.9639 - val_loss: 4.6478\n",
      "Epoch 3/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 3.6670\n",
      "Epoch 00003: val_loss improved from 4.64777 to 2.03882, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.1961 - val_loss: 2.0388\n",
      "Epoch 4/100\n",
      "25/39 [==================>...........] - ETA: 0s - loss: 1.7700\n",
      "Epoch 00004: val_loss improved from 2.03882 to 1.11043, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.6107 - val_loss: 1.1104\n",
      "Epoch 5/100\n",
      "25/39 [==================>...........] - ETA: 0s - loss: 1.0093\n",
      "Epoch 00005: val_loss improved from 1.11043 to 0.65234, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.9246 - val_loss: 0.6523\n",
      "Epoch 6/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.6345\n",
      "Epoch 00006: val_loss improved from 0.65234 to 0.43160, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.5884 - val_loss: 0.4316\n",
      "Epoch 7/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.4513\n",
      "Epoch 00007: val_loss improved from 0.43160 to 0.32373, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4268 - val_loss: 0.3237\n",
      "Epoch 8/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.3540\n",
      "Epoch 00008: val_loss improved from 0.32373 to 0.26830, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3502 - val_loss: 0.2683\n",
      "Epoch 9/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.3195\n",
      "Epoch 00009: val_loss improved from 0.26830 to 0.23784, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3104 - val_loss: 0.2378\n",
      "Epoch 10/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2928\n",
      "Epoch 00010: val_loss improved from 0.23784 to 0.21857, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2873 - val_loss: 0.2186\n",
      "Epoch 11/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2806\n",
      "Epoch 00011: val_loss improved from 0.21857 to 0.20530, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2736 - val_loss: 0.2053\n",
      "Epoch 12/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.2647\n",
      "Epoch 00012: val_loss improved from 0.20530 to 0.19527, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2602 - val_loss: 0.1953\n",
      "Epoch 13/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2559\n",
      "Epoch 00013: val_loss improved from 0.19527 to 0.18717, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2550 - val_loss: 0.1872\n",
      "Epoch 14/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2508\n",
      "Epoch 00014: val_loss improved from 0.18717 to 0.17983, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2459 - val_loss: 0.1798\n",
      "Epoch 15/100\n",
      "20/39 [==============>...............] - ETA: 0s - loss: 0.2406\n",
      "Epoch 00015: val_loss improved from 0.17983 to 0.17420, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2396 - val_loss: 0.1742\n",
      "Epoch 16/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2364\n",
      "Epoch 00016: val_loss improved from 0.17420 to 0.16873, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2364 - val_loss: 0.1687\n",
      "Epoch 17/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2372\n",
      "Epoch 00017: val_loss improved from 0.16873 to 0.16448, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2338 - val_loss: 0.1645\n",
      "Epoch 18/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2304\n",
      "Epoch 00018: val_loss improved from 0.16448 to 0.16127, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2289 - val_loss: 0.1613\n",
      "Epoch 19/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.2262\n",
      "Epoch 00019: val_loss improved from 0.16127 to 0.15752, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2263 - val_loss: 0.1575\n",
      "Epoch 20/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2184\n",
      "Epoch 00020: val_loss improved from 0.15752 to 0.15497, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2228 - val_loss: 0.1550\n",
      "Epoch 21/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2170\n",
      "Epoch 00021: val_loss improved from 0.15497 to 0.15290, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2178 - val_loss: 0.1529\n",
      "Epoch 22/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2213\n",
      "Epoch 00022: val_loss improved from 0.15290 to 0.15077, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2192 - val_loss: 0.1508\n",
      "Epoch 23/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2194\n",
      "Epoch 00023: val_loss improved from 0.15077 to 0.14859, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2166 - val_loss: 0.1486\n",
      "Epoch 24/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2116\n",
      "Epoch 00024: val_loss improved from 0.14859 to 0.14711, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2145 - val_loss: 0.1471\n",
      "Epoch 25/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2110\n",
      "Epoch 00025: val_loss improved from 0.14711 to 0.14555, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2115 - val_loss: 0.1455\n",
      "Epoch 26/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2118\n",
      "Epoch 00026: val_loss improved from 0.14555 to 0.14451, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2111 - val_loss: 0.1445\n",
      "Epoch 27/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2079\n",
      "Epoch 00027: val_loss improved from 0.14451 to 0.14363, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2090 - val_loss: 0.1436\n",
      "Epoch 28/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2083\n",
      "Epoch 00028: val_loss improved from 0.14363 to 0.14271, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2071 - val_loss: 0.1427\n",
      "Epoch 29/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2044\n",
      "Epoch 00029: val_loss improved from 0.14271 to 0.14168, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2038 - val_loss: 0.1417\n",
      "Epoch 30/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2044\n",
      "Epoch 00030: val_loss improved from 0.14168 to 0.14080, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2046 - val_loss: 0.1408\n",
      "Epoch 31/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2038\n",
      "Epoch 00031: val_loss improved from 0.14080 to 0.14004, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2015 - val_loss: 0.1400\n",
      "Epoch 32/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2030\n",
      "Epoch 00032: val_loss improved from 0.14004 to 0.13944, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2018 - val_loss: 0.1394\n",
      "Epoch 33/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2035\n",
      "Epoch 00033: val_loss did not improve from 0.13944\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2027 - val_loss: 0.1395\n",
      "Epoch 34/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1942\n",
      "Epoch 00034: val_loss improved from 0.13944 to 0.13846, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1994 - val_loss: 0.1385\n",
      "Epoch 35/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2021\n",
      "Epoch 00035: val_loss improved from 0.13846 to 0.13778, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2007 - val_loss: 0.1378\n",
      "Epoch 36/100\n",
      "20/39 [==============>...............] - ETA: 0s - loss: 0.1958\n",
      "Epoch 00036: val_loss improved from 0.13778 to 0.13760, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1985 - val_loss: 0.1376\n",
      "Epoch 37/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1963\n",
      "Epoch 00037: val_loss improved from 0.13760 to 0.13640, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1954 - val_loss: 0.1364\n",
      "Epoch 38/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1947\n",
      "Epoch 00038: val_loss improved from 0.13640 to 0.13611, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1947 - val_loss: 0.1361\n",
      "Epoch 39/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1983\n",
      "Epoch 00039: val_loss improved from 0.13611 to 0.13595, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1956 - val_loss: 0.1360\n",
      "Epoch 40/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1892\n",
      "Epoch 00040: val_loss improved from 0.13595 to 0.13588, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1921 - val_loss: 0.1359\n",
      "Epoch 41/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00041: val_loss improved from 0.13588 to 0.13532, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1981 - val_loss: 0.1353\n",
      "Epoch 42/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1916\n",
      "Epoch 00042: val_loss improved from 0.13532 to 0.13450, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1965 - val_loss: 0.1345\n",
      "Epoch 43/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1956\n",
      "Epoch 00043: val_loss improved from 0.13450 to 0.13435, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1937 - val_loss: 0.1343\n",
      "Epoch 44/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1909\n",
      "Epoch 00044: val_loss improved from 0.13435 to 0.13417, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1879 - val_loss: 0.1342\n",
      "Epoch 45/100\n",
      "20/39 [==============>...............] - ETA: 0s - loss: 0.1880\n",
      "Epoch 00045: val_loss improved from 0.13417 to 0.13362, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1896 - val_loss: 0.1336\n",
      "Epoch 46/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1949\n",
      "Epoch 00046: val_loss did not improve from 0.13362\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1931 - val_loss: 0.1337\n",
      "Epoch 47/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1935\n",
      "Epoch 00047: val_loss improved from 0.13362 to 0.13327, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1902 - val_loss: 0.1333\n",
      "Epoch 48/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.1953\n",
      "Epoch 00048: val_loss did not improve from 0.13327\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1922 - val_loss: 0.1342\n",
      "Epoch 49/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.1900\n",
      "Epoch 00049: val_loss improved from 0.13327 to 0.13280, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1888 - val_loss: 0.1328\n",
      "Epoch 50/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1914\n",
      "Epoch 00050: val_loss improved from 0.13280 to 0.13249, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1898 - val_loss: 0.1325\n",
      "Epoch 51/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.1831\n",
      "Epoch 00051: val_loss improved from 0.13249 to 0.13202, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1855 - val_loss: 0.1320\n",
      "Epoch 52/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1914\n",
      "Epoch 00052: val_loss improved from 0.13202 to 0.13198, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1868 - val_loss: 0.1320\n",
      "Epoch 53/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1861\n",
      "Epoch 00053: val_loss improved from 0.13198 to 0.13177, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1846 - val_loss: 0.1318\n",
      "Epoch 54/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1867\n",
      "Epoch 00054: val_loss improved from 0.13177 to 0.13150, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1849 - val_loss: 0.1315\n",
      "Epoch 55/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1888\n",
      "Epoch 00055: val_loss did not improve from 0.13150\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1868 - val_loss: 0.1315\n",
      "Epoch 56/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1829\n",
      "Epoch 00056: val_loss did not improve from 0.13150\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1842 - val_loss: 0.1326\n",
      " ###5 fold : val acc1 0.587, acc3 0.966, mae 0.237###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/39 [================>.............] - ETA: 0s - loss: 18.4769 \n",
      "Epoch 00001: val_loss improved from inf to 11.65106, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 16.5337 - val_loss: 11.6511\n",
      "Epoch 2/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 9.3631 \n",
      "Epoch 00002: val_loss improved from 11.65106 to 4.71515, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.1404 - val_loss: 4.7152\n",
      "Epoch 3/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 4.0097\n",
      "Epoch 00003: val_loss improved from 4.71515 to 2.07165, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.3871 - val_loss: 2.0716\n",
      "Epoch 4/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 2.0298\n",
      "Epoch 00004: val_loss improved from 2.07165 to 1.14873, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7498 - val_loss: 1.1487\n",
      "Epoch 5/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 1.3817\n",
      "Epoch 00005: val_loss improved from 1.14873 to 0.70174, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.1472 - val_loss: 0.7017\n",
      "Epoch 6/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.8683\n",
      "Epoch 00006: val_loss improved from 0.70174 to 0.47069, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.7379 - val_loss: 0.4707\n",
      "Epoch 7/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.4902\n",
      "Epoch 00007: val_loss improved from 0.47069 to 0.35209, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.5234 - val_loss: 0.3521\n",
      "Epoch 8/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3825\n",
      "Epoch 00008: val_loss improved from 0.35209 to 0.28643, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4392 - val_loss: 0.2864\n",
      "Epoch 9/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.3406\n",
      "Epoch 00009: val_loss improved from 0.28643 to 0.25095, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3788 - val_loss: 0.2510\n",
      "Epoch 10/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.3078\n",
      "Epoch 00010: val_loss improved from 0.25095 to 0.22912, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3015 - val_loss: 0.2291\n",
      "Epoch 11/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2868\n",
      "Epoch 00011: val_loss improved from 0.22912 to 0.21474, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2909 - val_loss: 0.2147\n",
      "Epoch 12/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2815\n",
      "Epoch 00012: val_loss improved from 0.21474 to 0.20410, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2775 - val_loss: 0.2041\n",
      "Epoch 13/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2762\n",
      "Epoch 00013: val_loss improved from 0.20410 to 0.19537, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2692 - val_loss: 0.1954\n",
      "Epoch 14/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2767\n",
      "Epoch 00014: val_loss improved from 0.19537 to 0.18783, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2650 - val_loss: 0.1878\n",
      "Epoch 15/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2455\n",
      "Epoch 00015: val_loss improved from 0.18783 to 0.18218, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2567 - val_loss: 0.1822\n",
      "Epoch 16/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2467\n",
      "Epoch 00016: val_loss improved from 0.18218 to 0.17489, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2454 - val_loss: 0.1749\n",
      "Epoch 17/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2410\n",
      "Epoch 00017: val_loss improved from 0.17489 to 0.16918, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2412 - val_loss: 0.1692\n",
      "Epoch 18/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2428\n",
      "Epoch 00018: val_loss improved from 0.16918 to 0.16670, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2424 - val_loss: 0.1667\n",
      "Epoch 19/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2486\n",
      "Epoch 00019: val_loss improved from 0.16670 to 0.16401, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2488 - val_loss: 0.1640\n",
      "Epoch 20/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.2315\n",
      "Epoch 00020: val_loss improved from 0.16401 to 0.15859, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2301 - val_loss: 0.1586\n",
      "Epoch 21/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2245\n",
      "Epoch 00021: val_loss improved from 0.15859 to 0.15474, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2248 - val_loss: 0.1547\n",
      "Epoch 22/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2259\n",
      "Epoch 00022: val_loss improved from 0.15474 to 0.15343, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2258 - val_loss: 0.1534\n",
      "Epoch 23/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2230\n",
      "Epoch 00023: val_loss improved from 0.15343 to 0.15037, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2216 - val_loss: 0.1504\n",
      "Epoch 24/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2185\n",
      "Epoch 00024: val_loss improved from 0.15037 to 0.14828, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2180 - val_loss: 0.1483\n",
      "Epoch 25/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2133\n",
      "Epoch 00025: val_loss improved from 0.14828 to 0.14635, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2145 - val_loss: 0.1464\n",
      "Epoch 26/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2146\n",
      "Epoch 00026: val_loss improved from 0.14635 to 0.14432, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2147 - val_loss: 0.1443\n",
      "Epoch 27/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2108\n",
      "Epoch 00027: val_loss improved from 0.14432 to 0.14307, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2110 - val_loss: 0.1431\n",
      "Epoch 28/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2146\n",
      "Epoch 00028: val_loss improved from 0.14307 to 0.14299, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2148 - val_loss: 0.1430\n",
      "Epoch 29/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2084\n",
      "Epoch 00029: val_loss improved from 0.14299 to 0.14150, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2082 - val_loss: 0.1415\n",
      "Epoch 30/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2107\n",
      "Epoch 00030: val_loss improved from 0.14150 to 0.13864, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2109 - val_loss: 0.1386\n",
      "Epoch 31/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00031: val_loss improved from 0.13864 to 0.13815, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2019 - val_loss: 0.1382\n",
      "Epoch 32/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00032: val_loss did not improve from 0.13815\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2062 - val_loss: 0.1382\n",
      "Epoch 33/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2077\n",
      "Epoch 00033: val_loss did not improve from 0.13815\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2063 - val_loss: 0.1385\n",
      " ###6 fold : val acc1 0.570, acc3 0.957, mae 0.241###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/39 [=================>............] - ETA: 0s - loss: 18.3455 \n",
      "Epoch 00001: val_loss improved from inf to 11.65986, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 16.5427 - val_loss: 11.6599\n",
      "Epoch 2/100\n",
      "25/39 [==================>...........] - ETA: 0s - loss: 9.2045 \n",
      "Epoch 00002: val_loss improved from 11.65986 to 4.71478, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.1463 - val_loss: 4.7148\n",
      "Epoch 3/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 3.9949\n",
      "Epoch 00003: val_loss improved from 4.71478 to 2.06330, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.3808 - val_loss: 2.0633\n",
      "Epoch 4/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 1.9921\n",
      "Epoch 00004: val_loss improved from 2.06330 to 1.14388, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7385 - val_loss: 1.1439\n",
      "Epoch 5/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 1.3694\n",
      "Epoch 00005: val_loss improved from 1.14388 to 0.70270, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.1397 - val_loss: 0.7027\n",
      "Epoch 6/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.8498\n",
      "Epoch 00006: val_loss improved from 0.70270 to 0.47316, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.7378 - val_loss: 0.4732\n",
      "Epoch 7/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.4876\n",
      "Epoch 00007: val_loss improved from 0.47316 to 0.35459, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.5229 - val_loss: 0.3546\n",
      "Epoch 8/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.3840\n",
      "Epoch 00008: val_loss improved from 0.35459 to 0.28829, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4390 - val_loss: 0.2883\n",
      "Epoch 9/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3400\n",
      "Epoch 00009: val_loss improved from 0.28829 to 0.25178, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3784 - val_loss: 0.2518\n",
      "Epoch 10/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3104\n",
      "Epoch 00010: val_loss improved from 0.25178 to 0.22913, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3027 - val_loss: 0.2291\n",
      "Epoch 11/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2930\n",
      "Epoch 00011: val_loss improved from 0.22913 to 0.21442, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2926 - val_loss: 0.2144\n",
      "Epoch 12/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2890\n",
      "Epoch 00012: val_loss improved from 0.21442 to 0.20321, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2820 - val_loss: 0.2032\n",
      "Epoch 13/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2785\n",
      "Epoch 00013: val_loss improved from 0.20321 to 0.19431, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2696 - val_loss: 0.1943\n",
      "Epoch 14/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2801\n",
      "Epoch 00014: val_loss improved from 0.19431 to 0.18689, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2675 - val_loss: 0.1869\n",
      "Epoch 15/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2454\n",
      "Epoch 00015: val_loss improved from 0.18689 to 0.18107, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2568 - val_loss: 0.1811\n",
      "Epoch 16/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2442\n",
      "Epoch 00016: val_loss improved from 0.18107 to 0.17366, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2442 - val_loss: 0.1737\n",
      "Epoch 17/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2462\n",
      "Epoch 00017: val_loss improved from 0.17366 to 0.16790, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2424 - val_loss: 0.1679\n",
      "Epoch 18/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2422\n",
      "Epoch 00018: val_loss improved from 0.16790 to 0.16529, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2422 - val_loss: 0.1653\n",
      "Epoch 19/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2470\n",
      "Epoch 00019: val_loss improved from 0.16529 to 0.16251, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2470 - val_loss: 0.1625\n",
      "Epoch 20/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2338\n",
      "Epoch 00020: val_loss improved from 0.16251 to 0.15714, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2305 - val_loss: 0.1571\n",
      "Epoch 21/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2218\n",
      "Epoch 00021: val_loss improved from 0.15714 to 0.15361, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2249 - val_loss: 0.1536\n",
      "Epoch 22/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2243\n",
      "Epoch 00022: val_loss improved from 0.15361 to 0.15229, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2245 - val_loss: 0.1523\n",
      "Epoch 23/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2229\n",
      "Epoch 00023: val_loss improved from 0.15229 to 0.14947, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2209 - val_loss: 0.1495\n",
      "Epoch 24/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2183\n",
      "Epoch 00024: val_loss improved from 0.14947 to 0.14728, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2189 - val_loss: 0.1473\n",
      "Epoch 25/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2158\n",
      "Epoch 00025: val_loss improved from 0.14728 to 0.14537, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2166 - val_loss: 0.1454\n",
      "Epoch 26/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2143\n",
      "Epoch 00026: val_loss improved from 0.14537 to 0.14390, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2150 - val_loss: 0.1439\n",
      "Epoch 27/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2110\n",
      "Epoch 00027: val_loss improved from 0.14390 to 0.14220, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2110 - val_loss: 0.1422\n",
      "Epoch 28/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2139\n",
      "Epoch 00028: val_loss did not improve from 0.14220\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2139 - val_loss: 0.1423\n",
      "Epoch 29/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2086\n",
      "Epoch 00029: val_loss improved from 0.14220 to 0.14100, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2086 - val_loss: 0.1410\n",
      "Epoch 30/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2105\n",
      "Epoch 00030: val_loss improved from 0.14100 to 0.13799, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2105 - val_loss: 0.1380\n",
      "Epoch 31/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2040\n",
      "Epoch 00031: val_loss improved from 0.13799 to 0.13770, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2028 - val_loss: 0.1377\n",
      "Epoch 32/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2082\n",
      "Epoch 00032: val_loss improved from 0.13770 to 0.13745, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2059 - val_loss: 0.1375\n",
      "Epoch 33/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2053\n",
      "Epoch 00033: val_loss did not improve from 0.13745\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2055 - val_loss: 0.1383\n",
      "Epoch 34/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2030\n",
      "Epoch 00034: val_loss improved from 0.13745 to 0.13699, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2044 - val_loss: 0.1370\n",
      "Epoch 35/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2059\n",
      "Epoch 00035: val_loss did not improve from 0.13699\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2059 - val_loss: 0.1375\n",
      "Epoch 36/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1992\n",
      "Epoch 00036: val_loss did not improve from 0.13699\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2097 - val_loss: 0.1394\n",
      " ###7 fold : val acc1 0.582, acc3 0.959, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/39 [================>.............] - ETA: 0s - loss: 18.4823 \n",
      "Epoch 00001: val_loss improved from inf to 11.71983, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 16.5371 - val_loss: 11.7198\n",
      "Epoch 2/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 9.2841 \n",
      "Epoch 00002: val_loss improved from 11.71983 to 4.76178, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.1459 - val_loss: 4.7618\n",
      "Epoch 3/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 3.9869\n",
      "Epoch 00003: val_loss improved from 4.76178 to 2.07891, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.3754 - val_loss: 2.0789\n",
      "Epoch 4/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 2.0122\n",
      "Epoch 00004: val_loss improved from 2.07891 to 1.15496, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7356 - val_loss: 1.1550\n",
      "Epoch 5/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 1.3816\n",
      "Epoch 00005: val_loss improved from 1.15496 to 0.70549, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.1386 - val_loss: 0.7055\n",
      "Epoch 6/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.8618\n",
      "Epoch 00006: val_loss improved from 0.70549 to 0.46897, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.7370 - val_loss: 0.4690\n",
      "Epoch 7/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.4872\n",
      "Epoch 00007: val_loss improved from 0.46897 to 0.34699, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.5233 - val_loss: 0.3470\n",
      "Epoch 8/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3863\n",
      "Epoch 00008: val_loss improved from 0.34699 to 0.27808, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4399 - val_loss: 0.2781\n",
      "Epoch 9/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3407\n",
      "Epoch 00009: val_loss improved from 0.27808 to 0.24095, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3808 - val_loss: 0.2409\n",
      "Epoch 10/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3101\n",
      "Epoch 00010: val_loss improved from 0.24095 to 0.21877, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3036 - val_loss: 0.2188\n",
      "Epoch 11/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2924\n",
      "Epoch 00011: val_loss improved from 0.21877 to 0.20394, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2936 - val_loss: 0.2039\n",
      "Epoch 12/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2907\n",
      "Epoch 00012: val_loss improved from 0.20394 to 0.19383, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2824 - val_loss: 0.1938\n",
      "Epoch 13/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2688\n",
      "Epoch 00013: val_loss improved from 0.19383 to 0.18525, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2686 - val_loss: 0.1853\n",
      "Epoch 14/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2680\n",
      "Epoch 00014: val_loss improved from 0.18525 to 0.17909, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2677 - val_loss: 0.1791\n",
      "Epoch 15/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2559\n",
      "Epoch 00015: val_loss improved from 0.17909 to 0.17359, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2559 - val_loss: 0.1736\n",
      "Epoch 16/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2450\n",
      "Epoch 00016: val_loss improved from 0.17359 to 0.16728, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2447 - val_loss: 0.1673\n",
      "Epoch 17/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2419\n",
      "Epoch 00017: val_loss improved from 0.16728 to 0.16196, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2419 - val_loss: 0.1620\n",
      "Epoch 18/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2427\n",
      "Epoch 00018: val_loss improved from 0.16196 to 0.15938, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2422 - val_loss: 0.1594\n",
      "Epoch 19/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2533\n",
      "Epoch 00019: val_loss improved from 0.15938 to 0.15814, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2469 - val_loss: 0.1581\n",
      "Epoch 20/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2301\n",
      "Epoch 00020: val_loss improved from 0.15814 to 0.15354, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2301 - val_loss: 0.1535\n",
      "Epoch 21/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2251\n",
      "Epoch 00021: val_loss improved from 0.15354 to 0.15049, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2251 - val_loss: 0.1505\n",
      "Epoch 22/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2235\n",
      "Epoch 00022: val_loss improved from 0.15049 to 0.14868, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2238 - val_loss: 0.1487\n",
      "Epoch 23/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2239\n",
      "Epoch 00023: val_loss improved from 0.14868 to 0.14635, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2207 - val_loss: 0.1463\n",
      "Epoch 24/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2194\n",
      "Epoch 00024: val_loss improved from 0.14635 to 0.14444, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2187 - val_loss: 0.1444\n",
      "Epoch 25/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2153\n",
      "Epoch 00025: val_loss improved from 0.14444 to 0.14254, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2160 - val_loss: 0.1425\n",
      "Epoch 26/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2142\n",
      "Epoch 00026: val_loss improved from 0.14254 to 0.14105, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2144 - val_loss: 0.1411\n",
      "Epoch 27/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2131\n",
      "Epoch 00027: val_loss improved from 0.14105 to 0.13974, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2100 - val_loss: 0.1397\n",
      "Epoch 28/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2172\n",
      "Epoch 00028: val_loss did not improve from 0.13974\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2136 - val_loss: 0.1402\n",
      "Epoch 29/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2085\n",
      "Epoch 00029: val_loss improved from 0.13974 to 0.13850, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2085 - val_loss: 0.1385\n",
      "Epoch 30/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2161\n",
      "Epoch 00030: val_loss improved from 0.13850 to 0.13635, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2099 - val_loss: 0.1363\n",
      "Epoch 31/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2040\n",
      "Epoch 00031: val_loss improved from 0.13635 to 0.13591, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2025 - val_loss: 0.1359\n",
      "Epoch 32/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2059\n",
      "Epoch 00032: val_loss improved from 0.13591 to 0.13572, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2059 - val_loss: 0.1357\n",
      "Epoch 33/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2045\n",
      "Epoch 00033: val_loss did not improve from 0.13572\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2045 - val_loss: 0.1363\n",
      "Epoch 34/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2043\n",
      "Epoch 00034: val_loss improved from 0.13572 to 0.13473, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2041 - val_loss: 0.1347\n",
      "Epoch 35/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.2037\n",
      "Epoch 00035: val_loss did not improve from 0.13473\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2048 - val_loss: 0.1353\n",
      "Epoch 36/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2095\n",
      "Epoch 00036: val_loss did not improve from 0.13473\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2091 - val_loss: 0.1371\n",
      " ###8 fold : val acc1 0.554, acc3 0.956, mae 0.250###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/39 [=================>............] - ETA: 0s - loss: 18.3613 \n",
      "Epoch 00001: val_loss improved from inf to 11.70515, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 16.5371 - val_loss: 11.7052\n",
      "Epoch 2/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 9.2841 \n",
      "Epoch 00002: val_loss improved from 11.70515 to 4.75479, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.1459 - val_loss: 4.7548\n",
      "Epoch 3/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 3.9357\n",
      "Epoch 00003: val_loss improved from 4.75479 to 2.09596, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.3754 - val_loss: 2.0960\n",
      "Epoch 4/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 1.9904\n",
      "Epoch 00004: val_loss improved from 2.09596 to 1.16875, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.7356 - val_loss: 1.1688\n",
      "Epoch 5/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 1.3430\n",
      "Epoch 00005: val_loss improved from 1.16875 to 0.71360, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.1386 - val_loss: 0.7136\n",
      "Epoch 6/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.8618\n",
      "Epoch 00006: val_loss improved from 0.71360 to 0.47200, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.7370 - val_loss: 0.4720\n",
      "Epoch 7/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.4886\n",
      "Epoch 00007: val_loss improved from 0.47200 to 0.34732, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.5233 - val_loss: 0.3473\n",
      "Epoch 8/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3863\n",
      "Epoch 00008: val_loss improved from 0.34732 to 0.27736, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4399 - val_loss: 0.2774\n",
      "Epoch 9/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3407\n",
      "Epoch 00009: val_loss improved from 0.27736 to 0.23943, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3808 - val_loss: 0.2394\n",
      "Epoch 10/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.3095\n",
      "Epoch 00010: val_loss improved from 0.23943 to 0.21627, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3036 - val_loss: 0.2163\n",
      "Epoch 11/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2924\n",
      "Epoch 00011: val_loss improved from 0.21627 to 0.20169, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2936 - val_loss: 0.2017\n",
      "Epoch 12/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2907\n",
      "Epoch 00012: val_loss improved from 0.20169 to 0.19111, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2824 - val_loss: 0.1911\n",
      "Epoch 13/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2741\n",
      "Epoch 00013: val_loss improved from 0.19111 to 0.18293, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2686 - val_loss: 0.1829\n",
      "Epoch 14/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2762\n",
      "Epoch 00014: val_loss improved from 0.18293 to 0.17633, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2677 - val_loss: 0.1763\n",
      "Epoch 15/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2551\n",
      "Epoch 00015: val_loss improved from 0.17633 to 0.17136, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2559 - val_loss: 0.1714\n",
      "Epoch 16/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2447\n",
      "Epoch 00016: val_loss improved from 0.17136 to 0.16487, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2447 - val_loss: 0.1649\n",
      "Epoch 17/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2419\n",
      "Epoch 00017: val_loss improved from 0.16487 to 0.15990, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2419 - val_loss: 0.1599\n",
      "Epoch 18/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2422\n",
      "Epoch 00018: val_loss improved from 0.15990 to 0.15785, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2422 - val_loss: 0.1579\n",
      "Epoch 19/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2469\n",
      "Epoch 00019: val_loss improved from 0.15785 to 0.15628, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2469 - val_loss: 0.1563\n",
      "Epoch 20/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2345\n",
      "Epoch 00020: val_loss improved from 0.15628 to 0.15173, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2301 - val_loss: 0.1517\n",
      "Epoch 21/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2251\n",
      "Epoch 00021: val_loss improved from 0.15173 to 0.14864, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2251 - val_loss: 0.1486\n",
      "Epoch 22/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2235\n",
      "Epoch 00022: val_loss improved from 0.14864 to 0.14744, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2238 - val_loss: 0.1474\n",
      "Epoch 23/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2239\n",
      "Epoch 00023: val_loss improved from 0.14744 to 0.14496, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2207 - val_loss: 0.1450\n",
      "Epoch 24/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2194\n",
      "Epoch 00024: val_loss improved from 0.14496 to 0.14322, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2187 - val_loss: 0.1432\n",
      "Epoch 25/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2151\n",
      "Epoch 00025: val_loss improved from 0.14322 to 0.14150, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2160 - val_loss: 0.1415\n",
      "Epoch 26/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2142\n",
      "Epoch 00026: val_loss improved from 0.14150 to 0.14025, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2144 - val_loss: 0.1402\n",
      "Epoch 27/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2131\n",
      "Epoch 00027: val_loss improved from 0.14025 to 0.13887, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2100 - val_loss: 0.1389\n",
      "Epoch 28/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2172\n",
      "Epoch 00028: val_loss did not improve from 0.13887\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2136 - val_loss: 0.1394\n",
      "Epoch 29/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2085\n",
      "Epoch 00029: val_loss improved from 0.13887 to 0.13813, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2085 - val_loss: 0.1381\n",
      "Epoch 30/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2099\n",
      "Epoch 00030: val_loss improved from 0.13813 to 0.13581, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2099 - val_loss: 0.1358\n",
      "Epoch 31/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2040\n",
      "Epoch 00031: val_loss improved from 0.13581 to 0.13553, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2025 - val_loss: 0.1355\n",
      "Epoch 32/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2078\n",
      "Epoch 00032: val_loss improved from 0.13553 to 0.13521, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2059 - val_loss: 0.1352\n",
      "Epoch 33/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2044\n",
      "Epoch 00033: val_loss did not improve from 0.13521\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2045 - val_loss: 0.1359\n",
      "Epoch 34/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2041\n",
      "Epoch 00034: val_loss improved from 0.13521 to 0.13480, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2041 - val_loss: 0.1348\n",
      "Epoch 35/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2048\n",
      "Epoch 00035: val_loss did not improve from 0.13480\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2048 - val_loss: 0.1352\n",
      "Epoch 36/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2091\n",
      "Epoch 00036: val_loss did not improve from 0.13480\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2091 - val_loss: 0.1372\n",
      " ###9 fold : val acc1 0.572, acc3 0.959, mae 0.240###\n",
      "acc10.571_acc30.960\n",
      "random search 19/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293/307 [===========================>..] - ETA: 0s - loss: 3.7946\n",
      "Epoch 00001: val_loss improved from inf to 0.34403, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.6651 - val_loss: 0.3440\n",
      "Epoch 2/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 1.0111\n",
      "Epoch 00002: val_loss improved from 0.34403 to 0.26972, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.0111 - val_loss: 0.2697\n",
      "Epoch 3/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.6566\n",
      "Epoch 00003: val_loss did not improve from 0.26972\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6566 - val_loss: 0.4100\n",
      "Epoch 4/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.6026\n",
      "Epoch 00004: val_loss improved from 0.26972 to 0.25284, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5982 - val_loss: 0.2528\n",
      "Epoch 5/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.4380\n",
      "Epoch 00005: val_loss improved from 0.25284 to 0.19858, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4373 - val_loss: 0.1986\n",
      "Epoch 6/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.4031\n",
      "Epoch 00006: val_loss improved from 0.19858 to 0.17626, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4022 - val_loss: 0.1763\n",
      "Epoch 7/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.4170\n",
      "Epoch 00007: val_loss did not improve from 0.17626\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4139 - val_loss: 0.1818\n",
      "Epoch 8/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.3706\n",
      "Epoch 00008: val_loss did not improve from 0.17626\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3684 - val_loss: 0.1766\n",
      " ###0 fold : val acc1 0.490, acc3 0.932, mae 0.296###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/307 [============================>.] - ETA: 0s - loss: 3.6483\n",
      "Epoch 00001: val_loss improved from inf to 0.34116, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_1.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.6351 - val_loss: 0.3412\n",
      "Epoch 2/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.9926\n",
      "Epoch 00002: val_loss improved from 0.34116 to 0.26634, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.9722 - val_loss: 0.2663\n",
      "Epoch 3/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.4591\n",
      "Epoch 00003: val_loss did not improve from 0.26634\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.6281 - val_loss: 0.4032\n",
      "Epoch 4/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.5798\n",
      "Epoch 00004: val_loss improved from 0.26634 to 0.25255, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5760 - val_loss: 0.2525\n",
      "Epoch 5/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.4360\n",
      "Epoch 00005: val_loss improved from 0.25255 to 0.19273, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4337 - val_loss: 0.1927\n",
      "Epoch 6/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.3991\n",
      "Epoch 00006: val_loss improved from 0.19273 to 0.17998, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3987 - val_loss: 0.1800\n",
      "Epoch 7/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.4298\n",
      "Epoch 00007: val_loss improved from 0.17998 to 0.17963, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4298 - val_loss: 0.1796\n",
      "Epoch 8/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.3744\n",
      "Epoch 00008: val_loss improved from 0.17963 to 0.17456, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3728 - val_loss: 0.1746\n",
      "Epoch 9/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.3433\n",
      "Epoch 00009: val_loss did not improve from 0.17456\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3427 - val_loss: 0.1801\n",
      "Epoch 10/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.3184\n",
      "Epoch 00010: val_loss improved from 0.17456 to 0.17377, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3184 - val_loss: 0.1738\n",
      "Epoch 11/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.3059\n",
      "Epoch 00011: val_loss improved from 0.17377 to 0.17114, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3052 - val_loss: 0.1711\n",
      "Epoch 12/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.2957\n",
      "Epoch 00012: val_loss did not improve from 0.17114\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2956 - val_loss: 0.1713\n",
      "Epoch 13/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.2998\n",
      "Epoch 00013: val_loss did not improve from 0.17114\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2993 - val_loss: 0.1868\n",
      " ###1 fold : val acc1 0.468, acc3 0.941, mae 0.298###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/307 [============================>.] - ETA: 0s - loss: 3.6599\n",
      "Epoch 00001: val_loss improved from inf to 0.33038, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_2.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.6098 - val_loss: 0.3304\n",
      "Epoch 2/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.9625\n",
      "Epoch 00002: val_loss improved from 0.33038 to 0.26825, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.9591 - val_loss: 0.2683\n",
      "Epoch 3/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.6327\n",
      "Epoch 00003: val_loss did not improve from 0.26825\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6303 - val_loss: 0.3826\n",
      "Epoch 4/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.5670\n",
      "Epoch 00004: val_loss improved from 0.26825 to 0.24871, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5656 - val_loss: 0.2487\n",
      "Epoch 5/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.4539\n",
      "Epoch 00005: val_loss improved from 0.24871 to 0.17959, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4530 - val_loss: 0.1796\n",
      "Epoch 6/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.4171\n",
      "Epoch 00006: val_loss did not improve from 0.17959\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4161 - val_loss: 0.1873\n",
      "Epoch 7/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.4317\n",
      "Epoch 00007: val_loss did not improve from 0.17959\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4308 - val_loss: 0.1868\n",
      " ###2 fold : val acc1 0.480, acc3 0.940, mae 0.296###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293/307 [===========================>..] - ETA: 0s - loss: 3.7739\n",
      "Epoch 00001: val_loss improved from inf to 0.31984, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_3.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.6475 - val_loss: 0.3198\n",
      "Epoch 2/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 1.0141\n",
      "Epoch 00002: val_loss improved from 0.31984 to 0.25771, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.0073 - val_loss: 0.2577\n",
      "Epoch 3/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.6339\n",
      "Epoch 00003: val_loss did not improve from 0.25771\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6324 - val_loss: 0.3657\n",
      "Epoch 4/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.5760\n",
      "Epoch 00004: val_loss improved from 0.25771 to 0.23207, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5750 - val_loss: 0.2321\n",
      "Epoch 5/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.4516\n",
      "Epoch 00005: val_loss improved from 0.23207 to 0.18160, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4510 - val_loss: 0.1816\n",
      "Epoch 6/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.4202\n",
      "Epoch 00006: val_loss did not improve from 0.18160\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4192 - val_loss: 0.1857\n",
      "Epoch 7/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.4308\n",
      "Epoch 00007: val_loss did not improve from 0.18160\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4287 - val_loss: 0.1869\n",
      " ###3 fold : val acc1 0.484, acc3 0.943, mae 0.291###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298/307 [============================>.] - ETA: 0s - loss: 3.0199\n",
      "Epoch 00001: val_loss improved from inf to 0.31319, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_4.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.9593 - val_loss: 0.3132\n",
      "Epoch 2/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.7376\n",
      "Epoch 00002: val_loss improved from 0.31319 to 0.22462, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7265 - val_loss: 0.2246\n",
      "Epoch 3/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.6948\n",
      "Epoch 00003: val_loss improved from 0.22462 to 0.21309, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6868 - val_loss: 0.2131\n",
      "Epoch 4/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.5241\n",
      "Epoch 00004: val_loss did not improve from 0.21309\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5203 - val_loss: 0.2332\n",
      "Epoch 5/100\n",
      "291/307 [===========================>..] - ETA: 0s - loss: 0.5307\n",
      "Epoch 00005: val_loss improved from 0.21309 to 0.16980, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5232 - val_loss: 0.1698\n",
      "Epoch 6/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.4196\n",
      "Epoch 00006: val_loss did not improve from 0.16980\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4181 - val_loss: 0.1956\n",
      "Epoch 7/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.3845\n",
      "Epoch 00007: val_loss did not improve from 0.16980\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3839 - val_loss: 0.1927\n",
      " ###4 fold : val acc1 0.520, acc3 0.943, mae 0.273###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/307 [===========================>..] - ETA: 0s - loss: 2.5975\n",
      "Epoch 00001: val_loss improved from inf to 0.32514, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_5.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.5375 - val_loss: 0.3251\n",
      "Epoch 2/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.5883\n",
      "Epoch 00002: val_loss improved from 0.32514 to 0.22902, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5827 - val_loss: 0.2290\n",
      "Epoch 3/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.4285\n",
      "Epoch 00003: val_loss improved from 0.22902 to 0.21248, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4278 - val_loss: 0.2125\n",
      "Epoch 4/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.3955\n",
      "Epoch 00004: val_loss did not improve from 0.21248\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3955 - val_loss: 0.2449\n",
      "Epoch 5/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.3713\n",
      "Epoch 00005: val_loss improved from 0.21248 to 0.17690, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3721 - val_loss: 0.1769\n",
      "Epoch 6/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.3569\n",
      "Epoch 00006: val_loss did not improve from 0.17690\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3568 - val_loss: 0.1837\n",
      "Epoch 7/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.3416\n",
      "Epoch 00007: val_loss did not improve from 0.17690\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3415 - val_loss: 0.2258\n",
      " ###5 fold : val acc1 0.500, acc3 0.943, mae 0.295###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 4.1057\n",
      "Epoch 00001: val_loss improved from inf to 0.34449, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_6.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 4.0083 - val_loss: 0.3445\n",
      "Epoch 2/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 1.0605\n",
      "Epoch 00002: val_loss improved from 0.34449 to 0.24360, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.0605 - val_loss: 0.2436\n",
      "Epoch 3/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.6789\n",
      "Epoch 00003: val_loss improved from 0.24360 to 0.20230, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6758 - val_loss: 0.2023\n",
      "Epoch 4/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.5749\n",
      "Epoch 00004: val_loss did not improve from 0.20230\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5718 - val_loss: 0.2567\n",
      "Epoch 5/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.5742\n",
      "Epoch 00005: val_loss improved from 0.20230 to 0.18232, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5718 - val_loss: 0.1823\n",
      "Epoch 6/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.4663\n",
      "Epoch 00006: val_loss did not improve from 0.18232\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4662 - val_loss: 0.1838\n",
      "Epoch 7/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.3651\n",
      "Epoch 00007: val_loss did not improve from 0.18232\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3647 - val_loss: 0.1976\n",
      " ###6 fold : val acc1 0.506, acc3 0.934, mae 0.287###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293/307 [===========================>..] - ETA: 0s - loss: 4.1731\n",
      "Epoch 00001: val_loss improved from inf to 0.35950, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_7.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 4.0316 - val_loss: 0.3595\n",
      "Epoch 2/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 1.0859\n",
      "Epoch 00002: val_loss improved from 0.35950 to 0.26688, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.0811 - val_loss: 0.2669\n",
      "Epoch 3/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.6941\n",
      "Epoch 00003: val_loss improved from 0.26688 to 0.20378, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6903 - val_loss: 0.2038\n",
      "Epoch 4/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.6104\n",
      "Epoch 00004: val_loss did not improve from 0.20378\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6095 - val_loss: 0.2473\n",
      "Epoch 5/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.6174\n",
      "Epoch 00005: val_loss improved from 0.20378 to 0.18948, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6121 - val_loss: 0.1895\n",
      "Epoch 6/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.4867\n",
      "Epoch 00006: val_loss improved from 0.18948 to 0.17401, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4823 - val_loss: 0.1740\n",
      "Epoch 7/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.3837\n",
      "Epoch 00007: val_loss did not improve from 0.17401\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3834 - val_loss: 0.1940\n",
      "Epoch 8/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.4003\n",
      "Epoch 00008: val_loss improved from 0.17401 to 0.16648, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3995 - val_loss: 0.1665\n",
      "Epoch 9/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.3441\n",
      "Epoch 00009: val_loss did not improve from 0.16648\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3437 - val_loss: 0.1710\n",
      "Epoch 10/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.3231\n",
      "Epoch 00010: val_loss improved from 0.16648 to 0.15728, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3233 - val_loss: 0.1573\n",
      "Epoch 11/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.3019\n",
      "Epoch 00011: val_loss did not improve from 0.15728\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3029 - val_loss: 0.1643\n",
      "Epoch 12/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.3508\n",
      "Epoch 00012: val_loss improved from 0.15728 to 0.15621, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3492 - val_loss: 0.1562\n",
      "Epoch 13/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.3092\n",
      "Epoch 00013: val_loss did not improve from 0.15621\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3076 - val_loss: 0.1664\n",
      "Epoch 14/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.2848\n",
      "Epoch 00014: val_loss did not improve from 0.15621\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2841 - val_loss: 0.1705\n",
      " ###7 fold : val acc1 0.518, acc3 0.953, mae 0.268###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/307 [============================>.] - ETA: 0s - loss: 4.0791\n",
      "Epoch 00001: val_loss improved from inf to 0.36358, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_8.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 4.0650 - val_loss: 0.3636\n",
      "Epoch 2/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 1.0820\n",
      "Epoch 00002: val_loss improved from 0.36358 to 0.26237, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.0794 - val_loss: 0.2624\n",
      "Epoch 3/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.6806\n",
      "Epoch 00003: val_loss improved from 0.26237 to 0.20284, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6778 - val_loss: 0.2028\n",
      "Epoch 4/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.5951\n",
      "Epoch 00004: val_loss did not improve from 0.20284\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5923 - val_loss: 0.2376\n",
      "Epoch 5/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.6102\n",
      "Epoch 00005: val_loss improved from 0.20284 to 0.18264, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6035 - val_loss: 0.1826\n",
      "Epoch 6/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.4762\n",
      "Epoch 00006: val_loss improved from 0.18264 to 0.16762, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4744 - val_loss: 0.1676\n",
      "Epoch 7/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.3781\n",
      "Epoch 00007: val_loss did not improve from 0.16762\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3778 - val_loss: 0.1846\n",
      "Epoch 8/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.3969\n",
      "Epoch 00008: val_loss improved from 0.16762 to 0.16004, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3958 - val_loss: 0.1600\n",
      "Epoch 9/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.3408\n",
      "Epoch 00009: val_loss did not improve from 0.16004\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3406 - val_loss: 0.1673\n",
      "Epoch 10/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.3237\n",
      "Epoch 00010: val_loss improved from 0.16004 to 0.15238, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3242 - val_loss: 0.1524\n",
      "Epoch 11/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.3019\n",
      "Epoch 00011: val_loss did not improve from 0.15238\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3019 - val_loss: 0.1585\n",
      "Epoch 12/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.3549\n",
      "Epoch 00012: val_loss improved from 0.15238 to 0.15159, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3514 - val_loss: 0.1516\n",
      "Epoch 13/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.3144\n",
      "Epoch 00013: val_loss did not improve from 0.15159\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3138 - val_loss: 0.1618\n",
      "Epoch 14/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.2773\n",
      "Epoch 00014: val_loss did not improve from 0.15159\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2764 - val_loss: 0.1653\n",
      " ###8 fold : val acc1 0.512, acc3 0.950, mae 0.273###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302/307 [============================>.] - ETA: 0s - loss: 4.1107\n",
      "Epoch 00001: val_loss improved from inf to 0.35311, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_9.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 4.0650 - val_loss: 0.3531\n",
      "Epoch 2/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 1.1017\n",
      "Epoch 00002: val_loss improved from 0.35311 to 0.26757, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.0794 - val_loss: 0.2676\n",
      "Epoch 3/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.6778\n",
      "Epoch 00003: val_loss improved from 0.26757 to 0.20662, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6778 - val_loss: 0.2066\n",
      "Epoch 4/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.5941\n",
      "Epoch 00004: val_loss did not improve from 0.20662\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5923 - val_loss: 0.2431\n",
      "Epoch 5/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.6064\n",
      "Epoch 00005: val_loss improved from 0.20662 to 0.18644, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6035 - val_loss: 0.1864\n",
      "Epoch 6/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.4788\n",
      "Epoch 00006: val_loss improved from 0.18644 to 0.17107, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4744 - val_loss: 0.1711\n",
      "Epoch 7/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.3780\n",
      "Epoch 00007: val_loss did not improve from 0.17107\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3778 - val_loss: 0.1888\n",
      "Epoch 8/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.3986\n",
      "Epoch 00008: val_loss improved from 0.17107 to 0.16260, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3958 - val_loss: 0.1626\n",
      "Epoch 9/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.3406\n",
      "Epoch 00009: val_loss did not improve from 0.16260\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3406 - val_loss: 0.1703\n",
      "Epoch 10/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.3247\n",
      "Epoch 00010: val_loss improved from 0.16260 to 0.15471, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3242 - val_loss: 0.1547\n",
      "Epoch 11/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.3009\n",
      "Epoch 00011: val_loss did not improve from 0.15471\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3019 - val_loss: 0.1613\n",
      "Epoch 12/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.3540\n",
      "Epoch 00012: val_loss improved from 0.15471 to 0.15205, saving model to result/size/DNN_size_both_y/batch64,dnodes32_dropout0.4,dnodes256_dropout0.5,lr0.001/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3514 - val_loss: 0.1521\n",
      "Epoch 13/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.3166\n",
      "Epoch 00013: val_loss did not improve from 0.15205\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3138 - val_loss: 0.1632\n",
      "Epoch 14/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.2769\n",
      "Epoch 00014: val_loss did not improve from 0.15205\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2764 - val_loss: 0.1672\n",
      " ###9 fold : val acc1 0.518, acc3 0.953, mae 0.270###\n",
      "acc10.500_acc30.943\n",
      "random search 20/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/39 [==========================>...] - ETA: 0s - loss: 12.6133\n",
      "Epoch 00001: val_loss improved from inf to 1.93997, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 12.0683 - val_loss: 1.9400\n",
      "Epoch 2/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 3.8112\n",
      "Epoch 00002: val_loss improved from 1.93997 to 0.68878, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.6733 - val_loss: 0.6888\n",
      "Epoch 3/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 1.8972\n",
      "Epoch 00003: val_loss improved from 0.68878 to 0.40475, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.0703 - val_loss: 0.4048\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.7511\n",
      "Epoch 00004: val_loss improved from 0.40475 to 0.32314, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7349 - val_loss: 0.3231\n",
      "Epoch 5/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.7745\n",
      "Epoch 00005: val_loss improved from 0.32314 to 0.28826, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7520 - val_loss: 0.2883\n",
      "Epoch 6/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.4985\n",
      "Epoch 00006: val_loss improved from 0.28826 to 0.27882, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4860 - val_loss: 0.2788\n",
      "Epoch 7/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.4662\n",
      "Epoch 00007: val_loss improved from 0.27882 to 0.23851, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4433 - val_loss: 0.2385\n",
      "Epoch 8/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.3722\n",
      "Epoch 00008: val_loss did not improve from 0.23851\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4838 - val_loss: 0.2664\n",
      "Epoch 9/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.3858\n",
      "Epoch 00009: val_loss improved from 0.23851 to 0.21893, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3811 - val_loss: 0.2189\n",
      "Epoch 10/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.3606\n",
      "Epoch 00010: val_loss did not improve from 0.21893\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4066 - val_loss: 0.2352\n",
      "Epoch 11/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.3259\n",
      "Epoch 00011: val_loss improved from 0.21893 to 0.20766, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3252 - val_loss: 0.2077\n",
      "Epoch 12/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.3296\n",
      "Epoch 00012: val_loss improved from 0.20766 to 0.20147, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3236 - val_loss: 0.2015\n",
      "Epoch 13/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.3201\n",
      "Epoch 00013: val_loss did not improve from 0.20147\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3155 - val_loss: 0.2285\n",
      "Epoch 14/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.2933\n",
      "Epoch 00014: val_loss improved from 0.20147 to 0.18935, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3005 - val_loss: 0.1894\n",
      "Epoch 15/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.2616\n",
      "Epoch 00015: val_loss did not improve from 0.18935\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2675 - val_loss: 0.2080\n",
      "Epoch 16/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.2628\n",
      "Epoch 00016: val_loss did not improve from 0.18935\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2634 - val_loss: 0.2012\n",
      " ###0 fold : val acc1 0.495, acc3 0.917, mae 0.302###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/39 [============================>.] - ETA: 0s - loss: 12.1502\n",
      "Epoch 00001: val_loss improved from inf to 1.93643, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 12.0840 - val_loss: 1.9364\n",
      "Epoch 2/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 3.7070\n",
      "Epoch 00002: val_loss improved from 1.93643 to 0.68380, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.6614 - val_loss: 0.6838\n",
      "Epoch 3/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.8353\n",
      "Epoch 00003: val_loss improved from 0.68380 to 0.40634, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.0814 - val_loss: 0.4063\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.7617\n",
      "Epoch 00004: val_loss improved from 0.40634 to 0.32304, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7453 - val_loss: 0.3230\n",
      "Epoch 5/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.7696\n",
      "Epoch 00005: val_loss improved from 0.32304 to 0.28386, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7582 - val_loss: 0.2839\n",
      "Epoch 6/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.4910\n",
      "Epoch 00006: val_loss improved from 0.28386 to 0.28001, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4854 - val_loss: 0.2800\n",
      "Epoch 7/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.4540\n",
      "Epoch 00007: val_loss improved from 0.28001 to 0.23778, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4450 - val_loss: 0.2378\n",
      "Epoch 8/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.3783\n",
      "Epoch 00008: val_loss did not improve from 0.23778\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.4914 - val_loss: 0.2689\n",
      "Epoch 9/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.3908\n",
      "Epoch 00009: val_loss improved from 0.23778 to 0.21386, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3930 - val_loss: 0.2139\n",
      "Epoch 10/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.3584\n",
      "Epoch 00010: val_loss did not improve from 0.21386\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.4023 - val_loss: 0.2264\n",
      "Epoch 11/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.3306\n",
      "Epoch 00011: val_loss did not improve from 0.21386\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3254 - val_loss: 0.2144\n",
      " ###1 fold : val acc1 0.459, acc3 0.914, mae 0.318###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/39 [==========================>...] - ETA: 0s - loss: 12.6435\n",
      "Epoch 00001: val_loss improved from inf to 1.94361, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 12.0889 - val_loss: 1.9436\n",
      "Epoch 2/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 3.7834\n",
      "Epoch 00002: val_loss improved from 1.94361 to 0.68739, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.6926 - val_loss: 0.6874\n",
      "Epoch 3/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.8509\n",
      "Epoch 00003: val_loss improved from 0.68739 to 0.40779, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.0983 - val_loss: 0.4078\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.7606\n",
      "Epoch 00004: val_loss improved from 0.40779 to 0.31781, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7450 - val_loss: 0.3178\n",
      "Epoch 5/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.7998\n",
      "Epoch 00005: val_loss improved from 0.31781 to 0.28436, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7686 - val_loss: 0.2844\n",
      "Epoch 6/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.4995\n",
      "Epoch 00006: val_loss improved from 0.28436 to 0.28230, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4865 - val_loss: 0.2823\n",
      "Epoch 7/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.4620\n",
      "Epoch 00007: val_loss improved from 0.28230 to 0.24377, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4479 - val_loss: 0.2438\n",
      "Epoch 8/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.3806\n",
      "Epoch 00008: val_loss did not improve from 0.24377\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.4977 - val_loss: 0.2651\n",
      "Epoch 9/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.3884\n",
      "Epoch 00009: val_loss improved from 0.24377 to 0.21346, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3898 - val_loss: 0.2135\n",
      "Epoch 10/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.3592\n",
      "Epoch 00010: val_loss did not improve from 0.21346\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.4034 - val_loss: 0.2291\n",
      "Epoch 11/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.3271\n",
      "Epoch 00011: val_loss improved from 0.21346 to 0.21270, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3287 - val_loss: 0.2127\n",
      "Epoch 12/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.3331\n",
      "Epoch 00012: val_loss improved from 0.21270 to 0.20648, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3359 - val_loss: 0.2065\n",
      "Epoch 13/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.3127\n",
      "Epoch 00013: val_loss did not improve from 0.20648\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3134 - val_loss: 0.2164\n",
      "Epoch 14/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.2999\n",
      "Epoch 00014: val_loss improved from 0.20648 to 0.19340, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2983 - val_loss: 0.1934\n",
      "Epoch 15/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.2707\n",
      "Epoch 00015: val_loss did not improve from 0.19340\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.2721 - val_loss: 0.2130\n",
      "Epoch 16/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.2622\n",
      "Epoch 00016: val_loss did not improve from 0.19340\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2663 - val_loss: 0.1967\n",
      " ###2 fold : val acc1 0.465, acc3 0.924, mae 0.311###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - ETA: 0s - loss: 12.0619\n",
      "Epoch 00001: val_loss improved from inf to 1.94016, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 12.0619 - val_loss: 1.9402\n",
      "Epoch 2/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 3.6718\n",
      "Epoch 00002: val_loss improved from 1.94016 to 0.67843, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.6615 - val_loss: 0.6784\n",
      "Epoch 3/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 2.0938\n",
      "Epoch 00003: val_loss improved from 0.67843 to 0.40719, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.0882 - val_loss: 0.4072\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.7642\n",
      "Epoch 00004: val_loss improved from 0.40719 to 0.31469, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7495 - val_loss: 0.3147\n",
      "Epoch 5/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.7769\n",
      "Epoch 00005: val_loss improved from 0.31469 to 0.28459, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.7636 - val_loss: 0.2846\n",
      "Epoch 6/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.4884\n",
      "Epoch 00006: val_loss improved from 0.28459 to 0.28074, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4852 - val_loss: 0.2807\n",
      "Epoch 7/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.4520\n",
      "Epoch 00007: val_loss improved from 0.28074 to 0.24081, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4500 - val_loss: 0.2408\n",
      "Epoch 8/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.4930\n",
      "Epoch 00008: val_loss did not improve from 0.24081\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.4930 - val_loss: 0.2693\n",
      "Epoch 9/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.3846\n",
      "Epoch 00009: val_loss improved from 0.24081 to 0.22199, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3854 - val_loss: 0.2220\n",
      "Epoch 10/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.3500\n",
      "Epoch 00010: val_loss did not improve from 0.22199\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3951 - val_loss: 0.2364\n",
      "Epoch 11/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.3225\n",
      "Epoch 00011: val_loss improved from 0.22199 to 0.21379, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3235 - val_loss: 0.2138\n",
      "Epoch 12/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.3310\n",
      "Epoch 00012: val_loss improved from 0.21379 to 0.20811, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3333 - val_loss: 0.2081\n",
      "Epoch 13/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.3181\n",
      "Epoch 00013: val_loss did not improve from 0.20811\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3152 - val_loss: 0.2167\n",
      "Epoch 14/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.2908\n",
      "Epoch 00014: val_loss improved from 0.20811 to 0.19407, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2969 - val_loss: 0.1941\n",
      "Epoch 15/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.2613\n",
      "Epoch 00015: val_loss did not improve from 0.19407\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2631 - val_loss: 0.2170\n",
      "Epoch 16/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.2817\n",
      "Epoch 00016: val_loss improved from 0.19407 to 0.18959, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2757 - val_loss: 0.1896\n",
      "Epoch 17/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.2834\n",
      "Epoch 00017: val_loss did not improve from 0.18959\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.2836 - val_loss: 0.2185\n",
      "Epoch 18/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.2305\n",
      "Epoch 00018: val_loss did not improve from 0.18959\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2362 - val_loss: 0.1993\n",
      " ###3 fold : val acc1 0.470, acc3 0.934, mae 0.302###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/39 [===========================>..] - ETA: 0s - loss: 11.9457\n",
      "Epoch 00001: val_loss improved from inf to 1.88197, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 11.6531 - val_loss: 1.8820\n",
      "Epoch 2/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 3.4086\n",
      "Epoch 00002: val_loss improved from 1.88197 to 0.78105, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 3.3321 - val_loss: 0.7811\n",
      "Epoch 3/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 2.0061\n",
      "Epoch 00003: val_loss improved from 0.78105 to 0.38186, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.9793 - val_loss: 0.3819\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.6949\n",
      "Epoch 00004: val_loss improved from 0.38186 to 0.29732, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6842 - val_loss: 0.2973\n",
      "Epoch 5/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.6166\n",
      "Epoch 00005: val_loss improved from 0.29732 to 0.26100, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6001 - val_loss: 0.2610\n",
      "Epoch 6/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.4889\n",
      "Epoch 00006: val_loss did not improve from 0.26100\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.4790 - val_loss: 0.2648\n",
      "Epoch 7/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.4469\n",
      "Epoch 00007: val_loss improved from 0.26100 to 0.23606, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4329 - val_loss: 0.2361\n",
      "Epoch 8/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.4657\n",
      "Epoch 00008: val_loss improved from 0.23606 to 0.23188, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4549 - val_loss: 0.2319\n",
      "Epoch 9/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.3602\n",
      "Epoch 00009: val_loss improved from 0.23188 to 0.22157, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3637 - val_loss: 0.2216\n",
      "Epoch 10/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.3586\n",
      "Epoch 00010: val_loss did not improve from 0.22157\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3668 - val_loss: 0.2264\n",
      "Epoch 11/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.3231\n",
      "Epoch 00011: val_loss improved from 0.22157 to 0.20151, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3219 - val_loss: 0.2015\n",
      "Epoch 12/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.3340\n",
      "Epoch 00012: val_loss did not improve from 0.20151\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3399 - val_loss: 0.2189\n",
      "Epoch 13/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.3172\n",
      "Epoch 00013: val_loss did not improve from 0.20151\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3118 - val_loss: 0.2017\n",
      " ###4 fold : val acc1 0.480, acc3 0.927, mae 0.302###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/39 [============================>.] - ETA: 0s - loss: 11.2897\n",
      "Epoch 00001: val_loss improved from inf to 1.81234, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 11.2298 - val_loss: 1.8123\n",
      "Epoch 2/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.4872\n",
      "Epoch 00002: val_loss improved from 1.81234 to 0.58972, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.4714 - val_loss: 0.5897\n",
      "Epoch 3/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.7072\n",
      "Epoch 00003: val_loss improved from 0.58972 to 0.34721, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6937 - val_loss: 0.3472\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.4865\n",
      "Epoch 00004: val_loss improved from 0.34721 to 0.28108, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4843 - val_loss: 0.2811\n",
      "Epoch 5/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.4424\n",
      "Epoch 00005: val_loss improved from 0.28108 to 0.26286, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4373 - val_loss: 0.2629\n",
      "Epoch 6/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.3839\n",
      "Epoch 00006: val_loss improved from 0.26286 to 0.26152, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3811 - val_loss: 0.2615\n",
      "Epoch 7/100\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 1.3716\n",
      "Epoch 00007: val_loss improved from 0.26152 to 0.22900, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3638 - val_loss: 0.2290\n",
      "Epoch 8/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.3651\n",
      "Epoch 00008: val_loss did not improve from 0.22900\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3594 - val_loss: 0.2370\n",
      "Epoch 9/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.3624\n",
      "Epoch 00009: val_loss improved from 0.22900 to 0.20146, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3628 - val_loss: 0.2015\n",
      "Epoch 10/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.3422\n",
      "Epoch 00010: val_loss did not improve from 0.20146\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3500 - val_loss: 0.2257\n",
      "Epoch 11/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.2944\n",
      "Epoch 00011: val_loss improved from 0.20146 to 0.19891, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3075 - val_loss: 0.1989\n",
      "Epoch 12/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.3275\n",
      "Epoch 00012: val_loss did not improve from 0.19891\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3245 - val_loss: 0.2120\n",
      "Epoch 13/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.3017\n",
      "Epoch 00013: val_loss did not improve from 0.19891\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.2995 - val_loss: 0.2030\n",
      " ###5 fold : val acc1 0.479, acc3 0.931, mae 0.322###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/39 [============================>.] - ETA: 0s - loss: 11.4901\n",
      "Epoch 00001: val_loss improved from inf to 1.77867, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 11.4270 - val_loss: 1.7787\n",
      "Epoch 2/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.7623\n",
      "Epoch 00002: val_loss improved from 1.77867 to 0.64160, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.7379 - val_loss: 0.6416\n",
      "Epoch 3/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.0427\n",
      "Epoch 00003: val_loss improved from 0.64160 to 0.33824, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.0249 - val_loss: 0.3382\n",
      "Epoch 4/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.6139\n",
      "Epoch 00004: val_loss improved from 0.33824 to 0.27626, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.6057 - val_loss: 0.2763\n",
      "Epoch 5/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.5902\n",
      "Epoch 00005: val_loss improved from 0.27626 to 0.25575, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.5884 - val_loss: 0.2557\n",
      "Epoch 6/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.4142\n",
      "Epoch 00006: val_loss did not improve from 0.25575\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.4143 - val_loss: 0.2639\n",
      "Epoch 7/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.3946\n",
      "Epoch 00007: val_loss improved from 0.25575 to 0.24632, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3860 - val_loss: 0.2463\n",
      "Epoch 8/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.4124\n",
      "Epoch 00008: val_loss improved from 0.24632 to 0.24165, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.4005 - val_loss: 0.2416\n",
      "Epoch 9/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.3888\n",
      "Epoch 00009: val_loss improved from 0.24165 to 0.21924, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3858 - val_loss: 0.2192\n",
      "Epoch 10/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.3504\n",
      "Epoch 00010: val_loss did not improve from 0.21924\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3562 - val_loss: 0.2220\n",
      "Epoch 11/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.2990\n",
      "Epoch 00011: val_loss did not improve from 0.21924\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3351 - val_loss: 0.2340\n",
      " ###6 fold : val acc1 0.462, acc3 0.911, mae 0.321###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - ETA: 0s - loss: 11.4131\n",
      "Epoch 00001: val_loss improved from inf to 1.77576, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 11.4131 - val_loss: 1.7758\n",
      "Epoch 2/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 2.8110\n",
      "Epoch 00002: val_loss improved from 1.77576 to 0.65974, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.7690 - val_loss: 0.6597\n",
      "Epoch 3/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 2.4843\n",
      "Epoch 00003: val_loss improved from 0.65974 to 0.33787, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.0835 - val_loss: 0.3379\n",
      "Epoch 4/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.6265\n",
      "Epoch 00004: val_loss improved from 0.33787 to 0.27726, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6255 - val_loss: 0.2773\n",
      "Epoch 5/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.6450\n",
      "Epoch 00005: val_loss improved from 0.27726 to 0.25249, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6235 - val_loss: 0.2525\n",
      "Epoch 6/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.4213\n",
      "Epoch 00006: val_loss did not improve from 0.25249\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.4213 - val_loss: 0.2607\n",
      "Epoch 7/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.3901\n",
      "Epoch 00007: val_loss improved from 0.25249 to 0.23855, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3874 - val_loss: 0.2386\n",
      "Epoch 8/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.4139\n",
      "Epoch 00008: val_loss did not improve from 0.23855\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.4120 - val_loss: 0.2431\n",
      "Epoch 9/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.3896\n",
      "Epoch 00009: val_loss improved from 0.23855 to 0.23316, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3889 - val_loss: 0.2332\n",
      "Epoch 10/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 1.3641\n",
      "Epoch 00010: val_loss improved from 0.23316 to 0.22102, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3611 - val_loss: 0.2210\n",
      "Epoch 11/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.3168\n",
      "Epoch 00011: val_loss did not improve from 0.22102\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3203 - val_loss: 0.2236\n",
      "Epoch 12/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.3233\n",
      "Epoch 00012: val_loss improved from 0.22102 to 0.21008, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3215 - val_loss: 0.2101\n",
      "Epoch 13/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.3056\n",
      "Epoch 00013: val_loss did not improve from 0.21008\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3023 - val_loss: 0.2118\n",
      "Epoch 14/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.3103\n",
      "Epoch 00014: val_loss improved from 0.21008 to 0.19537, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3109 - val_loss: 0.1954\n",
      "Epoch 15/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.2637\n",
      "Epoch 00015: val_loss did not improve from 0.19537\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.2628 - val_loss: 0.2220\n",
      "Epoch 16/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.2829\n",
      "Epoch 00016: val_loss improved from 0.19537 to 0.17615, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2788 - val_loss: 0.1761\n",
      "Epoch 17/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.2733\n",
      "Epoch 00017: val_loss did not improve from 0.17615\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.2757 - val_loss: 0.2052\n",
      "Epoch 18/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.2345\n",
      "Epoch 00018: val_loss did not improve from 0.17615\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.2419 - val_loss: 0.2114\n",
      " ###7 fold : val acc1 0.504, acc3 0.933, mae 0.286###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/39 [==========================>...] - ETA: 0s - loss: 11.9349\n",
      "Epoch 00001: val_loss improved from inf to 1.82500, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 1s 8ms/step - loss: 11.4119 - val_loss: 1.8250\n",
      "Epoch 2/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.7882\n",
      "Epoch 00002: val_loss improved from 1.82500 to 0.68846, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.7644 - val_loss: 0.6885\n",
      "Epoch 3/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 2.0945\n",
      "Epoch 00003: val_loss improved from 0.68846 to 0.33713, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.0757 - val_loss: 0.3371\n",
      "Epoch 4/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.6230\n",
      "Epoch 00004: val_loss improved from 0.33713 to 0.27233, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6194 - val_loss: 0.2723\n",
      "Epoch 5/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.6272\n",
      "Epoch 00005: val_loss improved from 0.27233 to 0.24224, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6159 - val_loss: 0.2422\n",
      "Epoch 6/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.4223\n",
      "Epoch 00006: val_loss did not improve from 0.24224\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.4173 - val_loss: 0.2472\n",
      "Epoch 7/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.3969\n",
      "Epoch 00007: val_loss improved from 0.24224 to 0.23029, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3868 - val_loss: 0.2303\n",
      "Epoch 8/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.4265\n",
      "Epoch 00008: val_loss did not improve from 0.23029\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.4145 - val_loss: 0.2355\n",
      "Epoch 9/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.3791\n",
      "Epoch 00009: val_loss improved from 0.23029 to 0.22586, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 1.3897 - val_loss: 0.2259\n",
      "Epoch 10/100\n",
      "30/39 [======================>.......] - ETA: 0s - loss: 1.3540\n",
      "Epoch 00010: val_loss improved from 0.22586 to 0.21255, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 1.3594 - val_loss: 0.2126\n",
      "Epoch 11/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.2948\n",
      "Epoch 00011: val_loss did not improve from 0.21255\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3161 - val_loss: 0.2166\n",
      "Epoch 12/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.3229\n",
      "Epoch 00012: val_loss improved from 0.21255 to 0.20307, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3193 - val_loss: 0.2031\n",
      "Epoch 13/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.3080\n",
      "Epoch 00013: val_loss improved from 0.20307 to 0.20168, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3018 - val_loss: 0.2017\n",
      "Epoch 14/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.3062\n",
      "Epoch 00014: val_loss improved from 0.20168 to 0.18939, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3078 - val_loss: 0.1894\n",
      "Epoch 15/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.2558\n",
      "Epoch 00015: val_loss did not improve from 0.18939\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2614 - val_loss: 0.2159\n",
      "Epoch 16/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.2760\n",
      "Epoch 00016: val_loss improved from 0.18939 to 0.17254, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2759 - val_loss: 0.1725\n",
      "Epoch 17/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.2734\n",
      "Epoch 00017: val_loss did not improve from 0.17254\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2731 - val_loss: 0.1990\n",
      "Epoch 18/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.2356\n",
      "Epoch 00018: val_loss did not improve from 0.17254\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2416 - val_loss: 0.2047\n",
      " ###8 fold : val acc1 0.485, acc3 0.926, mae 0.300###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/39 [============================>.] - ETA: 0s - loss: 11.4789\n",
      "Epoch 00001: val_loss improved from inf to 1.82634, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 11.4119 - val_loss: 1.8263\n",
      "Epoch 2/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 2.8017\n",
      "Epoch 00002: val_loss improved from 1.82634 to 0.68292, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.7644 - val_loss: 0.6829\n",
      "Epoch 3/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 2.0757\n",
      "Epoch 00003: val_loss improved from 0.68292 to 0.33296, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 2.0757 - val_loss: 0.3330\n",
      "Epoch 4/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.6194\n",
      "Epoch 00004: val_loss improved from 0.33296 to 0.26907, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6194 - val_loss: 0.2691\n",
      "Epoch 5/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.6366\n",
      "Epoch 00005: val_loss improved from 0.26907 to 0.24037, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.6159 - val_loss: 0.2404\n",
      "Epoch 6/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 1.4196\n",
      "Epoch 00006: val_loss did not improve from 0.24037\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.4173 - val_loss: 0.2480\n",
      "Epoch 7/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.3969\n",
      "Epoch 00007: val_loss improved from 0.24037 to 0.23044, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3868 - val_loss: 0.2304\n",
      "Epoch 8/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.4227\n",
      "Epoch 00008: val_loss did not improve from 0.23044\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.4145 - val_loss: 0.2369\n",
      "Epoch 9/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.3903\n",
      "Epoch 00009: val_loss improved from 0.23044 to 0.22722, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3897 - val_loss: 0.2272\n",
      "Epoch 10/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.3552\n",
      "Epoch 00010: val_loss improved from 0.22722 to 0.21342, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3594 - val_loss: 0.2134\n",
      "Epoch 11/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 1.3117\n",
      "Epoch 00011: val_loss did not improve from 0.21342\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3161 - val_loss: 0.2185\n",
      "Epoch 12/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.3221\n",
      "Epoch 00012: val_loss improved from 0.21342 to 0.20469, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3193 - val_loss: 0.2047\n",
      "Epoch 13/100\n",
      "34/39 [=========================>....] - ETA: 0s - loss: 1.3050\n",
      "Epoch 00013: val_loss did not improve from 0.20469\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3018 - val_loss: 0.2047\n",
      "Epoch 14/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.3094\n",
      "Epoch 00014: val_loss improved from 0.20469 to 0.19150, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.3078 - val_loss: 0.1915\n",
      "Epoch 15/100\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.2554\n",
      "Epoch 00015: val_loss did not improve from 0.19150\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2614 - val_loss: 0.2197\n",
      "Epoch 16/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.2760\n",
      "Epoch 00016: val_loss improved from 0.19150 to 0.17421, saving model to result/size/DNN_size_both_y/batch512,dnodes512_dropout0.2,dnodes16_dropout0.3,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2759 - val_loss: 0.1742\n",
      "Epoch 17/100\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 1.2734\n",
      "Epoch 00017: val_loss did not improve from 0.17421\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2731 - val_loss: 0.2027\n",
      "Epoch 18/100\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 1.2326\n",
      "Epoch 00018: val_loss did not improve from 0.17421\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 1.2416 - val_loss: 0.2084\n",
      " ###9 fold : val acc1 0.492, acc3 0.933, mae 0.294###\n",
      "acc10.479_acc30.925\n",
      "random search 21/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/154 [==========================>...] - ETA: 0s - loss: 10.4723\n",
      "Epoch 00001: val_loss improved from inf to 1.20313, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 9.8979 - val_loss: 1.2031\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 2.7339\n",
      "Epoch 00002: val_loss improved from 1.20313 to 0.48450, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 2.7339 - val_loss: 0.4845\n",
      "Epoch 3/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 1.9904\n",
      "Epoch 00003: val_loss improved from 0.48450 to 0.38132, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 2.5773 - val_loss: 0.3813\n",
      "Epoch 4/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 2.2510\n",
      "Epoch 00004: val_loss improved from 0.38132 to 0.32528, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 2.2415 - val_loss: 0.3253\n",
      "Epoch 5/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 1.5477\n",
      "Epoch 00005: val_loss improved from 0.32528 to 0.28401, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.5425 - val_loss: 0.2840\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.3885\n",
      "Epoch 00006: val_loss improved from 0.28401 to 0.24446, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.3836 - val_loss: 0.2445\n",
      "Epoch 7/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 1.2332\n",
      "Epoch 00007: val_loss improved from 0.24446 to 0.21493, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.2352 - val_loss: 0.2149\n",
      "Epoch 8/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 1.1121\n",
      "Epoch 00008: val_loss improved from 0.21493 to 0.19630, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.1116 - val_loss: 0.1963\n",
      "Epoch 9/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 1.0479\n",
      "Epoch 00009: val_loss did not improve from 0.19630\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.0495 - val_loss: 0.2002\n",
      "Epoch 10/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.9585\n",
      "Epoch 00010: val_loss did not improve from 0.19630\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.1971 - val_loss: 0.2201\n",
      " ###0 fold : val acc1 0.489, acc3 0.921, mae 0.303###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/154 [===========================>..] - ETA: 0s - loss: 9.9242 \n",
      "Epoch 00001: val_loss improved from inf to 1.20661, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 9.6537 - val_loss: 1.2066\n",
      "Epoch 2/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 2.8140\n",
      "Epoch 00002: val_loss improved from 1.20661 to 0.47917, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 2.7884 - val_loss: 0.4792\n",
      "Epoch 3/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 1.9957\n",
      "Epoch 00003: val_loss improved from 0.47917 to 0.38927, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 2.3508 - val_loss: 0.3893\n",
      "Epoch 4/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 2.1096\n",
      "Epoch 00004: val_loss improved from 0.38927 to 0.33376, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 2.1096 - val_loss: 0.3338\n",
      "Epoch 5/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 1.5717\n",
      "Epoch 00005: val_loss improved from 0.33376 to 0.28466, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.5617 - val_loss: 0.2847\n",
      "Epoch 6/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 1.3957\n",
      "Epoch 00006: val_loss improved from 0.28466 to 0.24953, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.3841 - val_loss: 0.2495\n",
      "Epoch 7/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 1.2813\n",
      "Epoch 00007: val_loss improved from 0.24953 to 0.21261, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.2782 - val_loss: 0.2126\n",
      "Epoch 8/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 1.1299\n",
      "Epoch 00008: val_loss improved from 0.21261 to 0.20401, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.1298 - val_loss: 0.2040\n",
      "Epoch 9/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 1.0511\n",
      "Epoch 00009: val_loss improved from 0.20401 to 0.20018, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.0475 - val_loss: 0.2002\n",
      "Epoch 10/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.9649\n",
      "Epoch 00010: val_loss did not improve from 0.20018\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.2022 - val_loss: 0.2133\n",
      "Epoch 11/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.9122\n",
      "Epoch 00011: val_loss improved from 0.20018 to 0.19058, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.9156 - val_loss: 0.1906\n",
      "Epoch 12/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.8641\n",
      "Epoch 00012: val_loss improved from 0.19058 to 0.18664, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.8629 - val_loss: 0.1866\n",
      "Epoch 13/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.8454\n",
      "Epoch 00013: val_loss improved from 0.18664 to 0.17496, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.8454 - val_loss: 0.1750\n",
      "Epoch 14/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.7973\n",
      "Epoch 00014: val_loss did not improve from 0.17496\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.7914 - val_loss: 0.1799\n",
      "Epoch 15/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.7742\n",
      "Epoch 00015: val_loss improved from 0.17496 to 0.16907, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.7734 - val_loss: 0.1691\n",
      "Epoch 16/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.6843\n",
      "Epoch 00016: val_loss improved from 0.16907 to 0.16759, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6842 - val_loss: 0.1676\n",
      "Epoch 17/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.6446\n",
      "Epoch 00017: val_loss did not improve from 0.16759\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.6467 - val_loss: 0.1679\n",
      "Epoch 18/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.6080\n",
      "Epoch 00018: val_loss improved from 0.16759 to 0.16541, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6105 - val_loss: 0.1654\n",
      "Epoch 19/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.5896\n",
      "Epoch 00019: val_loss improved from 0.16541 to 0.16070, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.5899 - val_loss: 0.1607\n",
      "Epoch 20/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.5386\n",
      "Epoch 00020: val_loss did not improve from 0.16070\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5336 - val_loss: 0.1691\n",
      "Epoch 21/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.5464\n",
      "Epoch 00021: val_loss did not improve from 0.16070\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5406 - val_loss: 0.1676\n",
      " ###1 fold : val acc1 0.536, acc3 0.945, mae 0.264###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/154 [==========================>...] - ETA: 0s - loss: 10.2317\n",
      "Epoch 00001: val_loss improved from inf to 1.21215, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 9.6659 - val_loss: 1.2121\n",
      "Epoch 2/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 2.8464\n",
      "Epoch 00002: val_loss improved from 1.21215 to 0.47742, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 2.7975 - val_loss: 0.4774\n",
      "Epoch 3/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 1.9991\n",
      "Epoch 00003: val_loss improved from 0.47742 to 0.39934, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 2.4757 - val_loss: 0.3993\n",
      "Epoch 4/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 2.1384\n",
      "Epoch 00004: val_loss improved from 0.39934 to 0.32889, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.1114 - val_loss: 0.3289\n",
      "Epoch 5/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 1.5827\n",
      "Epoch 00005: val_loss improved from 0.32889 to 0.28915, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.5741 - val_loss: 0.2891\n",
      "Epoch 6/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 1.3778\n",
      "Epoch 00006: val_loss improved from 0.28915 to 0.25234, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.3766 - val_loss: 0.2523\n",
      "Epoch 7/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 1.2607\n",
      "Epoch 00007: val_loss improved from 0.25234 to 0.20901, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.2585 - val_loss: 0.2090\n",
      "Epoch 8/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.1225\n",
      "Epoch 00008: val_loss did not improve from 0.20901\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.1213 - val_loss: 0.2117\n",
      "Epoch 9/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.0589\n",
      "Epoch 00009: val_loss improved from 0.20901 to 0.20072, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0593 - val_loss: 0.2007\n",
      "Epoch 10/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.9528\n",
      "Epoch 00010: val_loss did not improve from 0.20072\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.2450 - val_loss: 0.2276\n",
      "Epoch 11/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.9078\n",
      "Epoch 00011: val_loss improved from 0.20072 to 0.19081, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.9093 - val_loss: 0.1908\n",
      "Epoch 12/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.8605\n",
      "Epoch 00012: val_loss improved from 0.19081 to 0.18368, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8570 - val_loss: 0.1837\n",
      "Epoch 13/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.8240\n",
      "Epoch 00013: val_loss improved from 0.18368 to 0.17225, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8252 - val_loss: 0.1722\n",
      "Epoch 14/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.8090\n",
      "Epoch 00014: val_loss did not improve from 0.17225\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8017 - val_loss: 0.1810\n",
      "Epoch 15/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.7503\n",
      "Epoch 00015: val_loss improved from 0.17225 to 0.16513, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7488 - val_loss: 0.1651\n",
      "Epoch 16/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.6812\n",
      "Epoch 00016: val_loss improved from 0.16513 to 0.16258, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6822 - val_loss: 0.1626\n",
      "Epoch 17/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.6447\n",
      "Epoch 00017: val_loss did not improve from 0.16258\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6447 - val_loss: 0.1676\n",
      "Epoch 18/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.6121\n",
      "Epoch 00018: val_loss did not improve from 0.16258\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6108 - val_loss: 0.1630\n",
      " ###2 fold : val acc1 0.516, acc3 0.942, mae 0.277###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/154 [============================>.] - ETA: 0s - loss: 9.7865 \n",
      "Epoch 00001: val_loss improved from inf to 1.19254, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 9.7326 - val_loss: 1.1925\n",
      "Epoch 2/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 2.8092\n",
      "Epoch 00002: val_loss improved from 1.19254 to 0.48512, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 2.7984 - val_loss: 0.4851\n",
      "Epoch 3/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 1.9902\n",
      "Epoch 00003: val_loss improved from 0.48512 to 0.40848, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 2.4954 - val_loss: 0.4085\n",
      "Epoch 4/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 2.1654\n",
      "Epoch 00004: val_loss improved from 0.40848 to 0.33146, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 2.1503 - val_loss: 0.3315\n",
      "Epoch 5/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 1.5889\n",
      "Epoch 00005: val_loss improved from 0.33146 to 0.28330, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.5778 - val_loss: 0.2833\n",
      "Epoch 6/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 1.3669\n",
      "Epoch 00006: val_loss improved from 0.28330 to 0.25188, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.3597 - val_loss: 0.2519\n",
      "Epoch 7/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 1.2482\n",
      "Epoch 00007: val_loss improved from 0.25188 to 0.21414, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.2469 - val_loss: 0.2141\n",
      "Epoch 8/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.1254\n",
      "Epoch 00008: val_loss improved from 0.21414 to 0.21125, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.1219 - val_loss: 0.2112\n",
      "Epoch 9/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 1.0536\n",
      "Epoch 00009: val_loss improved from 0.21125 to 0.19864, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.0514 - val_loss: 0.1986\n",
      "Epoch 10/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.9478\n",
      "Epoch 00010: val_loss did not improve from 0.19864\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.2333 - val_loss: 0.2309\n",
      "Epoch 11/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.9079\n",
      "Epoch 00011: val_loss improved from 0.19864 to 0.18700, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.9102 - val_loss: 0.1870\n",
      "Epoch 12/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.8649\n",
      "Epoch 00012: val_loss improved from 0.18700 to 0.18202, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.8616 - val_loss: 0.1820\n",
      "Epoch 13/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.8398\n",
      "Epoch 00013: val_loss improved from 0.18202 to 0.17204, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.8394 - val_loss: 0.1720\n",
      "Epoch 14/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.8100\n",
      "Epoch 00014: val_loss did not improve from 0.17204\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.8039 - val_loss: 0.1819\n",
      "Epoch 15/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.7499\n",
      "Epoch 00015: val_loss improved from 0.17204 to 0.16501, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.7468 - val_loss: 0.1650\n",
      "Epoch 16/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.6896\n",
      "Epoch 00016: val_loss improved from 0.16501 to 0.16366, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6858 - val_loss: 0.1637\n",
      "Epoch 17/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.6466\n",
      "Epoch 00017: val_loss did not improve from 0.16366\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.6456 - val_loss: 0.1680\n",
      "Epoch 18/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.6146\n",
      "Epoch 00018: val_loss improved from 0.16366 to 0.16233, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.6135 - val_loss: 0.1623\n",
      "Epoch 19/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.6110\n",
      "Epoch 00019: val_loss did not improve from 0.16233\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.6147 - val_loss: 0.1648\n",
      "Epoch 20/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.5549\n",
      "Epoch 00020: val_loss did not improve from 0.16233\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5488 - val_loss: 0.1712\n",
      " ###3 fold : val acc1 0.534, acc3 0.947, mae 0.264###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/154 [===========================>..] - ETA: 0s - loss: 9.5807 \n",
      "Epoch 00001: val_loss improved from inf to 1.16739, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 9.2678 - val_loss: 1.1674\n",
      "Epoch 2/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 3.1756\n",
      "Epoch 00002: val_loss improved from 1.16739 to 0.48759, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 3.0985 - val_loss: 0.4876\n",
      "Epoch 3/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 2.1768\n",
      "Epoch 00003: val_loss improved from 0.48759 to 0.36224, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 2.1534 - val_loss: 0.3622\n",
      "Epoch 4/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 1.8210\n",
      "Epoch 00004: val_loss improved from 0.36224 to 0.30687, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.8155 - val_loss: 0.3069\n",
      "Epoch 5/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 1.5782\n",
      "Epoch 00005: val_loss improved from 0.30687 to 0.25587, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.5697 - val_loss: 0.2559\n",
      "Epoch 6/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 1.3651\n",
      "Epoch 00006: val_loss improved from 0.25587 to 0.24945, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.3572 - val_loss: 0.2495\n",
      "Epoch 7/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 1.2544\n",
      "Epoch 00007: val_loss improved from 0.24945 to 0.21692, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.2491 - val_loss: 0.2169\n",
      "Epoch 8/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 1.2950\n",
      "Epoch 00008: val_loss improved from 0.21692 to 0.20750, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.2742 - val_loss: 0.2075\n",
      "Epoch 9/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.0508\n",
      "Epoch 00009: val_loss improved from 0.20750 to 0.20338, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.0483 - val_loss: 0.2034\n",
      "Epoch 10/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 1.0118\n",
      "Epoch 00010: val_loss improved from 0.20338 to 0.20208, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.0096 - val_loss: 0.2021\n",
      "Epoch 11/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.9472\n",
      "Epoch 00011: val_loss improved from 0.20208 to 0.18555, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9473 - val_loss: 0.1856\n",
      "Epoch 12/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.8540\n",
      "Epoch 00012: val_loss improved from 0.18555 to 0.17942, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8544 - val_loss: 0.1794\n",
      "Epoch 13/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.8367\n",
      "Epoch 00013: val_loss improved from 0.17942 to 0.16924, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8379 - val_loss: 0.1692\n",
      "Epoch 14/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 1.0902\n",
      "Epoch 00014: val_loss did not improve from 0.16924\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.0750 - val_loss: 0.1863\n",
      "Epoch 15/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.7219\n",
      "Epoch 00015: val_loss did not improve from 0.16924\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.7237 - val_loss: 0.1728\n",
      " ###4 fold : val acc1 0.531, acc3 0.943, mae 0.268###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/154 [===========================>..] - ETA: 0s - loss: 9.2848 \n",
      "Epoch 00001: val_loss improved from inf to 1.11600, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 8.9807 - val_loss: 1.1160\n",
      "Epoch 2/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 2.5034\n",
      "Epoch 00002: val_loss improved from 1.11600 to 0.44155, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 2.4660 - val_loss: 0.4415\n",
      "Epoch 3/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 1.9849\n",
      "Epoch 00003: val_loss improved from 0.44155 to 0.34863, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.9814 - val_loss: 0.3486\n",
      "Epoch 4/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 1.7514\n",
      "Epoch 00004: val_loss improved from 0.34863 to 0.29254, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.7381 - val_loss: 0.2925\n",
      "Epoch 5/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 1.5824\n",
      "Epoch 00005: val_loss improved from 0.29254 to 0.26431, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.5695 - val_loss: 0.2643\n",
      "Epoch 6/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 1.3412\n",
      "Epoch 00006: val_loss improved from 0.26431 to 0.25348, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.3416 - val_loss: 0.2535\n",
      "Epoch 7/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 1.2390\n",
      "Epoch 00007: val_loss improved from 0.25348 to 0.22004, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.2390 - val_loss: 0.2200\n",
      "Epoch 8/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 1.1355\n",
      "Epoch 00008: val_loss improved from 0.22004 to 0.20616, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.1301 - val_loss: 0.2062\n",
      "Epoch 9/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 1.0289\n",
      "Epoch 00009: val_loss improved from 0.20616 to 0.19559, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 1.0254 - val_loss: 0.1956\n",
      "Epoch 10/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.9432\n",
      "Epoch 00010: val_loss did not improve from 0.19559\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.9429 - val_loss: 0.2013\n",
      "Epoch 11/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.8998\n",
      "Epoch 00011: val_loss improved from 0.19559 to 0.17969, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.8995 - val_loss: 0.1797\n",
      "Epoch 12/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.8474\n",
      "Epoch 00012: val_loss did not improve from 0.17969\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.8470 - val_loss: 0.1799\n",
      "Epoch 13/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.8156\n",
      "Epoch 00013: val_loss improved from 0.17969 to 0.16195, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.8154 - val_loss: 0.1619\n",
      "Epoch 14/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.7606\n",
      "Epoch 00014: val_loss did not improve from 0.16195\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.7585 - val_loss: 0.1867\n",
      "Epoch 15/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.7062\n",
      "Epoch 00015: val_loss did not improve from 0.16195\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.7082 - val_loss: 0.1673\n",
      " ###5 fold : val acc1 0.549, acc3 0.950, mae 0.266###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/154 [==========================>...] - ETA: 0s - loss: 9.5012 \n",
      "Epoch 00001: val_loss improved from inf to 1.14731, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 9.0268 - val_loss: 1.1473\n",
      "Epoch 2/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 2.5301\n",
      "Epoch 00002: val_loss improved from 1.14731 to 0.48062, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.6103 - val_loss: 0.4806\n",
      "Epoch 3/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 2.2152\n",
      "Epoch 00003: val_loss improved from 0.48062 to 0.35834, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 2.1826 - val_loss: 0.3583\n",
      "Epoch 4/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.9875\n",
      "Epoch 00004: val_loss improved from 0.35834 to 0.30577, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.9640 - val_loss: 0.3058\n",
      "Epoch 5/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 1.7369\n",
      "Epoch 00005: val_loss improved from 0.30577 to 0.28225, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.7165 - val_loss: 0.2822\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.4040\n",
      "Epoch 00006: val_loss improved from 0.28225 to 0.26105, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4014 - val_loss: 0.2611\n",
      "Epoch 7/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 1.2543\n",
      "Epoch 00007: val_loss improved from 0.26105 to 0.21810, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.2524 - val_loss: 0.2181\n",
      "Epoch 8/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 1.1338\n",
      "Epoch 00008: val_loss improved from 0.21810 to 0.21029, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1331 - val_loss: 0.2103\n",
      "Epoch 9/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 1.0651\n",
      "Epoch 00009: val_loss improved from 0.21029 to 0.19497, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0608 - val_loss: 0.1950\n",
      "Epoch 10/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 1.1177\n",
      "Epoch 00010: val_loss did not improve from 0.19497\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.1074 - val_loss: 0.1993\n",
      "Epoch 11/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.9526\n",
      "Epoch 00011: val_loss improved from 0.19497 to 0.18528, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9553 - val_loss: 0.1853\n",
      "Epoch 12/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.8590\n",
      "Epoch 00012: val_loss improved from 0.18528 to 0.18368, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.8590 - val_loss: 0.1837\n",
      "Epoch 13/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.8316\n",
      "Epoch 00013: val_loss improved from 0.18368 to 0.16683, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8311 - val_loss: 0.1668\n",
      "Epoch 14/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.8023\n",
      "Epoch 00014: val_loss did not improve from 0.16683\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.7993 - val_loss: 0.1838\n",
      "Epoch 15/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.7431\n",
      "Epoch 00015: val_loss did not improve from 0.16683\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.7439 - val_loss: 0.1765\n",
      " ###6 fold : val acc1 0.541, acc3 0.943, mae 0.264###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/154 [=========================>....] - ETA: 0s - loss: 9.7606 \n",
      "Epoch 00001: val_loss improved from inf to 1.10599, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 9.0455 - val_loss: 1.1060\n",
      "Epoch 2/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 2.6277\n",
      "Epoch 00002: val_loss improved from 1.10599 to 0.48054, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.6029 - val_loss: 0.4805\n",
      "Epoch 3/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 2.2251\n",
      "Epoch 00003: val_loss improved from 0.48054 to 0.35751, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.2129 - val_loss: 0.3575\n",
      "Epoch 4/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 2.0151\n",
      "Epoch 00004: val_loss improved from 0.35751 to 0.31592, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.9901 - val_loss: 0.3159\n",
      "Epoch 5/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 1.7942\n",
      "Epoch 00005: val_loss improved from 0.31592 to 0.28816, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.7764 - val_loss: 0.2882\n",
      "Epoch 6/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 1.4258\n",
      "Epoch 00006: val_loss improved from 0.28816 to 0.26441, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4243 - val_loss: 0.2644\n",
      "Epoch 7/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 1.2688\n",
      "Epoch 00007: val_loss improved from 0.26441 to 0.22208, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.2686 - val_loss: 0.2221\n",
      "Epoch 8/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 1.1407\n",
      "Epoch 00008: val_loss improved from 0.22208 to 0.21307, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1408 - val_loss: 0.2131\n",
      "Epoch 9/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 1.0748\n",
      "Epoch 00009: val_loss improved from 0.21307 to 0.19487, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.0746 - val_loss: 0.1949\n",
      "Epoch 10/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 1.1406\n",
      "Epoch 00010: val_loss did not improve from 0.19487\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.1329 - val_loss: 0.1994\n",
      "Epoch 11/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.9569\n",
      "Epoch 00011: val_loss improved from 0.19487 to 0.18765, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9579 - val_loss: 0.1877\n",
      "Epoch 12/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.8702\n",
      "Epoch 00012: val_loss improved from 0.18765 to 0.18257, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8644 - val_loss: 0.1826\n",
      "Epoch 13/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.8219\n",
      "Epoch 00013: val_loss improved from 0.18257 to 0.16838, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8211 - val_loss: 0.1684\n",
      "Epoch 14/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.8106\n",
      "Epoch 00014: val_loss did not improve from 0.16838\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.8066 - val_loss: 0.1771\n",
      "Epoch 15/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.7381\n",
      "Epoch 00015: val_loss did not improve from 0.16838\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.7371 - val_loss: 0.1763\n",
      " ###7 fold : val acc1 0.550, acc3 0.938, mae 0.261###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - ETA: 0s - loss: 9.0582\n",
      "Epoch 00001: val_loss improved from inf to 1.12934, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 9.0582 - val_loss: 1.1293\n",
      "Epoch 2/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 2.6260\n",
      "Epoch 00002: val_loss improved from 1.12934 to 0.48287, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.6029 - val_loss: 0.4829\n",
      "Epoch 3/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 2.2233\n",
      "Epoch 00003: val_loss improved from 0.48287 to 0.34493, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 2.2089 - val_loss: 0.3449\n",
      "Epoch 4/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.9987\n",
      "Epoch 00004: val_loss improved from 0.34493 to 0.30275, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.9765 - val_loss: 0.3027\n",
      "Epoch 5/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 1.7804\n",
      "Epoch 00005: val_loss improved from 0.30275 to 0.27387, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.7646 - val_loss: 0.2739\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.4261\n",
      "Epoch 00006: val_loss improved from 0.27387 to 0.25501, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4228 - val_loss: 0.2550\n",
      "Epoch 7/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.2747\n",
      "Epoch 00007: val_loss improved from 0.25501 to 0.21566, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.2694 - val_loss: 0.2157\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 1.1395\n",
      "Epoch 00008: val_loss improved from 0.21566 to 0.20554, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.1395 - val_loss: 0.2055\n",
      "Epoch 9/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.0710\n",
      "Epoch 00009: val_loss improved from 0.20554 to 0.18860, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0671 - val_loss: 0.1886\n",
      "Epoch 10/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 1.1431\n",
      "Epoch 00010: val_loss did not improve from 0.18860\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.1308 - val_loss: 0.1908\n",
      "Epoch 11/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.9600\n",
      "Epoch 00011: val_loss improved from 0.18860 to 0.18277, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9622 - val_loss: 0.1828\n",
      "Epoch 12/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.8653\n",
      "Epoch 00012: val_loss improved from 0.18277 to 0.17610, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.8629 - val_loss: 0.1761\n",
      "Epoch 13/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.8228\n",
      "Epoch 00013: val_loss improved from 0.17610 to 0.16429, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8223 - val_loss: 0.1643\n",
      "Epoch 14/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.8097\n",
      "Epoch 00014: val_loss did not improve from 0.16429\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.8063 - val_loss: 0.1720\n",
      "Epoch 15/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.7373\n",
      "Epoch 00015: val_loss did not improve from 0.16429\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.7364 - val_loss: 0.1715\n",
      " ###8 fold : val acc1 0.517, acc3 0.932, mae 0.281###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/154 [===========================>..] - ETA: 0s - loss: 9.3154 \n",
      "Epoch 00001: val_loss improved from inf to 1.13942, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 9.0582 - val_loss: 1.1394\n",
      "Epoch 2/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 2.6260\n",
      "Epoch 00002: val_loss improved from 1.13942 to 0.48796, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 2.6029 - val_loss: 0.4880\n",
      "Epoch 3/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 2.2188\n",
      "Epoch 00003: val_loss improved from 0.48796 to 0.34933, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.2089 - val_loss: 0.3493\n",
      "Epoch 4/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 1.9798\n",
      "Epoch 00004: val_loss improved from 0.34933 to 0.30747, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.9765 - val_loss: 0.3075\n",
      "Epoch 5/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 1.7936\n",
      "Epoch 00005: val_loss improved from 0.30747 to 0.27682, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.7646 - val_loss: 0.2768\n",
      "Epoch 6/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 1.4356\n",
      "Epoch 00006: val_loss improved from 0.27682 to 0.25768, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.4228 - val_loss: 0.2577\n",
      "Epoch 7/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 1.2739\n",
      "Epoch 00007: val_loss improved from 0.25768 to 0.21736, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.2694 - val_loss: 0.2174\n",
      "Epoch 8/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 1.1396\n",
      "Epoch 00008: val_loss improved from 0.21736 to 0.20762, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.1395 - val_loss: 0.2076\n",
      "Epoch 9/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 1.0669\n",
      "Epoch 00009: val_loss improved from 0.20762 to 0.19075, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.0671 - val_loss: 0.1907\n",
      "Epoch 10/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 1.1314\n",
      "Epoch 00010: val_loss did not improve from 0.19075\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 1.1308 - val_loss: 0.1929\n",
      "Epoch 11/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.9623\n",
      "Epoch 00011: val_loss improved from 0.19075 to 0.18377, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.9622 - val_loss: 0.1838\n",
      "Epoch 12/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.8669\n",
      "Epoch 00012: val_loss improved from 0.18377 to 0.17644, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.8629 - val_loss: 0.1764\n",
      "Epoch 13/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.8235\n",
      "Epoch 00013: val_loss improved from 0.17644 to 0.16364, saving model to result/size/DNN_size_both_y/batch128,dnodes32_dropout0.5,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.8223 - val_loss: 0.1636\n",
      "Epoch 14/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.8121\n",
      "Epoch 00014: val_loss did not improve from 0.16364\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.8063 - val_loss: 0.1737\n",
      "Epoch 15/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.7366\n",
      "Epoch 00015: val_loss did not improve from 0.16364\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.7364 - val_loss: 0.1726\n",
      " ###9 fold : val acc1 0.520, acc3 0.938, mae 0.278###\n",
      "acc10.528_acc30.940\n",
      "random search 22/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599/613 [============================>.] - ETA: 0s - loss: 2.1878\n",
      "Epoch 00001: val_loss improved from inf to 0.18794, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.1462 - val_loss: 0.1879\n",
      "Epoch 2/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.4006\n",
      "Epoch 00002: val_loss improved from 0.18794 to 0.14836, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.3990 - val_loss: 0.1484\n",
      "Epoch 3/100\n",
      "597/613 [============================>.] - ETA: 0s - loss: 0.2574\n",
      "Epoch 00003: val_loss improved from 0.14836 to 0.14420, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2589 - val_loss: 0.1442\n",
      "Epoch 4/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.2643\n",
      "Epoch 00004: val_loss improved from 0.14420 to 0.13875, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2637 - val_loss: 0.1388\n",
      "Epoch 5/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.2343\n",
      "Epoch 00005: val_loss improved from 0.13875 to 0.13105, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2341 - val_loss: 0.1310\n",
      "Epoch 6/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.2270\n",
      "Epoch 00006: val_loss did not improve from 0.13105\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2270 - val_loss: 0.1320\n",
      "Epoch 7/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.2197\n",
      "Epoch 00007: val_loss did not improve from 0.13105\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2193 - val_loss: 0.1346\n",
      " ###0 fold : val acc1 0.581, acc3 0.960, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610/613 [============================>.] - ETA: 0s - loss: 2.1542\n",
      "Epoch 00001: val_loss improved from inf to 0.18700, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_1.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 2.1459 - val_loss: 0.1870\n",
      "Epoch 2/100\n",
      "597/613 [============================>.] - ETA: 0s - loss: 0.4138\n",
      "Epoch 00002: val_loss improved from 0.18700 to 0.14392, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.4105 - val_loss: 0.1439\n",
      "Epoch 3/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.2568\n",
      "Epoch 00003: val_loss improved from 0.14392 to 0.14385, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2574 - val_loss: 0.1438\n",
      "Epoch 4/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.2690\n",
      "Epoch 00004: val_loss improved from 0.14385 to 0.13659, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2688 - val_loss: 0.1366\n",
      "Epoch 5/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.2344\n",
      "Epoch 00005: val_loss improved from 0.13659 to 0.13056, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2343 - val_loss: 0.1306\n",
      "Epoch 6/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.2248\n",
      "Epoch 00006: val_loss did not improve from 0.13056\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2249 - val_loss: 0.1309\n",
      "Epoch 7/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.2320\n",
      "Epoch 00007: val_loss did not improve from 0.13056\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2314 - val_loss: 0.1379\n",
      " ###1 fold : val acc1 0.581, acc3 0.959, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595/613 [============================>.] - ETA: 0s - loss: 2.2116\n",
      "Epoch 00001: val_loss improved from inf to 0.19202, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_2.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.1566 - val_loss: 0.1920\n",
      "Epoch 2/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.4095\n",
      "Epoch 00002: val_loss improved from 0.19202 to 0.14562, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_2.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.4069 - val_loss: 0.1456\n",
      "Epoch 3/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.2580\n",
      "Epoch 00003: val_loss did not improve from 0.14562\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2595 - val_loss: 0.1521\n",
      "Epoch 4/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.2692\n",
      "Epoch 00004: val_loss improved from 0.14562 to 0.13751, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_2.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2686 - val_loss: 0.1375\n",
      "Epoch 5/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.2350\n",
      "Epoch 00005: val_loss improved from 0.13751 to 0.13135, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_2.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2349 - val_loss: 0.1314\n",
      "Epoch 6/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.2238\n",
      "Epoch 00006: val_loss improved from 0.13135 to 0.12953, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_2.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2237 - val_loss: 0.1295\n",
      "Epoch 7/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.2511\n",
      "Epoch 00007: val_loss did not improve from 0.12953\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2506 - val_loss: 0.1396\n",
      "Epoch 8/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.2301\n",
      "Epoch 00008: val_loss did not improve from 0.12953\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2295 - val_loss: 0.1451\n",
      " ###2 fold : val acc1 0.592, acc3 0.963, mae 0.226###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606/613 [============================>.] - ETA: 0s - loss: 2.1904\n",
      "Epoch 00001: val_loss improved from inf to 0.18915, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.1703 - val_loss: 0.1891\n",
      "Epoch 2/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.4161\n",
      "Epoch 00002: val_loss improved from 0.18915 to 0.14426, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.4131 - val_loss: 0.1443\n",
      "Epoch 3/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.2570\n",
      "Epoch 00003: val_loss did not improve from 0.14426\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2582 - val_loss: 0.1544\n",
      "Epoch 4/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.2688\n",
      "Epoch 00004: val_loss improved from 0.14426 to 0.13908, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2688 - val_loss: 0.1391\n",
      "Epoch 5/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.2365\n",
      "Epoch 00005: val_loss improved from 0.13908 to 0.13228, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2358 - val_loss: 0.1323\n",
      "Epoch 6/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.2254\n",
      "Epoch 00006: val_loss improved from 0.13228 to 0.13081, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2254 - val_loss: 0.1308\n",
      "Epoch 7/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.2671\n",
      "Epoch 00007: val_loss did not improve from 0.13081\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2660 - val_loss: 0.1385\n",
      "Epoch 8/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.2287\n",
      "Epoch 00008: val_loss did not improve from 0.13081\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2289 - val_loss: 0.1400\n",
      " ###3 fold : val acc1 0.589, acc3 0.963, mae 0.227###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610/613 [============================>.] - ETA: 0s - loss: 2.1562\n",
      "Epoch 00001: val_loss improved from inf to 0.18196, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.1479 - val_loss: 0.1820\n",
      "Epoch 2/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.4126\n",
      "Epoch 00002: val_loss improved from 0.18196 to 0.14752, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.4103 - val_loss: 0.1475\n",
      "Epoch 3/100\n",
      "594/613 [============================>.] - ETA: 0s - loss: 0.2773\n",
      "Epoch 00003: val_loss improved from 0.14752 to 0.13530, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2759 - val_loss: 0.1353\n",
      "Epoch 4/100\n",
      "595/613 [============================>.] - ETA: 0s - loss: 0.2452\n",
      "Epoch 00004: val_loss improved from 0.13530 to 0.13091, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2445 - val_loss: 0.1309\n",
      "Epoch 5/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.2363\n",
      "Epoch 00005: val_loss did not improve from 0.13091\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2367 - val_loss: 0.1324\n",
      "Epoch 6/100\n",
      "594/613 [============================>.] - ETA: 0s - loss: 0.2425\n",
      "Epoch 00006: val_loss did not improve from 0.13091\n",
      "613/613 [==============================] - 2s 2ms/step - loss: 0.2416 - val_loss: 0.1342\n",
      " ###4 fold : val acc1 0.591, acc3 0.964, mae 0.227###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 2.0851\n",
      "Epoch 00001: val_loss improved from inf to 0.18149, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_5.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 2.0508 - val_loss: 0.1815\n",
      "Epoch 2/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.2821\n",
      "Epoch 00002: val_loss improved from 0.18149 to 0.14465, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2822 - val_loss: 0.1447\n",
      "Epoch 3/100\n",
      "608/613 [============================>.] - ETA: 0s - loss: 0.2569\n",
      "Epoch 00003: val_loss improved from 0.14465 to 0.13526, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 4ms/step - loss: 0.2564 - val_loss: 0.1353\n",
      "Epoch 4/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.2427\n",
      "Epoch 00004: val_loss improved from 0.13526 to 0.13333, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2424 - val_loss: 0.1333\n",
      "Epoch 5/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.2317\n",
      "Epoch 00005: val_loss improved from 0.13333 to 0.13123, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2320 - val_loss: 0.1312\n",
      "Epoch 6/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.2193\n",
      "Epoch 00006: val_loss did not improve from 0.13123\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2194 - val_loss: 0.1328\n",
      "Epoch 7/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.2167\n",
      "Epoch 00007: val_loss did not improve from 0.13123\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2162 - val_loss: 0.1348\n",
      " ###5 fold : val acc1 0.591, acc3 0.966, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611/613 [============================>.] - ETA: 0s - loss: 2.4221\n",
      "Epoch 00001: val_loss improved from inf to 0.18563, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.4161 - val_loss: 0.1856\n",
      "Epoch 2/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.4700\n",
      "Epoch 00002: val_loss improved from 0.18563 to 0.15088, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.4685 - val_loss: 0.1509\n",
      "Epoch 3/100\n",
      "608/613 [============================>.] - ETA: 0s - loss: 0.2607\n",
      "Epoch 00003: val_loss improved from 0.15088 to 0.13479, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2603 - val_loss: 0.1348\n",
      "Epoch 4/100\n",
      "595/613 [============================>.] - ETA: 0s - loss: 0.2674\n",
      "Epoch 00004: val_loss improved from 0.13479 to 0.13110, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2656 - val_loss: 0.1311\n",
      "Epoch 5/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.2399\n",
      "Epoch 00005: val_loss did not improve from 0.13110\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2401 - val_loss: 0.1327\n",
      "Epoch 6/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.2230\n",
      "Epoch 00006: val_loss did not improve from 0.13110\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2230 - val_loss: 0.1322\n",
      " ###6 fold : val acc1 0.571, acc3 0.960, mae 0.239###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597/613 [============================>.] - ETA: 0s - loss: 2.4501\n",
      "Epoch 00001: val_loss improved from inf to 0.18210, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.3948 - val_loss: 0.1821\n",
      "Epoch 2/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.5754\n",
      "Epoch 00002: val_loss improved from 0.18210 to 0.14919, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.5714 - val_loss: 0.1492\n",
      "Epoch 3/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.2810\n",
      "Epoch 00003: val_loss improved from 0.14919 to 0.13629, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2797 - val_loss: 0.1363\n",
      "Epoch 4/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.2739\n",
      "Epoch 00004: val_loss improved from 0.13629 to 0.12988, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2738 - val_loss: 0.1299\n",
      "Epoch 5/100\n",
      "608/613 [============================>.] - ETA: 0s - loss: 0.2375\n",
      "Epoch 00005: val_loss did not improve from 0.12988\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2375 - val_loss: 0.1330\n",
      "Epoch 6/100\n",
      "597/613 [============================>.] - ETA: 0s - loss: 0.2206\n",
      "Epoch 00006: val_loss did not improve from 0.12988\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2209 - val_loss: 0.1320\n",
      " ###7 fold : val acc1 0.585, acc3 0.964, mae 0.229###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605/613 [============================>.] - ETA: 0s - loss: 2.4356\n",
      "Epoch 00001: val_loss improved from inf to 0.18080, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.4084 - val_loss: 0.1808\n",
      "Epoch 2/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.5871\n",
      "Epoch 00002: val_loss improved from 0.18080 to 0.14703, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.5827 - val_loss: 0.1470\n",
      "Epoch 3/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.2766\n",
      "Epoch 00003: val_loss improved from 0.14703 to 0.13437, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2762 - val_loss: 0.1344\n",
      "Epoch 4/100\n",
      "596/613 [============================>.] - ETA: 0s - loss: 0.2727\n",
      "Epoch 00004: val_loss improved from 0.13437 to 0.12820, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2711 - val_loss: 0.1282\n",
      "Epoch 5/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.2382\n",
      "Epoch 00005: val_loss did not improve from 0.12820\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2376 - val_loss: 0.1303\n",
      "Epoch 6/100\n",
      "608/613 [============================>.] - ETA: 0s - loss: 0.2193\n",
      "Epoch 00006: val_loss did not improve from 0.12820\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2194 - val_loss: 0.1290\n",
      " ###8 fold : val acc1 0.576, acc3 0.955, mae 0.239###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601/613 [============================>.] - ETA: 0s - loss: 2.4499\n",
      "Epoch 00001: val_loss improved from inf to 0.17632, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.4084 - val_loss: 0.1763\n",
      "Epoch 2/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.5881\n",
      "Epoch 00002: val_loss improved from 0.17632 to 0.14510, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.5827 - val_loss: 0.1451\n",
      "Epoch 3/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.2774\n",
      "Epoch 00003: val_loss improved from 0.14510 to 0.13425, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2762 - val_loss: 0.1343\n",
      "Epoch 4/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.2722\n",
      "Epoch 00004: val_loss improved from 0.13425 to 0.12823, saving model to result/size/DNN_size_both_y/batch32,dnodes256_dropout0.3,lr0.001/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2711 - val_loss: 0.1282\n",
      "Epoch 5/100\n",
      "603/613 [============================>.] - ETA: 0s - loss: 0.2380\n",
      "Epoch 00005: val_loss did not improve from 0.12823\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2376 - val_loss: 0.1317\n",
      "Epoch 6/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.2193\n",
      "Epoch 00006: val_loss did not improve from 0.12823\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2194 - val_loss: 0.1295\n",
      " ###9 fold : val acc1 0.582, acc3 0.962, mae 0.232###\n",
      "acc10.584_acc30.962\n",
      "random search 23/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307/307 [==============================] - ETA: 0s - loss: 2.8016\n",
      "Epoch 00001: val_loss improved from inf to 0.21885, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.8016 - val_loss: 0.2189\n",
      "Epoch 2/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 1.1388\n",
      "Epoch 00002: val_loss improved from 0.21885 to 0.16061, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 1.1386 - val_loss: 0.1606\n",
      "Epoch 3/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.8519\n",
      "Epoch 00003: val_loss did not improve from 0.16061\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8778 - val_loss: 0.2824\n",
      "Epoch 4/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.8426\n",
      "Epoch 00004: val_loss improved from 0.16061 to 0.15107, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8415 - val_loss: 0.1511\n",
      "Epoch 5/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.7972\n",
      "Epoch 00005: val_loss did not improve from 0.15107\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7950 - val_loss: 0.1514\n",
      "Epoch 6/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.7868\n",
      "Epoch 00006: val_loss did not improve from 0.15107\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7850 - val_loss: 0.1702\n",
      " ###0 fold : val acc1 0.547, acc3 0.945, mae 0.260###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/307 [===========================>..] - ETA: 0s - loss: 2.8591\n",
      "Epoch 00001: val_loss improved from inf to 0.22517, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.7929 - val_loss: 0.2252\n",
      "Epoch 2/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 1.1241\n",
      "Epoch 00002: val_loss improved from 0.22517 to 0.16073, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.1236 - val_loss: 0.1607\n",
      "Epoch 3/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.8762\n",
      "Epoch 00003: val_loss did not improve from 0.16073\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8762 - val_loss: 0.2653\n",
      "Epoch 4/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.8432\n",
      "Epoch 00004: val_loss improved from 0.16073 to 0.14583, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8417 - val_loss: 0.1458\n",
      "Epoch 5/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.8007\n",
      "Epoch 00005: val_loss did not improve from 0.14583\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7995 - val_loss: 0.1572\n",
      "Epoch 6/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.7861\n",
      "Epoch 00006: val_loss did not improve from 0.14583\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7844 - val_loss: 0.1641\n",
      " ###1 fold : val acc1 0.560, acc3 0.953, mae 0.247###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302/307 [============================>.] - ETA: 0s - loss: 2.8381\n",
      "Epoch 00001: val_loss improved from inf to 0.22572, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.8125 - val_loss: 0.2257\n",
      "Epoch 2/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 1.1241\n",
      "Epoch 00002: val_loss improved from 0.22572 to 0.15830, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.1220 - val_loss: 0.1583\n",
      "Epoch 3/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.8746\n",
      "Epoch 00003: val_loss did not improve from 0.15830\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8742 - val_loss: 0.2522\n",
      "Epoch 4/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.8412\n",
      "Epoch 00004: val_loss improved from 0.15830 to 0.14474, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8399 - val_loss: 0.1447\n",
      "Epoch 5/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.8033\n",
      "Epoch 00005: val_loss did not improve from 0.14474\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8020 - val_loss: 0.1526\n",
      "Epoch 6/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.7822\n",
      "Epoch 00006: val_loss did not improve from 0.14474\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7802 - val_loss: 0.1576\n",
      " ###2 fold : val acc1 0.563, acc3 0.956, mae 0.245###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/307 [===========================>..] - ETA: 0s - loss: 2.9269\n",
      "Epoch 00001: val_loss improved from inf to 0.21984, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.8335 - val_loss: 0.2198\n",
      "Epoch 2/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 1.1503\n",
      "Epoch 00002: val_loss improved from 0.21984 to 0.16198, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.1498 - val_loss: 0.1620\n",
      "Epoch 3/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.8496\n",
      "Epoch 00003: val_loss did not improve from 0.16198\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8778 - val_loss: 0.2462\n",
      "Epoch 4/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.8420\n",
      "Epoch 00004: val_loss improved from 0.16198 to 0.14699, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8421 - val_loss: 0.1470\n",
      "Epoch 5/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.8006\n",
      "Epoch 00005: val_loss did not improve from 0.14699\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7994 - val_loss: 0.1541\n",
      "Epoch 6/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.7808\n",
      "Epoch 00006: val_loss did not improve from 0.14699\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7809 - val_loss: 0.1569\n",
      " ###3 fold : val acc1 0.556, acc3 0.958, mae 0.247###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/307 [============================>.] - ETA: 0s - loss: 2.8468\n",
      "Epoch 00001: val_loss improved from inf to 0.20424, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.8238 - val_loss: 0.2042\n",
      "Epoch 2/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 1.0953\n",
      "Epoch 00002: val_loss improved from 0.20424 to 0.17576, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.0833 - val_loss: 0.1758\n",
      "Epoch 3/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.9127\n",
      "Epoch 00003: val_loss improved from 0.17576 to 0.16998, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.9070 - val_loss: 0.1700\n",
      "Epoch 4/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.8521\n",
      "Epoch 00004: val_loss improved from 0.16998 to 0.14436, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8498 - val_loss: 0.1444\n",
      "Epoch 5/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.7919\n",
      "Epoch 00005: val_loss improved from 0.14436 to 0.14336, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7921 - val_loss: 0.1434\n",
      "Epoch 6/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.7756\n",
      "Epoch 00006: val_loss did not improve from 0.14336\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7746 - val_loss: 0.1579\n",
      "Epoch 7/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.7240\n",
      "Epoch 00007: val_loss did not improve from 0.14336\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7239 - val_loss: 0.1663\n",
      " ###4 fold : val acc1 0.564, acc3 0.954, mae 0.244###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/307 [============================>.] - ETA: 0s - loss: 2.5965\n",
      "Epoch 00001: val_loss improved from inf to 0.20976, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.5534 - val_loss: 0.2098\n",
      "Epoch 2/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.8796\n",
      "Epoch 00002: val_loss improved from 0.20976 to 0.16230, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8771 - val_loss: 0.1623\n",
      "Epoch 3/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.8476\n",
      "Epoch 00003: val_loss did not improve from 0.16230\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8455 - val_loss: 0.1840\n",
      "Epoch 4/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.8169\n",
      "Epoch 00004: val_loss improved from 0.16230 to 0.15335, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8156 - val_loss: 0.1533\n",
      "Epoch 5/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.7890\n",
      "Epoch 00005: val_loss improved from 0.15335 to 0.15073, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7891 - val_loss: 0.1507\n",
      "Epoch 6/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.7714\n",
      "Epoch 00006: val_loss did not improve from 0.15073\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7698 - val_loss: 0.1611\n",
      "Epoch 7/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.7210\n",
      "Epoch 00007: val_loss did not improve from 0.15073\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7206 - val_loss: 0.2086\n",
      " ###5 fold : val acc1 0.551, acc3 0.955, mae 0.259###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/307 [============================>.] - ETA: 0s - loss: 3.5263\n",
      "Epoch 00001: val_loss improved from inf to 0.21968, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.5055 - val_loss: 0.2197\n",
      "Epoch 2/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 1.1814\n",
      "Epoch 00002: val_loss improved from 0.21968 to 0.20943, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.1741 - val_loss: 0.2094\n",
      "Epoch 3/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.8987\n",
      "Epoch 00003: val_loss improved from 0.20943 to 0.17387, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8965 - val_loss: 0.1739\n",
      "Epoch 4/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.8363\n",
      "Epoch 00004: val_loss improved from 0.17387 to 0.15662, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8366 - val_loss: 0.1566\n",
      "Epoch 5/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.8056\n",
      "Epoch 00005: val_loss improved from 0.15662 to 0.14617, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8056 - val_loss: 0.1462\n",
      "Epoch 6/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.7726\n",
      "Epoch 00006: val_loss did not improve from 0.14617\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7723 - val_loss: 0.1562\n",
      "Epoch 7/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.7254\n",
      "Epoch 00007: val_loss did not improve from 0.14617\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7253 - val_loss: 0.1807\n",
      " ###6 fold : val acc1 0.553, acc3 0.950, mae 0.253###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298/307 [============================>.] - ETA: 0s - loss: 3.5090\n",
      "Epoch 00001: val_loss improved from inf to 0.22593, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.4353 - val_loss: 0.2259\n",
      "Epoch 2/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 1.2727\n",
      "Epoch 00002: val_loss did not improve from 0.22593\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.2685 - val_loss: 0.2432\n",
      "Epoch 3/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.9228\n",
      "Epoch 00003: val_loss improved from 0.22593 to 0.17656, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.9215 - val_loss: 0.1766\n",
      "Epoch 4/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.8514\n",
      "Epoch 00004: val_loss improved from 0.17656 to 0.16174, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8518 - val_loss: 0.1617\n",
      "Epoch 5/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.8005\n",
      "Epoch 00005: val_loss improved from 0.16174 to 0.14911, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8010 - val_loss: 0.1491\n",
      "Epoch 6/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.7686\n",
      "Epoch 00006: val_loss did not improve from 0.14911\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7679 - val_loss: 0.1524\n",
      "Epoch 7/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.7296\n",
      "Epoch 00007: val_loss did not improve from 0.14911\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7275 - val_loss: 0.1775\n",
      " ###7 fold : val acc1 0.552, acc3 0.953, mae 0.251###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/307 [===========================>..] - ETA: 0s - loss: 3.5801\n",
      "Epoch 00001: val_loss improved from inf to 0.23259, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.4574 - val_loss: 0.2326\n",
      "Epoch 2/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 1.2676\n",
      "Epoch 00002: val_loss did not improve from 0.23259\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 1.2632 - val_loss: 0.2329\n",
      "Epoch 3/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.9176\n",
      "Epoch 00003: val_loss improved from 0.23259 to 0.17222, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.9158 - val_loss: 0.1722\n",
      "Epoch 4/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.8500\n",
      "Epoch 00004: val_loss improved from 0.17222 to 0.15743, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8500 - val_loss: 0.1574\n",
      "Epoch 5/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.8064\n",
      "Epoch 00005: val_loss improved from 0.15743 to 0.14582, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8055 - val_loss: 0.1458\n",
      "Epoch 6/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.7673\n",
      "Epoch 00006: val_loss did not improve from 0.14582\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7673 - val_loss: 0.1506\n",
      "Epoch 7/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.7282\n",
      "Epoch 00007: val_loss did not improve from 0.14582\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7277 - val_loss: 0.1749\n",
      " ###8 fold : val acc1 0.535, acc3 0.949, mae 0.262###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/307 [===========================>..] - ETA: 0s - loss: 3.5801\n",
      "Epoch 00001: val_loss improved from inf to 0.22607, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 3.4574 - val_loss: 0.2261\n",
      "Epoch 2/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 1.2672\n",
      "Epoch 00002: val_loss did not improve from 0.22607\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 1.2632 - val_loss: 0.2334\n",
      "Epoch 3/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.9171\n",
      "Epoch 00003: val_loss improved from 0.22607 to 0.17119, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.9158 - val_loss: 0.1712\n",
      "Epoch 4/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.8501\n",
      "Epoch 00004: val_loss improved from 0.17119 to 0.16039, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8500 - val_loss: 0.1604\n",
      "Epoch 5/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.8055\n",
      "Epoch 00005: val_loss improved from 0.16039 to 0.14661, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.2,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8055 - val_loss: 0.1466\n",
      "Epoch 6/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.7672\n",
      "Epoch 00006: val_loss did not improve from 0.14661\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7673 - val_loss: 0.1521\n",
      "Epoch 7/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.7301\n",
      "Epoch 00007: val_loss did not improve from 0.14661\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7277 - val_loss: 0.1768\n",
      " ###9 fold : val acc1 0.541, acc3 0.954, mae 0.257###\n",
      "acc10.552_acc30.953\n",
      "random search 24/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/307 [============================>.] - ETA: 0s - loss: 1.9882\n",
      "Epoch 00001: val_loss improved from inf to 0.19250, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 1.9810 - val_loss: 0.1925\n",
      "Epoch 2/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.6013\n",
      "Epoch 00002: val_loss improved from 0.19250 to 0.15635, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5974 - val_loss: 0.1563\n",
      "Epoch 3/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.3942\n",
      "Epoch 00003: val_loss did not improve from 0.15635\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3938 - val_loss: 0.1813\n",
      "Epoch 4/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.3355\n",
      "Epoch 00004: val_loss improved from 0.15635 to 0.14495, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3348 - val_loss: 0.1450\n",
      "Epoch 5/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.3384\n",
      "Epoch 00005: val_loss improved from 0.14495 to 0.14081, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3370 - val_loss: 0.1408\n",
      "Epoch 6/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.3118\n",
      "Epoch 00006: val_loss improved from 0.14081 to 0.13815, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3118 - val_loss: 0.1381\n",
      "Epoch 7/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.2789\n",
      "Epoch 00007: val_loss did not improve from 0.13815\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2789 - val_loss: 0.1416\n",
      "Epoch 8/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.2667\n",
      "Epoch 00008: val_loss did not improve from 0.13815\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2671 - val_loss: 0.1539\n",
      " ###0 fold : val acc1 0.561, acc3 0.959, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/307 [============================>.] - ETA: 0s - loss: 1.9437\n",
      "Epoch 00001: val_loss improved from inf to 0.19042, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 1.9366 - val_loss: 0.1904\n",
      "Epoch 2/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.5630\n",
      "Epoch 00002: val_loss improved from 0.19042 to 0.15635, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5621 - val_loss: 0.1563\n",
      "Epoch 3/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.3732\n",
      "Epoch 00003: val_loss did not improve from 0.15635\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3730 - val_loss: 0.1906\n",
      "Epoch 4/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.3359\n",
      "Epoch 00004: val_loss improved from 0.15635 to 0.14322, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3350 - val_loss: 0.1432\n",
      "Epoch 5/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.3415\n",
      "Epoch 00005: val_loss improved from 0.14322 to 0.13810, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3412 - val_loss: 0.1381\n",
      "Epoch 6/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.3170\n",
      "Epoch 00006: val_loss improved from 0.13810 to 0.13705, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3169 - val_loss: 0.1371\n",
      "Epoch 7/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.2782\n",
      "Epoch 00007: val_loss did not improve from 0.13705\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2787 - val_loss: 0.1429\n",
      "Epoch 8/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.2629\n",
      "Epoch 00008: val_loss did not improve from 0.13705\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2626 - val_loss: 0.1443\n",
      " ###1 fold : val acc1 0.582, acc3 0.961, mae 0.232###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/307 [===========================>..] - ETA: 0s - loss: 1.9639\n",
      "Epoch 00001: val_loss improved from inf to 0.19016, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 1.9130 - val_loss: 0.1902\n",
      "Epoch 2/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.5702\n",
      "Epoch 00002: val_loss improved from 0.19016 to 0.15777, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5605 - val_loss: 0.1578\n",
      "Epoch 3/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.3880\n",
      "Epoch 00003: val_loss did not improve from 0.15777\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3880 - val_loss: 0.1900\n",
      "Epoch 4/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.3539\n",
      "Epoch 00004: val_loss improved from 0.15777 to 0.14188, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3519 - val_loss: 0.1419\n",
      "Epoch 5/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.3520\n",
      "Epoch 00005: val_loss improved from 0.14188 to 0.13764, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3513 - val_loss: 0.1376\n",
      "Epoch 6/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.3184\n",
      "Epoch 00006: val_loss improved from 0.13764 to 0.13702, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3180 - val_loss: 0.1370\n",
      "Epoch 7/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.2901\n",
      "Epoch 00007: val_loss did not improve from 0.13702\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2901 - val_loss: 0.1482\n",
      "Epoch 8/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.2640\n",
      "Epoch 00008: val_loss did not improve from 0.13702\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2640 - val_loss: 0.1432\n",
      " ###2 fold : val acc1 0.581, acc3 0.958, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/307 [===========================>..] - ETA: 0s - loss: 1.9764\n",
      "Epoch 00001: val_loss improved from inf to 0.19186, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 1.9244 - val_loss: 0.1919\n",
      "Epoch 2/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.6127\n",
      "Epoch 00002: val_loss improved from 0.19186 to 0.16156, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6015 - val_loss: 0.1616\n",
      "Epoch 3/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.3732\n",
      "Epoch 00003: val_loss did not improve from 0.16156\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3726 - val_loss: 0.1992\n",
      "Epoch 4/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.3479\n",
      "Epoch 00004: val_loss improved from 0.16156 to 0.14252, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3456 - val_loss: 0.1425\n",
      "Epoch 5/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.3452\n",
      "Epoch 00005: val_loss improved from 0.14252 to 0.14139, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3435 - val_loss: 0.1414\n",
      "Epoch 6/100\n",
      "292/307 [===========================>..] - ETA: 0s - loss: 0.3238\n",
      "Epoch 00006: val_loss improved from 0.14139 to 0.13669, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3226 - val_loss: 0.1367\n",
      "Epoch 7/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.2812\n",
      "Epoch 00007: val_loss did not improve from 0.13669\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2822 - val_loss: 0.1467\n",
      "Epoch 8/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.2620\n",
      "Epoch 00008: val_loss improved from 0.13669 to 0.13446, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2616 - val_loss: 0.1345\n",
      "Epoch 9/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.2898\n",
      "Epoch 00009: val_loss improved from 0.13446 to 0.13258, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2898 - val_loss: 0.1326\n",
      "Epoch 10/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.2564\n",
      "Epoch 00010: val_loss did not improve from 0.13258\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2569 - val_loss: 0.1339\n",
      "Epoch 11/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.2649\n",
      "Epoch 00011: val_loss improved from 0.13258 to 0.13040, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2642 - val_loss: 0.1304\n",
      "Epoch 12/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.3382\n",
      "Epoch 00012: val_loss did not improve from 0.13040\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3363 - val_loss: 0.1326\n",
      "Epoch 13/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.2448\n",
      "Epoch 00013: val_loss did not improve from 0.13040\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2442 - val_loss: 0.1387\n",
      " ###3 fold : val acc1 0.577, acc3 0.965, mae 0.232###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/307 [===========================>..] - ETA: 0s - loss: 1.9515\n",
      "Epoch 00001: val_loss improved from inf to 0.18898, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 1.8960 - val_loss: 0.1890\n",
      "Epoch 2/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.5139\n",
      "Epoch 00002: val_loss improved from 0.18898 to 0.16348, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5065 - val_loss: 0.1635\n",
      "Epoch 3/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.4786\n",
      "Epoch 00003: val_loss improved from 0.16348 to 0.14274, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4743 - val_loss: 0.1427\n",
      "Epoch 4/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.3647\n",
      "Epoch 00004: val_loss improved from 0.14274 to 0.14273, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3646 - val_loss: 0.1427\n",
      "Epoch 5/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.3850\n",
      "Epoch 00005: val_loss improved from 0.14273 to 0.14092, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3839 - val_loss: 0.1409\n",
      "Epoch 6/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.2980\n",
      "Epoch 00006: val_loss improved from 0.14092 to 0.13502, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2980 - val_loss: 0.1350\n",
      "Epoch 7/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.2777\n",
      "Epoch 00007: val_loss did not improve from 0.13502\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2775 - val_loss: 0.1464\n",
      "Epoch 8/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.2783\n",
      "Epoch 00008: val_loss did not improve from 0.13502\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2783 - val_loss: 0.1356\n",
      " ###4 fold : val acc1 0.584, acc3 0.960, mae 0.232###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/307 [============================>.] - ETA: 0s - loss: 1.1787\n",
      "Epoch 00001: val_loss improved from inf to 0.19419, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 1.1778 - val_loss: 0.1942\n",
      "Epoch 2/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.3298\n",
      "Epoch 00002: val_loss improved from 0.19419 to 0.15927, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3292 - val_loss: 0.1593\n",
      "Epoch 3/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.3014\n",
      "Epoch 00003: val_loss did not improve from 0.15927\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3015 - val_loss: 0.1597\n",
      "Epoch 4/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.2886\n",
      "Epoch 00004: val_loss improved from 0.15927 to 0.15318, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2886 - val_loss: 0.1532\n",
      "Epoch 5/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.2762\n",
      "Epoch 00005: val_loss improved from 0.15318 to 0.14363, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2762 - val_loss: 0.1436\n",
      "Epoch 6/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.2703\n",
      "Epoch 00006: val_loss improved from 0.14363 to 0.14007, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2705 - val_loss: 0.1401\n",
      "Epoch 7/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.2655\n",
      "Epoch 00007: val_loss did not improve from 0.14007\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2655 - val_loss: 0.1583\n",
      "Epoch 8/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.2563\n",
      "Epoch 00008: val_loss improved from 0.14007 to 0.13402, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2572 - val_loss: 0.1340\n",
      "Epoch 9/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.2558\n",
      "Epoch 00009: val_loss did not improve from 0.13402\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2561 - val_loss: 0.1405\n",
      "Epoch 10/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.2474\n",
      "Epoch 00010: val_loss did not improve from 0.13402\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2482 - val_loss: 0.1345\n",
      " ###5 fold : val acc1 0.557, acc3 0.970, mae 0.252###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/307 [============================>.] - ETA: 0s - loss: 2.3414\n",
      "Epoch 00001: val_loss improved from inf to 0.19675, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.2930 - val_loss: 0.1968\n",
      "Epoch 2/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.6203\n",
      "Epoch 00002: val_loss improved from 0.19675 to 0.17889, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6098 - val_loss: 0.1789\n",
      "Epoch 3/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.5758\n",
      "Epoch 00003: val_loss improved from 0.17889 to 0.15092, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5733 - val_loss: 0.1509\n",
      "Epoch 4/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.3346\n",
      "Epoch 00004: val_loss improved from 0.15092 to 0.13936, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3334 - val_loss: 0.1394\n",
      "Epoch 5/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.3332\n",
      "Epoch 00005: val_loss did not improve from 0.13936\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3332 - val_loss: 0.1415\n",
      "Epoch 6/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.3170\n",
      "Epoch 00006: val_loss improved from 0.13936 to 0.13875, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3159 - val_loss: 0.1387\n",
      "Epoch 7/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.3295\n",
      "Epoch 00007: val_loss did not improve from 0.13875\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3285 - val_loss: 0.1596\n",
      "Epoch 8/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.3142\n",
      "Epoch 00008: val_loss did not improve from 0.13875\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3123 - val_loss: 0.1454\n",
      " ###6 fold : val acc1 0.590, acc3 0.956, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/307 [===========================>..] - ETA: 0s - loss: 2.3046\n",
      "Epoch 00001: val_loss improved from inf to 0.21351, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.2267 - val_loss: 0.2135\n",
      "Epoch 2/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.9099\n",
      "Epoch 00002: val_loss improved from 0.21351 to 0.18999, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.9040 - val_loss: 0.1900\n",
      "Epoch 3/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.6858\n",
      "Epoch 00003: val_loss improved from 0.18999 to 0.15424, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6852 - val_loss: 0.1542\n",
      "Epoch 4/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.4299\n",
      "Epoch 00004: val_loss improved from 0.15424 to 0.13810, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4289 - val_loss: 0.1381\n",
      "Epoch 5/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.3809\n",
      "Epoch 00005: val_loss did not improve from 0.13810\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3794 - val_loss: 0.1472\n",
      "Epoch 6/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.3261\n",
      "Epoch 00006: val_loss did not improve from 0.13810\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3260 - val_loss: 0.1401\n",
      " ###7 fold : val acc1 0.576, acc3 0.960, mae 0.235###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/307 [============================>.] - ETA: 0s - loss: 2.2856\n",
      "Epoch 00001: val_loss improved from inf to 0.23184, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.2449 - val_loss: 0.2318\n",
      "Epoch 2/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.8873\n",
      "Epoch 00002: val_loss improved from 0.23184 to 0.19777, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.8753 - val_loss: 0.1978\n",
      "Epoch 3/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.6671\n",
      "Epoch 00003: val_loss improved from 0.19777 to 0.15304, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6515 - val_loss: 0.1530\n",
      "Epoch 4/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.4258\n",
      "Epoch 00004: val_loss improved from 0.15304 to 0.13716, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4242 - val_loss: 0.1372\n",
      "Epoch 5/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.3499\n",
      "Epoch 00005: val_loss did not improve from 0.13716\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3494 - val_loss: 0.1414\n",
      "Epoch 6/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.3359\n",
      "Epoch 00006: val_loss did not improve from 0.13716\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3359 - val_loss: 0.1388\n",
      " ###8 fold : val acc1 0.559, acc3 0.954, mae 0.248###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/307 [============================>.] - ETA: 0s - loss: 2.2472\n",
      "Epoch 00001: val_loss improved from inf to 0.21426, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.2449 - val_loss: 0.2143\n",
      "Epoch 2/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.8935\n",
      "Epoch 00002: val_loss improved from 0.21426 to 0.19061, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8753 - val_loss: 0.1906\n",
      "Epoch 3/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.6645\n",
      "Epoch 00003: val_loss improved from 0.19061 to 0.15057, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6515 - val_loss: 0.1506\n",
      "Epoch 4/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.4286\n",
      "Epoch 00004: val_loss improved from 0.15057 to 0.13543, saving model to result/size/DNN_size_both_y/batch64,dnodes512_dropout0.3,dnodes512_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4242 - val_loss: 0.1354\n",
      "Epoch 5/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.3516\n",
      "Epoch 00005: val_loss did not improve from 0.13543\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3494 - val_loss: 0.1436\n",
      "Epoch 6/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.3374\n",
      "Epoch 00006: val_loss did not improve from 0.13543\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3359 - val_loss: 0.1399\n",
      " ###9 fold : val acc1 0.567, acc3 0.960, mae 0.242###\n",
      "acc10.573_acc30.960\n",
      "random search 25/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608/613 [============================>.] - ETA: 0s - loss: 2.5110\n",
      "Epoch 00001: val_loss improved from inf to 0.19996, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.4931 - val_loss: 0.2000\n",
      "Epoch 2/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.2512\n",
      "Epoch 00002: val_loss improved from 0.19996 to 0.14913, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2497 - val_loss: 0.1491\n",
      "Epoch 3/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.1745\n",
      "Epoch 00003: val_loss did not improve from 0.14913\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1895 - val_loss: 0.1601\n",
      "Epoch 4/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.1745\n",
      "Epoch 00004: val_loss improved from 0.14913 to 0.13772, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1741 - val_loss: 0.1377\n",
      "Epoch 5/100\n",
      "597/613 [============================>.] - ETA: 0s - loss: 0.1665\n",
      "Epoch 00005: val_loss improved from 0.13772 to 0.13237, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_0.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1663 - val_loss: 0.1324\n",
      "Epoch 6/100\n",
      "597/613 [============================>.] - ETA: 0s - loss: 0.1697\n",
      "Epoch 00006: val_loss did not improve from 0.13237\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1694 - val_loss: 0.1329\n",
      "Epoch 7/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.1836\n",
      "Epoch 00007: val_loss did not improve from 0.13237\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1833 - val_loss: 0.1324\n",
      " ###0 fold : val acc1 0.573, acc3 0.960, mae 0.238###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/613 [============================>.] - ETA: 0s - loss: 2.5518\n",
      "Epoch 00001: val_loss improved from inf to 0.19895, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.4891 - val_loss: 0.1989\n",
      "Epoch 2/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.2570\n",
      "Epoch 00002: val_loss improved from 0.19895 to 0.14754, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2563 - val_loss: 0.1475\n",
      "Epoch 3/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.1762\n",
      "Epoch 00003: val_loss did not improve from 0.14754\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1929 - val_loss: 0.1661\n",
      "Epoch 4/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.1760\n",
      "Epoch 00004: val_loss improved from 0.14754 to 0.13590, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1753 - val_loss: 0.1359\n",
      "Epoch 5/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.1685\n",
      "Epoch 00005: val_loss improved from 0.13590 to 0.13006, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_1.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1684 - val_loss: 0.1301\n",
      "Epoch 6/100\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.1706\n",
      "Epoch 00006: val_loss did not improve from 0.13006\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1706 - val_loss: 0.1343\n",
      "Epoch 7/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.1846\n",
      "Epoch 00007: val_loss did not improve from 0.13006\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1844 - val_loss: 0.1317\n",
      " ###1 fold : val acc1 0.583, acc3 0.961, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601/613 [============================>.] - ETA: 0s - loss: 2.5467\n",
      "Epoch 00001: val_loss improved from inf to 0.20117, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 2.5032 - val_loss: 0.2012\n",
      "Epoch 2/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.2527\n",
      "Epoch 00002: val_loss improved from 0.20117 to 0.14861, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2521 - val_loss: 0.1486\n",
      "Epoch 3/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.1762\n",
      "Epoch 00003: val_loss did not improve from 0.14861\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1926 - val_loss: 0.1709\n",
      "Epoch 4/100\n",
      "596/613 [============================>.] - ETA: 0s - loss: 0.1754\n",
      "Epoch 00004: val_loss improved from 0.14861 to 0.13560, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1746 - val_loss: 0.1356\n",
      "Epoch 5/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.1681\n",
      "Epoch 00005: val_loss improved from 0.13560 to 0.12956, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_2.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1680 - val_loss: 0.1296\n",
      "Epoch 6/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.1702\n",
      "Epoch 00006: val_loss did not improve from 0.12956\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1701 - val_loss: 0.1347\n",
      "Epoch 7/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.1842\n",
      "Epoch 00007: val_loss did not improve from 0.12956\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1840 - val_loss: 0.1326\n",
      " ###2 fold : val acc1 0.588, acc3 0.964, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608/613 [============================>.] - ETA: 0s - loss: 2.5305\n",
      "Epoch 00001: val_loss improved from inf to 0.20009, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.5125 - val_loss: 0.2001\n",
      "Epoch 2/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.2555\n",
      "Epoch 00002: val_loss improved from 0.20009 to 0.14873, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2542 - val_loss: 0.1487\n",
      "Epoch 3/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.1963\n",
      "Epoch 00003: val_loss did not improve from 0.14873\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1962 - val_loss: 0.1670\n",
      "Epoch 4/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.1760\n",
      "Epoch 00004: val_loss improved from 0.14873 to 0.13643, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1757 - val_loss: 0.1364\n",
      "Epoch 5/100\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.1681\n",
      "Epoch 00005: val_loss improved from 0.13643 to 0.12986, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_3.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1681 - val_loss: 0.1299\n",
      "Epoch 6/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.1670\n",
      "Epoch 00006: val_loss did not improve from 0.12986\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1674 - val_loss: 0.1333\n",
      "Epoch 7/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.1819\n",
      "Epoch 00007: val_loss did not improve from 0.12986\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1816 - val_loss: 0.1324\n",
      " ###3 fold : val acc1 0.572, acc3 0.964, mae 0.235###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597/613 [============================>.] - ETA: 0s - loss: 2.6702\n",
      "Epoch 00001: val_loss improved from inf to 0.20224, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "613/613 [==============================] - 3s 4ms/step - loss: 2.6067 - val_loss: 0.2022\n",
      "Epoch 2/100\n",
      "598/613 [============================>.] - ETA: 0s - loss: 0.2859\n",
      "Epoch 00002: val_loss improved from 0.20224 to 0.15242, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2843 - val_loss: 0.1524\n",
      "Epoch 3/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.1914\n",
      "Epoch 00003: val_loss improved from 0.15242 to 0.13499, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1906 - val_loss: 0.1350\n",
      "Epoch 4/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.1838\n",
      "Epoch 00004: val_loss improved from 0.13499 to 0.13415, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1840 - val_loss: 0.1341\n",
      "Epoch 5/100\n",
      "607/613 [============================>.] - ETA: 0s - loss: 0.1662\n",
      "Epoch 00005: val_loss improved from 0.13415 to 0.12872, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_4.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1666 - val_loss: 0.1287\n",
      "Epoch 6/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.1798\n",
      "Epoch 00006: val_loss did not improve from 0.12872\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1798 - val_loss: 0.1354\n",
      "Epoch 7/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.1670\n",
      "Epoch 00007: val_loss did not improve from 0.12872\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1668 - val_loss: 0.1318\n",
      " ###4 fold : val acc1 0.592, acc3 0.963, mae 0.226###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609/613 [============================>.] - ETA: 0s - loss: 2.4081\n",
      "Epoch 00001: val_loss improved from inf to 0.19634, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.3945 - val_loss: 0.1963\n",
      "Epoch 2/100\n",
      "608/613 [============================>.] - ETA: 0s - loss: 0.1996\n",
      "Epoch 00002: val_loss improved from 0.19634 to 0.15062, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1994 - val_loss: 0.1506\n",
      "Epoch 3/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.1801\n",
      "Epoch 00003: val_loss improved from 0.15062 to 0.13909, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1797 - val_loss: 0.1391\n",
      "Epoch 4/100\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.1699\n",
      "Epoch 00004: val_loss improved from 0.13909 to 0.13551, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1699 - val_loss: 0.1355\n",
      "Epoch 5/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.1652\n",
      "Epoch 00005: val_loss improved from 0.13551 to 0.12932, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_5.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1654 - val_loss: 0.1293\n",
      "Epoch 6/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.1631\n",
      "Epoch 00006: val_loss did not improve from 0.12932\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1631 - val_loss: 0.1351\n",
      "Epoch 7/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.1620\n",
      "Epoch 00007: val_loss did not improve from 0.12932\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1618 - val_loss: 0.1336\n",
      " ###5 fold : val acc1 0.591, acc3 0.967, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595/613 [============================>.] - ETA: 0s - loss: 2.9693\n",
      "Epoch 00001: val_loss improved from inf to 0.20930, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.8891 - val_loss: 0.2093\n",
      "Epoch 2/100\n",
      "600/613 [============================>.] - ETA: 0s - loss: 0.3434\n",
      "Epoch 00002: val_loss improved from 0.20930 to 0.15987, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.3406 - val_loss: 0.1599\n",
      "Epoch 3/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.2072\n",
      "Epoch 00003: val_loss improved from 0.15987 to 0.13615, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2063 - val_loss: 0.1362\n",
      "Epoch 4/100\n",
      "612/613 [============================>.] - ETA: 0s - loss: 0.1896\n",
      "Epoch 00004: val_loss improved from 0.13615 to 0.13214, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1896 - val_loss: 0.1321\n",
      "Epoch 5/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.1653\n",
      "Epoch 00005: val_loss improved from 0.13214 to 0.12858, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_6.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1654 - val_loss: 0.1286\n",
      "Epoch 6/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.1644\n",
      "Epoch 00006: val_loss did not improve from 0.12858\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1645 - val_loss: 0.1315\n",
      "Epoch 7/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.1675\n",
      "Epoch 00007: val_loss did not improve from 0.12858\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1671 - val_loss: 0.1408\n",
      " ###6 fold : val acc1 0.574, acc3 0.963, mae 0.236###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602/613 [============================>.] - ETA: 0s - loss: 2.9086\n",
      "Epoch 00001: val_loss improved from inf to 0.20944, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.8614 - val_loss: 0.2094\n",
      "Epoch 2/100\n",
      "606/613 [============================>.] - ETA: 0s - loss: 0.4013\n",
      "Epoch 00002: val_loss improved from 0.20944 to 0.16045, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.3991 - val_loss: 0.1604\n",
      "Epoch 3/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.2275\n",
      "Epoch 00003: val_loss improved from 0.16045 to 0.13703, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2266 - val_loss: 0.1370\n",
      "Epoch 4/100\n",
      "605/613 [============================>.] - ETA: 0s - loss: 0.1986\n",
      "Epoch 00004: val_loss improved from 0.13703 to 0.13013, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1980 - val_loss: 0.1301\n",
      "Epoch 5/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.1660\n",
      "Epoch 00005: val_loss improved from 0.13013 to 0.12953, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_7.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1658 - val_loss: 0.1295\n",
      "Epoch 6/100\n",
      "610/613 [============================>.] - ETA: 0s - loss: 0.1639\n",
      "Epoch 00006: val_loss did not improve from 0.12953\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1640 - val_loss: 0.1309\n",
      "Epoch 7/100\n",
      "608/613 [============================>.] - ETA: 0s - loss: 0.1673\n",
      "Epoch 00007: val_loss did not improve from 0.12953\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1672 - val_loss: 0.1391\n",
      " ###7 fold : val acc1 0.589, acc3 0.965, mae 0.226###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/613 [============================>.] - ETA: 0s - loss: 2.8978\n",
      "Epoch 00001: val_loss improved from inf to 0.21659, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.8597 - val_loss: 0.2166\n",
      "Epoch 2/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.3931\n",
      "Epoch 00002: val_loss improved from 0.21659 to 0.15857, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.3897 - val_loss: 0.1586\n",
      "Epoch 3/100\n",
      "602/613 [============================>.] - ETA: 0s - loss: 0.2258\n",
      "Epoch 00003: val_loss improved from 0.15857 to 0.13685, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2249 - val_loss: 0.1369\n",
      "Epoch 4/100\n",
      "601/613 [============================>.] - ETA: 0s - loss: 0.1993\n",
      "Epoch 00004: val_loss improved from 0.13685 to 0.12902, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1984 - val_loss: 0.1290\n",
      "Epoch 5/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.1651\n",
      "Epoch 00005: val_loss improved from 0.12902 to 0.12779, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_8.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1651 - val_loss: 0.1278\n",
      "Epoch 6/100\n",
      "596/613 [============================>.] - ETA: 0s - loss: 0.1632\n",
      "Epoch 00006: val_loss did not improve from 0.12779\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1636 - val_loss: 0.1282\n",
      "Epoch 7/100\n",
      "608/613 [============================>.] - ETA: 0s - loss: 0.1658\n",
      "Epoch 00007: val_loss did not improve from 0.12779\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1658 - val_loss: 0.1404\n",
      " ###8 fold : val acc1 0.575, acc3 0.959, mae 0.237###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/613 [============================>.] - ETA: 0s - loss: 2.8978\n",
      "Epoch 00001: val_loss improved from inf to 0.20543, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 2.8597 - val_loss: 0.2054\n",
      "Epoch 2/100\n",
      "599/613 [============================>.] - ETA: 0s - loss: 0.3936\n",
      "Epoch 00002: val_loss improved from 0.20543 to 0.15650, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.3897 - val_loss: 0.1565\n",
      "Epoch 3/100\n",
      "608/613 [============================>.] - ETA: 0s - loss: 0.2257\n",
      "Epoch 00003: val_loss improved from 0.15650 to 0.13533, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.2249 - val_loss: 0.1353\n",
      "Epoch 4/100\n",
      "604/613 [============================>.] - ETA: 0s - loss: 0.1991\n",
      "Epoch 00004: val_loss improved from 0.13533 to 0.12922, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1984 - val_loss: 0.1292\n",
      "Epoch 5/100\n",
      "597/613 [============================>.] - ETA: 0s - loss: 0.1654\n",
      "Epoch 00005: val_loss improved from 0.12922 to 0.12845, saving model to result/size/DNN_size_both_y/batch32,dnodes512_dropout0.2,lr0.0005/weights_9.hdf5\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1651 - val_loss: 0.1285\n",
      "Epoch 6/100\n",
      "609/613 [============================>.] - ETA: 0s - loss: 0.1635\n",
      "Epoch 00006: val_loss did not improve from 0.12845\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1636 - val_loss: 0.1289\n",
      "Epoch 7/100\n",
      "611/613 [============================>.] - ETA: 0s - loss: 0.1656\n",
      "Epoch 00007: val_loss did not improve from 0.12845\n",
      "613/613 [==============================] - 2s 3ms/step - loss: 0.1658 - val_loss: 0.1393\n",
      " ###9 fold : val acc1 0.582, acc3 0.962, mae 0.232###\n",
      "acc10.582_acc30.963\n",
      "random search 26/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/154 [============================>.] - ETA: 0s - loss: 5.0878\n",
      "Epoch 00001: val_loss improved from inf to 0.32529, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 4.9903 - val_loss: 0.3253\n",
      "Epoch 2/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.4254\n",
      "Epoch 00002: val_loss improved from 0.32529 to 0.19080, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.4063 - val_loss: 0.1908\n",
      "Epoch 3/100\n",
      "135/154 [=========================>....] - ETA: 0s - loss: 0.2041\n",
      "Epoch 00003: val_loss improved from 0.19080 to 0.16542, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2894 - val_loss: 0.1654\n",
      "Epoch 4/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.2128\n",
      "Epoch 00004: val_loss improved from 0.16542 to 0.14729, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2112 - val_loss: 0.1473\n",
      "Epoch 5/100\n",
      "132/154 [========================>.....] - ETA: 0s - loss: 0.1834\n",
      "Epoch 00005: val_loss improved from 0.14729 to 0.13920, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1822 - val_loss: 0.1392\n",
      "Epoch 6/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1857\n",
      "Epoch 00006: val_loss improved from 0.13920 to 0.13889, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1853 - val_loss: 0.1389\n",
      "Epoch 7/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1751\n",
      "Epoch 00007: val_loss improved from 0.13889 to 0.13487, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1746 - val_loss: 0.1349\n",
      "Epoch 8/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1703\n",
      "Epoch 00008: val_loss did not improve from 0.13487\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1712 - val_loss: 0.1349\n",
      "Epoch 9/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1765\n",
      "Epoch 00009: val_loss improved from 0.13487 to 0.12964, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1758 - val_loss: 0.1296\n",
      "Epoch 10/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1658\n",
      "Epoch 00010: val_loss improved from 0.12964 to 0.12946, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1657 - val_loss: 0.1295\n",
      "Epoch 11/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1726\n",
      "Epoch 00011: val_loss did not improve from 0.12946\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1722 - val_loss: 0.1303\n",
      "Epoch 12/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1616\n",
      "Epoch 00012: val_loss improved from 0.12946 to 0.12862, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1631 - val_loss: 0.1286\n",
      "Epoch 13/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1739\n",
      "Epoch 00013: val_loss did not improve from 0.12862\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1750 - val_loss: 0.1380\n",
      "Epoch 14/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.2048\n",
      "Epoch 00014: val_loss did not improve from 0.12862\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.2002 - val_loss: 0.1317\n",
      " ###0 fold : val acc1 0.571, acc3 0.965, mae 0.236###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/154 [=========================>....] - ETA: 0s - loss: 5.5356\n",
      "Epoch 00001: val_loss improved from inf to 0.32748, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 4.9944 - val_loss: 0.3275\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.4236\n",
      "Epoch 00002: val_loss improved from 0.32748 to 0.18910, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4236 - val_loss: 0.1891\n",
      "Epoch 3/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.3025\n",
      "Epoch 00003: val_loss improved from 0.18910 to 0.16499, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.3012 - val_loss: 0.1650\n",
      "Epoch 4/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.2163\n",
      "Epoch 00004: val_loss improved from 0.16499 to 0.14477, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2148 - val_loss: 0.1448\n",
      "Epoch 5/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1840\n",
      "Epoch 00005: val_loss improved from 0.14477 to 0.13685, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1836 - val_loss: 0.1369\n",
      "Epoch 6/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1858\n",
      "Epoch 00006: val_loss improved from 0.13685 to 0.13590, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1854 - val_loss: 0.1359\n",
      "Epoch 7/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1758\n",
      "Epoch 00007: val_loss improved from 0.13590 to 0.13390, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1758 - val_loss: 0.1339\n",
      "Epoch 8/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1707\n",
      "Epoch 00008: val_loss did not improve from 0.13390\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1742 - val_loss: 0.1346\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1820\n",
      "Epoch 00009: val_loss improved from 0.13390 to 0.12949, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1820 - val_loss: 0.1295\n",
      "Epoch 10/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1662\n",
      "Epoch 00010: val_loss did not improve from 0.12949\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1672 - val_loss: 0.1303\n",
      "Epoch 11/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1695\n",
      "Epoch 00011: val_loss improved from 0.12949 to 0.12944, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1690 - val_loss: 0.1294\n",
      "Epoch 12/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1628\n",
      "Epoch 00012: val_loss improved from 0.12944 to 0.12862, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1631 - val_loss: 0.1286\n",
      "Epoch 13/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1693\n",
      "Epoch 00013: val_loss did not improve from 0.12862\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1702 - val_loss: 0.1348\n",
      "Epoch 14/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1975\n",
      "Epoch 00014: val_loss did not improve from 0.12862\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1938 - val_loss: 0.1316\n",
      " ###1 fold : val acc1 0.580, acc3 0.962, mae 0.232###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/154 [===========================>..] - ETA: 0s - loss: 5.2460\n",
      "Epoch 00001: val_loss improved from inf to 0.32721, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 4.9871 - val_loss: 0.3272\n",
      "Epoch 2/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.4167\n",
      "Epoch 00002: val_loss improved from 0.32721 to 0.19076, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4136 - val_loss: 0.1908\n",
      "Epoch 3/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.2053\n",
      "Epoch 00003: val_loss improved from 0.19076 to 0.16659, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2953 - val_loss: 0.1666\n",
      "Epoch 4/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.2141\n",
      "Epoch 00004: val_loss improved from 0.16659 to 0.14728, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2126 - val_loss: 0.1473\n",
      "Epoch 5/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1837\n",
      "Epoch 00005: val_loss improved from 0.14728 to 0.13718, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1836 - val_loss: 0.1372\n",
      "Epoch 6/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1843\n",
      "Epoch 00006: val_loss improved from 0.13718 to 0.13601, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1846 - val_loss: 0.1360\n",
      "Epoch 7/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1741\n",
      "Epoch 00007: val_loss improved from 0.13601 to 0.13371, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1752 - val_loss: 0.1337\n",
      "Epoch 8/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1745\n",
      "Epoch 00008: val_loss did not improve from 0.13371\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1742 - val_loss: 0.1348\n",
      "Epoch 9/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1826\n",
      "Epoch 00009: val_loss improved from 0.13371 to 0.13055, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1825 - val_loss: 0.1306\n",
      "Epoch 10/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.1657\n",
      "Epoch 00010: val_loss did not improve from 0.13055\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1676 - val_loss: 0.1307\n",
      "Epoch 11/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1677\n",
      "Epoch 00011: val_loss improved from 0.13055 to 0.12984, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1674 - val_loss: 0.1298\n",
      "Epoch 12/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1628\n",
      "Epoch 00012: val_loss improved from 0.12984 to 0.12909, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1630 - val_loss: 0.1291\n",
      "Epoch 13/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1647\n",
      "Epoch 00013: val_loss did not improve from 0.12909\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1655 - val_loss: 0.1312\n",
      "Epoch 14/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1814\n",
      "Epoch 00014: val_loss did not improve from 0.12909\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1784 - val_loss: 0.1310\n",
      " ###2 fold : val acc1 0.575, acc3 0.965, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/154 [==========================>...] - ETA: 0s - loss: 5.3035\n",
      "Epoch 00001: val_loss improved from inf to 0.32857, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 4.9458 - val_loss: 0.3286\n",
      "Epoch 2/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.4172\n",
      "Epoch 00002: val_loss improved from 0.32857 to 0.19025, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.4064 - val_loss: 0.1903\n",
      "Epoch 3/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.2923\n",
      "Epoch 00003: val_loss improved from 0.19025 to 0.16739, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2921 - val_loss: 0.1674\n",
      "Epoch 4/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.2087\n",
      "Epoch 00004: val_loss improved from 0.16739 to 0.14704, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2085 - val_loss: 0.1470\n",
      "Epoch 5/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1838\n",
      "Epoch 00005: val_loss improved from 0.14704 to 0.13731, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1837 - val_loss: 0.1373\n",
      "Epoch 6/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1828\n",
      "Epoch 00006: val_loss improved from 0.13731 to 0.13689, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1828 - val_loss: 0.1369\n",
      "Epoch 7/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1770\n",
      "Epoch 00007: val_loss improved from 0.13689 to 0.13393, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1779 - val_loss: 0.1339\n",
      "Epoch 8/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1771\n",
      "Epoch 00008: val_loss did not improve from 0.13393\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1765 - val_loss: 0.1357\n",
      "Epoch 9/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1849\n",
      "Epoch 00009: val_loss improved from 0.13393 to 0.13192, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1850 - val_loss: 0.1319\n",
      "Epoch 10/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1686\n",
      "Epoch 00010: val_loss improved from 0.13192 to 0.13078, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1686 - val_loss: 0.1308\n",
      "Epoch 11/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1683\n",
      "Epoch 00011: val_loss improved from 0.13078 to 0.12962, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1682 - val_loss: 0.1296\n",
      "Epoch 12/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1636\n",
      "Epoch 00012: val_loss improved from 0.12962 to 0.12894, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1639 - val_loss: 0.1289\n",
      "Epoch 13/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1690\n",
      "Epoch 00013: val_loss did not improve from 0.12894\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1682 - val_loss: 0.1305\n",
      "Epoch 14/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1829\n",
      "Epoch 00014: val_loss did not improve from 0.12894\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1812 - val_loss: 0.1310\n",
      " ###3 fold : val acc1 0.559, acc3 0.966, mae 0.240###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/154 [============================>.] - ETA: 0s - loss: 5.1170\n",
      "Epoch 00001: val_loss improved from inf to 0.35415, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 5.0491 - val_loss: 0.3541\n",
      "Epoch 2/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.5348\n",
      "Epoch 00002: val_loss improved from 0.35415 to 0.19266, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5303 - val_loss: 0.1927\n",
      "Epoch 3/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.2750\n",
      "Epoch 00003: val_loss improved from 0.19266 to 0.15928, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2691 - val_loss: 0.1593\n",
      "Epoch 4/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.2449\n",
      "Epoch 00004: val_loss improved from 0.15928 to 0.14489, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2414 - val_loss: 0.1449\n",
      "Epoch 5/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1885\n",
      "Epoch 00005: val_loss improved from 0.14489 to 0.13754, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1871 - val_loss: 0.1375\n",
      "Epoch 6/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1765\n",
      "Epoch 00006: val_loss improved from 0.13754 to 0.13702, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1775 - val_loss: 0.1370\n",
      "Epoch 7/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1791\n",
      "Epoch 00007: val_loss improved from 0.13702 to 0.13284, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1788 - val_loss: 0.1328\n",
      "Epoch 8/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1741\n",
      "Epoch 00008: val_loss improved from 0.13284 to 0.13195, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1731 - val_loss: 0.1319\n",
      "Epoch 9/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1807\n",
      "Epoch 00009: val_loss did not improve from 0.13195\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1799 - val_loss: 0.1361\n",
      "Epoch 10/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1740\n",
      "Epoch 00010: val_loss improved from 0.13195 to 0.12933, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_4.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1732 - val_loss: 0.1293\n",
      "Epoch 11/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.1669\n",
      "Epoch 00011: val_loss did not improve from 0.12933\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1668 - val_loss: 0.1323\n",
      "Epoch 12/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1687\n",
      "Epoch 00012: val_loss did not improve from 0.12933\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1684 - val_loss: 0.1305\n",
      " ###4 fold : val acc1 0.583, acc3 0.965, mae 0.230###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/154 [=========================>....] - ETA: 0s - loss: 4.9806\n",
      "Epoch 00001: val_loss improved from inf to 0.32376, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 4.5309 - val_loss: 0.3238\n",
      "Epoch 2/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.2566\n",
      "Epoch 00002: val_loss improved from 0.32376 to 0.18357, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2556 - val_loss: 0.1836\n",
      "Epoch 3/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.2014\n",
      "Epoch 00003: val_loss improved from 0.18357 to 0.15504, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2013 - val_loss: 0.1550\n",
      "Epoch 4/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1859\n",
      "Epoch 00004: val_loss improved from 0.15504 to 0.14399, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1862 - val_loss: 0.1440\n",
      "Epoch 5/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1798\n",
      "Epoch 00005: val_loss improved from 0.14399 to 0.13796, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1785 - val_loss: 0.1380\n",
      "Epoch 6/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1748\n",
      "Epoch 00006: val_loss improved from 0.13796 to 0.13715, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1748 - val_loss: 0.1372\n",
      "Epoch 7/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1752\n",
      "Epoch 00007: val_loss improved from 0.13715 to 0.13388, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1753 - val_loss: 0.1339\n",
      "Epoch 8/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1704\n",
      "Epoch 00008: val_loss improved from 0.13388 to 0.13322, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1704 - val_loss: 0.1332\n",
      "Epoch 9/100\n",
      "137/154 [=========================>....] - ETA: 0s - loss: 0.1691\n",
      "Epoch 00009: val_loss improved from 0.13322 to 0.13153, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1691 - val_loss: 0.1315\n",
      "Epoch 10/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1679\n",
      "Epoch 00010: val_loss improved from 0.13153 to 0.13028, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1678 - val_loss: 0.1303\n",
      "Epoch 11/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1636\n",
      "Epoch 00011: val_loss did not improve from 0.13028\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1640 - val_loss: 0.1323\n",
      "Epoch 12/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1651\n",
      "Epoch 00012: val_loss improved from 0.13028 to 0.12889, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1641 - val_loss: 0.1289\n",
      "Epoch 13/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1636\n",
      "Epoch 00013: val_loss did not improve from 0.12889\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1625 - val_loss: 0.1289\n",
      "Epoch 14/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.1613\n",
      "Epoch 00014: val_loss improved from 0.12889 to 0.12806, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1611 - val_loss: 0.1281\n",
      "Epoch 15/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1598\n",
      "Epoch 00015: val_loss did not improve from 0.12806\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1597 - val_loss: 0.1289\n",
      "Epoch 16/100\n",
      "136/154 [=========================>....] - ETA: 0s - loss: 0.1581\n",
      "Epoch 00016: val_loss did not improve from 0.12806\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1581 - val_loss: 0.1292\n",
      " ###5 fold : val acc1 0.596, acc3 0.968, mae 0.227###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - ETA: 0s - loss: 4.8865\n",
      "Epoch 00001: val_loss improved from inf to 0.36619, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 4.8865 - val_loss: 0.3662\n",
      "Epoch 2/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.2851\n",
      "Epoch 00002: val_loss improved from 0.36619 to 0.21345, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4513 - val_loss: 0.2134\n",
      "Epoch 3/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.2338\n",
      "Epoch 00003: val_loss improved from 0.21345 to 0.15905, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2307 - val_loss: 0.1590\n",
      "Epoch 4/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.2350\n",
      "Epoch 00004: val_loss improved from 0.15905 to 0.14401, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2317 - val_loss: 0.1440\n",
      "Epoch 5/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1882\n",
      "Epoch 00005: val_loss improved from 0.14401 to 0.13766, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1867 - val_loss: 0.1377\n",
      "Epoch 6/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1754\n",
      "Epoch 00006: val_loss improved from 0.13766 to 0.13560, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1760 - val_loss: 0.1356\n",
      "Epoch 7/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1753\n",
      "Epoch 00007: val_loss improved from 0.13560 to 0.13340, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1758 - val_loss: 0.1334\n",
      "Epoch 8/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1718\n",
      "Epoch 00008: val_loss improved from 0.13340 to 0.13135, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1720 - val_loss: 0.1313\n",
      "Epoch 9/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1783\n",
      "Epoch 00009: val_loss did not improve from 0.13135\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1784 - val_loss: 0.1380\n",
      "Epoch 10/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1725\n",
      "Epoch 00010: val_loss improved from 0.13135 to 0.12908, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1725 - val_loss: 0.1291\n",
      "Epoch 11/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1718\n",
      "Epoch 00011: val_loss did not improve from 0.12908\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1729 - val_loss: 0.1379\n",
      "Epoch 12/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.2105\n",
      "Epoch 00012: val_loss did not improve from 0.12908\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2063 - val_loss: 0.1298\n",
      " ###6 fold : val acc1 0.574, acc3 0.961, mae 0.236###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/154 [============================>.] - ETA: 0s - loss: 4.9498\n",
      "Epoch 00001: val_loss improved from inf to 0.37463, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 4.8851 - val_loss: 0.3746\n",
      "Epoch 2/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.2854\n",
      "Epoch 00002: val_loss improved from 0.37463 to 0.21556, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4873 - val_loss: 0.2156\n",
      "Epoch 3/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.2424\n",
      "Epoch 00003: val_loss improved from 0.21556 to 0.15830, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2397 - val_loss: 0.1583\n",
      "Epoch 4/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.2360\n",
      "Epoch 00004: val_loss improved from 0.15830 to 0.14337, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2342 - val_loss: 0.1434\n",
      "Epoch 5/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1909\n",
      "Epoch 00005: val_loss improved from 0.14337 to 0.13761, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1893 - val_loss: 0.1376\n",
      "Epoch 6/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1747\n",
      "Epoch 00006: val_loss improved from 0.13761 to 0.13462, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1755 - val_loss: 0.1346\n",
      "Epoch 7/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1782\n",
      "Epoch 00007: val_loss did not improve from 0.13462\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1787 - val_loss: 0.1353\n",
      "Epoch 8/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1751\n",
      "Epoch 00008: val_loss improved from 0.13462 to 0.13077, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1749 - val_loss: 0.1308\n",
      "Epoch 9/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1899\n",
      "Epoch 00009: val_loss did not improve from 0.13077\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1898 - val_loss: 0.1427\n",
      "Epoch 10/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1860\n",
      "Epoch 00010: val_loss improved from 0.13077 to 0.13004, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1848 - val_loss: 0.1300\n",
      "Epoch 11/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1784\n",
      "Epoch 00011: val_loss did not improve from 0.13004\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1782 - val_loss: 0.1344\n",
      "Epoch 12/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1926\n",
      "Epoch 00012: val_loss improved from 0.13004 to 0.12861, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1913 - val_loss: 0.1286\n",
      "Epoch 13/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1812\n",
      "Epoch 00013: val_loss did not improve from 0.12861\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1812 - val_loss: 0.1319\n",
      "Epoch 14/100\n",
      "138/154 [=========================>....] - ETA: 0s - loss: 0.1760\n",
      "Epoch 00014: val_loss improved from 0.12861 to 0.12840, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1737 - val_loss: 0.1284\n",
      "Epoch 15/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1922\n",
      "Epoch 00015: val_loss did not improve from 0.12840\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1901 - val_loss: 0.1356\n",
      "Epoch 16/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1912\n",
      "Epoch 00016: val_loss did not improve from 0.12840\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1912 - val_loss: 0.1313\n",
      " ###7 fold : val acc1 0.583, acc3 0.963, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/154 [==========================>...] - ETA: 0s - loss: 5.3091\n",
      "Epoch 00001: val_loss improved from inf to 0.38539, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 4.8916 - val_loss: 0.3854\n",
      "Epoch 2/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.4814\n",
      "Epoch 00002: val_loss improved from 0.38539 to 0.21044, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.4796 - val_loss: 0.2104\n",
      "Epoch 3/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.2384\n",
      "Epoch 00003: val_loss improved from 0.21044 to 0.15428, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2384 - val_loss: 0.1543\n",
      "Epoch 4/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.2352\n",
      "Epoch 00004: val_loss improved from 0.15428 to 0.14063, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2343 - val_loss: 0.1406\n",
      "Epoch 5/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1889\n",
      "Epoch 00005: val_loss improved from 0.14063 to 0.13570, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1888 - val_loss: 0.1357\n",
      "Epoch 6/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.1752\n",
      "Epoch 00006: val_loss improved from 0.13570 to 0.13246, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1754 - val_loss: 0.1325\n",
      "Epoch 7/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1748\n",
      "Epoch 00007: val_loss did not improve from 0.13246\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1749 - val_loss: 0.1333\n",
      "Epoch 8/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1735\n",
      "Epoch 00008: val_loss improved from 0.13246 to 0.12856, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1732 - val_loss: 0.1286\n",
      "Epoch 9/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1827\n",
      "Epoch 00009: val_loss did not improve from 0.12856\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1828 - val_loss: 0.1395\n",
      "Epoch 10/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1763\n",
      "Epoch 00010: val_loss did not improve from 0.12856\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1766 - val_loss: 0.1288\n",
      " ###8 fold : val acc1 0.570, acc3 0.957, mae 0.241###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/154 [=========================>....] - ETA: 0s - loss: 5.5276\n",
      "Epoch 00001: val_loss improved from inf to 0.37556, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 4.8916 - val_loss: 0.3756\n",
      "Epoch 2/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.4895\n",
      "Epoch 00002: val_loss improved from 0.37556 to 0.20673, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.4796 - val_loss: 0.2067\n",
      "Epoch 3/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.2394\n",
      "Epoch 00003: val_loss improved from 0.20673 to 0.15230, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2384 - val_loss: 0.1523\n",
      "Epoch 4/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2349\n",
      "Epoch 00004: val_loss improved from 0.15230 to 0.14028, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2343 - val_loss: 0.1403\n",
      "Epoch 5/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1905\n",
      "Epoch 00005: val_loss improved from 0.14028 to 0.13549, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1888 - val_loss: 0.1355\n",
      "Epoch 6/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1748\n",
      "Epoch 00006: val_loss improved from 0.13549 to 0.13360, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1754 - val_loss: 0.1336\n",
      "Epoch 7/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1749\n",
      "Epoch 00007: val_loss improved from 0.13360 to 0.13271, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1749 - val_loss: 0.1327\n",
      "Epoch 8/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1734\n",
      "Epoch 00008: val_loss improved from 0.13271 to 0.12898, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1732 - val_loss: 0.1290\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1828\n",
      "Epoch 00009: val_loss did not improve from 0.12898\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1828 - val_loss: 0.1398\n",
      "Epoch 10/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1768\n",
      "Epoch 00010: val_loss improved from 0.12898 to 0.12885, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1766 - val_loss: 0.1289\n",
      "Epoch 11/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1719\n",
      "Epoch 00011: val_loss did not improve from 0.12885\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1724 - val_loss: 0.1331\n",
      "Epoch 12/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1925\n",
      "Epoch 00012: val_loss improved from 0.12885 to 0.12851, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1916 - val_loss: 0.1285\n",
      "Epoch 13/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.1940\n",
      "Epoch 00013: val_loss did not improve from 0.12851\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1915 - val_loss: 0.1317\n",
      "Epoch 14/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1840\n",
      "Epoch 00014: val_loss improved from 0.12851 to 0.12838, saving model to result/size/DNN_size_both_y/batch128,dnodes512_dropout0.2,lr0.001/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1812 - val_loss: 0.1284\n",
      "Epoch 15/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1949\n",
      "Epoch 00015: val_loss did not improve from 0.12838\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1939 - val_loss: 0.1342\n",
      "Epoch 16/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1867\n",
      "Epoch 00016: val_loss did not improve from 0.12838\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.1850 - val_loss: 0.1313\n",
      " ###9 fold : val acc1 0.585, acc3 0.965, mae 0.230###\n",
      "acc10.578_acc30.964\n",
      "random search 27/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/39 [===============>..............] - ETA: 0s - loss: 19.5315 \n",
      "Epoch 00001: val_loss improved from inf to 11.91538, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 17.3640 - val_loss: 11.9154\n",
      "Epoch 2/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 10.1952\n",
      "Epoch 00002: val_loss improved from 11.91538 to 5.17025, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.6398 - val_loss: 5.1703\n",
      "Epoch 3/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 4.1046\n",
      "Epoch 00003: val_loss improved from 5.17025 to 2.11455, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 4.0120 - val_loss: 2.1146\n",
      "Epoch 4/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 1.8798\n",
      "Epoch 00004: val_loss improved from 2.11455 to 1.12320, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.9715 - val_loss: 1.1232\n",
      "Epoch 5/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 1.0532\n",
      "Epoch 00005: val_loss improved from 1.12320 to 0.68816, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.2633 - val_loss: 0.6882\n",
      "Epoch 6/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.6797\n",
      "Epoch 00006: val_loss improved from 0.68816 to 0.46345, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.8043 - val_loss: 0.4635\n",
      "Epoch 7/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.4971\n",
      "Epoch 00007: val_loss improved from 0.46345 to 0.34468, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.6460 - val_loss: 0.3447\n",
      "Epoch 8/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3898\n",
      "Epoch 00008: val_loss improved from 0.34468 to 0.27937, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4586 - val_loss: 0.2794\n",
      "Epoch 9/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.4288\n",
      "Epoch 00009: val_loss improved from 0.27937 to 0.24285, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3844 - val_loss: 0.2429\n",
      "Epoch 10/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2924\n",
      "Epoch 00010: val_loss improved from 0.24285 to 0.22184, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3226 - val_loss: 0.2218\n",
      "Epoch 11/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.3143\n",
      "Epoch 00011: val_loss improved from 0.22184 to 0.20914, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.3135 - val_loss: 0.2091\n",
      "Epoch 12/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.3097\n",
      "Epoch 00012: val_loss improved from 0.20914 to 0.20173, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.3093 - val_loss: 0.2017\n",
      "Epoch 13/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2683\n",
      "Epoch 00013: val_loss improved from 0.20173 to 0.19437, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2680 - val_loss: 0.1944\n",
      "Epoch 14/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2720\n",
      "Epoch 00014: val_loss improved from 0.19437 to 0.18752, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2715 - val_loss: 0.1875\n",
      "Epoch 15/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2702\n",
      "Epoch 00015: val_loss improved from 0.18752 to 0.18323, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2703 - val_loss: 0.1832\n",
      "Epoch 16/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2495\n",
      "Epoch 00016: val_loss improved from 0.18323 to 0.17738, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2500 - val_loss: 0.1774\n",
      "Epoch 17/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2480\n",
      "Epoch 00017: val_loss improved from 0.17738 to 0.17313, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2481 - val_loss: 0.1731\n",
      "Epoch 18/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2439\n",
      "Epoch 00018: val_loss improved from 0.17313 to 0.16658, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2439 - val_loss: 0.1666\n",
      "Epoch 19/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2474\n",
      "Epoch 00019: val_loss improved from 0.16658 to 0.16406, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2474 - val_loss: 0.1641\n",
      "Epoch 20/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2336\n",
      "Epoch 00020: val_loss improved from 0.16406 to 0.16054, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2335 - val_loss: 0.1605\n",
      "Epoch 21/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2333\n",
      "Epoch 00021: val_loss improved from 0.16054 to 0.15640, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2331 - val_loss: 0.1564\n",
      "Epoch 22/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2243\n",
      "Epoch 00022: val_loss improved from 0.15640 to 0.15244, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2243 - val_loss: 0.1524\n",
      "Epoch 23/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2265\n",
      "Epoch 00023: val_loss improved from 0.15244 to 0.15091, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2265 - val_loss: 0.1509\n",
      "Epoch 24/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2284\n",
      "Epoch 00024: val_loss improved from 0.15091 to 0.15009, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2292 - val_loss: 0.1501\n",
      "Epoch 25/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2175\n",
      "Epoch 00025: val_loss improved from 0.15009 to 0.14699, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2175 - val_loss: 0.1470\n",
      "Epoch 26/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2145\n",
      "Epoch 00026: val_loss improved from 0.14699 to 0.14530, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2148 - val_loss: 0.1453\n",
      "Epoch 27/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2176\n",
      "Epoch 00027: val_loss improved from 0.14530 to 0.14445, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2173 - val_loss: 0.1444\n",
      "Epoch 28/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2152\n",
      "Epoch 00028: val_loss improved from 0.14445 to 0.14245, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2152 - val_loss: 0.1424\n",
      "Epoch 29/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2138\n",
      "Epoch 00029: val_loss improved from 0.14245 to 0.14245, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2138 - val_loss: 0.1424\n",
      "Epoch 30/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2140\n",
      "Epoch 00030: val_loss improved from 0.14245 to 0.14014, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2140 - val_loss: 0.1401\n",
      "Epoch 31/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2091\n",
      "Epoch 00031: val_loss improved from 0.14014 to 0.13913, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2089 - val_loss: 0.1391\n",
      "Epoch 32/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2103\n",
      "Epoch 00032: val_loss did not improve from 0.13913\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2103 - val_loss: 0.1400\n",
      "Epoch 33/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2097\n",
      "Epoch 00033: val_loss improved from 0.13913 to 0.13836, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2095 - val_loss: 0.1384\n",
      "Epoch 34/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2082\n",
      "Epoch 00034: val_loss did not improve from 0.13836\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2082 - val_loss: 0.1387\n",
      "Epoch 35/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00035: val_loss improved from 0.13836 to 0.13604, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2061 - val_loss: 0.1360\n",
      "Epoch 36/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2002\n",
      "Epoch 00036: val_loss improved from 0.13604 to 0.13527, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2016 - val_loss: 0.1353\n",
      "Epoch 37/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2032\n",
      "Epoch 00037: val_loss did not improve from 0.13527\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2032 - val_loss: 0.1354\n",
      "Epoch 38/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.1986\n",
      "Epoch 00038: val_loss improved from 0.13527 to 0.13478, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.1986 - val_loss: 0.1348\n",
      "Epoch 39/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2000\n",
      "Epoch 00039: val_loss improved from 0.13478 to 0.13473, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.1998 - val_loss: 0.1347\n",
      "Epoch 40/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.1982\n",
      "Epoch 00040: val_loss improved from 0.13473 to 0.13353, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_0.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1983 - val_loss: 0.1335\n",
      "Epoch 41/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2051\n",
      "Epoch 00041: val_loss did not improve from 0.13353\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2050 - val_loss: 0.1368\n",
      "Epoch 42/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2017\n",
      "Epoch 00042: val_loss did not improve from 0.13353\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2018 - val_loss: 0.1349\n",
      " ###0 fold : val acc1 0.572, acc3 0.963, mae 0.237###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/39 [=================>............] - ETA: 0s - loss: 19.2143 \n",
      "Epoch 00001: val_loss improved from inf to 11.91006, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 17.3688 - val_loss: 11.9101\n",
      "Epoch 2/100\n",
      "25/39 [==================>...........] - ETA: 0s - loss: 9.9058 \n",
      "Epoch 00002: val_loss improved from 11.91006 to 5.16783, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 8.6588 - val_loss: 5.1678\n",
      "Epoch 3/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 4.0412\n",
      "Epoch 00003: val_loss improved from 5.16783 to 2.10599, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 4.0196 - val_loss: 2.1060\n",
      "Epoch 4/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 1.8343\n",
      "Epoch 00004: val_loss improved from 2.10599 to 1.11809, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.9651 - val_loss: 1.1181\n",
      "Epoch 5/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 1.5401\n",
      "Epoch 00005: val_loss improved from 1.11809 to 0.68693, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.2624 - val_loss: 0.6869\n",
      "Epoch 6/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.6822\n",
      "Epoch 00006: val_loss improved from 0.68693 to 0.46278, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.8063 - val_loss: 0.4628\n",
      "Epoch 7/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.4940\n",
      "Epoch 00007: val_loss improved from 0.46278 to 0.34326, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.6512 - val_loss: 0.3433\n",
      "Epoch 8/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3894\n",
      "Epoch 00008: val_loss improved from 0.34326 to 0.27735, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4609 - val_loss: 0.2774\n",
      "Epoch 9/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.4357\n",
      "Epoch 00009: val_loss improved from 0.27735 to 0.24104, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3879 - val_loss: 0.2410\n",
      "Epoch 10/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2923\n",
      "Epoch 00010: val_loss improved from 0.24104 to 0.21963, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3258 - val_loss: 0.2196\n",
      "Epoch 11/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.3418\n",
      "Epoch 00011: val_loss improved from 0.21963 to 0.20650, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3163 - val_loss: 0.2065\n",
      "Epoch 12/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2699\n",
      "Epoch 00012: val_loss improved from 0.20650 to 0.19851, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3126 - val_loss: 0.1985\n",
      "Epoch 13/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2560\n",
      "Epoch 00013: val_loss improved from 0.19851 to 0.19155, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2689 - val_loss: 0.1915\n",
      "Epoch 14/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2951\n",
      "Epoch 00014: val_loss improved from 0.19155 to 0.18422, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2740 - val_loss: 0.1842\n",
      "Epoch 15/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2812\n",
      "Epoch 00015: val_loss improved from 0.18422 to 0.17940, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2710 - val_loss: 0.1794\n",
      "Epoch 16/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2523\n",
      "Epoch 00016: val_loss improved from 0.17940 to 0.17361, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2510 - val_loss: 0.1736\n",
      "Epoch 17/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2479\n",
      "Epoch 00017: val_loss improved from 0.17361 to 0.16952, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2494 - val_loss: 0.1695\n",
      "Epoch 18/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2441\n",
      "Epoch 00018: val_loss improved from 0.16952 to 0.16325, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2441 - val_loss: 0.1633\n",
      "Epoch 19/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2505\n",
      "Epoch 00019: val_loss improved from 0.16325 to 0.16107, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2505 - val_loss: 0.1611\n",
      "Epoch 20/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2292\n",
      "Epoch 00020: val_loss improved from 0.16107 to 0.15740, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2325 - val_loss: 0.1574\n",
      "Epoch 21/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2335\n",
      "Epoch 00021: val_loss improved from 0.15740 to 0.15337, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2335 - val_loss: 0.1534\n",
      "Epoch 22/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2233\n",
      "Epoch 00022: val_loss improved from 0.15337 to 0.15004, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2222 - val_loss: 0.1500\n",
      "Epoch 23/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2274\n",
      "Epoch 00023: val_loss improved from 0.15004 to 0.14835, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2274 - val_loss: 0.1483\n",
      "Epoch 24/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2285\n",
      "Epoch 00024: val_loss improved from 0.14835 to 0.14761, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2285 - val_loss: 0.1476\n",
      "Epoch 25/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2208\n",
      "Epoch 00025: val_loss improved from 0.14761 to 0.14451, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2208 - val_loss: 0.1445\n",
      "Epoch 26/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2196\n",
      "Epoch 00026: val_loss improved from 0.14451 to 0.14295, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2163 - val_loss: 0.1429\n",
      "Epoch 27/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2138\n",
      "Epoch 00027: val_loss did not improve from 0.14295\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2195 - val_loss: 0.1433\n",
      "Epoch 28/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2152\n",
      "Epoch 00028: val_loss improved from 0.14295 to 0.14114, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2167 - val_loss: 0.1411\n",
      "Epoch 29/100\n",
      "20/39 [==============>...............] - ETA: 0s - loss: 0.2096\n",
      "Epoch 00029: val_loss improved from 0.14114 to 0.14098, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2141 - val_loss: 0.1410\n",
      "Epoch 30/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2188\n",
      "Epoch 00030: val_loss improved from 0.14098 to 0.13834, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2150 - val_loss: 0.1383\n",
      "Epoch 31/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2131\n",
      "Epoch 00031: val_loss improved from 0.13834 to 0.13786, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2109 - val_loss: 0.1379\n",
      "Epoch 32/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2136\n",
      "Epoch 00032: val_loss did not improve from 0.13786\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2112 - val_loss: 0.1387\n",
      "Epoch 33/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2093\n",
      "Epoch 00033: val_loss improved from 0.13786 to 0.13726, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2095 - val_loss: 0.1373\n",
      "Epoch 34/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2086\n",
      "Epoch 00034: val_loss did not improve from 0.13726\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2085 - val_loss: 0.1375\n",
      "Epoch 35/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2109\n",
      "Epoch 00035: val_loss improved from 0.13726 to 0.13505, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2074 - val_loss: 0.1350\n",
      "Epoch 36/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2009\n",
      "Epoch 00036: val_loss improved from 0.13505 to 0.13444, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2017 - val_loss: 0.1344\n",
      "Epoch 37/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1995\n",
      "Epoch 00037: val_loss did not improve from 0.13444\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2036 - val_loss: 0.1345\n",
      "Epoch 38/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2004\n",
      "Epoch 00038: val_loss improved from 0.13444 to 0.13374, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2004 - val_loss: 0.1337\n",
      "Epoch 39/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2073\n",
      "Epoch 00039: val_loss improved from 0.13374 to 0.13364, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2009 - val_loss: 0.1336\n",
      "Epoch 40/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1953\n",
      "Epoch 00040: val_loss improved from 0.13364 to 0.13276, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_1.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1984 - val_loss: 0.1328\n",
      "Epoch 41/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2022\n",
      "Epoch 00041: val_loss did not improve from 0.13276\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2054 - val_loss: 0.1362\n",
      "Epoch 42/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2011\n",
      "Epoch 00042: val_loss did not improve from 0.13276\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2035 - val_loss: 0.1339\n",
      " ###1 fold : val acc1 0.573, acc3 0.959, mae 0.237###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/39 [=================>............] - ETA: 0s - loss: 19.2447 \n",
      "Epoch 00001: val_loss improved from inf to 11.90548, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 17.3720 - val_loss: 11.9055\n",
      "Epoch 2/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 10.0137\n",
      "Epoch 00002: val_loss improved from 11.90548 to 5.17617, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.6436 - val_loss: 5.1762\n",
      "Epoch 3/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 4.1002\n",
      "Epoch 00003: val_loss improved from 5.17617 to 2.12118, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 4.0264 - val_loss: 2.1212\n",
      "Epoch 4/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 1.8292\n",
      "Epoch 00004: val_loss improved from 2.12118 to 1.12702, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.9732 - val_loss: 1.1270\n",
      "Epoch 5/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 1.5549\n",
      "Epoch 00005: val_loss improved from 1.12702 to 0.68961, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.2714 - val_loss: 0.6896\n",
      "Epoch 6/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.6851\n",
      "Epoch 00006: val_loss improved from 0.68961 to 0.46419, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.8099 - val_loss: 0.4642\n",
      "Epoch 7/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.4962\n",
      "Epoch 00007: val_loss improved from 0.46419 to 0.34520, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.6533 - val_loss: 0.3452\n",
      "Epoch 8/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.4594\n",
      "Epoch 00008: val_loss improved from 0.34520 to 0.27873, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.4594 - val_loss: 0.2787\n",
      "Epoch 9/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.4444\n",
      "Epoch 00009: val_loss improved from 0.27873 to 0.24223, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3881 - val_loss: 0.2422\n",
      "Epoch 10/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2939\n",
      "Epoch 00010: val_loss improved from 0.24223 to 0.22105, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3241 - val_loss: 0.2211\n",
      "Epoch 11/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.3401\n",
      "Epoch 00011: val_loss improved from 0.22105 to 0.20799, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3146 - val_loss: 0.2080\n",
      "Epoch 12/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2739\n",
      "Epoch 00012: val_loss improved from 0.20799 to 0.20009, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3139 - val_loss: 0.2001\n",
      "Epoch 13/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2559\n",
      "Epoch 00013: val_loss improved from 0.20009 to 0.19327, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2688 - val_loss: 0.1933\n",
      "Epoch 14/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2941\n",
      "Epoch 00014: val_loss improved from 0.19327 to 0.18608, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2747 - val_loss: 0.1861\n",
      "Epoch 15/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2814\n",
      "Epoch 00015: val_loss improved from 0.18608 to 0.18142, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2717 - val_loss: 0.1814\n",
      "Epoch 16/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2504\n",
      "Epoch 00016: val_loss improved from 0.18142 to 0.17562, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2519 - val_loss: 0.1756\n",
      "Epoch 17/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2468\n",
      "Epoch 00017: val_loss improved from 0.17562 to 0.17111, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2499 - val_loss: 0.1711\n",
      "Epoch 18/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2535\n",
      "Epoch 00018: val_loss improved from 0.17111 to 0.16488, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2440 - val_loss: 0.1649\n",
      "Epoch 19/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2613\n",
      "Epoch 00019: val_loss improved from 0.16488 to 0.16244, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2488 - val_loss: 0.1624\n",
      "Epoch 20/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2335\n",
      "Epoch 00020: val_loss improved from 0.16244 to 0.15912, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2332 - val_loss: 0.1591\n",
      "Epoch 21/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2306\n",
      "Epoch 00021: val_loss improved from 0.15912 to 0.15493, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2342 - val_loss: 0.1549\n",
      "Epoch 22/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2235\n",
      "Epoch 00022: val_loss improved from 0.15493 to 0.15128, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2223 - val_loss: 0.1513\n",
      "Epoch 23/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2214\n",
      "Epoch 00023: val_loss improved from 0.15128 to 0.14963, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2281 - val_loss: 0.1496\n",
      "Epoch 24/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2284\n",
      "Epoch 00024: val_loss improved from 0.14963 to 0.14903, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2268 - val_loss: 0.1490\n",
      "Epoch 25/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2170\n",
      "Epoch 00025: val_loss improved from 0.14903 to 0.14563, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2203 - val_loss: 0.1456\n",
      "Epoch 26/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2211\n",
      "Epoch 00026: val_loss improved from 0.14563 to 0.14386, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2153 - val_loss: 0.1439\n",
      "Epoch 27/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2116\n",
      "Epoch 00027: val_loss did not improve from 0.14386\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2192 - val_loss: 0.1443\n",
      "Epoch 28/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2158\n",
      "Epoch 00028: val_loss improved from 0.14386 to 0.14189, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2180 - val_loss: 0.1419\n",
      "Epoch 29/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2148\n",
      "Epoch 00029: val_loss improved from 0.14189 to 0.14177, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2148 - val_loss: 0.1418\n",
      "Epoch 30/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2139\n",
      "Epoch 00030: val_loss improved from 0.14177 to 0.13888, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2145 - val_loss: 0.1389\n",
      "Epoch 31/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2090\n",
      "Epoch 00031: val_loss improved from 0.13888 to 0.13867, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2086 - val_loss: 0.1387\n",
      "Epoch 32/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2119\n",
      "Epoch 00032: val_loss did not improve from 0.13867\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2119 - val_loss: 0.1391\n",
      "Epoch 33/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2084\n",
      "Epoch 00033: val_loss improved from 0.13867 to 0.13747, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2098 - val_loss: 0.1375\n",
      "Epoch 34/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2071\n",
      "Epoch 00034: val_loss did not improve from 0.13747\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2089 - val_loss: 0.1379\n",
      "Epoch 35/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2084\n",
      "Epoch 00035: val_loss improved from 0.13747 to 0.13532, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2084 - val_loss: 0.1353\n",
      "Epoch 36/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2039\n",
      "Epoch 00036: val_loss did not improve from 0.13532\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2039 - val_loss: 0.1355\n",
      "Epoch 37/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2037\n",
      "Epoch 00037: val_loss improved from 0.13532 to 0.13523, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2037 - val_loss: 0.1352\n",
      "Epoch 38/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2037\n",
      "Epoch 00038: val_loss improved from 0.13523 to 0.13449, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2007 - val_loss: 0.1345\n",
      "Epoch 39/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2084\n",
      "Epoch 00039: val_loss improved from 0.13449 to 0.13380, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2026 - val_loss: 0.1338\n",
      "Epoch 40/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1938\n",
      "Epoch 00040: val_loss improved from 0.13380 to 0.13359, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_2.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1992 - val_loss: 0.1336\n",
      "Epoch 41/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2047\n",
      "Epoch 00041: val_loss did not improve from 0.13359\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2058 - val_loss: 0.1366\n",
      "Epoch 42/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2032\n",
      "Epoch 00042: val_loss did not improve from 0.13359\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2032 - val_loss: 0.1338\n",
      " ###2 fold : val acc1 0.569, acc3 0.961, mae 0.239###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/39 [================>.............] - ETA: 0s - loss: 19.3631 \n",
      "Epoch 00001: val_loss improved from inf to 11.91448, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 17.3509 - val_loss: 11.9145\n",
      "Epoch 2/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 10.2357\n",
      "Epoch 00002: val_loss improved from 11.91448 to 5.18980, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.6475 - val_loss: 5.1898\n",
      "Epoch 3/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 4.1154\n",
      "Epoch 00003: val_loss improved from 5.18980 to 2.12203, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 4.0322 - val_loss: 2.1220\n",
      "Epoch 4/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 1.8382\n",
      "Epoch 00004: val_loss improved from 2.12203 to 1.12048, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.9625 - val_loss: 1.1205\n",
      "Epoch 5/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 1.5515\n",
      "Epoch 00005: val_loss improved from 1.12048 to 0.68468, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.2645 - val_loss: 0.6847\n",
      "Epoch 6/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.6792\n",
      "Epoch 00006: val_loss improved from 0.68468 to 0.46394, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.8068 - val_loss: 0.4639\n",
      "Epoch 7/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.4909\n",
      "Epoch 00007: val_loss improved from 0.46394 to 0.34590, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.6537 - val_loss: 0.3459\n",
      "Epoch 8/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3828\n",
      "Epoch 00008: val_loss improved from 0.34590 to 0.27959, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4581 - val_loss: 0.2796\n",
      "Epoch 9/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.4450\n",
      "Epoch 00009: val_loss improved from 0.27959 to 0.24336, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3888 - val_loss: 0.2434\n",
      "Epoch 10/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2941\n",
      "Epoch 00010: val_loss improved from 0.24336 to 0.22217, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3258 - val_loss: 0.2222\n",
      "Epoch 11/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.3441\n",
      "Epoch 00011: val_loss improved from 0.22217 to 0.20911, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3182 - val_loss: 0.2091\n",
      "Epoch 12/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2712\n",
      "Epoch 00012: val_loss improved from 0.20911 to 0.20109, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3131 - val_loss: 0.2011\n",
      "Epoch 13/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2589\n",
      "Epoch 00013: val_loss improved from 0.20109 to 0.19404, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2704 - val_loss: 0.1940\n",
      "Epoch 14/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2955\n",
      "Epoch 00014: val_loss improved from 0.19404 to 0.18688, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2783 - val_loss: 0.1869\n",
      "Epoch 15/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2833\n",
      "Epoch 00015: val_loss improved from 0.18688 to 0.18199, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2729 - val_loss: 0.1820\n",
      "Epoch 16/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2498\n",
      "Epoch 00016: val_loss improved from 0.18199 to 0.17610, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2537 - val_loss: 0.1761\n",
      "Epoch 17/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2469\n",
      "Epoch 00017: val_loss improved from 0.17610 to 0.17137, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2516 - val_loss: 0.1714\n",
      "Epoch 18/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2459\n",
      "Epoch 00018: val_loss improved from 0.17137 to 0.16501, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2458 - val_loss: 0.1650\n",
      "Epoch 19/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2513\n",
      "Epoch 00019: val_loss improved from 0.16501 to 0.16292, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2508 - val_loss: 0.1629\n",
      "Epoch 20/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2363\n",
      "Epoch 00020: val_loss improved from 0.16292 to 0.15958, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2352 - val_loss: 0.1596\n",
      "Epoch 21/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2358\n",
      "Epoch 00021: val_loss improved from 0.15958 to 0.15529, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2356 - val_loss: 0.1553\n",
      "Epoch 22/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2242\n",
      "Epoch 00022: val_loss improved from 0.15529 to 0.15152, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2240 - val_loss: 0.1515\n",
      "Epoch 23/100\n",
      "20/39 [==============>...............] - ETA: 0s - loss: 0.2219\n",
      "Epoch 00023: val_loss improved from 0.15152 to 0.14973, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2272 - val_loss: 0.1497\n",
      "Epoch 24/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2272\n",
      "Epoch 00024: val_loss improved from 0.14973 to 0.14886, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2272 - val_loss: 0.1489\n",
      "Epoch 25/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2217\n",
      "Epoch 00025: val_loss improved from 0.14886 to 0.14548, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2218 - val_loss: 0.1455\n",
      "Epoch 26/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2166\n",
      "Epoch 00026: val_loss improved from 0.14548 to 0.14376, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2171 - val_loss: 0.1438\n",
      "Epoch 27/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2198\n",
      "Epoch 00027: val_loss did not improve from 0.14376\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2192 - val_loss: 0.1438\n",
      "Epoch 28/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2177\n",
      "Epoch 00028: val_loss improved from 0.14376 to 0.14189, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2175 - val_loss: 0.1419\n",
      "Epoch 29/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2156\n",
      "Epoch 00029: val_loss improved from 0.14189 to 0.14134, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2156 - val_loss: 0.1413\n",
      "Epoch 30/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2148\n",
      "Epoch 00030: val_loss improved from 0.14134 to 0.13885, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2148 - val_loss: 0.1389\n",
      "Epoch 31/100\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.2094\n",
      "Epoch 00031: val_loss improved from 0.13885 to 0.13820, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2097 - val_loss: 0.1382\n",
      "Epoch 32/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2119\n",
      "Epoch 00032: val_loss did not improve from 0.13820\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2120 - val_loss: 0.1390\n",
      "Epoch 33/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2113\n",
      "Epoch 00033: val_loss improved from 0.13820 to 0.13740, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2104 - val_loss: 0.1374\n",
      "Epoch 34/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2074\n",
      "Epoch 00034: val_loss did not improve from 0.13740\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2072 - val_loss: 0.1377\n",
      "Epoch 35/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2088\n",
      "Epoch 00035: val_loss improved from 0.13740 to 0.13522, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2089 - val_loss: 0.1352\n",
      "Epoch 36/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2039\n",
      "Epoch 00036: val_loss did not improve from 0.13522\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2039 - val_loss: 0.1353\n",
      "Epoch 37/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2052\n",
      "Epoch 00037: val_loss improved from 0.13522 to 0.13484, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2049 - val_loss: 0.1348\n",
      "Epoch 38/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2003\n",
      "Epoch 00038: val_loss improved from 0.13484 to 0.13422, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2003 - val_loss: 0.1342\n",
      "Epoch 39/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2033\n",
      "Epoch 00039: val_loss improved from 0.13422 to 0.13363, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_3.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2036 - val_loss: 0.1336\n",
      "Epoch 40/100\n",
      "20/39 [==============>...............] - ETA: 0s - loss: 0.1958\n",
      "Epoch 00040: val_loss did not improve from 0.13363\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1993 - val_loss: 0.1339\n",
      "Epoch 41/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2074\n",
      "Epoch 00041: val_loss did not improve from 0.13363\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2073 - val_loss: 0.1358\n",
      " ###3 fold : val acc1 0.576, acc3 0.966, mae 0.233###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/39 [=================>............] - ETA: 0s - loss: 19.4206 \n",
      "Epoch 00001: val_loss improved from inf to 11.93648, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 17.3475 - val_loss: 11.9365\n",
      "Epoch 2/100\n",
      "26/39 [===================>..........] - ETA: 0s - loss: 9.4405 \n",
      "Epoch 00002: val_loss improved from 11.93648 to 5.08493, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.7025 - val_loss: 5.0849\n",
      "Epoch 3/100\n",
      "25/39 [==================>...........] - ETA: 0s - loss: 4.7906\n",
      "Epoch 00003: val_loss improved from 5.08493 to 2.15874, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 4.0123 - val_loss: 2.1587\n",
      "Epoch 4/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 2.6048\n",
      "Epoch 00004: val_loss improved from 2.15874 to 1.12560, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 2.1037 - val_loss: 1.1256\n",
      "Epoch 5/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 1.5266\n",
      "Epoch 00005: val_loss improved from 1.12560 to 0.68659, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.2498 - val_loss: 0.6866\n",
      "Epoch 6/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.9854\n",
      "Epoch 00006: val_loss improved from 0.68659 to 0.46547, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.8170 - val_loss: 0.4655\n",
      "Epoch 7/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.7591\n",
      "Epoch 00007: val_loss improved from 0.46547 to 0.34745, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.6312 - val_loss: 0.3474\n",
      "Epoch 8/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.6336\n",
      "Epoch 00008: val_loss improved from 0.34745 to 0.28208, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.5160 - val_loss: 0.2821\n",
      "Epoch 9/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3400\n",
      "Epoch 00009: val_loss improved from 0.28208 to 0.24586, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4221 - val_loss: 0.2459\n",
      "Epoch 10/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.3066\n",
      "Epoch 00010: val_loss improved from 0.24586 to 0.22523, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3606 - val_loss: 0.2252\n",
      "Epoch 11/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2902\n",
      "Epoch 00011: val_loss improved from 0.22523 to 0.21104, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2952 - val_loss: 0.2110\n",
      "Epoch 12/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2756\n",
      "Epoch 00012: val_loss improved from 0.21104 to 0.20093, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2951 - val_loss: 0.2009\n",
      "Epoch 13/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2920\n",
      "Epoch 00013: val_loss improved from 0.20093 to 0.19525, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2854 - val_loss: 0.1953\n",
      "Epoch 14/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2767\n",
      "Epoch 00014: val_loss improved from 0.19525 to 0.18784, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2682 - val_loss: 0.1878\n",
      "Epoch 15/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2485\n",
      "Epoch 00015: val_loss improved from 0.18784 to 0.18150, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2528 - val_loss: 0.1815\n",
      "Epoch 16/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2726\n",
      "Epoch 00016: val_loss improved from 0.18150 to 0.17646, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2616 - val_loss: 0.1765\n",
      "Epoch 17/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2458\n",
      "Epoch 00017: val_loss improved from 0.17646 to 0.17164, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2468 - val_loss: 0.1716\n",
      "Epoch 18/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2482\n",
      "Epoch 00018: val_loss improved from 0.17164 to 0.16783, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2478 - val_loss: 0.1678\n",
      "Epoch 19/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2373\n",
      "Epoch 00019: val_loss improved from 0.16783 to 0.16339, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2373 - val_loss: 0.1634\n",
      "Epoch 20/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2447\n",
      "Epoch 00020: val_loss improved from 0.16339 to 0.16097, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2442 - val_loss: 0.1610\n",
      "Epoch 21/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2339\n",
      "Epoch 00021: val_loss improved from 0.16097 to 0.15666, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2336 - val_loss: 0.1567\n",
      "Epoch 22/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2245\n",
      "Epoch 00022: val_loss improved from 0.15666 to 0.15271, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2243 - val_loss: 0.1527\n",
      "Epoch 23/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2253\n",
      "Epoch 00023: val_loss improved from 0.15271 to 0.14985, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2253 - val_loss: 0.1499\n",
      "Epoch 24/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2229\n",
      "Epoch 00024: val_loss improved from 0.14985 to 0.14952, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2370 - val_loss: 0.1495\n",
      "Epoch 25/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2203\n",
      "Epoch 00025: val_loss improved from 0.14952 to 0.14714, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2237 - val_loss: 0.1471\n",
      "Epoch 26/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2228\n",
      "Epoch 00026: val_loss improved from 0.14714 to 0.14551, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2250 - val_loss: 0.1455\n",
      "Epoch 27/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2177\n",
      "Epoch 00027: val_loss improved from 0.14551 to 0.14519, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2177 - val_loss: 0.1452\n",
      "Epoch 28/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2153\n",
      "Epoch 00028: val_loss improved from 0.14519 to 0.14381, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2147 - val_loss: 0.1438\n",
      "Epoch 29/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2152\n",
      "Epoch 00029: val_loss improved from 0.14381 to 0.14305, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2181 - val_loss: 0.1430\n",
      "Epoch 30/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2137\n",
      "Epoch 00030: val_loss improved from 0.14305 to 0.14034, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2137 - val_loss: 0.1403\n",
      "Epoch 31/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2119\n",
      "Epoch 00031: val_loss improved from 0.14034 to 0.13974, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2116 - val_loss: 0.1397\n",
      "Epoch 32/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2198\n",
      "Epoch 00032: val_loss improved from 0.13974 to 0.13757, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2198 - val_loss: 0.1376\n",
      "Epoch 33/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2112\n",
      "Epoch 00033: val_loss improved from 0.13757 to 0.13683, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2112 - val_loss: 0.1368\n",
      "Epoch 34/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2060\n",
      "Epoch 00034: val_loss improved from 0.13683 to 0.13566, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2103 - val_loss: 0.1357\n",
      "Epoch 35/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2195\n",
      "Epoch 00035: val_loss did not improve from 0.13566\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2195 - val_loss: 0.1364\n",
      "Epoch 36/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00036: val_loss improved from 0.13566 to 0.13518, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2059 - val_loss: 0.1352\n",
      "Epoch 37/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00037: val_loss improved from 0.13518 to 0.13460, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2027 - val_loss: 0.1346\n",
      "Epoch 38/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2022\n",
      "Epoch 00038: val_loss improved from 0.13460 to 0.13431, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2022 - val_loss: 0.1343\n",
      "Epoch 39/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2067\n",
      "Epoch 00039: val_loss improved from 0.13431 to 0.13311, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_4.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2067 - val_loss: 0.1331\n",
      "Epoch 40/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2010\n",
      "Epoch 00040: val_loss did not improve from 0.13311\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2201 - val_loss: 0.1373\n",
      "Epoch 41/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2047\n",
      "Epoch 00041: val_loss did not improve from 0.13311\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2047 - val_loss: 0.1338\n",
      " ###4 fold : val acc1 0.585, acc3 0.967, mae 0.228###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/39 [=================>............] - ETA: 0s - loss: 19.2423 \n",
      "Epoch 00001: val_loss improved from inf to 11.70893, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 17.1806 - val_loss: 11.7089\n",
      "Epoch 2/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 9.4457 \n",
      "Epoch 00002: val_loss improved from 11.70893 to 4.81905, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.0612 - val_loss: 4.8190\n",
      "Epoch 3/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 3.8867\n",
      "Epoch 00003: val_loss improved from 4.81905 to 2.08681, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.3325 - val_loss: 2.0868\n",
      "Epoch 4/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 1.8037\n",
      "Epoch 00004: val_loss improved from 2.08681 to 1.10059, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.6141 - val_loss: 1.1006\n",
      "Epoch 5/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 1.0235\n",
      "Epoch 00005: val_loss improved from 1.10059 to 0.64250, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.9291 - val_loss: 0.6425\n",
      "Epoch 6/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.6325\n",
      "Epoch 00006: val_loss improved from 0.64250 to 0.41581, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.5867 - val_loss: 0.4158\n",
      "Epoch 7/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.4410\n",
      "Epoch 00007: val_loss improved from 0.41581 to 0.30182, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4170 - val_loss: 0.3018\n",
      "Epoch 8/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.3415\n",
      "Epoch 00008: val_loss improved from 0.30182 to 0.24596, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3337 - val_loss: 0.2460\n",
      "Epoch 9/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3022\n",
      "Epoch 00009: val_loss improved from 0.24596 to 0.21878, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2942 - val_loss: 0.2188\n",
      "Epoch 10/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2808\n",
      "Epoch 00010: val_loss improved from 0.21878 to 0.20300, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2758 - val_loss: 0.2030\n",
      "Epoch 11/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2683\n",
      "Epoch 00011: val_loss improved from 0.20300 to 0.19254, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2647 - val_loss: 0.1925\n",
      "Epoch 12/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.2611\n",
      "Epoch 00012: val_loss improved from 0.19254 to 0.18524, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2563 - val_loss: 0.1852\n",
      "Epoch 13/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.2471\n",
      "Epoch 00013: val_loss improved from 0.18524 to 0.17893, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2493 - val_loss: 0.1789\n",
      "Epoch 14/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2478\n",
      "Epoch 00014: val_loss improved from 0.17893 to 0.17336, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2440 - val_loss: 0.1734\n",
      "Epoch 15/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2397\n",
      "Epoch 00015: val_loss improved from 0.17336 to 0.16909, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2391 - val_loss: 0.1691\n",
      "Epoch 16/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2402\n",
      "Epoch 00016: val_loss improved from 0.16909 to 0.16502, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2366 - val_loss: 0.1650\n",
      "Epoch 17/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2346\n",
      "Epoch 00017: val_loss improved from 0.16502 to 0.16147, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2343 - val_loss: 0.1615\n",
      "Epoch 18/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2344\n",
      "Epoch 00018: val_loss improved from 0.16147 to 0.15899, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2308 - val_loss: 0.1590\n",
      "Epoch 19/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2281\n",
      "Epoch 00019: val_loss improved from 0.15899 to 0.15579, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2281 - val_loss: 0.1558\n",
      "Epoch 20/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2216\n",
      "Epoch 00020: val_loss improved from 0.15579 to 0.15373, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2245 - val_loss: 0.1537\n",
      "Epoch 21/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2275\n",
      "Epoch 00021: val_loss improved from 0.15373 to 0.15166, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2271 - val_loss: 0.1517\n",
      "Epoch 22/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2183\n",
      "Epoch 00022: val_loss improved from 0.15166 to 0.14943, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2183 - val_loss: 0.1494\n",
      "Epoch 23/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2216\n",
      "Epoch 00023: val_loss improved from 0.14943 to 0.14773, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2207 - val_loss: 0.1477\n",
      "Epoch 24/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2168\n",
      "Epoch 00024: val_loss improved from 0.14773 to 0.14697, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2177 - val_loss: 0.1470\n",
      "Epoch 25/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2157\n",
      "Epoch 00025: val_loss improved from 0.14697 to 0.14488, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2157 - val_loss: 0.1449\n",
      "Epoch 26/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2144\n",
      "Epoch 00026: val_loss improved from 0.14488 to 0.14366, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2143 - val_loss: 0.1437\n",
      "Epoch 27/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2092\n",
      "Epoch 00027: val_loss improved from 0.14366 to 0.14290, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2092 - val_loss: 0.1429\n",
      "Epoch 28/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2086\n",
      "Epoch 00028: val_loss improved from 0.14290 to 0.14182, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2091 - val_loss: 0.1418\n",
      "Epoch 29/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2112\n",
      "Epoch 00029: val_loss improved from 0.14182 to 0.14067, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2082 - val_loss: 0.1407\n",
      "Epoch 30/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2065\n",
      "Epoch 00030: val_loss improved from 0.14067 to 0.13987, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2075 - val_loss: 0.1399\n",
      "Epoch 31/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2088\n",
      "Epoch 00031: val_loss improved from 0.13987 to 0.13907, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2061 - val_loss: 0.1391\n",
      "Epoch 32/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2073\n",
      "Epoch 00032: val_loss improved from 0.13907 to 0.13856, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2077 - val_loss: 0.1386\n",
      "Epoch 33/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2047\n",
      "Epoch 00033: val_loss improved from 0.13856 to 0.13797, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2065 - val_loss: 0.1380\n",
      "Epoch 34/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2048\n",
      "Epoch 00034: val_loss improved from 0.13797 to 0.13706, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2048 - val_loss: 0.1371\n",
      "Epoch 35/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2041\n",
      "Epoch 00035: val_loss improved from 0.13706 to 0.13632, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2025 - val_loss: 0.1363\n",
      "Epoch 36/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1991\n",
      "Epoch 00036: val_loss improved from 0.13632 to 0.13628, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2027 - val_loss: 0.1363\n",
      "Epoch 37/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2052\n",
      "Epoch 00037: val_loss improved from 0.13628 to 0.13563, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2012 - val_loss: 0.1356\n",
      "Epoch 38/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1990\n",
      "Epoch 00038: val_loss improved from 0.13563 to 0.13530, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1990 - val_loss: 0.1353\n",
      "Epoch 39/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2040\n",
      "Epoch 00039: val_loss improved from 0.13530 to 0.13472, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1998 - val_loss: 0.1347\n",
      "Epoch 40/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1976\n",
      "Epoch 00040: val_loss did not improve from 0.13472\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1976 - val_loss: 0.1348\n",
      "Epoch 41/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.1981\n",
      "Epoch 00041: val_loss improved from 0.13472 to 0.13415, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.1995 - val_loss: 0.1341\n",
      "Epoch 42/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.1987\n",
      "Epoch 00042: val_loss improved from 0.13415 to 0.13340, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.1987 - val_loss: 0.1334\n",
      "Epoch 43/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1971\n",
      "Epoch 00043: val_loss improved from 0.13340 to 0.13291, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1963 - val_loss: 0.1329\n",
      "Epoch 44/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1987\n",
      "Epoch 00044: val_loss improved from 0.13291 to 0.13252, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1963 - val_loss: 0.1325\n",
      "Epoch 45/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00045: val_loss improved from 0.13252 to 0.13221, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1941 - val_loss: 0.1322\n",
      "Epoch 46/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1943\n",
      "Epoch 00046: val_loss did not improve from 0.13221\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1951 - val_loss: 0.1322\n",
      "Epoch 47/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1973\n",
      "Epoch 00047: val_loss improved from 0.13221 to 0.13214, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1954 - val_loss: 0.1321\n",
      "Epoch 48/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1956\n",
      "Epoch 00048: val_loss did not improve from 0.13214\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1942 - val_loss: 0.1328\n",
      "Epoch 49/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1930\n",
      "Epoch 00049: val_loss improved from 0.13214 to 0.13206, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1930 - val_loss: 0.1321\n",
      "Epoch 50/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1917\n",
      "Epoch 00050: val_loss did not improve from 0.13206\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1917 - val_loss: 0.1323\n",
      "Epoch 51/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1917\n",
      "Epoch 00051: val_loss improved from 0.13206 to 0.13113, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1917 - val_loss: 0.1311\n",
      "Epoch 52/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1921\n",
      "Epoch 00052: val_loss did not improve from 0.13113\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1921 - val_loss: 0.1317\n",
      "Epoch 53/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1929\n",
      "Epoch 00053: val_loss improved from 0.13113 to 0.13109, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1929 - val_loss: 0.1311\n",
      "Epoch 54/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1915\n",
      "Epoch 00054: val_loss improved from 0.13109 to 0.13108, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_5.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1915 - val_loss: 0.1311\n",
      "Epoch 55/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1916\n",
      "Epoch 00055: val_loss did not improve from 0.13108\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1909 - val_loss: 0.1312\n",
      "Epoch 56/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.1854\n",
      "Epoch 00056: val_loss did not improve from 0.13108\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1873 - val_loss: 0.1311\n",
      " ###5 fold : val acc1 0.585, acc3 0.964, mae 0.237###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/39 [==================>...........] - ETA: 0s - loss: 19.2641 \n",
      "Epoch 00001: val_loss improved from inf to 12.05528, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 17.4205 - val_loss: 12.0553\n",
      "Epoch 2/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 9.6296 \n",
      "Epoch 00002: val_loss improved from 12.05528 to 5.06099, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.7732 - val_loss: 5.0610\n",
      "Epoch 3/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 4.4940\n",
      "Epoch 00003: val_loss improved from 5.06099 to 2.14004, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.7721 - val_loss: 2.1400\n",
      "Epoch 4/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 2.2884\n",
      "Epoch 00004: val_loss improved from 2.14004 to 1.10631, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.9299 - val_loss: 1.1063\n",
      "Epoch 5/100\n",
      "25/39 [==================>...........] - ETA: 0s - loss: 1.5872\n",
      "Epoch 00005: val_loss improved from 1.10631 to 0.67752, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3202 - val_loss: 0.6775\n",
      "Epoch 6/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 1.0767\n",
      "Epoch 00006: val_loss improved from 0.67752 to 0.45481, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.8814 - val_loss: 0.4548\n",
      "Epoch 7/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.4841\n",
      "Epoch 00007: val_loss improved from 0.45481 to 0.34038, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.6395 - val_loss: 0.3404\n",
      "Epoch 8/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.5508\n",
      "Epoch 00008: val_loss improved from 0.34038 to 0.27617, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4769 - val_loss: 0.2762\n",
      "Epoch 9/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.3327\n",
      "Epoch 00009: val_loss improved from 0.27617 to 0.24124, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3651 - val_loss: 0.2412\n",
      "Epoch 10/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.3798\n",
      "Epoch 00010: val_loss improved from 0.24124 to 0.22097, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3463 - val_loss: 0.2210\n",
      "Epoch 11/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.2839\n",
      "Epoch 00011: val_loss improved from 0.22097 to 0.20746, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3054 - val_loss: 0.2075\n",
      "Epoch 12/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.2956\n",
      "Epoch 00012: val_loss improved from 0.20746 to 0.19853, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2817 - val_loss: 0.1985\n",
      "Epoch 13/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.2918\n",
      "Epoch 00013: val_loss improved from 0.19853 to 0.19223, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2797 - val_loss: 0.1922\n",
      "Epoch 14/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2939\n",
      "Epoch 00014: val_loss improved from 0.19223 to 0.18596, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2779 - val_loss: 0.1860\n",
      "Epoch 15/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2520\n",
      "Epoch 00015: val_loss improved from 0.18596 to 0.17923, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2527 - val_loss: 0.1792\n",
      "Epoch 16/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.2706\n",
      "Epoch 00016: val_loss improved from 0.17923 to 0.17540, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2614 - val_loss: 0.1754\n",
      "Epoch 17/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.2484\n",
      "Epoch 00017: val_loss improved from 0.17540 to 0.16999, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2448 - val_loss: 0.1700\n",
      "Epoch 18/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2457\n",
      "Epoch 00018: val_loss improved from 0.16999 to 0.16646, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2442 - val_loss: 0.1665\n",
      "Epoch 19/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2390\n",
      "Epoch 00019: val_loss improved from 0.16646 to 0.16234, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2393 - val_loss: 0.1623\n",
      "Epoch 20/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2341\n",
      "Epoch 00020: val_loss improved from 0.16234 to 0.15902, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2337 - val_loss: 0.1590\n",
      "Epoch 21/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2380\n",
      "Epoch 00021: val_loss improved from 0.15902 to 0.15459, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2352 - val_loss: 0.1546\n",
      "Epoch 22/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2270\n",
      "Epoch 00022: val_loss improved from 0.15459 to 0.15064, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2250 - val_loss: 0.1506\n",
      "Epoch 23/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2238\n",
      "Epoch 00023: val_loss improved from 0.15064 to 0.14962, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2346 - val_loss: 0.1496\n",
      "Epoch 24/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2225\n",
      "Epoch 00024: val_loss improved from 0.14962 to 0.14765, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2213 - val_loss: 0.1477\n",
      "Epoch 25/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2253\n",
      "Epoch 00025: val_loss improved from 0.14765 to 0.14587, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2262 - val_loss: 0.1459\n",
      "Epoch 26/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2166\n",
      "Epoch 00026: val_loss improved from 0.14587 to 0.14535, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2247 - val_loss: 0.1454\n",
      "Epoch 27/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2172\n",
      "Epoch 00027: val_loss improved from 0.14535 to 0.14303, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2161 - val_loss: 0.1430\n",
      "Epoch 28/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2185\n",
      "Epoch 00028: val_loss improved from 0.14303 to 0.14180, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2172 - val_loss: 0.1418\n",
      "Epoch 29/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2259\n",
      "Epoch 00029: val_loss did not improve from 0.14180\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2206 - val_loss: 0.1421\n",
      "Epoch 30/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2103\n",
      "Epoch 00030: val_loss improved from 0.14180 to 0.14024, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2116 - val_loss: 0.1402\n",
      "Epoch 31/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2125\n",
      "Epoch 00031: val_loss improved from 0.14024 to 0.13870, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_6.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2100 - val_loss: 0.1387\n",
      "Epoch 32/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2226\n",
      "Epoch 00032: val_loss did not improve from 0.13870\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2144 - val_loss: 0.1390\n",
      "Epoch 33/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2289\n",
      "Epoch 00033: val_loss did not improve from 0.13870\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2210 - val_loss: 0.1405\n",
      " ###6 fold : val acc1 0.568, acc3 0.956, mae 0.243###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/39 [===============>..............] - ETA: 0s - loss: 19.7255 \n",
      "Epoch 00001: val_loss improved from inf to 12.06562, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 17.4222 - val_loss: 12.0656\n",
      "Epoch 2/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 9.7620 \n",
      "Epoch 00002: val_loss improved from 12.06562 to 5.06504, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.7825 - val_loss: 5.0650\n",
      "Epoch 3/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 4.5454\n",
      "Epoch 00003: val_loss improved from 5.06504 to 2.13874, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.7741 - val_loss: 2.1387\n",
      "Epoch 4/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 2.3181\n",
      "Epoch 00004: val_loss improved from 2.13874 to 1.10431, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.9233 - val_loss: 1.1043\n",
      "Epoch 5/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 1.6911\n",
      "Epoch 00005: val_loss improved from 1.10431 to 0.67844, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3185 - val_loss: 0.6784\n",
      "Epoch 6/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 1.1193\n",
      "Epoch 00006: val_loss improved from 0.67844 to 0.45611, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.8803 - val_loss: 0.4561\n",
      "Epoch 7/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.4766\n",
      "Epoch 00007: val_loss improved from 0.45611 to 0.34145, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.6419 - val_loss: 0.3414\n",
      "Epoch 8/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.3769\n",
      "Epoch 00008: val_loss improved from 0.34145 to 0.27744, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4813 - val_loss: 0.2774\n",
      "Epoch 9/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3360\n",
      "Epoch 00009: val_loss improved from 0.27744 to 0.24207, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3699 - val_loss: 0.2421\n",
      "Epoch 10/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3940\n",
      "Epoch 00010: val_loss improved from 0.24207 to 0.22128, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3490 - val_loss: 0.2213\n",
      "Epoch 11/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2923\n",
      "Epoch 00011: val_loss improved from 0.22128 to 0.20734, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3113 - val_loss: 0.2073\n",
      "Epoch 12/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2948\n",
      "Epoch 00012: val_loss improved from 0.20734 to 0.19773, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2843 - val_loss: 0.1977\n",
      "Epoch 13/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.3052\n",
      "Epoch 00013: val_loss improved from 0.19773 to 0.19116, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2814 - val_loss: 0.1912\n",
      "Epoch 14/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2798\n",
      "Epoch 00014: val_loss improved from 0.19116 to 0.18482, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2797 - val_loss: 0.1848\n",
      "Epoch 15/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2528\n",
      "Epoch 00015: val_loss improved from 0.18482 to 0.17811, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2531 - val_loss: 0.1781\n",
      "Epoch 16/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2629\n",
      "Epoch 00016: val_loss improved from 0.17811 to 0.17429, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2622 - val_loss: 0.1743\n",
      "Epoch 17/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2447\n",
      "Epoch 00017: val_loss improved from 0.17429 to 0.16894, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2451 - val_loss: 0.1689\n",
      "Epoch 18/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2440\n",
      "Epoch 00018: val_loss improved from 0.16894 to 0.16529, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2443 - val_loss: 0.1653\n",
      "Epoch 19/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2379\n",
      "Epoch 00019: val_loss improved from 0.16529 to 0.16120, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2386 - val_loss: 0.1612\n",
      "Epoch 20/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2335\n",
      "Epoch 00020: val_loss improved from 0.16120 to 0.15818, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2334 - val_loss: 0.1582\n",
      "Epoch 21/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2336\n",
      "Epoch 00021: val_loss improved from 0.15818 to 0.15355, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2335 - val_loss: 0.1536\n",
      "Epoch 22/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2245\n",
      "Epoch 00022: val_loss improved from 0.15355 to 0.14975, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2244 - val_loss: 0.1498\n",
      "Epoch 23/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2371\n",
      "Epoch 00023: val_loss improved from 0.14975 to 0.14902, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2362 - val_loss: 0.1490\n",
      "Epoch 24/100\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2241\n",
      "Epoch 00024: val_loss improved from 0.14902 to 0.14687, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2235 - val_loss: 0.1469\n",
      "Epoch 25/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2269\n",
      "Epoch 00025: val_loss improved from 0.14687 to 0.14541, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2269 - val_loss: 0.1454\n",
      "Epoch 26/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2177\n",
      "Epoch 00026: val_loss improved from 0.14541 to 0.14511, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2239 - val_loss: 0.1451\n",
      "Epoch 27/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2166\n",
      "Epoch 00027: val_loss improved from 0.14511 to 0.14241, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2163 - val_loss: 0.1424\n",
      "Epoch 28/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2183\n",
      "Epoch 00028: val_loss improved from 0.14241 to 0.14153, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2181 - val_loss: 0.1415\n",
      "Epoch 29/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2202\n",
      "Epoch 00029: val_loss did not improve from 0.14153\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2202 - val_loss: 0.1417\n",
      "Epoch 30/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2112\n",
      "Epoch 00030: val_loss improved from 0.14153 to 0.13974, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2112 - val_loss: 0.1397\n",
      "Epoch 31/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2096\n",
      "Epoch 00031: val_loss improved from 0.13974 to 0.13851, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_7.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2092 - val_loss: 0.1385\n",
      "Epoch 32/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2212\n",
      "Epoch 00032: val_loss did not improve from 0.13851\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2140 - val_loss: 0.1388\n",
      "Epoch 33/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2199\n",
      "Epoch 00033: val_loss did not improve from 0.13851\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2197 - val_loss: 0.1401\n",
      " ###7 fold : val acc1 0.580, acc3 0.961, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/39 [=================>............] - ETA: 0s - loss: 19.4566 \n",
      "Epoch 00001: val_loss improved from inf to 12.13025, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 1s 6ms/step - loss: 17.4207 - val_loss: 12.1302\n",
      "Epoch 2/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 9.7554 \n",
      "Epoch 00002: val_loss improved from 12.13025 to 5.12225, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.7801 - val_loss: 5.1222\n",
      "Epoch 3/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 4.4675\n",
      "Epoch 00003: val_loss improved from 5.12225 to 2.17094, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.7664 - val_loss: 2.1709\n",
      "Epoch 4/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 2.2790\n",
      "Epoch 00004: val_loss improved from 2.17094 to 1.13110, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.9173 - val_loss: 1.1311\n",
      "Epoch 5/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 1.6151\n",
      "Epoch 00005: val_loss improved from 1.13110 to 0.69666, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3147 - val_loss: 0.6967\n",
      "Epoch 6/100\n",
      "25/39 [==================>...........] - ETA: 0s - loss: 1.0535\n",
      "Epoch 00006: val_loss improved from 0.69666 to 0.46533, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.8782 - val_loss: 0.4653\n",
      "Epoch 7/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.4753\n",
      "Epoch 00007: val_loss improved from 0.46533 to 0.34487, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.6399 - val_loss: 0.3449\n",
      "Epoch 8/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.5545\n",
      "Epoch 00008: val_loss improved from 0.34487 to 0.27430, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4780 - val_loss: 0.2743\n",
      "Epoch 9/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.3345\n",
      "Epoch 00009: val_loss improved from 0.27430 to 0.23618, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3682 - val_loss: 0.2362\n",
      "Epoch 10/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.3842\n",
      "Epoch 00010: val_loss improved from 0.23618 to 0.21396, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3475 - val_loss: 0.2140\n",
      "Epoch 11/100\n",
      "24/39 [=================>............] - ETA: 0s - loss: 0.2873\n",
      "Epoch 00011: val_loss improved from 0.21396 to 0.19972, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3090 - val_loss: 0.1997\n",
      "Epoch 12/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2938\n",
      "Epoch 00012: val_loss improved from 0.19972 to 0.19051, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2825 - val_loss: 0.1905\n",
      "Epoch 13/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.3002\n",
      "Epoch 00013: val_loss improved from 0.19051 to 0.18367, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2814 - val_loss: 0.1837\n",
      "Epoch 14/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2969\n",
      "Epoch 00014: val_loss improved from 0.18367 to 0.17828, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2794 - val_loss: 0.1783\n",
      "Epoch 15/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2468\n",
      "Epoch 00015: val_loss improved from 0.17828 to 0.17198, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2513 - val_loss: 0.1720\n",
      "Epoch 16/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2707\n",
      "Epoch 00016: val_loss improved from 0.17198 to 0.16847, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2612 - val_loss: 0.1685\n",
      "Epoch 17/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2468\n",
      "Epoch 00017: val_loss improved from 0.16847 to 0.16390, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2449 - val_loss: 0.1639\n",
      "Epoch 18/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2427\n",
      "Epoch 00018: val_loss improved from 0.16390 to 0.16066, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2438 - val_loss: 0.1607\n",
      "Epoch 19/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2373\n",
      "Epoch 00019: val_loss improved from 0.16066 to 0.15733, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2374 - val_loss: 0.1573\n",
      "Epoch 20/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2351\n",
      "Epoch 00020: val_loss improved from 0.15733 to 0.15443, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2330 - val_loss: 0.1544\n",
      "Epoch 21/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2331\n",
      "Epoch 00021: val_loss improved from 0.15443 to 0.15051, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2331 - val_loss: 0.1505\n",
      "Epoch 22/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2255\n",
      "Epoch 00022: val_loss improved from 0.15051 to 0.14733, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2245 - val_loss: 0.1473\n",
      "Epoch 23/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2249\n",
      "Epoch 00023: val_loss improved from 0.14733 to 0.14640, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2358 - val_loss: 0.1464\n",
      "Epoch 24/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2250\n",
      "Epoch 00024: val_loss improved from 0.14640 to 0.14429, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2235 - val_loss: 0.1443\n",
      "Epoch 25/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 0.2256\n",
      "Epoch 00025: val_loss improved from 0.14429 to 0.14312, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2257 - val_loss: 0.1431\n",
      "Epoch 26/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2184\n",
      "Epoch 00026: val_loss improved from 0.14312 to 0.14271, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2234 - val_loss: 0.1427\n",
      "Epoch 27/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2143\n",
      "Epoch 00027: val_loss improved from 0.14271 to 0.14002, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2158 - val_loss: 0.1400\n",
      "Epoch 28/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2177\n",
      "Epoch 00028: val_loss improved from 0.14002 to 0.13974, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2181 - val_loss: 0.1397\n",
      "Epoch 29/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2259\n",
      "Epoch 00029: val_loss did not improve from 0.13974\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2201 - val_loss: 0.1401\n",
      "Epoch 30/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2110\n",
      "Epoch 00030: val_loss improved from 0.13974 to 0.13826, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2111 - val_loss: 0.1383\n",
      "Epoch 31/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2096\n",
      "Epoch 00031: val_loss improved from 0.13826 to 0.13695, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_8.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2083 - val_loss: 0.1370\n",
      "Epoch 32/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2192\n",
      "Epoch 00032: val_loss did not improve from 0.13695\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2126 - val_loss: 0.1372\n",
      "Epoch 33/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2257\n",
      "Epoch 00033: val_loss did not improve from 0.13695\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2190 - val_loss: 0.1388\n",
      " ###8 fold : val acc1 0.557, acc3 0.955, mae 0.248###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/39 [================>.............] - ETA: 0s - loss: 19.5980 \n",
      "Epoch 00001: val_loss improved from inf to 12.12798, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 1s 7ms/step - loss: 17.4207 - val_loss: 12.1280\n",
      "Epoch 2/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 9.7554 \n",
      "Epoch 00002: val_loss improved from 12.12798 to 5.10596, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 8.7801 - val_loss: 5.1060\n",
      "Epoch 3/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 4.5352\n",
      "Epoch 00003: val_loss improved from 5.10596 to 2.17505, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 3.7664 - val_loss: 2.1751\n",
      "Epoch 4/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 2.3128\n",
      "Epoch 00004: val_loss improved from 2.17505 to 1.13982, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.9173 - val_loss: 1.1398\n",
      "Epoch 5/100\n",
      "23/39 [================>.............] - ETA: 0s - loss: 1.6473\n",
      "Epoch 00005: val_loss improved from 1.13982 to 0.70242, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 1.3147 - val_loss: 0.7024\n",
      "Epoch 6/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 1.1433\n",
      "Epoch 00006: val_loss improved from 0.70242 to 0.46626, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.8782 - val_loss: 0.4663\n",
      "Epoch 7/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.4766\n",
      "Epoch 00007: val_loss improved from 0.46626 to 0.34355, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.6399 - val_loss: 0.3436\n",
      "Epoch 8/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.3775\n",
      "Epoch 00008: val_loss improved from 0.34355 to 0.27211, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.4780 - val_loss: 0.2721\n",
      "Epoch 9/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.3360\n",
      "Epoch 00009: val_loss improved from 0.27211 to 0.23344, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3682 - val_loss: 0.2334\n",
      "Epoch 10/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.3981\n",
      "Epoch 00010: val_loss improved from 0.23344 to 0.21078, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3475 - val_loss: 0.2108\n",
      "Epoch 11/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2905\n",
      "Epoch 00011: val_loss improved from 0.21078 to 0.19691, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.3090 - val_loss: 0.1969\n",
      "Epoch 12/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2965\n",
      "Epoch 00012: val_loss improved from 0.19691 to 0.18720, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2825 - val_loss: 0.1872\n",
      "Epoch 13/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2814\n",
      "Epoch 00013: val_loss improved from 0.18720 to 0.18114, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2814 - val_loss: 0.1811\n",
      "Epoch 14/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2794\n",
      "Epoch 00014: val_loss improved from 0.18114 to 0.17552, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2794 - val_loss: 0.1755\n",
      "Epoch 15/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2430\n",
      "Epoch 00015: val_loss improved from 0.17552 to 0.16969, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2513 - val_loss: 0.1697\n",
      "Epoch 16/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2459\n",
      "Epoch 00016: val_loss improved from 0.16969 to 0.16658, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2612 - val_loss: 0.1666\n",
      "Epoch 17/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2462\n",
      "Epoch 00017: val_loss improved from 0.16658 to 0.16191, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2449 - val_loss: 0.1619\n",
      "Epoch 18/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2429\n",
      "Epoch 00018: val_loss improved from 0.16191 to 0.15892, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2438 - val_loss: 0.1589\n",
      "Epoch 19/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2349\n",
      "Epoch 00019: val_loss improved from 0.15892 to 0.15552, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2374 - val_loss: 0.1555\n",
      "Epoch 20/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.2351\n",
      "Epoch 00020: val_loss improved from 0.15552 to 0.15281, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2330 - val_loss: 0.1528\n",
      "Epoch 21/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2331\n",
      "Epoch 00021: val_loss improved from 0.15281 to 0.14909, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2331 - val_loss: 0.1491\n",
      "Epoch 22/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2253\n",
      "Epoch 00022: val_loss improved from 0.14909 to 0.14604, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2245 - val_loss: 0.1460\n",
      "Epoch 23/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2241\n",
      "Epoch 00023: val_loss improved from 0.14604 to 0.14541, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2358 - val_loss: 0.1454\n",
      "Epoch 24/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2257\n",
      "Epoch 00024: val_loss improved from 0.14541 to 0.14330, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2235 - val_loss: 0.1433\n",
      "Epoch 25/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2262\n",
      "Epoch 00025: val_loss improved from 0.14330 to 0.14196, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2257 - val_loss: 0.1420\n",
      "Epoch 26/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2234\n",
      "Epoch 00026: val_loss improved from 0.14196 to 0.14167, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2234 - val_loss: 0.1417\n",
      "Epoch 27/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2162\n",
      "Epoch 00027: val_loss improved from 0.14167 to 0.13930, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2158 - val_loss: 0.1393\n",
      "Epoch 28/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2181\n",
      "Epoch 00028: val_loss improved from 0.13930 to 0.13873, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2181 - val_loss: 0.1387\n",
      "Epoch 29/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2259\n",
      "Epoch 00029: val_loss did not improve from 0.13873\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2201 - val_loss: 0.1393\n",
      "Epoch 30/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2111\n",
      "Epoch 00030: val_loss improved from 0.13873 to 0.13747, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2111 - val_loss: 0.1375\n",
      "Epoch 31/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2083\n",
      "Epoch 00031: val_loss improved from 0.13747 to 0.13653, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2083 - val_loss: 0.1365\n",
      "Epoch 32/100\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2125\n",
      "Epoch 00032: val_loss improved from 0.13653 to 0.13649, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2126 - val_loss: 0.1365\n",
      "Epoch 33/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2257\n",
      "Epoch 00033: val_loss did not improve from 0.13649\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2190 - val_loss: 0.1380\n",
      "Epoch 34/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2079\n",
      "Epoch 00034: val_loss improved from 0.13649 to 0.13582, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2078 - val_loss: 0.1358\n",
      "Epoch 35/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2117\n",
      "Epoch 00035: val_loss improved from 0.13582 to 0.13391, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2091 - val_loss: 0.1339\n",
      "Epoch 36/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2036\n",
      "Epoch 00036: val_loss improved from 0.13391 to 0.13337, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2040 - val_loss: 0.1334\n",
      "Epoch 37/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2116\n",
      "Epoch 00037: val_loss did not improve from 0.13337\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2077 - val_loss: 0.1337\n",
      "Epoch 38/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2012\n",
      "Epoch 00038: val_loss improved from 0.13337 to 0.13250, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2039 - val_loss: 0.1325\n",
      "Epoch 39/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00039: val_loss improved from 0.13250 to 0.13169, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2018 - val_loss: 0.1317\n",
      "Epoch 40/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2015\n",
      "Epoch 00040: val_loss did not improve from 0.13169\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2016 - val_loss: 0.1321\n",
      "Epoch 41/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2096\n",
      "Epoch 00041: val_loss improved from 0.13169 to 0.13143, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2053 - val_loss: 0.1314\n",
      "Epoch 42/100\n",
      "21/39 [===============>..............] - ETA: 0s - loss: 0.2015\n",
      "Epoch 00042: val_loss improved from 0.13143 to 0.13109, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.2010 - val_loss: 0.1311\n",
      "Epoch 43/100\n",
      "22/39 [===============>..............] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00043: val_loss improved from 0.13109 to 0.13095, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1982 - val_loss: 0.1310\n",
      "Epoch 44/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1996\n",
      "Epoch 00044: val_loss improved from 0.13095 to 0.12992, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1996 - val_loss: 0.1299\n",
      "Epoch 45/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1961\n",
      "Epoch 00045: val_loss improved from 0.12992 to 0.12945, saving model to result/size/DNN_size_both_y/batch512,dnodes128_dropout0.1,lr0.001/weights_9.hdf5\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1961 - val_loss: 0.1294\n",
      "Epoch 46/100\n",
      "20/39 [==============>...............] - ETA: 0s - loss: 0.1944\n",
      "Epoch 00046: val_loss did not improve from 0.12945\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1960 - val_loss: 0.1296\n",
      "Epoch 47/100\n",
      "20/39 [==============>...............] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00047: val_loss did not improve from 0.12945\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.1988 - val_loss: 0.1300\n",
      " ###9 fold : val acc1 0.576, acc3 0.964, mae 0.234###\n",
      "acc10.574_acc30.962\n",
      "random search 28/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/154 [============================>.] - ETA: 0s - loss: 1.9770\n",
      "Epoch 00001: val_loss improved from inf to 0.22016, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 1.9448 - val_loss: 0.2202\n",
      "Epoch 2/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.5242\n",
      "Epoch 00002: val_loss improved from 0.22016 to 0.16087, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5011 - val_loss: 0.1609\n",
      "Epoch 3/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1966\n",
      "Epoch 00003: val_loss did not improve from 0.16087\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2248 - val_loss: 0.1826\n",
      "Epoch 4/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.2008\n",
      "Epoch 00004: val_loss improved from 0.16087 to 0.15298, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2020 - val_loss: 0.1530\n",
      "Epoch 5/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.2040\n",
      "Epoch 00005: val_loss improved from 0.15298 to 0.14974, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2028 - val_loss: 0.1497\n",
      "Epoch 6/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1822\n",
      "Epoch 00006: val_loss improved from 0.14974 to 0.14645, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1823 - val_loss: 0.1465\n",
      "Epoch 7/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.2270\n",
      "Epoch 00007: val_loss improved from 0.14645 to 0.14596, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2267 - val_loss: 0.1460\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.3042\n",
      "Epoch 00008: val_loss did not improve from 0.14596\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3042 - val_loss: 0.1875\n",
      "Epoch 9/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.2127\n",
      "Epoch 00009: val_loss improved from 0.14596 to 0.13293, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2092 - val_loss: 0.1329\n",
      "Epoch 10/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1792\n",
      "Epoch 00010: val_loss improved from 0.13293 to 0.13243, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1792 - val_loss: 0.1324\n",
      "Epoch 11/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1793\n",
      "Epoch 00011: val_loss improved from 0.13243 to 0.13237, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1780 - val_loss: 0.1324\n",
      "Epoch 12/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1918\n",
      "Epoch 00012: val_loss did not improve from 0.13237\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1919 - val_loss: 0.1352\n",
      "Epoch 13/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1696\n",
      "Epoch 00013: val_loss improved from 0.13237 to 0.13064, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1695 - val_loss: 0.1306\n",
      "Epoch 14/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1699\n",
      "Epoch 00014: val_loss did not improve from 0.13064\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1698 - val_loss: 0.1390\n",
      "Epoch 15/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1741\n",
      "Epoch 00015: val_loss did not improve from 0.13064\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1741 - val_loss: 0.1347\n",
      " ###0 fold : val acc1 0.561, acc3 0.964, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/154 [============================>.] - ETA: 0s - loss: 1.9312\n",
      "Epoch 00001: val_loss improved from inf to 0.21668, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 1.8996 - val_loss: 0.2167\n",
      "Epoch 2/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.4629\n",
      "Epoch 00002: val_loss improved from 0.21668 to 0.16002, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4618 - val_loss: 0.1600\n",
      "Epoch 3/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.2217\n",
      "Epoch 00003: val_loss did not improve from 0.16002\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2217 - val_loss: 0.2014\n",
      "Epoch 4/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1962\n",
      "Epoch 00004: val_loss improved from 0.16002 to 0.14986, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1972 - val_loss: 0.1499\n",
      "Epoch 5/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.2204\n",
      "Epoch 00005: val_loss did not improve from 0.14986\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2204 - val_loss: 0.1506\n",
      "Epoch 6/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1893\n",
      "Epoch 00006: val_loss improved from 0.14986 to 0.13556, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1893 - val_loss: 0.1356\n",
      "Epoch 7/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1882\n",
      "Epoch 00007: val_loss did not improve from 0.13556\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1882 - val_loss: 0.1490\n",
      "Epoch 8/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.1786\n",
      "Epoch 00008: val_loss improved from 0.13556 to 0.13120, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1775 - val_loss: 0.1312\n",
      "Epoch 9/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1790\n",
      "Epoch 00009: val_loss did not improve from 0.13120\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1776 - val_loss: 0.1314\n",
      "Epoch 10/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.1695\n",
      "Epoch 00010: val_loss did not improve from 0.13120\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.2029 - val_loss: 0.1602\n",
      " ###1 fold : val acc1 0.581, acc3 0.961, mae 0.234###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 1.9172\n",
      "Epoch 00001: val_loss improved from inf to 0.21757, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.8751 - val_loss: 0.2176\n",
      "Epoch 2/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.4708\n",
      "Epoch 00002: val_loss improved from 0.21757 to 0.16141, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4620 - val_loss: 0.1614\n",
      "Epoch 3/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.1961\n",
      "Epoch 00003: val_loss did not improve from 0.16141\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2280 - val_loss: 0.1996\n",
      "Epoch 4/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.1995\n",
      "Epoch 00004: val_loss improved from 0.16141 to 0.15381, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2019 - val_loss: 0.1538\n",
      "Epoch 5/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.2070\n",
      "Epoch 00005: val_loss improved from 0.15381 to 0.14874, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2058 - val_loss: 0.1487\n",
      "Epoch 6/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1835\n",
      "Epoch 00006: val_loss improved from 0.14874 to 0.13243, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1835 - val_loss: 0.1324\n",
      "Epoch 7/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1723\n",
      "Epoch 00007: val_loss did not improve from 0.13243\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1723 - val_loss: 0.1400\n",
      "Epoch 8/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.2256\n",
      "Epoch 00008: val_loss did not improve from 0.13243\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2256 - val_loss: 0.1918\n",
      " ###2 fold : val acc1 0.574, acc3 0.959, mae 0.238###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149/154 [============================>.] - ETA: 0s - loss: 1.8480\n",
      "Epoch 00001: val_loss improved from inf to 0.21429, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.8073 - val_loss: 0.2143\n",
      "Epoch 2/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.5136\n",
      "Epoch 00002: val_loss improved from 0.21429 to 0.16347, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5133 - val_loss: 0.1635\n",
      "Epoch 3/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1977\n",
      "Epoch 00003: val_loss did not improve from 0.16347\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2224 - val_loss: 0.1821\n",
      "Epoch 4/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1917\n",
      "Epoch 00004: val_loss improved from 0.16347 to 0.13833, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1917 - val_loss: 0.1383\n",
      "Epoch 5/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.2256\n",
      "Epoch 00005: val_loss did not improve from 0.13833\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2246 - val_loss: 0.1474\n",
      "Epoch 6/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1836\n",
      "Epoch 00006: val_loss improved from 0.13833 to 0.13645, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1843 - val_loss: 0.1365\n",
      "Epoch 7/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.2030\n",
      "Epoch 00007: val_loss did not improve from 0.13645\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2027 - val_loss: 0.1447\n",
      "Epoch 8/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.1756\n",
      "Epoch 00008: val_loss did not improve from 0.13645\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3384 - val_loss: 0.1955\n",
      " ###3 fold : val acc1 0.568, acc3 0.963, mae 0.238###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/154 [==========================>...] - ETA: 0s - loss: 1.9893\n",
      "Epoch 00001: val_loss improved from inf to 0.20195, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 1.8427 - val_loss: 0.2020\n",
      "Epoch 2/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.4407\n",
      "Epoch 00002: val_loss improved from 0.20195 to 0.17234, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4376 - val_loss: 0.1723\n",
      "Epoch 3/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.2566\n",
      "Epoch 00003: val_loss improved from 0.17234 to 0.14554, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2538 - val_loss: 0.1455\n",
      "Epoch 4/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1968\n",
      "Epoch 00004: val_loss improved from 0.14554 to 0.14432, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1957 - val_loss: 0.1443\n",
      "Epoch 5/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.1894\n",
      "Epoch 00005: val_loss improved from 0.14432 to 0.14028, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1893 - val_loss: 0.1403\n",
      "Epoch 6/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.1820\n",
      "Epoch 00006: val_loss improved from 0.14028 to 0.13310, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1825 - val_loss: 0.1331\n",
      "Epoch 7/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.1783\n",
      "Epoch 00007: val_loss did not improve from 0.13310\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1791 - val_loss: 0.1416\n",
      "Epoch 8/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.1744\n",
      "Epoch 00008: val_loss did not improve from 0.13310\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1744 - val_loss: 0.1348\n",
      " ###4 fold : val acc1 0.590, acc3 0.963, mae 0.227###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/154 [===========================>..] - ETA: 0s - loss: 1.3898\n",
      "Epoch 00001: val_loss improved from inf to 0.20258, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 1.3375 - val_loss: 0.2026\n",
      "Epoch 2/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.2147\n",
      "Epoch 00002: val_loss improved from 0.20258 to 0.15437, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2137 - val_loss: 0.1544\n",
      "Epoch 3/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1895\n",
      "Epoch 00003: val_loss improved from 0.15437 to 0.13790, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1895 - val_loss: 0.1379\n",
      "Epoch 4/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1793\n",
      "Epoch 00004: val_loss improved from 0.13790 to 0.13250, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1793 - val_loss: 0.1325\n",
      "Epoch 5/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1748\n",
      "Epoch 00005: val_loss did not improve from 0.13250\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1751 - val_loss: 0.1402\n",
      "Epoch 6/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1765\n",
      "Epoch 00006: val_loss did not improve from 0.13250\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1764 - val_loss: 0.1338\n",
      " ###5 fold : val acc1 0.577, acc3 0.968, mae 0.247###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/154 [===========================>..] - ETA: 0s - loss: 2.6440\n",
      "Epoch 00001: val_loss improved from inf to 0.20324, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 2.5170 - val_loss: 0.2032\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.4220\n",
      "Epoch 00002: val_loss improved from 0.20324 to 0.17397, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4220 - val_loss: 0.1740\n",
      "Epoch 3/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.2928\n",
      "Epoch 00003: val_loss improved from 0.17397 to 0.13830, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2840 - val_loss: 0.1383\n",
      "Epoch 4/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.2213\n",
      "Epoch 00004: val_loss did not improve from 0.13830\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2189 - val_loss: 0.1444\n",
      "Epoch 5/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2058\n",
      "Epoch 00005: val_loss improved from 0.13830 to 0.13744, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2056 - val_loss: 0.1374\n",
      "Epoch 6/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.3383\n",
      "Epoch 00006: val_loss did not improve from 0.13744\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3383 - val_loss: 0.1378\n",
      "Epoch 7/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.2007\n",
      "Epoch 00007: val_loss did not improve from 0.13744\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2010 - val_loss: 0.1477\n",
      " ###6 fold : val acc1 0.553, acc3 0.962, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/154 [===========================>..] - ETA: 0s - loss: 2.5645\n",
      "Epoch 00001: val_loss improved from inf to 0.20381, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 2.4720 - val_loss: 0.2038\n",
      "Epoch 2/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.5272\n",
      "Epoch 00002: val_loss improved from 0.20381 to 0.18134, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5254 - val_loss: 0.1813\n",
      "Epoch 3/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.3665\n",
      "Epoch 00003: val_loss improved from 0.18134 to 0.14058, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3637 - val_loss: 0.1406\n",
      "Epoch 4/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.1960\n",
      "Epoch 00004: val_loss did not improve from 0.14058\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1960 - val_loss: 0.1415\n",
      "Epoch 5/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.1853\n",
      "Epoch 00005: val_loss improved from 0.14058 to 0.13495, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1851 - val_loss: 0.1350\n",
      "Epoch 6/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.2302\n",
      "Epoch 00006: val_loss did not improve from 0.13495\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2303 - val_loss: 0.1392\n",
      "Epoch 7/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.2288\n",
      "Epoch 00007: val_loss did not improve from 0.13495\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2287 - val_loss: 0.1468\n",
      " ###7 fold : val acc1 0.574, acc3 0.960, mae 0.236###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/154 [============================>.] - ETA: 0s - loss: 2.5401\n",
      "Epoch 00001: val_loss improved from inf to 0.21312, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 2.4918 - val_loss: 0.2131\n",
      "Epoch 2/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.2236\n",
      "Epoch 00002: val_loss improved from 0.21312 to 0.18147, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5536 - val_loss: 0.1815\n",
      "Epoch 3/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.3700\n",
      "Epoch 00003: val_loss improved from 0.18147 to 0.13789, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3698 - val_loss: 0.1379\n",
      "Epoch 4/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.2022\n",
      "Epoch 00004: val_loss did not improve from 0.13789\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2019 - val_loss: 0.1392\n",
      "Epoch 5/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1800\n",
      "Epoch 00005: val_loss improved from 0.13789 to 0.13158, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1802 - val_loss: 0.1316\n",
      "Epoch 6/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.2035\n",
      "Epoch 00006: val_loss did not improve from 0.13158\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2036 - val_loss: 0.1367\n",
      "Epoch 7/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.1985\n",
      "Epoch 00007: val_loss did not improve from 0.13158\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1994 - val_loss: 0.1538\n",
      " ###8 fold : val acc1 0.565, acc3 0.957, mae 0.243###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/154 [===========================>..] - ETA: 0s - loss: 2.5852\n",
      "Epoch 00001: val_loss improved from inf to 0.20312, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 2.4918 - val_loss: 0.2031\n",
      "Epoch 2/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.2233\n",
      "Epoch 00002: val_loss improved from 0.20312 to 0.18384, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5536 - val_loss: 0.1838\n",
      "Epoch 3/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.3698\n",
      "Epoch 00003: val_loss improved from 0.18384 to 0.13837, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3698 - val_loss: 0.1384\n",
      "Epoch 4/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.2023\n",
      "Epoch 00004: val_loss improved from 0.13837 to 0.13802, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2019 - val_loss: 0.1380\n",
      "Epoch 5/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.1805\n",
      "Epoch 00005: val_loss improved from 0.13802 to 0.13121, saving model to result/size/DNN_size_both_y/batch128,dnodes64_dropout0,dnodes512_dropout0.3,lr0.002/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1802 - val_loss: 0.1312\n",
      "Epoch 6/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.2036\n",
      "Epoch 00006: val_loss did not improve from 0.13121\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2036 - val_loss: 0.1354\n",
      "Epoch 7/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.1994\n",
      "Epoch 00007: val_loss did not improve from 0.13121\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1994 - val_loss: 0.1526\n",
      " ###9 fold : val acc1 0.571, acc3 0.962, mae 0.238###\n",
      "acc10.571_acc30.962\n",
      "random search 29/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/154 [===========================>..] - ETA: 0s - loss: 12.2178\n",
      "Epoch 00001: val_loss improved from inf to 1.80783, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 11.7127 - val_loss: 1.8078\n",
      "Epoch 2/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 3.3178\n",
      "Epoch 00002: val_loss improved from 1.80783 to 0.57971, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 3.2582 - val_loss: 0.5797\n",
      "Epoch 3/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.1620\n",
      "Epoch 00003: val_loss improved from 0.57971 to 0.41502, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 3.2700 - val_loss: 0.4150\n",
      "Epoch 4/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 2.4858\n",
      "Epoch 00004: val_loss improved from 0.41502 to 0.34482, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.4858 - val_loss: 0.3448\n",
      "Epoch 5/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 1.5043\n",
      "Epoch 00005: val_loss improved from 0.34482 to 0.25026, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4557 - val_loss: 0.2503\n",
      "Epoch 6/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 1.1523\n",
      "Epoch 00006: val_loss improved from 0.25026 to 0.22873, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1246 - val_loss: 0.2287\n",
      "Epoch 7/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 1.4177\n",
      "Epoch 00007: val_loss improved from 0.22873 to 0.21313, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.3635 - val_loss: 0.2131\n",
      "Epoch 8/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.9262\n",
      "Epoch 00008: val_loss did not improve from 0.21313\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9258 - val_loss: 0.2236\n",
      "Epoch 9/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.7767\n",
      "Epoch 00009: val_loss improved from 0.21313 to 0.19518, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7645 - val_loss: 0.1952\n",
      "Epoch 10/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.5862\n",
      "Epoch 00010: val_loss did not improve from 0.19518\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9408 - val_loss: 0.2203\n",
      "Epoch 11/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.7385\n",
      "Epoch 00011: val_loss improved from 0.19518 to 0.18292, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7274 - val_loss: 0.1829\n",
      "Epoch 12/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.8666\n",
      "Epoch 00012: val_loss did not improve from 0.18292\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8504 - val_loss: 0.1830\n",
      "Epoch 13/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.6170\n",
      "Epoch 00013: val_loss improved from 0.18292 to 0.18141, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6113 - val_loss: 0.1814\n",
      "Epoch 14/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.5784\n",
      "Epoch 00014: val_loss did not improve from 0.18141\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5681 - val_loss: 0.1856\n",
      "Epoch 15/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.6120\n",
      "Epoch 00015: val_loss improved from 0.18141 to 0.17499, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6010 - val_loss: 0.1750\n",
      "Epoch 16/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.5167\n",
      "Epoch 00016: val_loss did not improve from 0.17499\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5108 - val_loss: 0.1769\n",
      "Epoch 17/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.4160\n",
      "Epoch 00017: val_loss did not improve from 0.17499\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4156 - val_loss: 0.1762\n",
      " ###0 fold : val acc1 0.504, acc3 0.943, mae 0.283###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/154 [============================>.] - ETA: 0s - loss: 11.8892\n",
      "Epoch 00001: val_loss improved from inf to 1.79622, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.6954 - val_loss: 1.7962\n",
      "Epoch 2/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 3.2384\n",
      "Epoch 00002: val_loss improved from 1.79622 to 0.57530, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 3.2355 - val_loss: 0.5753\n",
      "Epoch 3/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 1.1527\n",
      "Epoch 00003: val_loss improved from 0.57530 to 0.41759, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 3.2149 - val_loss: 0.4176\n",
      "Epoch 4/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 2.5608\n",
      "Epoch 00004: val_loss improved from 0.41759 to 0.34356, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.4622 - val_loss: 0.3436\n",
      "Epoch 5/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 1.4742\n",
      "Epoch 00005: val_loss improved from 0.34356 to 0.24763, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4384 - val_loss: 0.2476\n",
      "Epoch 6/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.1676\n",
      "Epoch 00006: val_loss improved from 0.24763 to 0.22957, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1367 - val_loss: 0.2296\n",
      "Epoch 7/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.3767\n",
      "Epoch 00007: val_loss improved from 0.22957 to 0.21390, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.3496 - val_loss: 0.2139\n",
      "Epoch 8/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.9374\n",
      "Epoch 00008: val_loss did not improve from 0.21390\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9191 - val_loss: 0.2214\n",
      "Epoch 9/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.7665\n",
      "Epoch 00009: val_loss improved from 0.21390 to 0.19497, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7599 - val_loss: 0.1950\n",
      "Epoch 10/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.5858\n",
      "Epoch 00010: val_loss did not improve from 0.19497\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9297 - val_loss: 0.2237\n",
      "Epoch 11/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.7400\n",
      "Epoch 00011: val_loss improved from 0.19497 to 0.18436, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7230 - val_loss: 0.1844\n",
      "Epoch 12/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.8793\n",
      "Epoch 00012: val_loss improved from 0.18436 to 0.18182, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8559 - val_loss: 0.1818\n",
      "Epoch 13/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.6131\n",
      "Epoch 00013: val_loss improved from 0.18182 to 0.18013, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6093 - val_loss: 0.1801\n",
      "Epoch 14/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.5646\n",
      "Epoch 00014: val_loss did not improve from 0.18013\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5543 - val_loss: 0.1838\n",
      "Epoch 15/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.6025\n",
      "Epoch 00015: val_loss improved from 0.18013 to 0.17636, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5930 - val_loss: 0.1764\n",
      "Epoch 16/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.5237\n",
      "Epoch 00016: val_loss did not improve from 0.17636\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5181 - val_loss: 0.1793\n",
      "Epoch 17/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.4185\n",
      "Epoch 00017: val_loss improved from 0.17636 to 0.17506, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4185 - val_loss: 0.1751\n",
      "Epoch 18/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.4725\n",
      "Epoch 00018: val_loss did not improve from 0.17506\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4662 - val_loss: 0.1796\n",
      "Epoch 19/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.4102\n",
      "Epoch 00019: val_loss did not improve from 0.17506\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4127 - val_loss: 0.1796\n",
      " ###1 fold : val acc1 0.512, acc3 0.948, mae 0.274###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/154 [==========================>...] - ETA: 0s - loss: 12.6319\n",
      "Epoch 00001: val_loss improved from inf to 1.80155, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 11.7189 - val_loss: 1.8015\n",
      "Epoch 2/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 3.3106\n",
      "Epoch 00002: val_loss improved from 1.80155 to 0.57849, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 3.2513 - val_loss: 0.5785\n",
      "Epoch 3/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 1.1588\n",
      "Epoch 00003: val_loss improved from 0.57849 to 0.42001, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 3.2419 - val_loss: 0.4200\n",
      "Epoch 4/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 2.5772\n",
      "Epoch 00004: val_loss improved from 0.42001 to 0.33617, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.4891 - val_loss: 0.3362\n",
      "Epoch 5/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 1.4911\n",
      "Epoch 00005: val_loss improved from 0.33617 to 0.24912, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4487 - val_loss: 0.2491\n",
      "Epoch 6/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 1.1414\n",
      "Epoch 00006: val_loss improved from 0.24912 to 0.23172, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1382 - val_loss: 0.2317\n",
      "Epoch 7/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.4044\n",
      "Epoch 00007: val_loss improved from 0.23172 to 0.21217, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.3572 - val_loss: 0.2122\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.9213\n",
      "Epoch 00008: val_loss did not improve from 0.21217\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9213 - val_loss: 0.2213\n",
      "Epoch 9/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.7607\n",
      "Epoch 00009: val_loss improved from 0.21217 to 0.19452, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7603 - val_loss: 0.1945\n",
      "Epoch 10/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.5792\n",
      "Epoch 00010: val_loss did not improve from 0.19452\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9325 - val_loss: 0.2289\n",
      "Epoch 11/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.7238\n",
      "Epoch 00011: val_loss improved from 0.19452 to 0.18775, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7226 - val_loss: 0.1878\n",
      "Epoch 12/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.8206\n",
      "Epoch 00012: val_loss improved from 0.18775 to 0.18254, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8206 - val_loss: 0.1825\n",
      "Epoch 13/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.6126\n",
      "Epoch 00013: val_loss improved from 0.18254 to 0.17848, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6126 - val_loss: 0.1785\n",
      "Epoch 14/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.5699\n",
      "Epoch 00014: val_loss did not improve from 0.17848\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5607 - val_loss: 0.1867\n",
      "Epoch 15/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.6149\n",
      "Epoch 00015: val_loss did not improve from 0.17848\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6013 - val_loss: 0.1803\n",
      " ###2 fold : val acc1 0.488, acc3 0.945, mae 0.289###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/154 [============================>.] - ETA: 0s - loss: 11.8249\n",
      "Epoch 00001: val_loss improved from inf to 1.79817, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 11.6928 - val_loss: 1.7982\n",
      "Epoch 2/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 3.2781\n",
      "Epoch 00002: val_loss improved from 1.79817 to 0.57852, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 3.2751 - val_loss: 0.5785\n",
      "Epoch 3/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.1625\n",
      "Epoch 00003: val_loss improved from 0.57852 to 0.41936, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 3.2703 - val_loss: 0.4194\n",
      "Epoch 4/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 2.5723\n",
      "Epoch 00004: val_loss improved from 0.41936 to 0.33190, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.5040 - val_loss: 0.3319\n",
      "Epoch 5/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 1.4601\n",
      "Epoch 00005: val_loss improved from 0.33190 to 0.24870, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4596 - val_loss: 0.2487\n",
      "Epoch 6/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 1.1328\n",
      "Epoch 00006: val_loss improved from 0.24870 to 0.23484, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1290 - val_loss: 0.2348\n",
      "Epoch 7/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 1.4342\n",
      "Epoch 00007: val_loss improved from 0.23484 to 0.21453, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.3809 - val_loss: 0.2145\n",
      "Epoch 8/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.9464\n",
      "Epoch 00008: val_loss did not improve from 0.21453\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9197 - val_loss: 0.2246\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.7763\n",
      "Epoch 00009: val_loss improved from 0.21453 to 0.19239, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7763 - val_loss: 0.1924\n",
      "Epoch 10/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.5819\n",
      "Epoch 00010: val_loss did not improve from 0.19239\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9432 - val_loss: 0.2252\n",
      "Epoch 11/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.7318\n",
      "Epoch 00011: val_loss improved from 0.19239 to 0.18572, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7318 - val_loss: 0.1857\n",
      "Epoch 12/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.8989\n",
      "Epoch 00012: val_loss improved from 0.18572 to 0.18048, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8772 - val_loss: 0.1805\n",
      "Epoch 13/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.6093\n",
      "Epoch 00013: val_loss improved from 0.18048 to 0.17844, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6068 - val_loss: 0.1784\n",
      "Epoch 14/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.5704\n",
      "Epoch 00014: val_loss did not improve from 0.17844\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5631 - val_loss: 0.1823\n",
      "Epoch 15/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.6073\n",
      "Epoch 00015: val_loss did not improve from 0.17844\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5927 - val_loss: 0.1819\n",
      " ###3 fold : val acc1 0.514, acc3 0.949, mae 0.273###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/154 [==========================>...] - ETA: 0s - loss: 11.7267\n",
      "Epoch 00001: val_loss improved from inf to 1.74965, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 10.8969 - val_loss: 1.7497\n",
      "Epoch 2/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 2.7603\n",
      "Epoch 00002: val_loss improved from 1.74965 to 0.55843, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.7074 - val_loss: 0.5584\n",
      "Epoch 3/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 2.8728\n",
      "Epoch 00003: val_loss improved from 0.55843 to 0.33264, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.7600 - val_loss: 0.3326\n",
      "Epoch 4/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 2.1517\n",
      "Epoch 00004: val_loss improved from 0.33264 to 0.26734, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.0916 - val_loss: 0.2673\n",
      "Epoch 5/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.4776\n",
      "Epoch 00005: val_loss improved from 0.26734 to 0.23720, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.4490 - val_loss: 0.2372\n",
      "Epoch 6/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 1.5359\n",
      "Epoch 00006: val_loss improved from 0.23720 to 0.22296, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.5089 - val_loss: 0.2230\n",
      "Epoch 7/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 1.2177\n",
      "Epoch 00007: val_loss improved from 0.22296 to 0.21171, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.2171 - val_loss: 0.2117\n",
      "Epoch 8/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 1.0832\n",
      "Epoch 00008: val_loss improved from 0.21171 to 0.19718, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0507 - val_loss: 0.1972\n",
      "Epoch 9/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.8292\n",
      "Epoch 00009: val_loss improved from 0.19718 to 0.18356, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8270 - val_loss: 0.1836\n",
      "Epoch 10/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.7964\n",
      "Epoch 00010: val_loss did not improve from 0.18356\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7867 - val_loss: 0.1891\n",
      "Epoch 11/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.6383\n",
      "Epoch 00011: val_loss improved from 0.18356 to 0.17733, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6310 - val_loss: 0.1773\n",
      "Epoch 12/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.6356\n",
      "Epoch 00012: val_loss did not improve from 0.17733\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6272 - val_loss: 0.1964\n",
      "Epoch 13/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.5089\n",
      "Epoch 00013: val_loss did not improve from 0.17733\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5068 - val_loss: 0.1810\n",
      " ###4 fold : val acc1 0.512, acc3 0.946, mae 0.276###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/154 [==========================>...] - ETA: 0s - loss: 10.8638\n",
      "Epoch 00001: val_loss improved from inf to 1.58419, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 10.1504 - val_loss: 1.5842\n",
      "Epoch 2/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 1.6602\n",
      "Epoch 00002: val_loss improved from 1.58419 to 0.53603, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.6469 - val_loss: 0.5360\n",
      "Epoch 3/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.1595\n",
      "Epoch 00003: val_loss improved from 0.53603 to 0.34837, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1455 - val_loss: 0.3484\n",
      "Epoch 4/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.9541\n",
      "Epoch 00004: val_loss improved from 0.34837 to 0.29289, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9480 - val_loss: 0.2929\n",
      "Epoch 5/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.8353\n",
      "Epoch 00005: val_loss improved from 0.29289 to 0.25841, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8303 - val_loss: 0.2584\n",
      "Epoch 6/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.7468\n",
      "Epoch 00006: val_loss improved from 0.25841 to 0.24643, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7417 - val_loss: 0.2464\n",
      "Epoch 7/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.6680\n",
      "Epoch 00007: val_loss improved from 0.24643 to 0.22846, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6655 - val_loss: 0.2285\n",
      "Epoch 8/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.5867\n",
      "Epoch 00008: val_loss improved from 0.22846 to 0.21132, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5865 - val_loss: 0.2113\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.5417\n",
      "Epoch 00009: val_loss improved from 0.21132 to 0.19843, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5417 - val_loss: 0.1984\n",
      "Epoch 10/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.4974\n",
      "Epoch 00010: val_loss did not improve from 0.19843\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4958 - val_loss: 0.2051\n",
      "Epoch 11/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.4624\n",
      "Epoch 00011: val_loss did not improve from 0.19843\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4623 - val_loss: 0.1994\n",
      " ###5 fold : val acc1 0.508, acc3 0.949, mae 0.308###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/154 [==========================>...] - ETA: 0s - loss: 11.2564\n",
      "Epoch 00001: val_loss improved from inf to 1.60517, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 10.5086 - val_loss: 1.6052\n",
      "Epoch 2/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 2.5922\n",
      "Epoch 00002: val_loss improved from 1.60517 to 0.56822, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.5623 - val_loss: 0.5682\n",
      "Epoch 3/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 2.7697\n",
      "Epoch 00003: val_loss improved from 0.56822 to 0.31627, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.6985 - val_loss: 0.3163\n",
      "Epoch 4/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 1.1620\n",
      "Epoch 00004: val_loss improved from 0.31627 to 0.26673, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1511 - val_loss: 0.2667\n",
      "Epoch 5/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 1.5682\n",
      "Epoch 00005: val_loss improved from 0.26673 to 0.23280, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.5576 - val_loss: 0.2328\n",
      "Epoch 6/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 1.1100\n",
      "Epoch 00006: val_loss improved from 0.23280 to 0.22581, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1021 - val_loss: 0.2258\n",
      "Epoch 7/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 1.0424\n",
      "Epoch 00007: val_loss did not improve from 0.22581\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0366 - val_loss: 0.2527\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.8111\n",
      "Epoch 00008: val_loss improved from 0.22581 to 0.19534, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8111 - val_loss: 0.1953\n",
      "Epoch 9/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.8506\n",
      "Epoch 00009: val_loss improved from 0.19534 to 0.18489, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8296 - val_loss: 0.1849\n",
      "Epoch 10/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.5712\n",
      "Epoch 00010: val_loss improved from 0.18489 to 0.18482, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5689 - val_loss: 0.1848\n",
      "Epoch 11/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 0.7438\n",
      "Epoch 00011: val_loss did not improve from 0.18482\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7228 - val_loss: 0.1947\n",
      "Epoch 12/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.6395\n",
      "Epoch 00012: val_loss improved from 0.18482 to 0.17755, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6378 - val_loss: 0.1776\n",
      "Epoch 13/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.5027\n",
      "Epoch 00013: val_loss did not improve from 0.17755\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5011 - val_loss: 0.1792\n",
      "Epoch 14/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.4716\n",
      "Epoch 00014: val_loss did not improve from 0.17755\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4727 - val_loss: 0.1795\n",
      " ###6 fold : val acc1 0.509, acc3 0.942, mae 0.280###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/154 [===========================>..] - ETA: 0s - loss: 11.0138\n",
      "Epoch 00001: val_loss improved from inf to 1.59212, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 10.4990 - val_loss: 1.5921\n",
      "Epoch 2/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 1.6388\n",
      "Epoch 00002: val_loss improved from 1.59212 to 0.57117, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.5892 - val_loss: 0.5712\n",
      "Epoch 3/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 2.9743\n",
      "Epoch 00003: val_loss improved from 0.57117 to 0.32413, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.8566 - val_loss: 0.3241\n",
      "Epoch 4/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 1.1827\n",
      "Epoch 00004: val_loss improved from 0.32413 to 0.27724, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1755 - val_loss: 0.2772\n",
      "Epoch 5/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 1.6363\n",
      "Epoch 00005: val_loss improved from 0.27724 to 0.23718, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.5915 - val_loss: 0.2372\n",
      "Epoch 6/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 1.1247\n",
      "Epoch 00006: val_loss improved from 0.23718 to 0.22701, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1199 - val_loss: 0.2270\n",
      "Epoch 7/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.0833\n",
      "Epoch 00007: val_loss did not improve from 0.22701\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0634 - val_loss: 0.2550\n",
      "Epoch 8/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.8606\n",
      "Epoch 00008: val_loss improved from 0.22701 to 0.19600, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8573 - val_loss: 0.1960\n",
      "Epoch 9/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.8988\n",
      "Epoch 00009: val_loss improved from 0.19600 to 0.19037, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8882 - val_loss: 0.1904\n",
      "Epoch 10/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.5782\n",
      "Epoch 00010: val_loss improved from 0.19037 to 0.18757, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5776 - val_loss: 0.1876\n",
      "Epoch 11/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.7746\n",
      "Epoch 00011: val_loss did not improve from 0.18757\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7668 - val_loss: 0.1937\n",
      "Epoch 12/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.6653\n",
      "Epoch 00012: val_loss improved from 0.18757 to 0.17592, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6593 - val_loss: 0.1759\n",
      "Epoch 13/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.5179\n",
      "Epoch 00013: val_loss did not improve from 0.17592\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5145 - val_loss: 0.1801\n",
      "Epoch 14/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.4844\n",
      "Epoch 00014: val_loss did not improve from 0.17592\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4848 - val_loss: 0.1792\n",
      " ###7 fold : val acc1 0.519, acc3 0.942, mae 0.274###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/154 [==========================>...] - ETA: 0s - loss: 11.1958\n",
      "Epoch 00001: val_loss improved from inf to 1.64615, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 10.5101 - val_loss: 1.6462\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 2.5842\n",
      "Epoch 00002: val_loss improved from 1.64615 to 0.63572, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.5842 - val_loss: 0.6357\n",
      "Epoch 3/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 2.8682\n",
      "Epoch 00003: val_loss improved from 0.63572 to 0.35552, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.8415 - val_loss: 0.3555\n",
      "Epoch 4/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.1838\n",
      "Epoch 00004: val_loss improved from 0.35552 to 0.30249, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1707 - val_loss: 0.3025\n",
      "Epoch 5/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 1.5973\n",
      "Epoch 00005: val_loss improved from 0.30249 to 0.25123, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.5638 - val_loss: 0.2512\n",
      "Epoch 6/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 1.1309\n",
      "Epoch 00006: val_loss improved from 0.25123 to 0.23649, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1155 - val_loss: 0.2365\n",
      "Epoch 7/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 1.0612\n",
      "Epoch 00007: val_loss did not improve from 0.23649\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0447 - val_loss: 0.2561\n",
      "Epoch 8/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.8537\n",
      "Epoch 00008: val_loss improved from 0.23649 to 0.19779, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8456 - val_loss: 0.1978\n",
      "Epoch 9/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.8968\n",
      "Epoch 00009: val_loss improved from 0.19779 to 0.18722, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8735 - val_loss: 0.1872\n",
      "Epoch 10/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.5786\n",
      "Epoch 00010: val_loss improved from 0.18722 to 0.18494, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5768 - val_loss: 0.1849\n",
      "Epoch 11/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.7840\n",
      "Epoch 00011: val_loss did not improve from 0.18494\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.7834 - val_loss: 0.1877\n",
      "Epoch 12/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.6739\n",
      "Epoch 00012: val_loss improved from 0.18494 to 0.16869, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6598 - val_loss: 0.1687\n",
      "Epoch 13/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.5202\n",
      "Epoch 00013: val_loss did not improve from 0.16869\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5202 - val_loss: 0.1722\n",
      "Epoch 14/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.4828\n",
      "Epoch 00014: val_loss did not improve from 0.16869\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4812 - val_loss: 0.1699\n",
      " ###8 fold : val acc1 0.488, acc3 0.928, mae 0.299###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/154 [============================>.] - ETA: 0s - loss: 10.6312\n",
      "Epoch 00001: val_loss improved from inf to 1.65256, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 10.5101 - val_loss: 1.6526\n",
      "Epoch 2/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 1.6282\n",
      "Epoch 00002: val_loss improved from 1.65256 to 0.61024, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.5842 - val_loss: 0.6102\n",
      "Epoch 3/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 2.8440\n",
      "Epoch 00003: val_loss improved from 0.61024 to 0.33959, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.8415 - val_loss: 0.3396\n",
      "Epoch 4/100\n",
      "139/154 [==========================>...] - ETA: 0s - loss: 1.1950\n",
      "Epoch 00004: val_loss improved from 0.33959 to 0.28844, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1707 - val_loss: 0.2884\n",
      "Epoch 5/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 1.6171\n",
      "Epoch 00005: val_loss improved from 0.28844 to 0.24336, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.5638 - val_loss: 0.2434\n",
      "Epoch 6/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 1.1465\n",
      "Epoch 00006: val_loss improved from 0.24336 to 0.23002, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.1155 - val_loss: 0.2300\n",
      "Epoch 7/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 1.0718\n",
      "Epoch 00007: val_loss did not improve from 0.23002\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 1.0447 - val_loss: 0.2549\n",
      "Epoch 8/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.8629\n",
      "Epoch 00008: val_loss improved from 0.23002 to 0.19598, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8456 - val_loss: 0.1960\n",
      "Epoch 9/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.8905\n",
      "Epoch 00009: val_loss improved from 0.19598 to 0.18884, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8735 - val_loss: 0.1888\n",
      "Epoch 10/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.5778\n",
      "Epoch 00010: val_loss improved from 0.18884 to 0.18553, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5768 - val_loss: 0.1855\n",
      "Epoch 11/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.8063\n",
      "Epoch 00011: val_loss did not improve from 0.18553\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7834 - val_loss: 0.1917\n",
      "Epoch 12/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.6754\n",
      "Epoch 00012: val_loss improved from 0.18553 to 0.17059, saving model to result/size/DNN_size_both_y/batch128,dnodes16_dropout0.2,dnodes128_dropout0.3,lr0.0005/weights_9.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6598 - val_loss: 0.1706\n",
      "Epoch 13/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.5253\n",
      "Epoch 00013: val_loss did not improve from 0.17059\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5202 - val_loss: 0.1749\n",
      "Epoch 14/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.4812\n",
      "Epoch 00014: val_loss did not improve from 0.17059\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4812 - val_loss: 0.1719\n",
      " ###9 fold : val acc1 0.490, acc3 0.941, mae 0.291###\n",
      "acc10.504_acc30.943\n",
      "random search 30/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/77 [=========================>....] - ETA: 0s - loss: 23.4147\n",
      "Epoch 00001: val_loss improved from inf to 19.00243, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 23.0277 - val_loss: 19.0024\n",
      "Epoch 2/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 16.0890\n",
      "Epoch 00002: val_loss improved from 19.00243 to 12.45377, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 15.8225 - val_loss: 12.4538\n",
      "Epoch 3/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 10.3336\n",
      "Epoch 00003: val_loss improved from 12.45377 to 7.10224, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 9.9189 - val_loss: 7.1022\n",
      "Epoch 4/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 5.9534\n",
      "Epoch 00004: val_loss improved from 7.10224 to 3.74889, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 5.7639 - val_loss: 3.7489\n",
      "Epoch 5/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 3.7510\n",
      "Epoch 00005: val_loss improved from 3.74889 to 2.25001, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.6552 - val_loss: 2.2500\n",
      "Epoch 6/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 2.9809\n",
      "Epoch 00006: val_loss improved from 2.25001 to 1.66268, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.9235 - val_loss: 1.6627\n",
      "Epoch 7/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 2.5616\n",
      "Epoch 00007: val_loss improved from 1.66268 to 1.35845, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.5253 - val_loss: 1.3584\n",
      "Epoch 8/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 2.3456\n",
      "Epoch 00008: val_loss improved from 1.35845 to 1.14756, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.3033 - val_loss: 1.1476\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 2.1287\n",
      "Epoch 00009: val_loss improved from 1.14756 to 0.97767, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.1287 - val_loss: 0.9777\n",
      "Epoch 10/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.9881\n",
      "Epoch 00010: val_loss improved from 0.97767 to 0.83811, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.9774 - val_loss: 0.8381\n",
      "Epoch 11/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.8561\n",
      "Epoch 00011: val_loss improved from 0.83811 to 0.72611, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.8316 - val_loss: 0.7261\n",
      "Epoch 12/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.7623\n",
      "Epoch 00012: val_loss improved from 0.72611 to 0.63257, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7525 - val_loss: 0.6326\n",
      "Epoch 13/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.5994\n",
      "Epoch 00013: val_loss improved from 0.63257 to 0.56355, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5901 - val_loss: 0.5635\n",
      "Epoch 14/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.5647\n",
      "Epoch 00014: val_loss improved from 0.56355 to 0.49795, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5526 - val_loss: 0.4979\n",
      "Epoch 15/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.5105\n",
      "Epoch 00015: val_loss improved from 0.49795 to 0.44072, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4973 - val_loss: 0.4407\n",
      "Epoch 16/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.4516\n",
      "Epoch 00016: val_loss improved from 0.44072 to 0.39982, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4354 - val_loss: 0.3998\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.3668\n",
      "Epoch 00017: val_loss improved from 0.39982 to 0.36826, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3668 - val_loss: 0.3683\n",
      "Epoch 18/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.3046\n",
      "Epoch 00018: val_loss improved from 0.36826 to 0.34555, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2953 - val_loss: 0.3455\n",
      "Epoch 19/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.2656\n",
      "Epoch 00019: val_loss improved from 0.34555 to 0.31006, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2455 - val_loss: 0.3101\n",
      "Epoch 20/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.2153\n",
      "Epoch 00020: val_loss improved from 0.31006 to 0.29461, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2321 - val_loss: 0.2946\n",
      "Epoch 21/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.1430\n",
      "Epoch 00021: val_loss improved from 0.29461 to 0.28330, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1354 - val_loss: 0.2833\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.1401\n",
      "Epoch 00022: val_loss improved from 0.28330 to 0.26105, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1401 - val_loss: 0.2610\n",
      "Epoch 23/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.0985\n",
      "Epoch 00023: val_loss improved from 0.26105 to 0.24916, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0955 - val_loss: 0.2492\n",
      "Epoch 24/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.0541\n",
      "Epoch 00024: val_loss improved from 0.24916 to 0.23827, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0465 - val_loss: 0.2383\n",
      "Epoch 25/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.9918\n",
      "Epoch 00025: val_loss improved from 0.23827 to 0.23355, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0133 - val_loss: 0.2335\n",
      "Epoch 26/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.9617\n",
      "Epoch 00026: val_loss improved from 0.23355 to 0.22306, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9555 - val_loss: 0.2231\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.9084\n",
      "Epoch 00027: val_loss improved from 0.22306 to 0.21148, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9084 - val_loss: 0.2115\n",
      "Epoch 28/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.8987\n",
      "Epoch 00028: val_loss improved from 0.21148 to 0.20748, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8941 - val_loss: 0.2075\n",
      "Epoch 29/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8724\n",
      "Epoch 00029: val_loss improved from 0.20748 to 0.20314, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8758 - val_loss: 0.2031\n",
      "Epoch 30/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.8527\n",
      "Epoch 00030: val_loss improved from 0.20314 to 0.19405, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8472 - val_loss: 0.1940\n",
      "Epoch 31/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.8393\n",
      "Epoch 00031: val_loss improved from 0.19405 to 0.19196, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8426 - val_loss: 0.1920\n",
      "Epoch 32/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8348\n",
      "Epoch 00032: val_loss improved from 0.19196 to 0.19078, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8188 - val_loss: 0.1908\n",
      "Epoch 33/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7876\n",
      "Epoch 00033: val_loss improved from 0.19078 to 0.18434, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7862 - val_loss: 0.1843\n",
      "Epoch 34/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7644\n",
      "Epoch 00034: val_loss did not improve from 0.18434\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7634 - val_loss: 0.1865\n",
      "Epoch 35/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7460\n",
      "Epoch 00035: val_loss improved from 0.18434 to 0.18383, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7445 - val_loss: 0.1838\n",
      "Epoch 36/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7371\n",
      "Epoch 00036: val_loss improved from 0.18383 to 0.17924, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7425 - val_loss: 0.1792\n",
      "Epoch 37/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7280\n",
      "Epoch 00037: val_loss improved from 0.17924 to 0.17566, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7275 - val_loss: 0.1757\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.7107\n",
      "Epoch 00038: val_loss did not improve from 0.17566\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7107 - val_loss: 0.1784\n",
      "Epoch 39/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7165\n",
      "Epoch 00039: val_loss improved from 0.17566 to 0.17503, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7144 - val_loss: 0.1750\n",
      "Epoch 40/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6946\n",
      "Epoch 00040: val_loss improved from 0.17503 to 0.16733, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_0.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6843 - val_loss: 0.1673\n",
      "Epoch 41/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6758\n",
      "Epoch 00041: val_loss did not improve from 0.16733\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7114 - val_loss: 0.1784\n",
      "Epoch 42/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6787\n",
      "Epoch 00042: val_loss did not improve from 0.16733\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6738 - val_loss: 0.1719\n",
      " ###0 fold : val acc1 0.526, acc3 0.939, mae 0.276###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/77 [=======================>......] - ETA: 0s - loss: 23.6672\n",
      "Epoch 00001: val_loss improved from inf to 19.00943, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 23.0214 - val_loss: 19.0094\n",
      "Epoch 2/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 16.2946\n",
      "Epoch 00002: val_loss improved from 19.00943 to 12.46520, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 15.8189 - val_loss: 12.4652\n",
      "Epoch 3/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 10.3861\n",
      "Epoch 00003: val_loss improved from 12.46520 to 7.11311, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 9.9389 - val_loss: 7.1131\n",
      "Epoch 4/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 6.1386\n",
      "Epoch 00004: val_loss improved from 7.11311 to 3.75159, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 5.7831 - val_loss: 3.7516\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 3.6514\n",
      "Epoch 00005: val_loss improved from 3.75159 to 2.24851, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 3.6514 - val_loss: 2.2485\n",
      "Epoch 6/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 2.9876\n",
      "Epoch 00006: val_loss improved from 2.24851 to 1.65804, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.9189 - val_loss: 1.6580\n",
      "Epoch 7/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 2.5247\n",
      "Epoch 00007: val_loss improved from 1.65804 to 1.35435, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.5217 - val_loss: 1.3543\n",
      "Epoch 8/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 2.3026\n",
      "Epoch 00008: val_loss improved from 1.35435 to 1.14203, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.2979 - val_loss: 1.1420\n",
      "Epoch 9/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 2.1344\n",
      "Epoch 00009: val_loss improved from 1.14203 to 0.97237, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.1328 - val_loss: 0.9724\n",
      "Epoch 10/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.9643\n",
      "Epoch 00010: val_loss improved from 0.97237 to 0.83101, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.9629 - val_loss: 0.8310\n",
      "Epoch 11/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.8382\n",
      "Epoch 00011: val_loss improved from 0.83101 to 0.72038, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.8375 - val_loss: 0.7204\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.7444\n",
      "Epoch 00012: val_loss improved from 0.72038 to 0.62754, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7444 - val_loss: 0.6275\n",
      "Epoch 13/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.6023\n",
      "Epoch 00013: val_loss improved from 0.62754 to 0.55915, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5963 - val_loss: 0.5592\n",
      "Epoch 14/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.5575\n",
      "Epoch 00014: val_loss improved from 0.55915 to 0.49370, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5535 - val_loss: 0.4937\n",
      "Epoch 15/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.5157\n",
      "Epoch 00015: val_loss improved from 0.49370 to 0.43476, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5139 - val_loss: 0.4348\n",
      "Epoch 16/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.4471\n",
      "Epoch 00016: val_loss improved from 0.43476 to 0.39637, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4306 - val_loss: 0.3964\n",
      "Epoch 17/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.3776\n",
      "Epoch 00017: val_loss improved from 0.39637 to 0.36258, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3766 - val_loss: 0.3626\n",
      "Epoch 18/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.3072\n",
      "Epoch 00018: val_loss improved from 0.36258 to 0.33742, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3060 - val_loss: 0.3374\n",
      "Epoch 19/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 1.2664\n",
      "Epoch 00019: val_loss improved from 0.33742 to 0.30548, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.2510 - val_loss: 0.3055\n",
      "Epoch 20/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 1.2091\n",
      "Epoch 00020: val_loss improved from 0.30548 to 0.28933, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.2272 - val_loss: 0.2893\n",
      "Epoch 21/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.1361\n",
      "Epoch 00021: val_loss improved from 0.28933 to 0.27856, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1302 - val_loss: 0.2786\n",
      "Epoch 22/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.1227\n",
      "Epoch 00022: val_loss improved from 0.27856 to 0.25590, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1364 - val_loss: 0.2559\n",
      "Epoch 23/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.1051\n",
      "Epoch 00023: val_loss improved from 0.25590 to 0.24739, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1025 - val_loss: 0.2474\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.0415\n",
      "Epoch 00024: val_loss improved from 0.24739 to 0.23446, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0415 - val_loss: 0.2345\n",
      "Epoch 25/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0171\n",
      "Epoch 00025: val_loss improved from 0.23446 to 0.22927, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0173 - val_loss: 0.2293\n",
      "Epoch 26/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9547\n",
      "Epoch 00026: val_loss improved from 0.22927 to 0.21803, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9560 - val_loss: 0.2180\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.9056\n",
      "Epoch 00027: val_loss improved from 0.21803 to 0.20753, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9056 - val_loss: 0.2075\n",
      "Epoch 28/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8930\n",
      "Epoch 00028: val_loss improved from 0.20753 to 0.20409, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8903 - val_loss: 0.2041\n",
      "Epoch 29/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8793\n",
      "Epoch 00029: val_loss improved from 0.20409 to 0.19937, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8793 - val_loss: 0.1994\n",
      "Epoch 30/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.8468\n",
      "Epoch 00030: val_loss improved from 0.19937 to 0.19067, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8481 - val_loss: 0.1907\n",
      "Epoch 31/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8323\n",
      "Epoch 00031: val_loss improved from 0.19067 to 0.18977, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8359 - val_loss: 0.1898\n",
      "Epoch 32/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8421\n",
      "Epoch 00032: val_loss improved from 0.18977 to 0.18665, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8242 - val_loss: 0.1866\n",
      "Epoch 33/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7853\n",
      "Epoch 00033: val_loss improved from 0.18665 to 0.18228, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7868 - val_loss: 0.1823\n",
      "Epoch 34/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7617\n",
      "Epoch 00034: val_loss did not improve from 0.18228\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7650 - val_loss: 0.1831\n",
      "Epoch 35/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7539\n",
      "Epoch 00035: val_loss improved from 0.18228 to 0.18077, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7540 - val_loss: 0.1808\n",
      "Epoch 36/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7422\n",
      "Epoch 00036: val_loss improved from 0.18077 to 0.17648, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7440 - val_loss: 0.1765\n",
      "Epoch 37/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7260\n",
      "Epoch 00037: val_loss improved from 0.17648 to 0.17268, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_1.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7256 - val_loss: 0.1727\n",
      "Epoch 38/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7125\n",
      "Epoch 00038: val_loss did not improve from 0.17268\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7129 - val_loss: 0.1760\n",
      "Epoch 39/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7139\n",
      "Epoch 00039: val_loss did not improve from 0.17268\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7136 - val_loss: 0.1731\n",
      " ###1 fold : val acc1 0.511, acc3 0.942, mae 0.278###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/77 [==========================>...] - ETA: 0s - loss: 23.4075\n",
      "Epoch 00001: val_loss improved from inf to 18.99388, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 23.0398 - val_loss: 18.9939\n",
      "Epoch 2/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 16.0503\n",
      "Epoch 00002: val_loss improved from 18.99388 to 12.41878, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 15.8185 - val_loss: 12.4188\n",
      "Epoch 3/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 10.1113\n",
      "Epoch 00003: val_loss improved from 12.41878 to 7.05553, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 9.8700 - val_loss: 7.0555\n",
      "Epoch 4/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 5.8189\n",
      "Epoch 00004: val_loss improved from 7.05553 to 3.74652, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 5.7442 - val_loss: 3.7465\n",
      "Epoch 5/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 3.6936\n",
      "Epoch 00005: val_loss improved from 3.74652 to 2.26428, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.6491 - val_loss: 2.2643\n",
      "Epoch 6/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 2.9540\n",
      "Epoch 00006: val_loss improved from 2.26428 to 1.67238, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.9329 - val_loss: 1.6724\n",
      "Epoch 7/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 2.5551\n",
      "Epoch 00007: val_loss improved from 1.67238 to 1.36673, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.5242 - val_loss: 1.3667\n",
      "Epoch 8/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 2.3455\n",
      "Epoch 00008: val_loss improved from 1.36673 to 1.15329, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.3108 - val_loss: 1.1533\n",
      "Epoch 9/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 2.1506\n",
      "Epoch 00009: val_loss improved from 1.15329 to 0.98009, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.1343 - val_loss: 0.9801\n",
      "Epoch 10/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.9629\n",
      "Epoch 00010: val_loss improved from 0.98009 to 0.84028, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.9617 - val_loss: 0.8403\n",
      "Epoch 11/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.8774\n",
      "Epoch 00011: val_loss improved from 0.84028 to 0.73001, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.8534 - val_loss: 0.7300\n",
      "Epoch 12/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.7629\n",
      "Epoch 00012: val_loss improved from 0.73001 to 0.63723, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.7535 - val_loss: 0.6372\n",
      "Epoch 13/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.6038\n",
      "Epoch 00013: val_loss improved from 0.63723 to 0.56304, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.6026 - val_loss: 0.5630\n",
      "Epoch 14/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.5653\n",
      "Epoch 00014: val_loss improved from 0.56304 to 0.49629, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5511 - val_loss: 0.4963\n",
      "Epoch 15/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.5444\n",
      "Epoch 00015: val_loss improved from 0.49629 to 0.43902, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5248 - val_loss: 0.4390\n",
      "Epoch 16/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.4448\n",
      "Epoch 00016: val_loss improved from 0.43902 to 0.40062, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4375 - val_loss: 0.4006\n",
      "Epoch 17/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.3602\n",
      "Epoch 00017: val_loss improved from 0.40062 to 0.36433, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3641 - val_loss: 0.3643\n",
      "Epoch 18/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.3194\n",
      "Epoch 00018: val_loss improved from 0.36433 to 0.33983, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.3102 - val_loss: 0.3398\n",
      "Epoch 19/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.2574\n",
      "Epoch 00019: val_loss improved from 0.33983 to 0.30759, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2423 - val_loss: 0.3076\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.2224\n",
      "Epoch 00020: val_loss improved from 0.30759 to 0.28946, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2224 - val_loss: 0.2895\n",
      "Epoch 21/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.1303\n",
      "Epoch 00021: val_loss improved from 0.28946 to 0.27996, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1255 - val_loss: 0.2800\n",
      "Epoch 22/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.1209\n",
      "Epoch 00022: val_loss improved from 0.27996 to 0.25870, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1309 - val_loss: 0.2587\n",
      "Epoch 23/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.0972\n",
      "Epoch 00023: val_loss improved from 0.25870 to 0.25009, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0965 - val_loss: 0.2501\n",
      "Epoch 24/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.0506\n",
      "Epoch 00024: val_loss improved from 0.25009 to 0.23479, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0449 - val_loss: 0.2348\n",
      "Epoch 25/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.9927\n",
      "Epoch 00025: val_loss improved from 0.23479 to 0.22910, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0097 - val_loss: 0.2291\n",
      "Epoch 26/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.9647\n",
      "Epoch 00026: val_loss improved from 0.22910 to 0.21870, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9619 - val_loss: 0.2187\n",
      "Epoch 27/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.9046\n",
      "Epoch 00027: val_loss improved from 0.21870 to 0.20810, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9075 - val_loss: 0.2081\n",
      "Epoch 28/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.8835\n",
      "Epoch 00028: val_loss improved from 0.20810 to 0.20367, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8812 - val_loss: 0.2037\n",
      "Epoch 29/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.8756\n",
      "Epoch 00029: val_loss improved from 0.20367 to 0.20113, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8728 - val_loss: 0.2011\n",
      "Epoch 30/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.8541\n",
      "Epoch 00030: val_loss improved from 0.20113 to 0.19066, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8485 - val_loss: 0.1907\n",
      "Epoch 31/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8303\n",
      "Epoch 00031: val_loss improved from 0.19066 to 0.19051, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8307 - val_loss: 0.1905\n",
      "Epoch 32/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.8393\n",
      "Epoch 00032: val_loss improved from 0.19051 to 0.18804, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8214 - val_loss: 0.1880\n",
      "Epoch 33/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7868\n",
      "Epoch 00033: val_loss improved from 0.18804 to 0.18367, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7873 - val_loss: 0.1837\n",
      "Epoch 34/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7586\n",
      "Epoch 00034: val_loss improved from 0.18367 to 0.18303, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7625 - val_loss: 0.1830\n",
      "Epoch 35/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7519\n",
      "Epoch 00035: val_loss improved from 0.18303 to 0.18024, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7519 - val_loss: 0.1802\n",
      "Epoch 36/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7430\n",
      "Epoch 00036: val_loss improved from 0.18024 to 0.17787, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7444 - val_loss: 0.1779\n",
      "Epoch 37/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7335\n",
      "Epoch 00037: val_loss improved from 0.17787 to 0.17461, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7285 - val_loss: 0.1746\n",
      "Epoch 38/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.7000\n",
      "Epoch 00038: val_loss did not improve from 0.17461\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7074 - val_loss: 0.1761\n",
      "Epoch 39/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7190\n",
      "Epoch 00039: val_loss improved from 0.17461 to 0.17337, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7136 - val_loss: 0.1734\n",
      "Epoch 40/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6855\n",
      "Epoch 00040: val_loss improved from 0.17337 to 0.16675, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_2.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6793 - val_loss: 0.1667\n",
      "Epoch 41/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.7202\n",
      "Epoch 00041: val_loss did not improve from 0.16675\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7099 - val_loss: 0.1801\n",
      "Epoch 42/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6815\n",
      "Epoch 00042: val_loss did not improve from 0.16675\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6784 - val_loss: 0.1752\n",
      " ###2 fold : val acc1 0.511, acc3 0.941, mae 0.280###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 23.5501\n",
      "Epoch 00001: val_loss improved from inf to 19.01273, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 23.0282 - val_loss: 19.0127\n",
      "Epoch 2/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 16.2744\n",
      "Epoch 00002: val_loss improved from 19.01273 to 12.47052, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 15.8337 - val_loss: 12.4705\n",
      "Epoch 3/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 10.1666\n",
      "Epoch 00003: val_loss improved from 12.47052 to 7.10234, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 9.9410 - val_loss: 7.1023\n",
      "Epoch 4/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 5.8749\n",
      "Epoch 00004: val_loss improved from 7.10234 to 3.75421, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 5.7698 - val_loss: 3.7542\n",
      "Epoch 5/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 3.7541\n",
      "Epoch 00005: val_loss improved from 3.75421 to 2.24875, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.6631 - val_loss: 2.2488\n",
      "Epoch 6/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 2.9774\n",
      "Epoch 00006: val_loss improved from 2.24875 to 1.65935, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.9272 - val_loss: 1.6594\n",
      "Epoch 7/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 2.5119\n",
      "Epoch 00007: val_loss improved from 1.65935 to 1.35361, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.5005 - val_loss: 1.3536\n",
      "Epoch 8/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 2.3034\n",
      "Epoch 00008: val_loss improved from 1.35361 to 1.14217, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.2972 - val_loss: 1.1422\n",
      "Epoch 9/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 2.1351\n",
      "Epoch 00009: val_loss improved from 1.14217 to 0.96854, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.1208 - val_loss: 0.9685\n",
      "Epoch 10/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.9529\n",
      "Epoch 00010: val_loss improved from 0.96854 to 0.82849, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.9519 - val_loss: 0.8285\n",
      "Epoch 11/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.8931\n",
      "Epoch 00011: val_loss improved from 0.82849 to 0.71831, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.8613 - val_loss: 0.7183\n",
      "Epoch 12/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.7507\n",
      "Epoch 00012: val_loss improved from 0.71831 to 0.63134, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7448 - val_loss: 0.6313\n",
      "Epoch 13/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.5980\n",
      "Epoch 00013: val_loss improved from 0.63134 to 0.55920, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5919 - val_loss: 0.5592\n",
      "Epoch 14/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.5697\n",
      "Epoch 00014: val_loss improved from 0.55920 to 0.49244, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5587 - val_loss: 0.4924\n",
      "Epoch 15/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.5478\n",
      "Epoch 00015: val_loss improved from 0.49244 to 0.43656, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5265 - val_loss: 0.4366\n",
      "Epoch 16/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.4523\n",
      "Epoch 00016: val_loss improved from 0.43656 to 0.39929, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4356 - val_loss: 0.3993\n",
      "Epoch 17/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.3498\n",
      "Epoch 00017: val_loss improved from 0.39929 to 0.36367, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3550 - val_loss: 0.3637\n",
      "Epoch 18/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.3335\n",
      "Epoch 00018: val_loss improved from 0.36367 to 0.33633, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3241 - val_loss: 0.3363\n",
      "Epoch 19/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.2537\n",
      "Epoch 00019: val_loss improved from 0.33633 to 0.30695, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2445 - val_loss: 0.3069\n",
      "Epoch 20/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.2063\n",
      "Epoch 00020: val_loss improved from 0.30695 to 0.28849, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2150 - val_loss: 0.2885\n",
      "Epoch 21/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.1379\n",
      "Epoch 00021: val_loss improved from 0.28849 to 0.27936, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1317 - val_loss: 0.2794\n",
      "Epoch 22/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.1215\n",
      "Epoch 00022: val_loss improved from 0.27936 to 0.25765, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1268 - val_loss: 0.2577\n",
      "Epoch 23/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.1015\n",
      "Epoch 00023: val_loss improved from 0.25765 to 0.24810, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0978 - val_loss: 0.2481\n",
      "Epoch 24/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.0639\n",
      "Epoch 00024: val_loss improved from 0.24810 to 0.23442, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0569 - val_loss: 0.2344\n",
      "Epoch 25/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.9876\n",
      "Epoch 00025: val_loss improved from 0.23442 to 0.22945, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0076 - val_loss: 0.2294\n",
      "Epoch 26/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.9705\n",
      "Epoch 00026: val_loss improved from 0.22945 to 0.21868, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9658 - val_loss: 0.2187\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.9153\n",
      "Epoch 00027: val_loss improved from 0.21868 to 0.20960, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9154 - val_loss: 0.2096\n",
      "Epoch 28/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8805\n",
      "Epoch 00028: val_loss improved from 0.20960 to 0.20499, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8779 - val_loss: 0.2050\n",
      "Epoch 29/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.8782\n",
      "Epoch 00029: val_loss improved from 0.20499 to 0.20177, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8736 - val_loss: 0.2018\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8488\n",
      "Epoch 00030: val_loss improved from 0.20177 to 0.19089, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8488 - val_loss: 0.1909\n",
      "Epoch 31/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8378\n",
      "Epoch 00031: val_loss improved from 0.19089 to 0.19044, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8375 - val_loss: 0.1904\n",
      "Epoch 32/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8357\n",
      "Epoch 00032: val_loss improved from 0.19044 to 0.18949, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8216 - val_loss: 0.1895\n",
      "Epoch 33/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7909\n",
      "Epoch 00033: val_loss improved from 0.18949 to 0.18308, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7891 - val_loss: 0.1831\n",
      "Epoch 34/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7612\n",
      "Epoch 00034: val_loss did not improve from 0.18308\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7654 - val_loss: 0.1844\n",
      "Epoch 35/100\n",
      "58/77 [=====================>........] - ETA: 0s - loss: 0.7596\n",
      "Epoch 00035: val_loss improved from 0.18308 to 0.17872, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7568 - val_loss: 0.1787\n",
      "Epoch 36/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.7450\n",
      "Epoch 00036: val_loss improved from 0.17872 to 0.17700, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7459 - val_loss: 0.1770\n",
      "Epoch 37/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7364\n",
      "Epoch 00037: val_loss improved from 0.17700 to 0.17552, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7322 - val_loss: 0.1755\n",
      "Epoch 38/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6999\n",
      "Epoch 00038: val_loss did not improve from 0.17552\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7076 - val_loss: 0.1756\n",
      "Epoch 39/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7204\n",
      "Epoch 00039: val_loss improved from 0.17552 to 0.17208, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7175 - val_loss: 0.1721\n",
      "Epoch 40/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6841\n",
      "Epoch 00040: val_loss improved from 0.17208 to 0.16681, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_3.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6765 - val_loss: 0.1668\n",
      "Epoch 41/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.7254\n",
      "Epoch 00041: val_loss did not improve from 0.16681\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7119 - val_loss: 0.1805\n",
      "Epoch 42/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6814\n",
      "Epoch 00042: val_loss did not improve from 0.16681\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6815 - val_loss: 0.1752\n",
      " ###3 fold : val acc1 0.526, acc3 0.946, mae 0.269###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/77 [=========================>....] - ETA: 0s - loss: 23.5420\n",
      "Epoch 00001: val_loss improved from inf to 19.03775, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 23.0914 - val_loss: 19.0378\n",
      "Epoch 2/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 16.2063\n",
      "Epoch 00002: val_loss improved from 19.03775 to 12.42833, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 15.8035 - val_loss: 12.4283\n",
      "Epoch 3/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 10.3083\n",
      "Epoch 00003: val_loss improved from 12.42833 to 7.04775, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 9.8935 - val_loss: 7.0478\n",
      "Epoch 4/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 5.9472\n",
      "Epoch 00004: val_loss improved from 7.04775 to 3.73840, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 5.7201 - val_loss: 3.7384\n",
      "Epoch 5/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 3.8139\n",
      "Epoch 00005: val_loss improved from 3.73840 to 2.26783, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.7221 - val_loss: 2.2678\n",
      "Epoch 6/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 2.9516\n",
      "Epoch 00006: val_loss improved from 2.26783 to 1.67404, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.9054 - val_loss: 1.6740\n",
      "Epoch 7/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 2.5113\n",
      "Epoch 00007: val_loss improved from 1.67404 to 1.36947, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.4750 - val_loss: 1.3695\n",
      "Epoch 8/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 2.3249\n",
      "Epoch 00008: val_loss improved from 1.36947 to 1.15649, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.3184 - val_loss: 1.1565\n",
      "Epoch 9/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 2.1436\n",
      "Epoch 00009: val_loss improved from 1.15649 to 0.98398, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.1353 - val_loss: 0.9840\n",
      "Epoch 10/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.9858\n",
      "Epoch 00010: val_loss improved from 0.98398 to 0.83807, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.9785 - val_loss: 0.8381\n",
      "Epoch 11/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.9142\n",
      "Epoch 00011: val_loss improved from 0.83807 to 0.72873, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.8926 - val_loss: 0.7287\n",
      "Epoch 12/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.7322\n",
      "Epoch 00012: val_loss improved from 0.72873 to 0.63449, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7251 - val_loss: 0.6345\n",
      "Epoch 13/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.6948\n",
      "Epoch 00013: val_loss improved from 0.63449 to 0.55846, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.6796 - val_loss: 0.5585\n",
      "Epoch 14/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.5507\n",
      "Epoch 00014: val_loss improved from 0.55846 to 0.49542, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.5592 - val_loss: 0.4954\n",
      "Epoch 15/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.5128\n",
      "Epoch 00015: val_loss improved from 0.49542 to 0.44561, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.4991 - val_loss: 0.4456\n",
      "Epoch 16/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.4486\n",
      "Epoch 00016: val_loss improved from 0.44561 to 0.41212, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4415 - val_loss: 0.4121\n",
      "Epoch 17/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.3876\n",
      "Epoch 00017: val_loss improved from 0.41212 to 0.37558, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3670 - val_loss: 0.3756\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.3345\n",
      "Epoch 00018: val_loss improved from 0.37558 to 0.34442, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3345 - val_loss: 0.3444\n",
      "Epoch 19/100\n",
      "58/77 [=====================>........] - ETA: 0s - loss: 1.2870\n",
      "Epoch 00019: val_loss improved from 0.34442 to 0.31411, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2896 - val_loss: 0.3141\n",
      "Epoch 20/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.2298\n",
      "Epoch 00020: val_loss improved from 0.31411 to 0.29661, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2236 - val_loss: 0.2966\n",
      "Epoch 21/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.1810\n",
      "Epoch 00021: val_loss improved from 0.29661 to 0.28780, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1644 - val_loss: 0.2878\n",
      "Epoch 22/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.1373\n",
      "Epoch 00022: val_loss improved from 0.28780 to 0.27043, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1349 - val_loss: 0.2704\n",
      "Epoch 23/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.1188\n",
      "Epoch 00023: val_loss improved from 0.27043 to 0.25875, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1175 - val_loss: 0.2587\n",
      "Epoch 24/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.0723\n",
      "Epoch 00024: val_loss improved from 0.25875 to 0.24291, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0726 - val_loss: 0.2429\n",
      "Epoch 25/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.0125\n",
      "Epoch 00025: val_loss improved from 0.24291 to 0.23436, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0192 - val_loss: 0.2344\n",
      "Epoch 26/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.0015\n",
      "Epoch 00026: val_loss improved from 0.23436 to 0.22589, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0034 - val_loss: 0.2259\n",
      "Epoch 27/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.9764\n",
      "Epoch 00027: val_loss did not improve from 0.22589\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.9804 - val_loss: 0.2266\n",
      "Epoch 28/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.9380\n",
      "Epoch 00028: val_loss improved from 0.22589 to 0.21030, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9325 - val_loss: 0.2103\n",
      "Epoch 29/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.8907\n",
      "Epoch 00029: val_loss improved from 0.21030 to 0.20772, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8884 - val_loss: 0.2077\n",
      "Epoch 30/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8581\n",
      "Epoch 00030: val_loss improved from 0.20772 to 0.19788, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8505 - val_loss: 0.1979\n",
      "Epoch 31/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8556\n",
      "Epoch 00031: val_loss improved from 0.19788 to 0.19294, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8437 - val_loss: 0.1929\n",
      "Epoch 32/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.8367\n",
      "Epoch 00032: val_loss did not improve from 0.19294\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8238 - val_loss: 0.1936\n",
      "Epoch 33/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8130\n",
      "Epoch 00033: val_loss improved from 0.19294 to 0.18945, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7996 - val_loss: 0.1894\n",
      "Epoch 34/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7792\n",
      "Epoch 00034: val_loss improved from 0.18945 to 0.18390, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7808 - val_loss: 0.1839\n",
      "Epoch 35/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7622\n",
      "Epoch 00035: val_loss improved from 0.18390 to 0.18128, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7669 - val_loss: 0.1813\n",
      "Epoch 36/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7430\n",
      "Epoch 00036: val_loss improved from 0.18128 to 0.17941, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7470 - val_loss: 0.1794\n",
      "Epoch 37/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7352\n",
      "Epoch 00037: val_loss improved from 0.17941 to 0.17408, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7304 - val_loss: 0.1741\n",
      "Epoch 38/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.7110\n",
      "Epoch 00038: val_loss did not improve from 0.17408\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7151 - val_loss: 0.1748\n",
      "Epoch 39/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6935\n",
      "Epoch 00039: val_loss improved from 0.17408 to 0.17332, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6917 - val_loss: 0.1733\n",
      "Epoch 40/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6816\n",
      "Epoch 00040: val_loss did not improve from 0.17332\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6954 - val_loss: 0.1749\n",
      "Epoch 41/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6893\n",
      "Epoch 00041: val_loss improved from 0.17332 to 0.16723, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6817 - val_loss: 0.1672\n",
      "Epoch 42/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6684\n",
      "Epoch 00042: val_loss did not improve from 0.16723\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6686 - val_loss: 0.1682\n",
      "Epoch 43/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6626\n",
      "Epoch 00043: val_loss improved from 0.16723 to 0.16488, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_4.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6548 - val_loss: 0.1649\n",
      "Epoch 44/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6604\n",
      "Epoch 00044: val_loss did not improve from 0.16488\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6578 - val_loss: 0.1688\n",
      "Epoch 45/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6478\n",
      "Epoch 00045: val_loss did not improve from 0.16488\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6430 - val_loss: 0.1652\n",
      " ###4 fold : val acc1 0.546, acc3 0.942, mae 0.261###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 23.4737\n",
      "Epoch 00001: val_loss improved from inf to 19.01030, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 22.9812 - val_loss: 19.0103\n",
      "Epoch 2/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 16.1408\n",
      "Epoch 00002: val_loss improved from 19.01030 to 12.41223, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 15.7815 - val_loss: 12.4122\n",
      "Epoch 3/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 10.1712\n",
      "Epoch 00003: val_loss improved from 12.41223 to 7.01705, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 9.8541 - val_loss: 7.0170\n",
      "Epoch 4/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 5.9291\n",
      "Epoch 00004: val_loss improved from 7.01705 to 3.70813, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 5.7003 - val_loss: 3.7081\n",
      "Epoch 5/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 3.7559\n",
      "Epoch 00005: val_loss improved from 3.70813 to 2.24811, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 3.6457 - val_loss: 2.2481\n",
      "Epoch 6/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 2.9566\n",
      "Epoch 00006: val_loss improved from 2.24811 to 1.68192, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.9064 - val_loss: 1.6819\n",
      "Epoch 7/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 2.5174\n",
      "Epoch 00007: val_loss improved from 1.68192 to 1.37992, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.4862 - val_loss: 1.3799\n",
      "Epoch 8/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 2.3253\n",
      "Epoch 00008: val_loss improved from 1.37992 to 1.15831, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.3107 - val_loss: 1.1583\n",
      "Epoch 9/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 2.0924\n",
      "Epoch 00009: val_loss improved from 1.15831 to 0.98048, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.0880 - val_loss: 0.9805\n",
      "Epoch 10/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.9970\n",
      "Epoch 00010: val_loss improved from 0.98048 to 0.83377, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.9880 - val_loss: 0.8338\n",
      "Epoch 11/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.8396\n",
      "Epoch 00011: val_loss improved from 0.83377 to 0.72298, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.8309 - val_loss: 0.7230\n",
      "Epoch 12/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.7325\n",
      "Epoch 00012: val_loss improved from 0.72298 to 0.62938, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7215 - val_loss: 0.6294\n",
      "Epoch 13/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.6365\n",
      "Epoch 00013: val_loss improved from 0.62938 to 0.55382, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.6278 - val_loss: 0.5538\n",
      "Epoch 14/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.5476\n",
      "Epoch 00014: val_loss improved from 0.55382 to 0.48447, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.5523 - val_loss: 0.4845\n",
      "Epoch 15/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.4925\n",
      "Epoch 00015: val_loss improved from 0.48447 to 0.43617, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4897 - val_loss: 0.4362\n",
      "Epoch 16/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.4074\n",
      "Epoch 00016: val_loss improved from 0.43617 to 0.40283, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4095 - val_loss: 0.4028\n",
      "Epoch 17/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.3687\n",
      "Epoch 00017: val_loss improved from 0.40283 to 0.36241, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.3516 - val_loss: 0.3624\n",
      "Epoch 18/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.3152\n",
      "Epoch 00018: val_loss improved from 0.36241 to 0.33051, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3142 - val_loss: 0.3305\n",
      "Epoch 19/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.2595\n",
      "Epoch 00019: val_loss improved from 0.33051 to 0.30027, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2574 - val_loss: 0.3003\n",
      "Epoch 20/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.1939\n",
      "Epoch 00020: val_loss improved from 0.30027 to 0.28307, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.1872 - val_loss: 0.2831\n",
      "Epoch 21/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.1240\n",
      "Epoch 00021: val_loss improved from 0.28307 to 0.27484, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1162 - val_loss: 0.2748\n",
      "Epoch 22/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.0842\n",
      "Epoch 00022: val_loss improved from 0.27484 to 0.25495, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0849 - val_loss: 0.2549\n",
      "Epoch 23/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.0572\n",
      "Epoch 00023: val_loss improved from 0.25495 to 0.24335, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0561 - val_loss: 0.2434\n",
      "Epoch 24/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.0187\n",
      "Epoch 00024: val_loss improved from 0.24335 to 0.22956, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0196 - val_loss: 0.2296\n",
      "Epoch 25/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.9830\n",
      "Epoch 00025: val_loss improved from 0.22956 to 0.22013, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9857 - val_loss: 0.2201\n",
      "Epoch 26/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.9527\n",
      "Epoch 00026: val_loss improved from 0.22013 to 0.21294, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.9515 - val_loss: 0.2129\n",
      "Epoch 27/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.9125\n",
      "Epoch 00027: val_loss improved from 0.21294 to 0.20770, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.9174 - val_loss: 0.2077\n",
      "Epoch 28/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.8632\n",
      "Epoch 00028: val_loss improved from 0.20770 to 0.20017, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8647 - val_loss: 0.2002\n",
      "Epoch 29/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.8551\n",
      "Epoch 00029: val_loss did not improve from 0.20017\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8540 - val_loss: 0.2009\n",
      "Epoch 30/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.8331\n",
      "Epoch 00030: val_loss improved from 0.20017 to 0.18987, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8290 - val_loss: 0.1899\n",
      "Epoch 31/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.8319\n",
      "Epoch 00031: val_loss improved from 0.18987 to 0.18646, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8218 - val_loss: 0.1865\n",
      "Epoch 32/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8144\n",
      "Epoch 00032: val_loss improved from 0.18646 to 0.18609, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8042 - val_loss: 0.1861\n",
      "Epoch 33/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.7808\n",
      "Epoch 00033: val_loss improved from 0.18609 to 0.18268, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7742 - val_loss: 0.1827\n",
      "Epoch 34/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.7644\n",
      "Epoch 00034: val_loss improved from 0.18268 to 0.17850, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7682 - val_loss: 0.1785\n",
      "Epoch 35/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.7572\n",
      "Epoch 00035: val_loss improved from 0.17850 to 0.17766, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7606 - val_loss: 0.1777\n",
      "Epoch 36/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7392\n",
      "Epoch 00036: val_loss improved from 0.17766 to 0.17665, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7434 - val_loss: 0.1767\n",
      "Epoch 37/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.7199\n",
      "Epoch 00037: val_loss improved from 0.17665 to 0.17250, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7139 - val_loss: 0.1725\n",
      "Epoch 38/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.6965\n",
      "Epoch 00038: val_loss improved from 0.17250 to 0.17196, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7023 - val_loss: 0.1720\n",
      "Epoch 39/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6919\n",
      "Epoch 00039: val_loss did not improve from 0.17196\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6922 - val_loss: 0.1726\n",
      "Epoch 40/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6799\n",
      "Epoch 00040: val_loss improved from 0.17196 to 0.16730, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6782 - val_loss: 0.1673\n",
      "Epoch 41/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6760\n",
      "Epoch 00041: val_loss improved from 0.16730 to 0.16647, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6722 - val_loss: 0.1665\n",
      "Epoch 42/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6716\n",
      "Epoch 00042: val_loss did not improve from 0.16647\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6717 - val_loss: 0.1684\n",
      "Epoch 43/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.6578\n",
      "Epoch 00043: val_loss improved from 0.16647 to 0.16542, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6499 - val_loss: 0.1654\n",
      "Epoch 44/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6526\n",
      "Epoch 00044: val_loss did not improve from 0.16542\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6512 - val_loss: 0.1679\n",
      "Epoch 45/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.6383\n",
      "Epoch 00045: val_loss improved from 0.16542 to 0.16436, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6346 - val_loss: 0.1644\n",
      "Epoch 46/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6332\n",
      "Epoch 00046: val_loss improved from 0.16436 to 0.16329, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6319 - val_loss: 0.1633\n",
      "Epoch 47/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6248\n",
      "Epoch 00047: val_loss improved from 0.16329 to 0.16324, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6278 - val_loss: 0.1632\n",
      "Epoch 48/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6280\n",
      "Epoch 00048: val_loss did not improve from 0.16324\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6262 - val_loss: 0.1658\n",
      "Epoch 49/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.6012\n",
      "Epoch 00049: val_loss improved from 0.16324 to 0.16153, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5992 - val_loss: 0.1615\n",
      "Epoch 50/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.5959\n",
      "Epoch 00050: val_loss improved from 0.16153 to 0.15893, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_5.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.5912 - val_loss: 0.1589\n",
      "Epoch 51/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.5857\n",
      "Epoch 00051: val_loss did not improve from 0.15893\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5867 - val_loss: 0.1599\n",
      "Epoch 52/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.5787\n",
      "Epoch 00052: val_loss did not improve from 0.15893\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5723 - val_loss: 0.1600\n",
      " ###5 fold : val acc1 0.543, acc3 0.952, mae 0.268###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/77 [=======================>......] - ETA: 0s - loss: 23.6428\n",
      "Epoch 00001: val_loss improved from inf to 19.03640, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 22.9888 - val_loss: 19.0364\n",
      "Epoch 2/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 16.3142\n",
      "Epoch 00002: val_loss improved from 19.03640 to 12.44619, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 15.8486 - val_loss: 12.4462\n",
      "Epoch 3/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 10.3536\n",
      "Epoch 00003: val_loss improved from 12.44619 to 7.06452, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 9.8977 - val_loss: 7.0645\n",
      "Epoch 4/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 6.0575\n",
      "Epoch 00004: val_loss improved from 7.06452 to 3.73170, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 5.7383 - val_loss: 3.7317\n",
      "Epoch 5/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 3.6866\n",
      "Epoch 00005: val_loss improved from 3.73170 to 2.25273, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 3.6852 - val_loss: 2.2527\n",
      "Epoch 6/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 3.0804\n",
      "Epoch 00006: val_loss improved from 2.25273 to 1.67113, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.9932 - val_loss: 1.6711\n",
      "Epoch 7/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 2.5228\n",
      "Epoch 00007: val_loss improved from 1.67113 to 1.36882, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.5576 - val_loss: 1.3688\n",
      "Epoch 8/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 2.3319\n",
      "Epoch 00008: val_loss improved from 1.36882 to 1.15205, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.3167 - val_loss: 1.1521\n",
      "Epoch 9/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 2.1158\n",
      "Epoch 00009: val_loss improved from 1.15205 to 0.97966, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.1446 - val_loss: 0.9797\n",
      "Epoch 10/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 2.0057\n",
      "Epoch 00010: val_loss improved from 0.97966 to 0.83555, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.0058 - val_loss: 0.8356\n",
      "Epoch 11/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.8426\n",
      "Epoch 00011: val_loss improved from 0.83555 to 0.72510, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.8415 - val_loss: 0.7251\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.7360\n",
      "Epoch 00012: val_loss improved from 0.72510 to 0.63210, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7360 - val_loss: 0.6321\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.6193\n",
      "Epoch 00013: val_loss improved from 0.63210 to 0.55848, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.6193 - val_loss: 0.5585\n",
      "Epoch 14/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.5571\n",
      "Epoch 00014: val_loss improved from 0.55848 to 0.49446, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5573 - val_loss: 0.4945\n",
      "Epoch 15/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.5069\n",
      "Epoch 00015: val_loss improved from 0.49446 to 0.44699, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5097 - val_loss: 0.4470\n",
      "Epoch 16/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.4354\n",
      "Epoch 00016: val_loss improved from 0.44699 to 0.41161, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4351 - val_loss: 0.4116\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.3676\n",
      "Epoch 00017: val_loss improved from 0.41161 to 0.37011, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3676 - val_loss: 0.3701\n",
      "Epoch 18/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.3508\n",
      "Epoch 00018: val_loss improved from 0.37011 to 0.33922, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3511 - val_loss: 0.3392\n",
      "Epoch 19/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.2869\n",
      "Epoch 00019: val_loss improved from 0.33922 to 0.31165, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2920 - val_loss: 0.3117\n",
      "Epoch 20/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.2367\n",
      "Epoch 00020: val_loss improved from 0.31165 to 0.29357, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2208 - val_loss: 0.2936\n",
      "Epoch 21/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.1568\n",
      "Epoch 00021: val_loss improved from 0.29357 to 0.28565, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1543 - val_loss: 0.2857\n",
      "Epoch 22/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.1227\n",
      "Epoch 00022: val_loss improved from 0.28565 to 0.26675, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1219 - val_loss: 0.2668\n",
      "Epoch 23/100\n",
      "58/77 [=====================>........] - ETA: 0s - loss: 1.1061\n",
      "Epoch 00023: val_loss improved from 0.26675 to 0.25650, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1037 - val_loss: 0.2565\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.0792\n",
      "Epoch 00024: val_loss improved from 0.25650 to 0.23901, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0792 - val_loss: 0.2390\n",
      "Epoch 25/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0305\n",
      "Epoch 00025: val_loss improved from 0.23901 to 0.22930, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0295 - val_loss: 0.2293\n",
      "Epoch 26/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.0174\n",
      "Epoch 00026: val_loss improved from 0.22930 to 0.22823, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0186 - val_loss: 0.2282\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.9637\n",
      "Epoch 00027: val_loss improved from 0.22823 to 0.21759, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9674 - val_loss: 0.2176\n",
      "Epoch 28/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.9025\n",
      "Epoch 00028: val_loss improved from 0.21759 to 0.20977, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9082 - val_loss: 0.2098\n",
      "Epoch 29/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8843\n",
      "Epoch 00029: val_loss improved from 0.20977 to 0.20820, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8829 - val_loss: 0.2082\n",
      "Epoch 30/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8542\n",
      "Epoch 00030: val_loss improved from 0.20820 to 0.19774, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8529 - val_loss: 0.1977\n",
      "Epoch 31/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8482\n",
      "Epoch 00031: val_loss improved from 0.19774 to 0.19413, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8468 - val_loss: 0.1941\n",
      "Epoch 32/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8330\n",
      "Epoch 00032: val_loss improved from 0.19413 to 0.19302, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8330 - val_loss: 0.1930\n",
      "Epoch 33/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7992\n",
      "Epoch 00033: val_loss improved from 0.19302 to 0.18709, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7911 - val_loss: 0.1871\n",
      "Epoch 34/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7797\n",
      "Epoch 00034: val_loss improved from 0.18709 to 0.18262, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7812 - val_loss: 0.1826\n",
      "Epoch 35/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7830\n",
      "Epoch 00035: val_loss improved from 0.18262 to 0.18083, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7831 - val_loss: 0.1808\n",
      "Epoch 36/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7493\n",
      "Epoch 00036: val_loss did not improve from 0.18083\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7499 - val_loss: 0.1817\n",
      "Epoch 37/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7188\n",
      "Epoch 00037: val_loss improved from 0.18083 to 0.17528, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7156 - val_loss: 0.1753\n",
      "Epoch 38/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6990\n",
      "Epoch 00038: val_loss improved from 0.17528 to 0.17433, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6992 - val_loss: 0.1743\n",
      "Epoch 39/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7059\n",
      "Epoch 00039: val_loss improved from 0.17433 to 0.17248, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7075 - val_loss: 0.1725\n",
      "Epoch 40/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.6784\n",
      "Epoch 00040: val_loss improved from 0.17248 to 0.16778, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_6.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6788 - val_loss: 0.1678\n",
      "Epoch 41/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.6854\n",
      "Epoch 00041: val_loss did not improve from 0.16778\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6854 - val_loss: 0.1682\n",
      "Epoch 42/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6807\n",
      "Epoch 00042: val_loss did not improve from 0.16778\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6788 - val_loss: 0.1681\n",
      " ###6 fold : val acc1 0.528, acc3 0.943, mae 0.271###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/77 [===========================>..] - ETA: 0s - loss: 23.1040\n",
      "Epoch 00001: val_loss improved from inf to 19.05347, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 22.9743 - val_loss: 19.0535\n",
      "Epoch 2/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 16.0281\n",
      "Epoch 00002: val_loss improved from 19.05347 to 12.47204, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 15.8531 - val_loss: 12.4720\n",
      "Epoch 3/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 10.1381\n",
      "Epoch 00003: val_loss improved from 12.47204 to 7.08891, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 9.9069 - val_loss: 7.0889\n",
      "Epoch 4/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 5.8831\n",
      "Epoch 00004: val_loss improved from 7.08891 to 3.72717, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 5.7314 - val_loss: 3.7272\n",
      "Epoch 5/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 3.7214\n",
      "Epoch 00005: val_loss improved from 3.72717 to 2.24861, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.6495 - val_loss: 2.2486\n",
      "Epoch 6/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 3.0319\n",
      "Epoch 00006: val_loss improved from 2.24861 to 1.67168, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.9714 - val_loss: 1.6717\n",
      "Epoch 7/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 2.5263\n",
      "Epoch 00007: val_loss improved from 1.67168 to 1.37281, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.5627 - val_loss: 1.3728\n",
      "Epoch 8/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 2.3154\n",
      "Epoch 00008: val_loss improved from 1.37281 to 1.15706, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.2970 - val_loss: 1.1571\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 2.1257\n",
      "Epoch 00009: val_loss improved from 1.15706 to 0.98641, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.1257 - val_loss: 0.9864\n",
      "Epoch 10/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.9967\n",
      "Epoch 00010: val_loss improved from 0.98641 to 0.84195, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.9968 - val_loss: 0.8420\n",
      "Epoch 11/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.8567\n",
      "Epoch 00011: val_loss improved from 0.84195 to 0.73199, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.8576 - val_loss: 0.7320\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.7286\n",
      "Epoch 00012: val_loss improved from 0.73199 to 0.63805, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7286 - val_loss: 0.6381\n",
      "Epoch 13/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.6305\n",
      "Epoch 00013: val_loss improved from 0.63805 to 0.56600, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.6170 - val_loss: 0.5660\n",
      "Epoch 14/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.5746\n",
      "Epoch 00014: val_loss improved from 0.56600 to 0.50132, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5694 - val_loss: 0.5013\n",
      "Epoch 15/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.5222\n",
      "Epoch 00015: val_loss improved from 0.50132 to 0.45251, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5141 - val_loss: 0.4525\n",
      "Epoch 16/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.4535\n",
      "Epoch 00016: val_loss improved from 0.45251 to 0.41721, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4469 - val_loss: 0.4172\n",
      "Epoch 17/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 1.3868\n",
      "Epoch 00017: val_loss improved from 0.41721 to 0.37596, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3722 - val_loss: 0.3760\n",
      "Epoch 18/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.3640\n",
      "Epoch 00018: val_loss improved from 0.37596 to 0.34390, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3549 - val_loss: 0.3439\n",
      "Epoch 19/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.3020\n",
      "Epoch 00019: val_loss improved from 0.34390 to 0.31474, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2990 - val_loss: 0.3147\n",
      "Epoch 20/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.2329\n",
      "Epoch 00020: val_loss improved from 0.31474 to 0.29524, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2237 - val_loss: 0.2952\n",
      "Epoch 21/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.1716\n",
      "Epoch 00021: val_loss improved from 0.29524 to 0.28805, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1570 - val_loss: 0.2881\n",
      "Epoch 22/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.1040\n",
      "Epoch 00022: val_loss improved from 0.28805 to 0.26784, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1127 - val_loss: 0.2678\n",
      "Epoch 23/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.0939\n",
      "Epoch 00023: val_loss improved from 0.26784 to 0.25731, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0922 - val_loss: 0.2573\n",
      "Epoch 24/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.0838\n",
      "Epoch 00024: val_loss improved from 0.25731 to 0.23885, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0799 - val_loss: 0.2389\n",
      "Epoch 25/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.0327\n",
      "Epoch 00025: val_loss improved from 0.23885 to 0.22912, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0401 - val_loss: 0.2291\n",
      "Epoch 26/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.0212\n",
      "Epoch 00026: val_loss improved from 0.22912 to 0.22771, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0130 - val_loss: 0.2277\n",
      "Epoch 27/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.9662\n",
      "Epoch 00027: val_loss improved from 0.22771 to 0.21472, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9638 - val_loss: 0.2147\n",
      "Epoch 28/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.9069\n",
      "Epoch 00028: val_loss improved from 0.21472 to 0.20748, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9128 - val_loss: 0.2075\n",
      "Epoch 29/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.8978\n",
      "Epoch 00029: val_loss improved from 0.20748 to 0.20496, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8933 - val_loss: 0.2050\n",
      "Epoch 30/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.8592\n",
      "Epoch 00030: val_loss improved from 0.20496 to 0.19552, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8561 - val_loss: 0.1955\n",
      "Epoch 31/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.8494\n",
      "Epoch 00031: val_loss improved from 0.19552 to 0.19315, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8442 - val_loss: 0.1931\n",
      "Epoch 32/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.8489\n",
      "Epoch 00032: val_loss improved from 0.19315 to 0.19228, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8324 - val_loss: 0.1923\n",
      "Epoch 33/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 0.7924\n",
      "Epoch 00033: val_loss improved from 0.19228 to 0.18659, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7919 - val_loss: 0.1866\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.7835\n",
      "Epoch 00034: val_loss improved from 0.18659 to 0.18162, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7835 - val_loss: 0.1816\n",
      "Epoch 35/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7821\n",
      "Epoch 00035: val_loss improved from 0.18162 to 0.18040, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7815 - val_loss: 0.1804\n",
      "Epoch 36/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7490\n",
      "Epoch 00036: val_loss did not improve from 0.18040\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7469 - val_loss: 0.1814\n",
      "Epoch 37/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7206\n",
      "Epoch 00037: val_loss improved from 0.18040 to 0.17361, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7172 - val_loss: 0.1736\n",
      "Epoch 38/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6897\n",
      "Epoch 00038: val_loss did not improve from 0.17361\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6957 - val_loss: 0.1748\n",
      "Epoch 39/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7042\n",
      "Epoch 00039: val_loss improved from 0.17361 to 0.17113, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7111 - val_loss: 0.1711\n",
      "Epoch 40/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.6820\n",
      "Epoch 00040: val_loss improved from 0.17113 to 0.16577, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_7.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6824 - val_loss: 0.1658\n",
      "Epoch 41/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 0.6860\n",
      "Epoch 00041: val_loss did not improve from 0.16577\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6800 - val_loss: 0.1668\n",
      "Epoch 42/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.6754\n",
      "Epoch 00042: val_loss did not improve from 0.16577\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6763 - val_loss: 0.1669\n",
      " ###7 fold : val acc1 0.545, acc3 0.943, mae 0.260###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/77 [======================>.......] - ETA: 0s - loss: 23.7971\n",
      "Epoch 00001: val_loss improved from inf to 19.08401, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 22.9735 - val_loss: 19.0840\n",
      "Epoch 2/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 16.4041\n",
      "Epoch 00002: val_loss improved from 19.08401 to 12.49031, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 15.8489 - val_loss: 12.4903\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 9.8879 \n",
      "Epoch 00003: val_loss improved from 12.49031 to 7.08862, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 9.8879 - val_loss: 7.0886\n",
      "Epoch 4/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 5.7690\n",
      "Epoch 00004: val_loss improved from 7.08862 to 3.73923, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 5.7225 - val_loss: 3.7392\n",
      "Epoch 5/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 3.7883\n",
      "Epoch 00005: val_loss improved from 3.73923 to 2.26177, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 3.6493 - val_loss: 2.2618\n",
      "Epoch 6/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 2.9847\n",
      "Epoch 00006: val_loss improved from 2.26177 to 1.67959, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.9722 - val_loss: 1.6796\n",
      "Epoch 7/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 2.5599\n",
      "Epoch 00007: val_loss improved from 1.67959 to 1.37595, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.5567 - val_loss: 1.3760\n",
      "Epoch 8/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 2.3049\n",
      "Epoch 00008: val_loss improved from 1.37595 to 1.15645, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.2994 - val_loss: 1.1565\n",
      "Epoch 9/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 2.1289\n",
      "Epoch 00009: val_loss improved from 1.15645 to 0.98111, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 2.1203 - val_loss: 0.9811\n",
      "Epoch 10/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 2.0047\n",
      "Epoch 00010: val_loss improved from 0.98111 to 0.83593, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.9958 - val_loss: 0.8359\n",
      "Epoch 11/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.8489\n",
      "Epoch 00011: val_loss improved from 0.83593 to 0.72325, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.8536 - val_loss: 0.7232\n",
      "Epoch 12/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.7222\n",
      "Epoch 00012: val_loss improved from 0.72325 to 0.62826, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7201 - val_loss: 0.6283\n",
      "Epoch 13/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.6175\n",
      "Epoch 00013: val_loss improved from 0.62826 to 0.55500, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.6101 - val_loss: 0.5550\n",
      "Epoch 14/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.5665\n",
      "Epoch 00014: val_loss improved from 0.55500 to 0.48905, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5681 - val_loss: 0.4891\n",
      "Epoch 15/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.5107\n",
      "Epoch 00015: val_loss improved from 0.48905 to 0.43860, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5100 - val_loss: 0.4386\n",
      "Epoch 16/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.4540\n",
      "Epoch 00016: val_loss improved from 0.43860 to 0.40143, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4522 - val_loss: 0.4014\n",
      "Epoch 17/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.3716\n",
      "Epoch 00017: val_loss improved from 0.40143 to 0.36059, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3649 - val_loss: 0.3606\n",
      "Epoch 18/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.3591\n",
      "Epoch 00018: val_loss improved from 0.36059 to 0.32881, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3547 - val_loss: 0.3288\n",
      "Epoch 19/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.2960\n",
      "Epoch 00019: val_loss improved from 0.32881 to 0.29868, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2979 - val_loss: 0.2987\n",
      "Epoch 20/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.2226\n",
      "Epoch 00020: val_loss improved from 0.29868 to 0.27907, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2224 - val_loss: 0.2791\n",
      "Epoch 21/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 1.1549\n",
      "Epoch 00021: val_loss improved from 0.27907 to 0.27210, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1528 - val_loss: 0.2721\n",
      "Epoch 22/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.1116\n",
      "Epoch 00022: val_loss improved from 0.27210 to 0.25257, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1107 - val_loss: 0.2526\n",
      "Epoch 23/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.0973\n",
      "Epoch 00023: val_loss improved from 0.25257 to 0.24244, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0919 - val_loss: 0.2424\n",
      "Epoch 24/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.0826\n",
      "Epoch 00024: val_loss improved from 0.24244 to 0.22657, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0798 - val_loss: 0.2266\n",
      "Epoch 25/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 1.0329\n",
      "Epoch 00025: val_loss improved from 0.22657 to 0.21800, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0378 - val_loss: 0.2180\n",
      "Epoch 26/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0131\n",
      "Epoch 00026: val_loss improved from 0.21800 to 0.21620, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0133 - val_loss: 0.2162\n",
      "Epoch 27/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.9638\n",
      "Epoch 00027: val_loss improved from 0.21620 to 0.20434, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9644 - val_loss: 0.2043\n",
      "Epoch 28/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.9102\n",
      "Epoch 00028: val_loss improved from 0.20434 to 0.19762, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.9116 - val_loss: 0.1976\n",
      "Epoch 29/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.8981\n",
      "Epoch 00029: val_loss improved from 0.19762 to 0.19435, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8936 - val_loss: 0.1943\n",
      "Epoch 30/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.8572\n",
      "Epoch 00030: val_loss improved from 0.19435 to 0.18687, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8553 - val_loss: 0.1869\n",
      "Epoch 31/100\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8402\n",
      "Epoch 00031: val_loss improved from 0.18687 to 0.18529, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8407 - val_loss: 0.1853\n",
      "Epoch 32/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.8288\n",
      "Epoch 00032: val_loss improved from 0.18529 to 0.18401, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8283 - val_loss: 0.1840\n",
      "Epoch 33/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7895\n",
      "Epoch 00033: val_loss improved from 0.18401 to 0.17922, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7885 - val_loss: 0.1792\n",
      "Epoch 34/100\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.7822\n",
      "Epoch 00034: val_loss improved from 0.17922 to 0.17488, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7821 - val_loss: 0.1749\n",
      "Epoch 35/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.7803\n",
      "Epoch 00035: val_loss improved from 0.17488 to 0.17349, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7818 - val_loss: 0.1735\n",
      "Epoch 36/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7458\n",
      "Epoch 00036: val_loss did not improve from 0.17349\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7455 - val_loss: 0.1755\n",
      "Epoch 37/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.7198\n",
      "Epoch 00037: val_loss improved from 0.17349 to 0.16776, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7167 - val_loss: 0.1678\n",
      "Epoch 38/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6934\n",
      "Epoch 00038: val_loss did not improve from 0.16776\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6947 - val_loss: 0.1698\n",
      "Epoch 39/100\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.7069\n",
      "Epoch 00039: val_loss improved from 0.16776 to 0.16643, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7114 - val_loss: 0.1664\n",
      "Epoch 40/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6818\n",
      "Epoch 00040: val_loss improved from 0.16643 to 0.16162, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_8.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6804 - val_loss: 0.1616\n",
      "Epoch 41/100\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.6797\n",
      "Epoch 00041: val_loss did not improve from 0.16162\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6777 - val_loss: 0.1627\n",
      "Epoch 42/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 0.6764\n",
      "Epoch 00042: val_loss did not improve from 0.16162\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6752 - val_loss: 0.1619\n",
      " ###8 fold : val acc1 0.519, acc3 0.935, mae 0.279###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/77 [=========================>....] - ETA: 0s - loss: 23.4774\n",
      "Epoch 00001: val_loss improved from inf to 19.13623, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 1s 4ms/step - loss: 22.9735 - val_loss: 19.1362\n",
      "Epoch 2/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 16.2759\n",
      "Epoch 00002: val_loss improved from 19.13623 to 12.54547, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 15.8489 - val_loss: 12.5455\n",
      "Epoch 3/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 10.3069\n",
      "Epoch 00003: val_loss improved from 12.54547 to 7.14045, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 9.8879 - val_loss: 7.1404\n",
      "Epoch 4/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 5.9124\n",
      "Epoch 00004: val_loss improved from 7.14045 to 3.78179, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 5.7225 - val_loss: 3.7818\n",
      "Epoch 5/100\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 3.7074\n",
      "Epoch 00005: val_loss improved from 3.78179 to 2.29514, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 3.6493 - val_loss: 2.2951\n",
      "Epoch 6/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 3.0357\n",
      "Epoch 00006: val_loss improved from 2.29514 to 1.70752, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.9722 - val_loss: 1.7075\n",
      "Epoch 7/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 2.5125\n",
      "Epoch 00007: val_loss improved from 1.70752 to 1.39877, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.5567 - val_loss: 1.3988\n",
      "Epoch 8/100\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 2.3041\n",
      "Epoch 00008: val_loss improved from 1.39877 to 1.17271, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.2994 - val_loss: 1.1727\n",
      "Epoch 9/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 2.1399\n",
      "Epoch 00009: val_loss improved from 1.17271 to 0.99372, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 2.1203 - val_loss: 0.9937\n",
      "Epoch 10/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.9941\n",
      "Epoch 00010: val_loss improved from 0.99372 to 0.84364, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.9958 - val_loss: 0.8436\n",
      "Epoch 11/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.8677\n",
      "Epoch 00011: val_loss improved from 0.84364 to 0.72849, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.8536 - val_loss: 0.7285\n",
      "Epoch 12/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.7373\n",
      "Epoch 00012: val_loss improved from 0.72849 to 0.63164, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.7201 - val_loss: 0.6316\n",
      "Epoch 13/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.6311\n",
      "Epoch 00013: val_loss improved from 0.63164 to 0.55747, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.6101 - val_loss: 0.5575\n",
      "Epoch 14/100\n",
      "75/77 [============================>.] - ETA: 0s - loss: 1.5632\n",
      "Epoch 00014: val_loss improved from 0.55747 to 0.49073, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.5681 - val_loss: 0.4907\n",
      "Epoch 15/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 1.5159\n",
      "Epoch 00015: val_loss improved from 0.49073 to 0.43910, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.5100 - val_loss: 0.4391\n",
      "Epoch 16/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.4555\n",
      "Epoch 00016: val_loss improved from 0.43910 to 0.40300, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.4522 - val_loss: 0.4030\n",
      "Epoch 17/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.3747\n",
      "Epoch 00017: val_loss improved from 0.40300 to 0.36138, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3649 - val_loss: 0.3614\n",
      "Epoch 18/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.3669\n",
      "Epoch 00018: val_loss improved from 0.36138 to 0.32949, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.3547 - val_loss: 0.3295\n",
      "Epoch 19/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 1.2989\n",
      "Epoch 00019: val_loss improved from 0.32949 to 0.29985, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2979 - val_loss: 0.2998\n",
      "Epoch 20/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 1.2325\n",
      "Epoch 00020: val_loss improved from 0.29985 to 0.28047, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.2224 - val_loss: 0.2805\n",
      "Epoch 21/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 1.1720\n",
      "Epoch 00021: val_loss improved from 0.28047 to 0.27412, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1528 - val_loss: 0.2741\n",
      "Epoch 22/100\n",
      "62/77 [=======================>......] - ETA: 0s - loss: 1.1039\n",
      "Epoch 00022: val_loss improved from 0.27412 to 0.25467, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.1107 - val_loss: 0.2547\n",
      "Epoch 23/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 1.1010\n",
      "Epoch 00023: val_loss improved from 0.25467 to 0.24422, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0919 - val_loss: 0.2442\n",
      "Epoch 24/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 1.0833\n",
      "Epoch 00024: val_loss improved from 0.24422 to 0.22700, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0798 - val_loss: 0.2270\n",
      "Epoch 25/100\n",
      "69/77 [=========================>....] - ETA: 0s - loss: 1.0326\n",
      "Epoch 00025: val_loss improved from 0.22700 to 0.21744, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0378 - val_loss: 0.2174\n",
      "Epoch 26/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 1.0144\n",
      "Epoch 00026: val_loss improved from 0.21744 to 0.21672, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.0133 - val_loss: 0.2167\n",
      "Epoch 27/100\n",
      "67/77 [=========================>....] - ETA: 0s - loss: 0.9626\n",
      "Epoch 00027: val_loss improved from 0.21672 to 0.20380, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.9644 - val_loss: 0.2038\n",
      "Epoch 28/100\n",
      "68/77 [=========================>....] - ETA: 0s - loss: 0.9104\n",
      "Epoch 00028: val_loss improved from 0.20380 to 0.19728, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.9116 - val_loss: 0.1973\n",
      "Epoch 29/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.8977\n",
      "Epoch 00029: val_loss improved from 0.19728 to 0.19422, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8936 - val_loss: 0.1942\n",
      "Epoch 30/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.8583\n",
      "Epoch 00030: val_loss improved from 0.19422 to 0.18648, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8553 - val_loss: 0.1865\n",
      "Epoch 31/100\n",
      "65/77 [========================>.....] - ETA: 0s - loss: 0.8460\n",
      "Epoch 00031: val_loss improved from 0.18648 to 0.18526, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8407 - val_loss: 0.1853\n",
      "Epoch 32/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.8377\n",
      "Epoch 00032: val_loss improved from 0.18526 to 0.18418, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.8283 - val_loss: 0.1842\n",
      "Epoch 33/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7955\n",
      "Epoch 00033: val_loss improved from 0.18418 to 0.17883, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7885 - val_loss: 0.1788\n",
      "Epoch 34/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7752\n",
      "Epoch 00034: val_loss improved from 0.17883 to 0.17486, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7821 - val_loss: 0.1749\n",
      "Epoch 35/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.7831\n",
      "Epoch 00035: val_loss improved from 0.17486 to 0.17355, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7818 - val_loss: 0.1735\n",
      "Epoch 36/100\n",
      "61/77 [======================>.......] - ETA: 0s - loss: 0.7485\n",
      "Epoch 00036: val_loss did not improve from 0.17355\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7455 - val_loss: 0.1759\n",
      "Epoch 37/100\n",
      "60/77 [======================>.......] - ETA: 0s - loss: 0.7192\n",
      "Epoch 00037: val_loss improved from 0.17355 to 0.16786, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7167 - val_loss: 0.1679\n",
      "Epoch 38/100\n",
      "63/77 [=======================>......] - ETA: 0s - loss: 0.6870\n",
      "Epoch 00038: val_loss did not improve from 0.16786\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6947 - val_loss: 0.1693\n",
      "Epoch 39/100\n",
      "59/77 [=====================>........] - ETA: 0s - loss: 0.7044\n",
      "Epoch 00039: val_loss improved from 0.16786 to 0.16574, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7114 - val_loss: 0.1657\n",
      "Epoch 40/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.6798\n",
      "Epoch 00040: val_loss improved from 0.16574 to 0.16105, saving model to result/size/DNN_size_both_y/batch256,dnodes16_dropout0.2,lr0.001/weights_9.hdf5\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6804 - val_loss: 0.1610\n",
      "Epoch 41/100\n",
      "66/77 [========================>.....] - ETA: 0s - loss: 0.6845\n",
      "Epoch 00041: val_loss did not improve from 0.16105\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6777 - val_loss: 0.1622\n",
      "Epoch 42/100\n",
      "64/77 [=======================>......] - ETA: 0s - loss: 0.6724\n",
      "Epoch 00042: val_loss did not improve from 0.16105\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.6752 - val_loss: 0.1618\n",
      " ###9 fold : val acc1 0.523, acc3 0.939, mae 0.276###\n",
      "acc10.528_acc30.942\n",
      "random search 31/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297/307 [============================>.] - ETA: 0s - loss: 2.3709\n",
      "Epoch 00001: val_loss improved from inf to 0.22326, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.3107 - val_loss: 0.2233\n",
      "Epoch 2/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.5939\n",
      "Epoch 00002: val_loss improved from 0.22326 to 0.16228, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.5898 - val_loss: 0.1623\n",
      "Epoch 3/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.2754\n",
      "Epoch 00003: val_loss did not improve from 0.16228\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3632 - val_loss: 0.1866\n",
      "Epoch 4/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.3116\n",
      "Epoch 00004: val_loss improved from 0.16228 to 0.14552, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3097 - val_loss: 0.1455\n",
      "Epoch 5/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.3073\n",
      "Epoch 00005: val_loss improved from 0.14552 to 0.14254, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3060 - val_loss: 0.1425\n",
      "Epoch 6/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.2885\n",
      "Epoch 00006: val_loss improved from 0.14254 to 0.14111, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2882 - val_loss: 0.1411\n",
      "Epoch 7/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.2502\n",
      "Epoch 00007: val_loss improved from 0.14111 to 0.13387, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_0.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2503 - val_loss: 0.1339\n",
      "Epoch 8/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.2439\n",
      "Epoch 00008: val_loss did not improve from 0.13387\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2435 - val_loss: 0.1633\n",
      "Epoch 9/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.2500\n",
      "Epoch 00009: val_loss did not improve from 0.13387\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2500 - val_loss: 0.1364\n",
      " ###0 fold : val acc1 0.580, acc3 0.959, mae 0.234###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307/307 [==============================] - ETA: 0s - loss: 2.2364\n",
      "Epoch 00001: val_loss improved from inf to 0.22086, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 2.2364 - val_loss: 0.2209\n",
      "Epoch 2/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.5634\n",
      "Epoch 00002: val_loss improved from 0.22086 to 0.16269, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.5600 - val_loss: 0.1627\n",
      "Epoch 3/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.2751\n",
      "Epoch 00003: val_loss did not improve from 0.16269\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3510 - val_loss: 0.1956\n",
      "Epoch 4/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.3136\n",
      "Epoch 00004: val_loss improved from 0.16269 to 0.14297, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3121 - val_loss: 0.1430\n",
      "Epoch 5/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.3007\n",
      "Epoch 00005: val_loss improved from 0.14297 to 0.13891, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_1.hdf5\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.2994 - val_loss: 0.1389\n",
      "Epoch 6/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.3123\n",
      "Epoch 00006: val_loss did not improve from 0.13891\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3113 - val_loss: 0.1396\n",
      "Epoch 7/100\n",
      "296/307 [===========================>..] - ETA: 0s - loss: 0.2555\n",
      "Epoch 00007: val_loss did not improve from 0.13891\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2575 - val_loss: 0.1431\n",
      " ###1 fold : val acc1 0.568, acc3 0.961, mae 0.239###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/307 [============================>.] - ETA: 0s - loss: 2.1782\n",
      "Epoch 00001: val_loss improved from inf to 0.22026, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.1760 - val_loss: 0.2203\n",
      "Epoch 2/100\n",
      "307/307 [==============================] - ETA: 0s - loss: 0.5562\n",
      "Epoch 00002: val_loss improved from 0.22026 to 0.16116, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5562 - val_loss: 0.1612\n",
      "Epoch 3/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.3577\n",
      "Epoch 00003: val_loss did not improve from 0.16116\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3575 - val_loss: 0.1906\n",
      "Epoch 4/100\n",
      "294/307 [===========================>..] - ETA: 0s - loss: 0.3303\n",
      "Epoch 00004: val_loss improved from 0.16116 to 0.14253, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3279 - val_loss: 0.1425\n",
      "Epoch 5/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.3094\n",
      "Epoch 00005: val_loss improved from 0.14253 to 0.13854, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_2.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3091 - val_loss: 0.1385\n",
      "Epoch 6/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.3208\n",
      "Epoch 00006: val_loss did not improve from 0.13854\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3200 - val_loss: 0.1429\n",
      "Epoch 7/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.2569\n",
      "Epoch 00007: val_loss did not improve from 0.13854\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2569 - val_loss: 0.1459\n",
      " ###2 fold : val acc1 0.567, acc3 0.960, mae 0.240###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/307 [============================>.] - ETA: 0s - loss: 2.2190\n",
      "Epoch 00001: val_loss improved from inf to 0.21778, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.1761 - val_loss: 0.2178\n",
      "Epoch 2/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.5760\n",
      "Epoch 00002: val_loss improved from 0.21778 to 0.15990, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5670 - val_loss: 0.1599\n",
      "Epoch 3/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.2781\n",
      "Epoch 00003: val_loss did not improve from 0.15990\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3532 - val_loss: 0.2011\n",
      "Epoch 4/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.3080\n",
      "Epoch 00004: val_loss improved from 0.15990 to 0.14280, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3070 - val_loss: 0.1428\n",
      "Epoch 5/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.2999\n",
      "Epoch 00005: val_loss improved from 0.14280 to 0.13863, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_3.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2997 - val_loss: 0.1386\n",
      "Epoch 6/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.3432\n",
      "Epoch 00006: val_loss did not improve from 0.13863\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3433 - val_loss: 0.1403\n",
      "Epoch 7/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.2555\n",
      "Epoch 00007: val_loss did not improve from 0.13863\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2553 - val_loss: 0.1417\n",
      " ###3 fold : val acc1 0.566, acc3 0.965, mae 0.238###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307/307 [==============================] - ETA: 0s - loss: 1.8906\n",
      "Epoch 00001: val_loss improved from inf to 0.20598, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 1.8906 - val_loss: 0.2060\n",
      "Epoch 2/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.6296\n",
      "Epoch 00002: val_loss improved from 0.20598 to 0.16973, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6247 - val_loss: 0.1697\n",
      "Epoch 3/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.3371\n",
      "Epoch 00003: val_loss improved from 0.16973 to 0.14326, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3357 - val_loss: 0.1433\n",
      "Epoch 4/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.3008\n",
      "Epoch 00004: val_loss did not improve from 0.14326\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3001 - val_loss: 0.1450\n",
      "Epoch 5/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.2856\n",
      "Epoch 00005: val_loss improved from 0.14326 to 0.13800, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_4.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2853 - val_loss: 0.1380\n",
      "Epoch 6/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.2539\n",
      "Epoch 00006: val_loss did not improve from 0.13800\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2529 - val_loss: 0.1380\n",
      "Epoch 7/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.2677\n",
      "Epoch 00007: val_loss did not improve from 0.13800\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2675 - val_loss: 0.1434\n",
      " ###4 fold : val acc1 0.574, acc3 0.962, mae 0.236###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/307 [============================>.] - ETA: 0s - loss: 1.3523\n",
      "Epoch 00001: val_loss improved from inf to 0.20752, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 1.3310 - val_loss: 0.2075\n",
      "Epoch 2/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.3006\n",
      "Epoch 00002: val_loss improved from 0.20752 to 0.16468, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3009 - val_loss: 0.1647\n",
      "Epoch 3/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.2740\n",
      "Epoch 00003: val_loss improved from 0.16468 to 0.14664, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2735 - val_loss: 0.1466\n",
      "Epoch 4/100\n",
      "303/307 [============================>.] - ETA: 0s - loss: 0.2618\n",
      "Epoch 00004: val_loss improved from 0.14664 to 0.14636, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2610 - val_loss: 0.1464\n",
      "Epoch 5/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.2517\n",
      "Epoch 00005: val_loss improved from 0.14636 to 0.13911, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_5.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2516 - val_loss: 0.1391\n",
      "Epoch 6/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.2490\n",
      "Epoch 00006: val_loss did not improve from 0.13911\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2488 - val_loss: 0.1405\n",
      "Epoch 7/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.2438\n",
      "Epoch 00007: val_loss did not improve from 0.13911\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2437 - val_loss: 0.1483\n",
      " ###5 fold : val acc1 0.576, acc3 0.962, mae 0.246###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/307 [===========================>..] - ETA: 0s - loss: 2.9197\n",
      "Epoch 00001: val_loss improved from inf to 0.21228, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.8170 - val_loss: 0.2123\n",
      "Epoch 2/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.6166\n",
      "Epoch 00002: val_loss improved from 0.21228 to 0.16368, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.6086 - val_loss: 0.1637\n",
      "Epoch 3/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.4226\n",
      "Epoch 00003: val_loss improved from 0.16368 to 0.14195, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4211 - val_loss: 0.1419\n",
      "Epoch 4/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.3337\n",
      "Epoch 00004: val_loss improved from 0.14195 to 0.13800, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3318 - val_loss: 0.1380\n",
      "Epoch 5/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.2703\n",
      "Epoch 00005: val_loss improved from 0.13800 to 0.13648, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_6.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2702 - val_loss: 0.1365\n",
      "Epoch 6/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.3070\n",
      "Epoch 00006: val_loss did not improve from 0.13648\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3057 - val_loss: 0.1416\n",
      "Epoch 7/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.2540\n",
      "Epoch 00007: val_loss did not improve from 0.13648\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2544 - val_loss: 0.1519\n",
      " ###6 fold : val acc1 0.570, acc3 0.959, mae 0.240###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/307 [===========================>..] - ETA: 0s - loss: 2.8292\n",
      "Epoch 00001: val_loss improved from inf to 0.22661, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.7449 - val_loss: 0.2266\n",
      "Epoch 2/100\n",
      "305/307 [============================>.] - ETA: 0s - loss: 0.8080\n",
      "Epoch 00002: val_loss improved from 0.22661 to 0.16782, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.8058 - val_loss: 0.1678\n",
      "Epoch 3/100\n",
      "295/307 [===========================>..] - ETA: 0s - loss: 0.5702\n",
      "Epoch 00003: val_loss improved from 0.16782 to 0.14741, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.5601 - val_loss: 0.1474\n",
      "Epoch 4/100\n",
      "299/307 [============================>.] - ETA: 0s - loss: 0.3754\n",
      "Epoch 00004: val_loss improved from 0.14741 to 0.13541, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_7.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3725 - val_loss: 0.1354\n",
      "Epoch 5/100\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.2783\n",
      "Epoch 00005: val_loss did not improve from 0.13541\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2782 - val_loss: 0.1390\n",
      "Epoch 6/100\n",
      "300/307 [============================>.] - ETA: 0s - loss: 0.3174\n",
      "Epoch 00006: val_loss did not improve from 0.13541\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3160 - val_loss: 0.1400\n",
      " ###7 fold : val acc1 0.581, acc3 0.962, mae 0.231###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/307 [============================>.] - ETA: 0s - loss: 2.7875\n",
      "Epoch 00001: val_loss improved from inf to 0.24882, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.7685 - val_loss: 0.2488\n",
      "Epoch 2/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.7533\n",
      "Epoch 00002: val_loss improved from 0.24882 to 0.17003, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7461 - val_loss: 0.1700\n",
      "Epoch 3/100\n",
      "293/307 [===========================>..] - ETA: 0s - loss: 0.5036\n",
      "Epoch 00003: val_loss improved from 0.17003 to 0.14611, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4943 - val_loss: 0.1461\n",
      "Epoch 4/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.3716\n",
      "Epoch 00004: val_loss improved from 0.14611 to 0.13301, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_8.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3693 - val_loss: 0.1330\n",
      "Epoch 5/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.2783\n",
      "Epoch 00005: val_loss did not improve from 0.13301\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2782 - val_loss: 0.1363\n",
      "Epoch 6/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.2897\n",
      "Epoch 00006: val_loss did not improve from 0.13301\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2889 - val_loss: 0.1390\n",
      " ###8 fold : val acc1 0.571, acc3 0.956, mae 0.241###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/307 [============================>.] - ETA: 0s - loss: 2.7796\n",
      "Epoch 00001: val_loss improved from inf to 0.22932, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "307/307 [==============================] - 2s 4ms/step - loss: 2.7685 - val_loss: 0.2293\n",
      "Epoch 2/100\n",
      "302/307 [============================>.] - ETA: 0s - loss: 0.7521\n",
      "Epoch 00002: val_loss improved from 0.22932 to 0.16360, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.7461 - val_loss: 0.1636\n",
      "Epoch 3/100\n",
      "298/307 [============================>.] - ETA: 0s - loss: 0.4996\n",
      "Epoch 00003: val_loss improved from 0.16360 to 0.14502, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.4943 - val_loss: 0.1450\n",
      "Epoch 4/100\n",
      "304/307 [============================>.] - ETA: 0s - loss: 0.3705\n",
      "Epoch 00004: val_loss improved from 0.14502 to 0.13202, saving model to result/size/DNN_size_both_y/batch64,dnodes256_dropout0.1,dnodes512_dropout0.5,lr0.0005/weights_9.hdf5\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.3693 - val_loss: 0.1320\n",
      "Epoch 5/100\n",
      "301/307 [============================>.] - ETA: 0s - loss: 0.2784\n",
      "Epoch 00005: val_loss did not improve from 0.13202\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2782 - val_loss: 0.1370\n",
      "Epoch 6/100\n",
      "297/307 [============================>.] - ETA: 0s - loss: 0.2902\n",
      "Epoch 00006: val_loss did not improve from 0.13202\n",
      "307/307 [==============================] - 1s 4ms/step - loss: 0.2890 - val_loss: 0.1382\n",
      " ###9 fold : val acc1 0.580, acc3 0.960, mae 0.235###\n",
      "acc10.573_acc30.961\n",
      "random search 32/500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/154 [==========================>...] - ETA: 0s - loss: 2.4188\n",
      "Epoch 00001: val_loss improved from inf to 0.22298, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 2.2853 - val_loss: 0.2230\n",
      "Epoch 2/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.9400\n",
      "Epoch 00002: val_loss improved from 0.22298 to 0.19267, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9293 - val_loss: 0.1927\n",
      "Epoch 3/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.4305\n",
      "Epoch 00003: val_loss did not improve from 0.19267\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5259 - val_loss: 0.2559\n",
      "Epoch 4/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.5523\n",
      "Epoch 00004: val_loss improved from 0.19267 to 0.16386, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5442 - val_loss: 0.1639\n",
      "Epoch 5/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3916\n",
      "Epoch 00005: val_loss improved from 0.16386 to 0.15897, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3895 - val_loss: 0.1590\n",
      "Epoch 6/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.3814\n",
      "Epoch 00006: val_loss improved from 0.15897 to 0.15578, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3778 - val_loss: 0.1558\n",
      "Epoch 7/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.3200\n",
      "Epoch 00007: val_loss improved from 0.15578 to 0.14341, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3200 - val_loss: 0.1434\n",
      "Epoch 8/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.3094\n",
      "Epoch 00008: val_loss improved from 0.14341 to 0.14316, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3076 - val_loss: 0.1432\n",
      "Epoch 9/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.2804\n",
      "Epoch 00009: val_loss improved from 0.14316 to 0.13853, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_0.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2798 - val_loss: 0.1385\n",
      "Epoch 10/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.2878\n",
      "Epoch 00010: val_loss did not improve from 0.13853\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2874 - val_loss: 0.1423\n",
      "Epoch 11/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2609\n",
      "Epoch 00011: val_loss did not improve from 0.13853\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2602 - val_loss: 0.1405\n",
      " ###0 fold : val acc1 0.557, acc3 0.956, mae 0.249###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/154 [===========================>..] - ETA: 0s - loss: 2.3382\n",
      "Epoch 00001: val_loss improved from inf to 0.22246, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 2.2677 - val_loss: 0.2225\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.9044\n",
      "Epoch 00002: val_loss improved from 0.22246 to 0.19662, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9044 - val_loss: 0.1966\n",
      "Epoch 3/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.5152\n",
      "Epoch 00003: val_loss did not improve from 0.19662\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5153 - val_loss: 0.2644\n",
      "Epoch 4/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.5357\n",
      "Epoch 00004: val_loss improved from 0.19662 to 0.16256, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5345 - val_loss: 0.1626\n",
      "Epoch 5/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.3882\n",
      "Epoch 00005: val_loss improved from 0.16256 to 0.14937, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3881 - val_loss: 0.1494\n",
      "Epoch 6/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.3742\n",
      "Epoch 00006: val_loss did not improve from 0.14937\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3735 - val_loss: 0.1573\n",
      "Epoch 7/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.3167\n",
      "Epoch 00007: val_loss improved from 0.14937 to 0.14408, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3149 - val_loss: 0.1441\n",
      "Epoch 8/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.3309\n",
      "Epoch 00008: val_loss did not improve from 0.14408\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3306 - val_loss: 0.1442\n",
      "Epoch 9/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2851\n",
      "Epoch 00009: val_loss improved from 0.14408 to 0.14288, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2853 - val_loss: 0.1429\n",
      "Epoch 10/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.2918\n",
      "Epoch 00010: val_loss did not improve from 0.14288\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2917 - val_loss: 0.1437\n",
      "Epoch 11/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.2628\n",
      "Epoch 00011: val_loss improved from 0.14288 to 0.14203, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2627 - val_loss: 0.1420\n",
      "Epoch 12/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.2511\n",
      "Epoch 00012: val_loss improved from 0.14203 to 0.13469, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_1.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2511 - val_loss: 0.1347\n",
      "Epoch 13/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.2512\n",
      "Epoch 00013: val_loss did not improve from 0.13469\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2509 - val_loss: 0.1397\n",
      "Epoch 14/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2504\n",
      "Epoch 00014: val_loss did not improve from 0.13469\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2497 - val_loss: 0.1489\n",
      " ###1 fold : val acc1 0.584, acc3 0.961, mae 0.231###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 2.3365\n",
      "Epoch 00001: val_loss improved from inf to 0.21690, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 2.2439 - val_loss: 0.2169\n",
      "Epoch 2/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.9346\n",
      "Epoch 00002: val_loss improved from 0.21690 to 0.20923, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9340 - val_loss: 0.2092\n",
      "Epoch 3/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.5387\n",
      "Epoch 00003: val_loss did not improve from 0.20923\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5365 - val_loss: 0.2650\n",
      "Epoch 4/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.5300\n",
      "Epoch 00004: val_loss improved from 0.20923 to 0.15667, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5297 - val_loss: 0.1567\n",
      "Epoch 5/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.3975\n",
      "Epoch 00005: val_loss improved from 0.15667 to 0.14676, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3973 - val_loss: 0.1468\n",
      "Epoch 6/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.3767\n",
      "Epoch 00006: val_loss did not improve from 0.14676\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3766 - val_loss: 0.1478\n",
      "Epoch 7/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.3151\n",
      "Epoch 00007: val_loss improved from 0.14676 to 0.14253, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3151 - val_loss: 0.1425\n",
      "Epoch 8/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.3254\n",
      "Epoch 00008: val_loss improved from 0.14253 to 0.14106, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3211 - val_loss: 0.1411\n",
      "Epoch 9/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.2817\n",
      "Epoch 00009: val_loss improved from 0.14106 to 0.13915, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_2.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2817 - val_loss: 0.1392\n",
      "Epoch 10/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.2662\n",
      "Epoch 00010: val_loss did not improve from 0.13915\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2791 - val_loss: 0.1415\n",
      "Epoch 11/100\n",
      "140/154 [==========================>...] - ETA: 0s - loss: 0.2633\n",
      "Epoch 00011: val_loss did not improve from 0.13915\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2605 - val_loss: 0.1437\n",
      " ###2 fold : val acc1 0.563, acc3 0.962, mae 0.242###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/154 [==========================>...] - ETA: 0s - loss: 2.3195\n",
      "Epoch 00001: val_loss improved from inf to 0.21592, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 2.1934 - val_loss: 0.2159\n",
      "Epoch 2/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.9810\n",
      "Epoch 00002: val_loss improved from 0.21592 to 0.21127, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9778 - val_loss: 0.2113\n",
      "Epoch 3/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.4291\n",
      "Epoch 00003: val_loss did not improve from 0.21127\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5374 - val_loss: 0.2616\n",
      "Epoch 4/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.5472\n",
      "Epoch 00004: val_loss improved from 0.21127 to 0.15910, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5459 - val_loss: 0.1591\n",
      "Epoch 5/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3955\n",
      "Epoch 00005: val_loss improved from 0.15910 to 0.14652, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3942 - val_loss: 0.1465\n",
      "Epoch 6/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.3812\n",
      "Epoch 00006: val_loss did not improve from 0.14652\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3801 - val_loss: 0.1490\n",
      "Epoch 7/100\n",
      "143/154 [==========================>...] - ETA: 0s - loss: 0.3179\n",
      "Epoch 00007: val_loss improved from 0.14652 to 0.13996, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3173 - val_loss: 0.1400\n",
      "Epoch 8/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.3159\n",
      "Epoch 00008: val_loss did not improve from 0.13996\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3131 - val_loss: 0.1415\n",
      "Epoch 9/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.2801\n",
      "Epoch 00009: val_loss improved from 0.13996 to 0.13976, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2803 - val_loss: 0.1398\n",
      "Epoch 10/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.2664\n",
      "Epoch 00010: val_loss improved from 0.13976 to 0.13896, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2990 - val_loss: 0.1390\n",
      "Epoch 11/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.2734\n",
      "Epoch 00011: val_loss improved from 0.13896 to 0.13896, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2710 - val_loss: 0.1390\n",
      "Epoch 12/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.2515\n",
      "Epoch 00012: val_loss improved from 0.13896 to 0.13263, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_3.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2515 - val_loss: 0.1326\n",
      "Epoch 13/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.2452\n",
      "Epoch 00013: val_loss did not improve from 0.13263\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2438 - val_loss: 0.1376\n",
      "Epoch 14/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.2488\n",
      "Epoch 00014: val_loss did not improve from 0.13263\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2480 - val_loss: 0.1382\n",
      " ###3 fold : val acc1 0.577, acc3 0.965, mae 0.232###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/154 [==========================>...] - ETA: 0s - loss: 2.5716\n",
      "Epoch 00001: val_loss improved from inf to 0.19965, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 2.3995 - val_loss: 0.1996\n",
      "Epoch 2/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.7720\n",
      "Epoch 00002: val_loss improved from 0.19965 to 0.16941, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7659 - val_loss: 0.1694\n",
      "Epoch 3/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.5277\n",
      "Epoch 00003: val_loss improved from 0.16941 to 0.15907, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5261 - val_loss: 0.1591\n",
      "Epoch 4/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.5105\n",
      "Epoch 00004: val_loss improved from 0.15907 to 0.15245, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5050 - val_loss: 0.1524\n",
      "Epoch 5/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.4689\n",
      "Epoch 00005: val_loss improved from 0.15245 to 0.14512, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4637 - val_loss: 0.1451\n",
      "Epoch 6/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.4136\n",
      "Epoch 00006: val_loss did not improve from 0.14512\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4113 - val_loss: 0.1605\n",
      "Epoch 7/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.3397\n",
      "Epoch 00007: val_loss improved from 0.14512 to 0.14451, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3394 - val_loss: 0.1445\n",
      "Epoch 8/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2915\n",
      "Epoch 00008: val_loss improved from 0.14451 to 0.14307, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2917 - val_loss: 0.1431\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.3403\n",
      "Epoch 00009: val_loss did not improve from 0.14307\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3403 - val_loss: 0.1459\n",
      "Epoch 10/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.2799\n",
      "Epoch 00010: val_loss improved from 0.14307 to 0.14117, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2802 - val_loss: 0.1412\n",
      "Epoch 11/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.2675\n",
      "Epoch 00011: val_loss improved from 0.14117 to 0.13602, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2673 - val_loss: 0.1360\n",
      "Epoch 12/100\n",
      "141/154 [==========================>...] - ETA: 0s - loss: 0.2532\n",
      "Epoch 00012: val_loss improved from 0.13602 to 0.13294, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_4.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2578 - val_loss: 0.1329\n",
      "Epoch 13/100\n",
      "145/154 [===========================>..] - ETA: 0s - loss: 0.2467\n",
      "Epoch 00013: val_loss did not improve from 0.13294\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2466 - val_loss: 0.1352\n",
      "Epoch 14/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.2498\n",
      "Epoch 00014: val_loss did not improve from 0.13294\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2489 - val_loss: 0.1401\n",
      " ###4 fold : val acc1 0.584, acc3 0.964, mae 0.229###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/154 [============================>.] - ETA: 0s - loss: 1.6347\n",
      "Epoch 00001: val_loss improved from inf to 0.20702, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 1.6335 - val_loss: 0.2070\n",
      "Epoch 2/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.4631\n",
      "Epoch 00002: val_loss improved from 0.20702 to 0.18110, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4604 - val_loss: 0.1811\n",
      "Epoch 3/100\n",
      "146/154 [===========================>..] - ETA: 0s - loss: 0.3884\n",
      "Epoch 00003: val_loss did not improve from 0.18110\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3877 - val_loss: 0.1969\n",
      "Epoch 4/100\n",
      "142/154 [==========================>...] - ETA: 0s - loss: 0.3532\n",
      "Epoch 00004: val_loss improved from 0.18110 to 0.14729, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3528 - val_loss: 0.1473\n",
      "Epoch 5/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.3196\n",
      "Epoch 00005: val_loss improved from 0.14729 to 0.14152, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_5.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3202 - val_loss: 0.1415\n",
      "Epoch 6/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.2972\n",
      "Epoch 00006: val_loss did not improve from 0.14152\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2972 - val_loss: 0.1532\n",
      "Epoch 7/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.2840\n",
      "Epoch 00007: val_loss did not improve from 0.14152\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2840 - val_loss: 0.1720\n",
      " ###5 fold : val acc1 0.574, acc3 0.962, mae 0.250###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - ETA: 0s - loss: 3.5077\n",
      "Epoch 00001: val_loss improved from inf to 0.21252, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 3.5077 - val_loss: 0.2125\n",
      "Epoch 2/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.7952\n",
      "Epoch 00002: val_loss improved from 0.21252 to 0.19669, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7928 - val_loss: 0.1967\n",
      "Epoch 3/100\n",
      "147/154 [===========================>..] - ETA: 0s - loss: 0.8164\n",
      "Epoch 00003: val_loss improved from 0.19669 to 0.16920, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7997 - val_loss: 0.1692\n",
      "Epoch 4/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.6056\n",
      "Epoch 00004: val_loss improved from 0.16920 to 0.16023, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.6052 - val_loss: 0.1602\n",
      "Epoch 5/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.4115\n",
      "Epoch 00005: val_loss improved from 0.16023 to 0.15199, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4100 - val_loss: 0.1520\n",
      "Epoch 6/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.4049\n",
      "Epoch 00006: val_loss improved from 0.15199 to 0.14320, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_6.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4008 - val_loss: 0.1432\n",
      "Epoch 7/100\n",
      "144/154 [===========================>..] - ETA: 0s - loss: 0.4355\n",
      "Epoch 00007: val_loss did not improve from 0.14320\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4289 - val_loss: 0.1607\n",
      "Epoch 8/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.3105\n",
      "Epoch 00008: val_loss did not improve from 0.14320\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3084 - val_loss: 0.1456\n",
      " ###6 fold : val acc1 0.576, acc3 0.953, mae 0.241###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/154 [============================>.] - ETA: 0s - loss: 3.5314\n",
      "Epoch 00001: val_loss improved from inf to 0.22478, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 3.4691 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.9260\n",
      "Epoch 00002: val_loss improved from 0.22478 to 0.20707, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9172 - val_loss: 0.2071\n",
      "Epoch 3/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.9175\n",
      "Epoch 00003: val_loss improved from 0.20707 to 0.17728, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.9175 - val_loss: 0.1773\n",
      "Epoch 4/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.7354\n",
      "Epoch 00004: val_loss improved from 0.17728 to 0.15903, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7324 - val_loss: 0.1590\n",
      "Epoch 5/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.4283\n",
      "Epoch 00005: val_loss improved from 0.15903 to 0.15226, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4285 - val_loss: 0.1523\n",
      "Epoch 6/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.3896\n",
      "Epoch 00006: val_loss improved from 0.15226 to 0.14194, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_7.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3893 - val_loss: 0.1419\n",
      "Epoch 7/100\n",
      "153/154 [============================>.] - ETA: 0s - loss: 0.4678\n",
      "Epoch 00007: val_loss did not improve from 0.14194\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4676 - val_loss: 0.1684\n",
      "Epoch 8/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.3140\n",
      "Epoch 00008: val_loss did not improve from 0.14194\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3133 - val_loss: 0.1422\n",
      " ###7 fold : val acc1 0.580, acc3 0.956, mae 0.235###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/154 [==========================>...] - ETA: 0s - loss: 3.6471\n",
      "Epoch 00001: val_loss improved from inf to 0.24501, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 3.4434 - val_loss: 0.2450\n",
      "Epoch 2/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.8911\n",
      "Epoch 00002: val_loss improved from 0.24501 to 0.20906, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8854 - val_loss: 0.2091\n",
      "Epoch 3/100\n",
      "148/154 [===========================>..] - ETA: 0s - loss: 0.9055\n",
      "Epoch 00003: val_loss improved from 0.20906 to 0.17886, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.8880 - val_loss: 0.1789\n",
      "Epoch 4/100\n",
      "149/154 [============================>.] - ETA: 0s - loss: 0.7331\n",
      "Epoch 00004: val_loss improved from 0.17886 to 0.15541, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.7234 - val_loss: 0.1554\n",
      "Epoch 5/100\n",
      "152/154 [============================>.] - ETA: 0s - loss: 0.4204\n",
      "Epoch 00005: val_loss improved from 0.15541 to 0.14408, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4205 - val_loss: 0.1441\n",
      "Epoch 6/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.3848\n",
      "Epoch 00006: val_loss improved from 0.14408 to 0.13982, saving model to result/size/DNN_size_both_y/batch128,dnodes128_dropout0.3,dnodes256_dropout0.4,lr0.002/weights_8.hdf5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3848 - val_loss: 0.1398\n",
      "Epoch 7/100\n",
      "150/154 [============================>.] - ETA: 0s - loss: 0.4553\n",
      "Epoch 00007: val_loss did not improve from 0.13982\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4522 - val_loss: 0.1626\n",
      "Epoch 8/100\n",
      "151/154 [============================>.] - ETA: 0s - loss: 0.3250\n",
      "Epoch 00008: val_loss did not improve from 0.13982\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.3241 - val_loss: 0.1469\n",
      " ###8 fold : val acc1 0.552, acc3 0.951, mae 0.254###\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35/154 [=====>........................] - ETA: 0s - loss: 4.9896 "
     ]
    }
   ],
   "source": [
    "from keras import metrics\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "# random search for hyperparameter\n",
    "ntrial = ntest\n",
    "train_errs, val_errs = [] ,[]\n",
    "test_acc, test_roc, test_prc = [], [], []\n",
    "#test_rmse, test_mae, test_auc = [], [], []\n",
    "random_settings = []\n",
    "\n",
    "\n",
    "for itrial in range(ntrial):\n",
    "    # grid search\n",
    "    # test_setting = test_settings[itrial]\n",
    "\n",
    "    # random search\n",
    "    print('random search {}/{}'.format(itrial, ntrial))\n",
    "    \n",
    "    # total conv layers of the model\n",
    "    nlayer = random.choice([1,2]) \n",
    "    # test settings\n",
    "    dnodes[0], dropouts[0], dnodes[1], dropouts[1], batch_size, learning_rate = random.choice(test_settings)\n",
    "    \n",
    "\n",
    "    # 이번 옵션에 대한 결과 디렉토리\n",
    "    odir_f = f'batch{batch_size},'\n",
    "    for i in range(nlayer):\n",
    "        odir_f += f'dnodes{dnodes[i]}_dropout{dropouts[i]},'\n",
    "    odir_f += f'lr{learning_rate}'\n",
    "    random_settings.append(odir_f)\n",
    "    \n",
    "    odir = rootdir + '/' + odir_f\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "\n",
    "    # build a model\n",
    "    inp = Input(shape=(x_train.shape[1],))\n",
    "    out = inp\n",
    "\n",
    "    \n",
    "    for i in range(nlayer):      \n",
    "        out = Dense(dnodes[i], activation='relu')(out)\n",
    "        out = Dropout(dropouts[i])(out)\n",
    "    \n",
    "    out = Dense(1)(out)\n",
    "\n",
    "\n",
    "    model = Model(inputs=[inp], outputs=[out])\n",
    "    model.save_weights(f'{odir}/initial_weights.hdf5')\n",
    "        \n",
    "\n",
    "    # 4-fold cv\n",
    "    kfold = KFold(nfold)\n",
    "    acc1s, acc3s, maes = [], [], []\n",
    "\n",
    "    switch = 0\n",
    "    for fold, (train_mask, test_mask) in enumerate(kfold.split(y_train)):\n",
    "        X_train = x_train_imputed[train_mask]\n",
    "        X_test = x_train_imputed[test_mask] \n",
    "        \n",
    "        Y_train = y_train[train_mask] \n",
    "        Y_test = y_train[test_mask]\n",
    "\n",
    "\n",
    "        # model 학습\n",
    "        try:\n",
    "            weightcache = f\"{odir}/weights_{fold}.hdf5\"\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=learning_rate), metrics=[])\n",
    "            hist = model.fit(X_train, Y_train, validation_split=0.2, epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=2, verbose=0, mode='auto')])\n",
    "\n",
    "            model.load_weights(weightcache)\n",
    "            y_pred = model.predict(X_test).flatten() \n",
    "            y_pred = np.round(y_pred * 2) / 2\n",
    "            \n",
    "            acc1 = np.mean(y_pred==Y_test)\n",
    "            acc3 = np.mean((y_pred >= Y_test-0.5) & (y_pred <= Y_test+0.5))\n",
    "            mae = mean_absolute_error(Y_test, y_pred)           \n",
    "            \n",
    "            acc1s.append(acc1)\n",
    "            acc3s.append(acc3)\n",
    "            maes.append(mae)\n",
    "\n",
    "            print(f' ###{fold} fold : val acc1 {acc1:.3f}, acc3 {acc3:.3f}, mae {mae:.3f}###')\n",
    "            tf.keras.backend.clear_session()\n",
    "            model.load_weights(f'{odir}/initial_weights.hdf5')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            switch = 1\n",
    "            shutil.rmtree(odir)\n",
    "            itrial -= 1\n",
    "            break\n",
    "\n",
    "    if switch:\n",
    "        switch = 0\n",
    "        continue\n",
    "    \n",
    "\n",
    "    print(f'acc1{np.mean(acc1s):.3f}_acc3{np.mean(acc3s):.3f}')\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    os.rename(odir, rootdir+f'/acc1-{np.mean(acc1s):.3f}_acc3-{np.mean(acc3s):.3f}_{odir_f}')\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf846690-8fbf-4328-8fa3-42eb56924ffa",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ddac1-3af3-438e-a4d6-324e7bee93c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgbr model\n",
    "xgbr = xgb.XGBRegressor()\n",
    "xgbr.load_model('result/size/acc1-0.601_acc3-0.966_XGBR_10fold/model.model')\n",
    "y_xgbr = xgbr.predict(x_test)\n",
    "\n",
    "# RF model\n",
    "rfr = pickle.load(open(f'{odir}/gridSearch','rb'))\n",
    "y_rbr = rbr.predict(x_test_imputed).flatten()\n",
    "#y_rbr = np.round(y_rbr * 2) / 2\n",
    "\n",
    "# DNN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc5119-2f0f-4969-a4a3-333ee4216d8a",
   "metadata": {},
   "source": [
    "# Using BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389d6f7-1d35-4e29-8c84-85ca900467fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3792f7c2-1d6b-4205-82f3-0472e9baec8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75e9a751-014c-4715-9716-0ad9407a7b8e",
   "metadata": {},
   "source": [
    "# Age-based vs xgbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e36a3e6-6d10-4114-8ea1-ac834650cbc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T15:08:04.449944Z",
     "iopub.status.busy": "2023-02-17T15:08:04.449384Z",
     "iopub.status.idle": "2023-02-17T15:08:04.462959Z",
     "shell.execute_reply": "2023-02-17T15:08:04.461906Z",
     "shell.execute_reply.started": "2023-02-17T15:08:04.449889Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xgbr 모델 예측값\n",
    "xgbr = xgb.XGBRegressor()\n",
    "xgbr.load_model('result/size/acc1-0.601_acc3-0.966_XGBR_10fold/model.model')\n",
    "\n",
    "y_pred = xgbr.predict(x_test).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "# age-based formula 값 : y_test_old\n",
    "\n",
    "# 두 공식 간에 다른값 비율\n",
    "#y_pred == y_test_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffe16240-0412-4a80-98e4-9bf1b682cab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T15:08:11.415523Z",
     "iopub.status.busy": "2023-02-17T15:08:11.414967Z",
     "iopub.status.idle": "2023-02-17T15:08:11.423931Z",
     "shell.execute_reply": "2023-02-17T15:08:11.422881Z",
     "shell.execute_reply.started": "2023-02-17T15:08:11.415472Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3641304347826087"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred == y_test_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3142a146-9d41-464a-b61d-ca7222a19686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T15:30:41.287359Z",
     "iopub.status.busy": "2023-02-17T15:30:41.286848Z",
     "iopub.status.idle": "2023-02-17T15:30:44.743797Z",
     "shell.execute_reply": "2023-02-17T15:30:44.743175Z",
     "shell.execute_reply.started": "2023-02-17T15:30:41.287304Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f09b426d760>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAHkCAYAAAAepQd0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABGwElEQVR4nO3debxkZX3n8c/vVt19raVpmq27EQXEQdAGFRVFJCIRXMYoxGVGE43GcSKZZJLJZMZljHEyzmhcYkSNojFGI8orKi6gREUUaZBFUECFZhGarlNVd629nvnjqdvc7r59b93uOnVq+b5fr3p136pT5/z6dvf91rOc5zHnHCIiItK5BqIuQERERNamsBYREelwCmsREZEOp7AWERHpcAprERGRDqewFhER6XChhrWZXWpmd5jZz8zs82Y2Eub1REREelFoYW1mRwP/GdjhnHsSEAMuDut6IiIivSrsbvA4MGpmcWAM+E3I1xMREek5oYW1c+4h4H3A/cDDwKxz7tthXU9ERKRXxcM6sZklgBcD24E88C9m9mrn3D/ud9wbgTcCjI+PP/Wkk04KqyQREZGOUK1Wcc5x2223ZZxzm9Y7PrSwBp4P3Ouc2wNgZl8GzgL2CWvn3GXAZQA7duxwO3fuDLEkERGR6DjnyOVyFItFpqenmZiY2NXM+8Ics74feLqZjZmZAecCPw/xeiIiIh3LOUc2m90b1OPj402/N8wx6xuALwE3A7c3rnVZWNcTERHpVMtBXSqVmJmZ2VBQQ7jd4Djn3g68PcxriIiIdDLnHEEQUC6XSSQSjI6ObvgcoYa1iIhIP6vX62Sz2cMKalBYi4iIhKJerxMEAdVqlWQyycjIoS/iqbXBRUREWmxlUCcSicMKalDLWkREpKXq9TqZTIZarUYymWR4ePiwz6mwFhERaZFarUYQBC0NalBYi4iItMTKoE6lUgwNDbXs3AprERGRw1Sr1chkMtTr9ZYHNSisRUREDku1WiUIApxzpNNpBgcHW34NzQYXERE5RCuDOpVKhRLUoJa1iIjIIalWq2QyGQDS6TTxeHiRqrAWERHZoEqlQhAEmBmpVCrUoAZ1g4uIiGxIu4Ma1LIWERFpWrlcJpvNYmak02lisVhbrquWtYiISBPK5TJBEDAwMNDWoAa1rEVERNZVKpXIZrPEYjFSqVRbgxoU1iIiImtaGdTpdJqBgfZ3SiusRUREDqJYLJLL5YjH46RSqUiCGjRmLSIisqpOCWpQy1pEROQAhUKBXC7H0NAQyWQy0qAGhbWIiMg+VgZ1KpXCzKIuSWEtIiKybGlpiXw+z/DwMMlksiOCGjRmLSIiAnRuUINa1iIiIiwuLjI7O8vIyAiJRKKjghoU1iIi0ucWFhaYm5vr2KAGhbWIiPSx5aAeHR1lZmamI4MaFNYiItKn5ufnmZ+fZ3R0lEQiEXU5a1JYi4hI35mbm2NhYYGxsTFmZmaiLmddmg0uIiJ9pduCGtSyFhGRPjI7O8vi4iLj4+NMT09HXU7TFNYiItIX8vk8S0tLTExMMDU1FXU5G6JucBER6XndHNSglrWIiPS4XC5HoVBgcnKSycnJqMs5JAprERHpSc458vk8hUKBqakpJiYmoi7pkCmsRUSk5zjnyOVyFIvFrg9qUFiLiEiPWRnU09PTjI+PR13SYVNYi4hIz3DOkc1mKZVKzMzMMDY2FnVJLaHZ4CIi0hN6NahBLWsREekBzjmCIKBcLpNIJBgdHY26pJZSWIuISFer1+tks9meDWpQWIuISBer1+sEQUC1WiWZTDIyMhJ1SaHQmLWIiHSllUGdSCR6NqhBLWsREelC9XqdTCZDrVYjmUwyPDwcdUmhUliLiEhXqdVqBEHQN0ENCmsREekiK4M6lUoxNDQUdUltobAWEZGuUKvVyGQyOOf6KqhBYS0iIl2gWq0SBMHeoB4cHIy6pLbSbHAREelo/R7UoJa1iIh0sGq1SiaTASCdThOP92ds9eefWkREOl6lUiEIAsyMVCrVt0EN6gYXEZEOpKDeV3//6UVEpOOUy2WCIGBgYIB0Ok0sFou6pMgprEVEpGMsB3UsFiOVSimoGxTWIiLSEUqlEtlsVkG9CoW1iIhEbmVQp9NpBgY0pWolhbWIiESqWCySy+WIx+OkUikF9Sr0HRERkcgoqJujlrWIiESiUCiQy+UYGhoimUwqqNegsBYRkbZbGdSpVAozi7qkjqawFhGRtlpaWiKfzzM8PEwymVRQN0FhLSIibbO4uMjs7KyCeoMU1iIi0hbLQT0yMkIikVBQb4DCWkREQrewsMDc3JyC+hAprEVEJFTz8/PMz88zOjrKzMyMgvoQKKxFRCQ0K4M6kUhEXU7XCu2mNjM70cxuWfGYM7O3hXU9ERHpLHNzc8zPzzM2NqagPkyhtaydc3cBpwGYWQx4CPhKWNcTEZHOMTc3x8LCAuPj40xPT0ddTtdrVzf4ucCvnHO72nQ9ERGJyOzsLIuLiwrqFmrX2m4XA59f7QUze6OZ7TSznXv27GlTOSIiEoZ8Ps/i4iITExMK6hYKPazNbAi4CPiX1V53zl3mnNvhnNuxadOmsMsREZGQ5PN5lpaWmJycZGpqKupyeko7usFfCNzsnNvdhmuJiEgEcrkchUKByclJJicnoy6n57QjrC/hIF3gIiLS3Zxz5PN5CoUCU1NTTExMRF1STwq1G9zMxoHzgC+HeR0REWk/59zeFrWCOlyhtqydc4tAKsxriIhI+znnyGazlEolpqenGR8fj7qknqYVzEREZENWBvXMzAxjY2NRl9Tz2nXrloiI9AAFdTTUshYRkaY45wiCgHK5TCKRYHR0NOqS+obCWkRE1lWv18lmswrqiCisRURkTfV6nSAIqFarJJNJRkZGoi6p7yisRUTkoPYP6uHh4ahL6ksKaxERWVWtViMIAmq1moI6YpoNLiIiB1BQdxa1rEVEZB+1Wo1MJkO9XieVSjE0NBR1SX1PLWsREdlrOaidcwrqDqKWtYiIAFCtVgmCYG9QDw4ORl2SNCisRUSEarVKJpMBUFB3IIW1iEifq1QqBEEAQDqdJh5XNHQa/Y2IiPSx5aA2M1KplIK6Q2mCmYhIn1JQdw/9zYiI9KFyuUwQBAwMDJBOp4nFYlGXJGtQWIuI9JnloI7FYqRSKQV1F1BYi4j0kVKpRDabVVB3GYW1iEifWBnU6XSagQFNW+oWCmsRkT5QLBbJ5XLE43FSqZSCusvob0tEpMcVi0Wy2ayCuoupZS0i0sMKhQK5XI6hoSGSyaSCuksprEVEetTKoE6lUphZ1CXJIVJYi4j0oKWlJfL5PMPDwySTSQV1l1NYi4j0mMXFRWZnZxXUPURhLSLSQ5aDemRkhEQioaDuEQprEZEesbCwwNzcnIK6BymsRUR6wPz8PPPz84yOjjIzM6Og7jEKaxGRLrcyqBOJRNTlSAgU1iIiXWxubo6FhQXGxsaYmZmJuhwJicJaRKRLzc7Osri4yPj4ONPT01GXIyHSUjYiIl1IQd1f1LIWEeky+XyepaUlJiYmmJqairocaQOFtYhIF1kO6snJSSYnJ6MuR9pEYS0i0gWcc+TzeQqFgoK6DymsRUQ6nHOOXC5HsVhkamqKiYmJqEuSNlNYi4h0sJVBPT09zfj4eNQlSQQU1iIiHco5RzabpVQqKaj7nMJaRKQDrQzqmZkZxsbGoi5JIqSwFhHpMM45giCgXC4rqAVQWIuIdJR6vU42m6VcLpNIJBgdHY26JOkACmsRkQ5Rr9cJgoBKpUIymWRkZCTqkqRDaLlREZEOsBzU1WpVQS0HUMtaRCRi+wf18PBw1CVJh1FYi4hEqFarEQQBtVpNQS0HpbAWEYnIyqBOpVIMDQ1FXZJ0KIW1iEgEarUamUyGer2uoJZ1KaxFRNqsWq0SBAHOOQW1NEVhLSLSRvsH9eDgYNQlSRdQWIuItEm1WiWTyQAoqGVDFNYiIm1QqVQIggCAdDpNPK4fv9I8/WsREQnZclCbGalUSkEtG6Z/MSIiISqXy2SzWcyMdDpNLBaLuiTpQlpuVEQkJOVyeW+LWkEth0MtaxGRECwHdSwWI5VKKajlsCisRURarFQqkc1mFdTSMgprEZEWWg7qeDxOKpViYECjjXL4FNYiIi1SLBbJ5XIKamk5/UsSEWmBYrFINptlcHBQQS0tp5a1iMhhKhQK5HI5hoaGSKVSmFnUJUmPUViLiBxMtQp33gnf/jb86lf+61QKnv98OPNMmJpiaWmJfD6voJZQKaxFRFbzyCPwgQ/A7t0wPu5D2gwKBfjc5+ALX2Dp4ovJn3wyw8PDJJNJBbWERmEtIrK/PXvgPe+BWg22bt33tYkJmJhgcW6O2b/7O4Zf/3qSF1zQ3qCenYUf/AC++12Yn4ctW+AFL/CtfW0O0pNCnQFhZjNm9iUz+4WZ/dzMnhHm9UREWuLyy6FUgk2bVn15sVxm1oyRY44h+eUvY/l8+2p79FF4xzvgiisgHocjj/SB/bGP+Z6AUql9tUjbhD1d8W+BbzrnTgKeDPw85OuJiByeRx6Bn/3Mh+AqFsplZkslRuJxEjMzWL0OP/5xe2pzDj76USgWfYt/bAxiMZiehu3bfd1f/3p7apG2Ci2szWwaOBv4JIBzruycy4d1PRHpEfU6XH893HCDD6d2u/FGGBjw49P7mS+VmCuVGI3HSY6O+q7vTZv8BLR22LUL7r0XjjjiwNfM4Kij4Oqr1bruQWG2rLcDe4BPmdlPzewTZjYe4vVEpBfceit85CPw4Q/DzyPojNu9G0ZGDnh6vlRivlxmbHCQxOjoYy+MjkIu58e3w/bgg/7Xg42PDw9DuQyZTPi1SFuFGdZx4CnAR51zpwOLwJ/vf5CZvdHMdprZzj179oRYjoh0hclJHzrDw34WdrvFYge06OdWBPXMKkGOmW+Nt6O29SayOdeeWqStwvwbfRB40Dl3Q+PrL+HDex/Oucucczucczs2HWQyh4j0kRNOgHe/2z/2n4ndDtu2+THhhtlikYVymfGDBfXsLBx77Poh2gonnOB/rddXf31hAZJJ2Lw5/FqkrUILa+fcI8ADZnZi46lzgTvDup6I9JCjjjroBK/QPfWpvmVarTJbLLJYqTAxNMT0akENkM/DBRe0p7ZNm+DpT4cHHjhwPL9a9V34F12klnUPCvs+67cCnzOzIeDXwOtCvp6IyOGZmoLnPIf8N7/J0pFHMjE8zNTw8OrH5vP++NNOa199r32tb0HffvtjwwWLi37M/KUvhWc/u321SNuEGtbOuVuAHWFeQ0Sk1XLnnUfhjjuYfOghJrdvP/AA5yAIoFKBP/uzVSekhWZ0FC69FO65B370I98Nf/TRcNZZvkdCepJWMBMRaXDOkc/nKTjH5J/8CZNf/Spcd50P5/FxPy5dLPqQPu44+P3f97+228AAnHiif0hfUFiLiOCDOpfLUSwWmZqaYmJiAl7/enjZy/yiJ8sbeWzaBM94hp+IprXApU0U1iLS91YG9fT0NOMrbxmbmYHzz4+sNhFQWItIn3POkc1mKZVKBwa1SIdQWItI31oZ1DMzM4yNjUVdksiqFNYi0peccwRBQLlcJpFIMLpyCVGRDqOwFpG+U6/XyWazCmrpGgprEekr9XqdIAioVqskk0lG2nmPtMgh0pp0ItI3VgZ1IpFQUEvXUMtaRPpCvV4nk8lQq9VIJpMMH2wJUZEOpLAWkZ5Xq9UIgkBBLV1LYS0iPW1lUKdSKYaGhqIuSWTDFNYi0rNqtRqZTIZ6va6glq6msBaRnlStVgmCAOcc6XSawcHBqEsSOWSaDS4iPWdlUKdSKQW1dD21rEWkp1SrVTKZDADpdJp4XD/mpPvpX7GI9IxKpUIQBJgZqVRKQS09Q93gItITFNTSy/SvWUS6XrlcJpvNYmak02lisVjUJYm0lFrWItLVyuUyQRAwMDCgoJaepZa1iHStUqlENpslFouRSqUU1NKzFNYi0pVWBnU6nWZgIISOwnodfvlL+OlPIZ+HahUmJ+GEE+D000Fba0qbrBvWZvZ04EPAycAQEAMWnXNTIdcmIrKqYrFILpcjHo+TSqVaH9SFAvzkJ3DVVbB7N8TjMDwMZj6wr70WhobgnHPguc+FI49s7fVF9tNMy/rDwMXAvwA7gNcCTwizKBGRgwk9qPfsgf/3/+A3v4F0GrZtW/24chmuucY//uAP4IwzWluHyApN/St3zv0SiDnnas65TwHnh1uWiMiBCoUC2WyWwcHBcII6k4G/+ivI5WD7dt/lfTBDQ3DMMZBMwoc+BNdf39paRFZopmW9ZGZDwC1m9jfAw2gWuYiEoVaDu+7yrVozH4aPfzwMDFAoFMjlcgwNDZFKpTCz1l67WIT3vx9KpY11a4+NwVFHwWWXwaZNvl6RFmsmrF+DH6f+T8ClwLHAvw+zKBHpQ7fdBpdfDtksOPfY80ccwdIll5DfvJnh4WGSyWTrgxrg5pvhoYcO3u29lpERmJiAK66AP//zlpcmsm5YO+d2NX5bAN4Zbjki0pduuQU+8AHfpbx16z4vLWUy5N/7Xob/838medZZ4QS1c34yWSJx6OdIpeAXv/C9Akcd1braRFijO9vMbjez2w72aGeRItLDSqXHupCn9r3JZLFcJj88zEgqRfKLX8RqtXBquPdeePDBA66/IWYQi8H3v9+6ukQa1mpZv6htVYhIdMplf2tSGPcpN+PWW2FpCY44Yp+nF8pl5kolRuJxEps2YfffD3feCaee2voa7rzT//kPt9W+aRP8+Mdw8cWtqUuk4aBhvaL7W0R61QMPwLvfDU98IvzRH0VTw513+jHfFZaDejQeZ2ZkxHd9x+N+8lkYYZ3P+9ndh2toyM8od+7wg19khWYWRZkHlmd7DAGDaFEUkd4wP+9btbt3R1dDrbZPq36+VGK+XGY0HiexcoUwM39sGKrV1pzHzK96JtJizUww23ujofmZHS8Gnh5mUSLSJiefDO96l5/YFZVt2+C66wCYK5VYKJcZGxxkZr/WNpUKHHtsODVMTbUmsCsVfyuXWtXSYhsapHLelcALwilHRNrKzC/+MT0dXQ07dsDAAHOLiwcP6nIZBgf9etxheNzjWhPW2awfUhBpsWa6wV+24ssB/JKjxdAqEpH+Mj3N7LnnsnjFFYxv3870ai3q+++H177Wt1rDcMopvnVdKBze5hzFIjz/+a2rS6ShmUVRLlzx+ypwH74rXETksOXzeZbOPpsJ55i65hrfOl1e5nN+3k/WuuQSOO+88IqIx+H88+FLX4Ljjju0c8zPw+bNfkcukRZrZsz6de0oRET6Tz6fZ2lpiYmpKaZe9Sp4wQv8Gtt33+0POOkkOOus9oypn3UWfPWrMDe38futazV49FF405uiuwVOeloz3eDbgbcC21Ye75y7KLyyRKTX5XI5CoUCk5OTTC63pNNpuCiiHy0zM/C2t8F73+sDd2KiuffVanDffb5lftZZIRYo/ayZbvArgU8CXwV0T4KIHBbnHPl8nkKhwNTUFBPNhmI7nHgi/Jf/An/7t75b+4gj/Kpkq3HOt8IzGXjhC+GVr9QscAlNM2FddM59MPRKRKTnOefI5XIUi8XOC+plT3oSvP3tvkv8xht9KCeTMDzsw7ha9Vtolst+V7CLL4Yzz1RQS6jMrdzdZrUDzH4XeDzwbaC0/Lxz7uZWF7Njxw63c+fOVp9WRDrAyqCenp5mfHw86pLWl8vBj34EP/yhb0VXq757/KST4Jxz/G1vCmk5DGZ2k3Nux3rHNdOy/nf4bTKfx2Pd4K7xtYjIupxzZLNZSqUSMzMzjIV1C1arJRJwwQX+IRKhZsL6d4DjnXPlsIsRkd7TtUEt0kGaucfgZ8BMyHWISA9yzhEEAaVSiUQioaAWOUTNtKxngF+Y2Y3sO2atW7dE5KDq9TrZbJZyuUwikWD0cFYGE+lzzYT120OvQkR6Sr1eJwgCqtUqyWSSkf2XEBWRDVkzrM0sBnzMOXdSm+oRkS63MqgTiYSCWqQF1hyzds7VgLvM7BAXyxWRflKv18lkMmpRi7RYM93gCeAOM/sJsLj8pMasRWSlWq1GEATUajWSySTDw8NRlyTSM5oJ6/8RehUi0tVWBnUqlWJoaCjqkkR6SjO7bn3PzDYDZzSe+olz7tFwyxKRblGr1chkMjjnFNQiIVn3PmszewXwE/ziKK8AbjCzl4ddmIh0vmq1qqAWaYNmusH/O3DGcmvazDYB1wBfCrMwEels1WqVIAj2BvXg4GDUJYn0rGbCemC/bu+A5lY+E5EetdyiBkin08TjzfwoEZFD1cz/sG+a2beAzze+fiVwVXgliUgnq1QqBEGAmZFKpRTUIm1w0P9lZjbsnCs55/7UzF4GPKvx0mXOua+0pzwR6SQKapForPU/7UfAU8zss8651wBfblNNItKByuUyQRAwMDBAOp0mFotFXZJI31grrIfM7HeBsxot63045xTeIn1iOahjsRipVKr1QV2vwz33wC23wF13QSYDZpBOw8knw2mnweMe558T6UNrhfWbgFfhd926cL/XHGppi3S3SgUeeQSKRYjHYWoKUqkDDiuVSmSz2fCC+uc/h8svh927IRbzdUxM+NeyWfjGN+BrX4Ojj4b/+B/h8Y9v7fVFusBBw9o5dx1wnZntdM59so01iUiYggB+9CP41rdgacm3Vp3zrduTToLzz4dTToF4fJ+gTqfTDAy08EaQWg2++EX45jchkYCtWw88ZmgIpqd9fbkcvPvd8OIXw0teAq2sRaTDNbOCmYJapBc4B9/+NnzhC/7rTZv2bUk7Bw88AO9/P2zZQvFNbyI3NEQ8HieVSrU2qOt1+Oxn4bvf9SG9XmvdDJJJ3+r+yld8b8All6hbXPqGPpqK9APn4Mor4XOfgyOPhOOOg9HRfY9ZHiPeto1iEJB75zuJB0Hrgxrg+ut9UG/btn5QrxSP+/d885tw882trUmkgymsRfrBDTfAl7/sQ3qdJUELlQrZyUkGgdTllzNQKrW2ltlZ36resuXQurJjMdi8Gf7hH2Bxcf3jW2VpCebmfK+ASJutdZ/1U9Z6o3NOH2tFukG97oP6iCN8y3QNhUqFXLHIUCxG6thjsV27fAv2mc9sXT3XX+8nt+3fst+I8XE/Y/zGG+G5z21Zaau67z7f9X7bbY/1Plx4ITzrWeqGl7ZZ63/u/238OgLsAG4FDDgV2Ak8Y72Tm9l9wDxQA6rOuR2HU6yIHIK774ZHH/Xdx2tYqlTIF4sMx2IkR0cxMz/x6xvfgLPOak0wOQdXX+0D73AlEv5cYYb1L38J730vDA7Cscf6noD5ebjsMnj4YXjFK8K7tsgKB+2Dcs6d45w7B3gYeIpzbodz7qnA6cBDG7jGOc650xTU0veqVbj/fj85qp3+7d9gZGTNQxbL5QODGvyErocegl27WlPL3Jyf1X04replk5Pwm99AoXD451qNc/DpT/tW/ObNj3XZT076Dz7f+Ia/vkgbNDNgdKJz7vblL5xzPwNODq8kkR714Q/D//gf8J73QLncvus++KAPmINYLJeZLZUYicf3DWrwremBAR+wrbBnjz9fK1rpy7U1NhRpuYce8o+ZmQNfi8X8tW+8MZxri+ynmbC+zcw+YWbPbTw+DtzW5Pkd8G0zu8nM3rjaAWb2RjPbaWY79+zZ02zdIt2lVoNbb/Uzse+/30+yapdS6aATuRZWBHViZGTfoF7mnB9jboVarTXnWWbmeyzCsLTkQ/lgHywGB1v3IUZkHc2E9euAO4A/ajzubDzXjGc5554CvBB4i5mdvf8BzrnLGl3sOzZt2tTkaUW6TCwGF1/sQ/rcc1szZtussbFVA22hXGauVGJ0raBeNjzcmlpadZ5lzq3bxX/IUin/4eJgs79LJT+7XqQNmlkUpWhmfw9c5Zy7ayMnd8491Pj1UTP7CnAm8P1DqlSk273gBf7Rbqec4lcrW9EVPl8qMV8u+6Bea/y4XveBePTRranlyCMfO+fhdoXXav4cYX3IT6XgKU/xPSLHHLPva4uLvmW9Q1NxpD3WbVmb2UXALcA3G1+fZmb/2sT7xs1scvn3wG8BPzusakVk4579bB9szgEw1wjqscHBtYMa/Bjzaae1ridgZAS2b4d8/vDPlcv55VHD3KbzNa/xt7zde6+veWHBr/KWy8Ef/qGfgCfSBs10g78d3yLOAzjnbgG2N/G+zfi1xW8FfgJ83Tn3zUMrU0QO2ZYtfueqPXuYK5VYKJcZHxxkZr3uY+f8uO1557W2nvPPb82Y/cIC/NZvHf551jIz4ycFvv71vgU/Nuav+e53+w8xIm3SzEfSinNudr/xLLfem5xzvwaefKiFiUgLXXIJs3/5lyzW64zPzDDdTFDv2uW7eU86qbW1nHaa/wARBKvu8tWURx/1a4o/8YktLW1Vo6PwnOf4h0hEmmlZ39HY1zpmZo83sw8B14dcl4i0UH5qisXXvY6JxUWmFxf3domvqlr1q3adfDK84Q2t391qaMifd37+0O45X1ryk7ve8IZwu8BFOkgz/wvfCpwClIDPA3PA20KsSURaKJ/Ps7S0xOSOHUy9612+a3fXLr+gR6Xy2PaYCwv++Ycf9l29l14a3kzr44+HP/gDf635+ebfNzfnW9VvecuBk75Eepi5tT5h73+wWQwYd87NhVHMjh073M6dO8M4tUhfyuVyFAoFJicnmVyeDe6cnzB17bV+3e/l+4nTaR/SZ5yx5iIqLXXbbfDxj/sPClu2HHyTkVIJHnnE7239pje1vmteJCJmdlMzK3yuG9Zm9k/Am/Dre98ITAF/65z7P60odCWFtUhrOOfI5/MUCgWmpqaYmJhY62D/a1SbUszP+6U7v/Odx1r6w8P+13LZ1zU87Ce6veAFfvlPkR7RbFg3M+DzROfcnJm9CvgG8OfATUDLw1pEDp9zjlwuR7FYXD+oIfqdoyYn/YYYF13kW/wPPAC7d/u6Nm/2G2hs3976BVVEukgzYT1oZoPAS4APO+cqZtZ837mItM3KoJ6enma8m1qhIyN+UtvJ2npAZH/NTDD7GHAfMA5838y24ieZiUgHcc6RzWYpFovMzMx0V1CLyJqaWW70g8AHVzy1y8zOCa8kEdmo5aAulUrMzMwwNjYWdUki0kJN3aRoZr+Nv31r5X0c7wqlIhHZEOccQRBQLpdJJBKMtmKvaBHpKOuGdWMTjzHgHOATwMvxy4eKSMTq9TrZbFZBLdLjmhmzPss591og55x7J/AM4AnhliUi66nX6wRBQKVSIZlMKqhFelgzYV1o/LpkZkcBFWBLeCWJyHqWg7parZJIJBgJa6UxEekIzYxZf83MZvD3Vd+M38Tj42EWJSIHV6vVCIKAWq1GMplkWPcfi/S8ZmaD/6/Gb68ws68BI865FuxvJyIbpaAW6U/NTDAbAf4QeBa+VX2dmX3UOXcI2+WIyKFaGdSpVIqhg62jLSI9p5lu8M8A88CHGl//LvBZ4HfCKkpE9lWr1chkMjjnFNQifaiZsH6Sc27lDu/XmtmdYRUkIvuqVqsEQbA3qAcHB6MuSUTarJnZ4Deb2dOXvzCzpwHaGkukDRTUIgJrtKzN7Hb8GPUgcL2Z3d946TjgF22oTaSvVatVMpkMAOl0mni8qQUHRaQHrfW//0Vtq0JE9lGpVAiCADMjlUopqEX63EF/Ajjndu3/nJm90Tl3WbglifQ3BbWI7K+ZMeuV3hRKFSICQLlcJpPJYGbq+haRvTb6k8BCqUKklxUK8NOfwl13weIijI7C9u2wYwdMTe09rFwuEwQBsViMVCpFLBYLty7nYHYWlpYgHve1aNlSkY600bC+MJQqRHrR3BxcdRVcey2USj6kYzGo1+G66+Bzn4NnPhNe9CJK09Nks9n2BHW5DLfe6mu77z4YaHSwxWLwnOf4x9FHg+mzuUinaGYFs83Ae4CjnHMvNLMnAs9wzn0y9OpEutWePfC+98Gjj8KWLbDaIibVKlx/PaWf/ITs615HbPt20uk0AwMbHZ3agN/8Bt7/fl/f9DQcd9xjoVyp+A8WV18NL3gBvOIVPsBFJHLN/FT4NPAt4KjG13cDbwupHpHuNzfng3puDrZuXT2oAeJxikceSbZWI/73f0+6Ugk3qB95BN7zHt/tvW0bJBL7tp4HB+GYY+DYY+Eb34DPftZ3lYtI5Jr5yZB2zn0RqAM456pALdSqRLrZ17/uW9SbN695WLFaJVsoEJ+ZITU0xMDnPx9eTfU6fOQj/tdNm9Y+NhbzYf6d78ANN4RXk4g0rZmwXjSzFH6BFBqrmWnXLZHVLC35ruSjjlrzsEKlQrZQYCgWIzU2xsCRR8LPfgYPPxxOXffcAw8+CEcc0dzxAwOQSvkPHmpdi0SumbD+Y+BfgceZ2Q/xG3u8NdSqRLrVzTf7CVxrLAtaqFTIFYs+qEdHGTDz3dGxmJ94FobvfAc2up3m9DQ88ADce284Na2lVvMPEQGaCGvn3M3Ac4CzgD8ATnHO3RZ2YSKHpVYLr5W6ljvvhLGxg7681Ajq4UZQ28ox45kZ37puNef8rWPp9Mbet1zbr37V+prW8qtfwVvfCm97G9x//7qHi/SDdcPazF4GXAScCDwBuNDMzjWzJvvTRCJw003wv/6XHztup8VFf8/yai+Vy+QbQZ3cP6jBv29pqfU11Wp+pvehLLASj8PCQutrWsu11/qZ8ktL8MMftvfaIh2qmf+9vwc8A7i28fVzgZuA7Wb2LufcZ0OqTeTQnXIK/P7vb7w1ebhGR1ftvl0sl5ktlRiJx0mMjBwY1OAnf220q7oZsZgfg67XH7unulm12po9BaE49VQf0mb+71FEmgrrOHCyc2437L3v+jPA04DvAwpr6Tzj4/CUp7T/utu2HTCDeqFcZm69oAa/mtjTntb6mszghBP8sEAqtbH3OucXSGmnM8/0t7yZNT8hTqTHNfMx+9jloG54tPFcFqiEU5ZIl1oO20brer5UYq5UYnS9oHbOT0x77nPDqeuFL4T5+Y29Z3HRj6OffHIoJa1p82YFtcgKzYT1v5nZ18zsP5jZf8DPDP+emY0D+VCrE+k2iQSccQbs3s18qcR8ueyDerUx6pXyeb8gyfHHh1PXk57k1/6em2vueOdg92644AKtYibSAZoJ67cAnwJOazwud8692Tm36Jw7J8TaRLrTS1/KXKXCfBAwNjhIYnR07eOLRd8F/qpXhbce9+Ag/OEfQi7nW8xrcc7fk/3EJ4bX0heRDWnm1i3nnLvCOXepc+5SYLeZfaQNtYl0pdmxMRZe/3rGy2Vm5ucPvqiIc75F/cgj8KY3hd/dfNJJcOml/poPPOC73fevJ5fz91WfdJK/fepgS6WKSFuZa2J1IjM7HbgEeAVwL/Bl59yHWl3Mjh073M6dO1t9WpG2mZ2dZXFxkfHxcabn5uDyy+GXv/S3QCUS/tdazbekSyW/ycdrXuNbse3y6KPwve/BNdf4W7qWOecndl1wAZx++poLu4hIa5jZTc65Hesed7CwNrMn4AP6EiADfAH4E+fc1lYWupLCWrpZPp9naWmJiYkJppb3qV7uUv7e9+COO/ze1sPDfnb2OefA4x4X3VaUxSL8+te+pngckkk/bq6tMUXaptmwXuvWrV8APwBe5Jz7ZeOkl7aoPpGeshzUk5OTTE5OPvaCmd/F6tWvjq64gxkZaW+LXkQO2Vpj1i8DHgauNbOPm9m5gD5yi+wnl8utHtQiIi1y0LB2zl3pnLsYOAm/etnbgCPM7KNm9lttqk+kYznnyGazFAoFpqamFNQiEppmZoMvOuf+yTl3IXAM8FPgz0KvTKSDOefI5XIUi0Wmp6eZmJiIuiQR6WEbWijYOZdzzl3mnDs3rIJEOt1yi3o5qMfHx6MuSUR63CFswyPSv5aDulQqMTMzw1i7N7kQkb6ksBZpknOOIAgol8sKahFpK4W1SBPq9TrZbJZyuUwikWB0vSVERURaSGEtso56vU4QBFQqFZLJJCMjI1GXJCJ9ZoM70Yv0l+WgrlarCmoRiYxa1iIHsX9QDw8PR12SiPQphbXIKmq1GkEQUKvVFNQiEjmFtch+VgZ1KpViSNtEikjEFNYiK9RqNTKZDPV6XUEtIh1DYS3SUK1WCYIA55yCWkQ6isJahAODenBwMOqSRET2UlhL36tWq2QyGQAFtYh0JIW19LVKpUIQBACk02ni8Rb/l3AOdu+G+++H++6DpSUYHoZt2+Doo+HYY8G0TbyIrE1hLX1rOajNjFQq1dqgdg7uvBOuvBJ++Uv/XDzuH7UaVCr+mKOOgosugjPPhAGtUSQiq1NYS18ql8tks9lwgnppCf7pn+D734fpaTjuuNVbz87B3Bx85CNw3XXw+tdDMtm6OkSkZ+ijvPSdcrm8t0Xd8q7vxUV43/vghz/0Xd2p1MG7uc18mB9/PNx1F/zVX0Fj7FxEZCWFtfSV5aCOxWKk02lisVjrTl6vw2WXwa5dsHVr893aZn78enER3v9+KJVaV5OI9ASFtfQm5x4bF24olUp7gzqVSrU2qAGuvx5++lM45phDe/+RR8JDD8HXvtbaukSk62nMWnpLJgPXXAPXXutbqMkknH8+pTPPJFsoEI/HSaVSDLR6MlelAl/8og/cw5ndfcwxcNVV8Pzn+y5yERHa0LI2s5iZ/dTM1FyQcD3wALz97XD11X6seNs2iMUofuYzZN/9buKlUjhBDX7m99wcjI0d3nnicd+dfsMNralLRHpCO7rB/wj4eRuuI/2sXoe/+zvfqj32WGgsFVocHia7eTODDz1E6gc/CCeoAW6/fe81D9v0NNx8c2vOJSI9IdSwNrNjgN8GPhHmdaQDrBgbjsQ998Ajj0A6vfepQqVCtlBgKBYjtX07A9//vr+tKgx33w2Tk60518QE/PrX/gOIiAjht6w/APxXQD91etlVV8G73x1tYD/yyD7XX6pUyBWLPqhHR7GhIf/6nj3hXH92tnUt63jcj4GXy605n4h0vdDC2sxeBDzqnLtpnePeaGY7zWznnrB+kEq4tm6FU0+NtoYVM7uXKhXyxSLDy0G9POGrXt/nuJZqZfe6c/6hFc1EpCHMnwbPBC4ys/uAfwaeZ2b/uP9BzrnLnHM7nHM7Nm3aFGI5EppTToEXvzjaNa4f/3gAFovFvUGdXBnUi4swMwNbtoRz/SOPbF0Xe7nsx621oYiINIQW1s65/+acO8Y5tw24GPiuc+7VYV1P+tzmzSyefjqz997LyP5BXa36bvIXvzi8lvUTnwgLC60519wcPOEJ2uBDRPbSfdbSExYWFph70YsYmZ8ncc892PCw391qaclvnPHSl8LZZ4dXwJOfDFdc4buvDzdkFxfhrLNaU5eI9IS2hLVz7t+Af2vHtaT/zM/PMz8/z2gySeIv/sLvcvXjH/tJX0cfDc94Rnjd38uOPRZOOAEefhgOZzhnYQGmpuBJT2pdbSLS9dSylq62HNRjY2PMzMz4J5/wBP9oJzN4zWvgHe/wM7kPZby5XodHH4VLL9V4tYjsQ9NNpWvNzc0dGNRR2roVfud34P77/Tj5RtTrfgOQc86B004LpTwR6V5qWUtXmp2dZXFxkfHxcaY7aQ3tF74QikW48krfHd7MQinFot/A49nP9q1zTSwTkf0orKXrdGxQgw/al77U71H9yU/Cfff5dconJg4M4aUlv/FIPA6/93t+ApzurRaRVSispavk83mWlpaYmJhgamoq6nJWZ+a7st/7XrjxRvjmN33X+HIQL6+0NjUFL3+5nwCXSERWroh0PoW1dI1cLkehUGBycpLJVq3DHabxcXjuc/1jYcHf610u+5b0EUf4hU/U5S0iTVBYS8dzzpHP57srqPc3MeFv7RIROQQKa+lozjlyuRzFYpGpqSkmJiaiLklEpO0U1tKxVgb19PQ04+PjUZckIhIJhbV0JOcc2WyWUqmkoBaRvqewlo6zMqhnZmYYGxuLuiQRkUgprKWjOOcIgoByuUwikWB0dDTqkkREIqewlo5Rr9fJZrMKahGR/SispSPU63WCIKBarZJMJhkZGYm6JBGRjqG1DSVyK4M6kUgoqEVE9qOWtUSqXq+TyWSo1Wokk0mGh4ejLklEpOMorCUytVqNIAgU1CIi61BYSyRWBnUqlWJoaCjqkkREOpbCWtquVquRyWSo1+sKahGRJiispa2q1SpBEOCcI51OMzg4GHVJIiIdT2EtbbMyqFOpVOuDul6H++6Dhx7y+0eXSn63q23b4Nhj4cgjtSWliHQlhbW0RbVaJZPJAJBOp4nHW/hPr1KB666Dr38dgsA/NzQEAwNQrfoHwOMeBy9+MTzpSQptEekqCms5dJUK3HMPzM35Vu3ICBx/PMzM7HdYhSAIMDNSqVRrg/qhh+Cyy3yLetMm2Lp19eOcg0cfhf/zf+CZz4RXvcq3ukVEuoDCWjYum4Xrr4dvfQsWFnwr1bnHWqtPfzo873nwuMdRaXR9hxLUv/41/O//DbEYbN++9rFmkEz6DxI33AAPPgh/+qcwNdW6ekREQqKwlo35xS/gAx+Actm3ZFOpfV+v1eCmm+CHP6R8/vlkzz4bi8dJp9PEYrHW1REE8L73wejoAS35NQ0MwHHHwW9+Ax/6EPzZn0ErP0CIiIRAP6W6Rb0Ov/wlPPyw//roo/0YbDvHXu+6C/7mbyCR8JO1VhOLwZYtlEslgi99iVguR+qNb2xtUNfr8OlP+7HoI444tHNs2QJ33w3XXAPnn9+62kREQqCw7ga/+AX8wz/Anj2+u3nZUUfB618PJ5wQfg3z8/DBD/pW7OTkmoeWqlWy5TKxrVtJ/fCHxM44A844o3W13HUX3Habn+V9qMz8B54vfQme9SyNX4tIR9NGHp3ujjv8uGyl4idPbdvmH1u3wuIi/PVf+0leYbvxRlhaWneMt1Stki0UiA0MkJ6cJLZpE3z1q/t+yDhcV18NY2OH36swNOS77W+8sTV1iYiERGHdyapVP9M5mYTp6X1fM/Pd0ZOT8PGP+67hsNRqcNVVB45P76fYCOr4wADpsTEGzHzdu3b52dqtUC7Drbf68fJWmJ6GH/+4NedaTybjewQeeqg91xORnqFu8E52xx0wO7t2d+/MjA/Cu++Gk04Kp4777/czwI877qCHFKtVco2gTi0HNfgPFYODvvW63oztZjzyiG+lD7Toc+bEBNx7r/9A0spx9f3ddRf83//rP4A5B7//+/4WMhGRJqhl3cnuuae5mcpmrWu5rmb59qyDKFQqZAsFBmOxfYN62fDwY4uVHK7Z2dacZ1k87gN0aam1593fP/+zvw/9uONg82b47GdbOzQgIj1NYd3J6vXmxmXNfMswLGuESqFSIVcsMhSLkRodPTCol+trVTf9yvu5u0m5/NgHr1jssRa2iEgTFNadbOtWP7FsPfW6n9kclrGxVYNlqRHUw42gtoOFaLm8sXuh1zIx0dqQq9V8l/roaOvOuZoLL/Rj1vfd54cVLrywdV35ItLzNGbdyZ78ZN+FXCz6LtTVLC35SWannBJeHVu3+jBbUcdSpUK+EdTJtYIafFg/5SmtqWXLFh/WrWphLy76TT7CXhjl6U/3EwV37fL3hp96arjXE5Geoo/2nWxkxK9h/Zvf+KDcX6HgJ1y99rV+EldYBgfhBS/w93kDi+Uy+WKRkXh8/aBeWIB0Gk48sTW1jI7C4x8PuVxrzpfPw1Of2ppzrecJT4DzzvMfwrqxK19EIqOw7nTPfjb83u/52di7dsHu3T6g77vPB82b39zaBUcOpjFzeWFujtlSiZF4nMTIyNpB7Zyv94ILWtvle/75rZlotjzOf9ZZh38uEZEQqRu805nBc57jA/mmm+BXv/LPP+EJcPrp4Y+1LkulWLjkEuY++lFGjzuOmWaCetcuX/fZZ7e2ln/373zX/J49h3e/9YMP+pZuMtm62kREQmCug2ak7tixw+3cuTPqMmQV8/PzzM/PM3rzzSSuuMKv/nXEEQfem+yc3zIzk4Ezz4Q3vOHg4+2H44EH4B3v8Au1jI1t/P2ZjP+g8653te8Dj4jIfszsJufcjvWOU8ta1jU3N8fCwgJjY2PMvOhF8MQn+u0xb7zRz0QfHPQ9ANWq//qYY+B3fgee9rTwFho59lg/BPChD/kx8XXWK99reV/reBz++I8V1CLSFdSyljXtE9T7336Vy8Ett/gFTyoVv3TnSSf5lcraNYHqZz+Dj33Mz4rfsmXtWd3LE/KOPx7e9Ca/OImISISabVkrrOWgZmdnWVxcZHx8nOn91ybvJPPzcOWV8L3v+db90BCMj/tJbdWqn5Feq/nW94UXwvOepz2sRaQjqBtcDstyUE9MTDC1zk5bkZuchNe8Bl76UrjzTr9M6333+db++LjfQvSEE3yrP8xb3EREQqKwlgPk83mWlpa6I6hXmpjwk9rOPDPqSkREWkphLfvI5XIUCgUmJyeZbHbSloiIhEphLQA458jn8xQKBaamppiYmIi6JBERaVBYC845crkcxWJRQS0i0oEU1n1uZVBPT08zPj4edUkiIrIfhXUfc86RzWYplUrMzMwwdigrgYmISOi0kUefUlCLiHQPtaz7kHOOIAgol8skEglGteSmiEhHU1j3mXq9TjabVVCLiHQRhXUfqdfrBEFAtVolmUwyEsZuWCIi0nIas+4TK4M6kUgoqEVEuoha1n2gXq+TyWSo1Wokk0mGh4ejLklERDZAYd3jarUaQRAoqEVEupjCuoetDOpUKsXQ0FDUJYmIyCFQWPeoWq1GJpPBOaegFhHpcgrrHlStVgmCYG9QD2oPZxGRrqbZ4D1GQS0i0nvUsu4h1WqVTCYDQDqdJh7XX6+ISC/QT/MeUalUCIIAMyOVSimoRUR6iH6id4sggB//GH79azCDE06Apz0NEonogrpahTvvhLvvhkoFjjwSnvpUmJpqz/VFRPqEOeeirmGvHTt2uJ07d0ZdRmepVOALX4Dvftd/PT4OzsHiIphRPu88gmc/m4HBQdLpNLFYrD113XILfOpTMDcH8TgMDEC57H897zx4+cv98yIiclBmdpNzbsd6x+mnaSer1+HTn4brroPjjoOVQZxOUy6XCa64glgmQ+rNb25fUN90E3zwg7BpE2zduu9r1Sp84xuQz8Mb3rBvzSIickhCmw1uZiNm9hMzu9XM7jCzd4Z1rZ51553wgx/4QNwv9ErVKkGpRGzrVlI/+Qmxe+9tT02FAnz847B5M0xMHPh6PA7btsH118Ptt7enJhGRHhfmrVsl4HnOuScDpwHnm9nTQ7xe77n6ah+IA/v+NZWqVbKFArGBAdITE8TGxh7rJg/bzTdDsQhjYwc/xgymp30LOyrOQa0W3fVFRFootLB23kLjy8HGo3MGyDtdtQq33Qbp9D5PFxtBHR8YID02xoAZHHEE3HijD6iw7dy5eot6f8mkn3hWKIRf02o+8xnfDf+jH0VzfRGRFgp1URQzi5nZLcCjwNXOuRtWOeaNZrbTzHbu2bMnzHK6S6XiW6grWtXFapVcI6hTy0EN/phqtT1hXSw2N3HMzD/K5fBrWs0NN8DSkp8IJyLS5UINa+dczTl3GnAMcKaZPWmVYy5zzu1wzu3YtGlTmOV0l+FhGBraG3aFSoVsocBgLLZvUIMP0OnpA7rLQ3HEEc21lisVP86+Vnd5mN7yFrjgAnjZy6K5vohIC7VluVHnXB64Fji/HdfrCQMDcO65sHs3hUqFXLHIUCxGanR036AG2LPH3y7VDs98JpRK6x/36KNw9tkQ1XKnp5wCr32tnwgnItLlwpwNvsnMZhq/HwXOA34R1vV60tlns1StksvlGG4Ete0f1IuLvlv6rLPaU9MJJ8D27fDIIwc/plDwk7ue97z21CQi0uPCbFlvAa41s9uAG/Fj1l8L8Xo9Z3FigvzFFzOczZJcWGCfmK7Xfes1COA//Sc/oasdBgb89aamYNeufVvZ9Trs3u0fb34zHH10e2oSEelxWsGsQy0uLjI7O8vIyAiJTAa78kr4+c8fG5eu1+HUU+ElL/Et3Xabm4NrrvGP5cB2Dk4/HX77t+H449tfk4hIl2l2BTOFdQdaWFhgbm7OB3Ui8VjX9+7dfnwa/FhsJ0zIK5V8XbUaJBIwMxN1RSIiXUPLjXap5aAeHR1lZmZm3zHqzZs7b8LU8LBfClVEREKjsO4g8/PzzM/PMzo6SiKRiLocERHpEArrDjE3N8fCwgJjY2PMqCtZRERWUFh3gOWgHh8fZ3p6OupyRESkwyisIzY7O8vi4qKCWkREDkphHaF8Ps/S0hITExNMTU1FXY6IiHSotiw3KgdSUIuISLPUso5ALpejUCgwOTnJ5ORk1OWIiEiHU1i3kXOOfD5PoVBgamqKiWb2hRYRkb6nsG4T5xy5XI5isaigFhGRDVFYt8HKoJ6enmZ8fDzqkkREpIsorEPmnCObzVIqlZiZmWFsbCzqkkREpMtoNniIFNQiItIKalmHxDlHEASUy2USiQSjo6NRlyQiIl1KYR2Cer1ONptVUIuISEsorFusXq8TBAHVapVkMsnIyEjUJYmISJfTmHULrQzqRCKhoBYRkZZQy7pF6vU6mUyGWq1GMplkeHg46pJERKRHKKxboFarEQSBglpEREKhsD5MK4M6lUoxNDQUdUkiItJjFNaHoVarkclkcM4pqEVEJDQK60NUrVYJgmBvUA8ODkZdkoiI9CjNBj8ECmoREWkntaw3qFqtkslkAEin08Tj+haKiEi4lDQbUKlUCIIAMyOVSimoRUSkLdQN3iQFtYiIREWJ04RyuUwQBAwMDJBOp4nFYlGXJCIifURhvY7loI7FYqRSKQW1iIi0ncJ6DaVSiWw2q6AWEZFIKawPYmVQp9NpBgY0vC8iItFQWK+iWCySy+WIx+OkUikFtYiIREoptB8FtYiIdBq1rFcoFArkcjmGhoZIJpMKahER6QgK64aVQZ1KpTCzqEsSEREBFNYALC0tkc/nGR4eJplMKqhFRKSj9H1YLy4uMjs7q6AWEZGO1ddhvRzUIyMjJBIJBbWIiHSkvg3rhYUF5ubmFNQiItLx+jKs5+fnmZ+fZ3R0lJmZGQW1iIh0tL4L65VBnUgkoi5HRERkXX0V1nNzcywsLDA2NsbMzEzU5YiIiDSlb8J6dnaWxcVFxsfHmZ6ejrocERGRpvXFEl0KahER6WY937LO5/MsLS0xMTHB1NRU1OWIiIhsWE+H9XJQT05OMjk5GXU5IiIih6RnwzqXy1EoFBTUIiLS9XourJ1z5HI5isUiU1NTTExMRF2SiIjIYempsF4Z1NPT04yPj0ddkoiIyGHrmbB2zpHNZimVSgpqERHpKT0R1iuDemZmhrGxsahLEhERaZmuD2vnHEEQUC6XFdQiItKTujqs6/U62WyWcrlMIpFgdHQ06pJERERarmvDul6vEwQBlUqFZDLJyMhI1CWJiIiEoiuXG10O6mq1qqAWEZGe13Ut6/2Denh4OOqSREREQtVVYV2r1QiCgFqtpqAWEZG+0TVhvTKoU6kUQ0NDUZckIiLSFl0R1rVajUwmQ71eV1CLiEjf6fgJZstB7ZxTUIuISF/q6JZ1tVolCIK9QT04OBh1SSIiIm3XsWFdrVbJZDIACmoREelrHRnWlUqFIAgASKfTxOMdWaaIiEhbhDZmbWbHmtm1Znanmd1hZn/UzPuWg9rMFNQiIiKE27KuAv/FOXezmU0CN5nZ1c65Ow/2huVNOcyMVCqloBYRESHElrVz7mHn3M2N388DPweOXus91WpVLWoREZH9tOXWLTPbBpwO3LDOcaTTaWKxWDvKEhER6QqhN1/NbAK4Anibc25uldffCLyx8WUpHo//LOyaekAayERdRBfQ96l5+l41R9+n5ul71ZwTmznInHOhVWBmg8DXgG855/5fE8fvdM7tCK2gHqHvU3P0fWqevlfN0fepefpeNafZ71OYs8EN+CTw82aCWkRERFYX5pj1M4HXAM8zs1sajwtCvJ6IiEhPCm3M2jl3HWAbfNtlYdTSg/R9ao6+T83T96o5+j41T9+r5jT1fQp1zFpEREQOX8fvuiUiItLvIg/rQ12WtB+Z2YiZ/cTMbm18r94ZdU2dzMxiZvZTM/ta1LV0KjO7z8xub8wp2Rl1PZ3MzGbM7Etm9gsz+7mZPSPqmjqNmZ24Yo7SLWY2Z2Zvi7quTmRmlzZ+jv/MzD5vZiNrHh91N7iZbQG2rFyWFHjJWsuS9qvGDPtx59xC47a464A/cs79OOLSOpKZ/TGwA5hyzr0o6no6kZndB+xwzul+2HWY2eXAD5xznzCzIWDMOZePuKyOZWYx4CHgac65XVHX00nM7Gj8z+8nOucKZvZF4Crn3KcP9p7IW9aHsixpv3LeQuPLwcZDkw5WYWbHAL8NfCLqWqT7mdk0cDb+dlScc2UF9brOBX6loD6oODBqZnFgDPjNWgdHHtYrNbssaT9rdO3eAjwKXO2c0/dqdR8A/itQj7iOTueAb5vZTY3VBGV124E9wKcaQyufMLPxqIvqcBcDn4+6iE7knHsIeB9wP/AwMOuc+/Za7+mYsF5vWVLxnHM159xpwDHAmWb2pIhL6jhm9iLgUefcTVHX0gWe5Zx7CvBC4C1mdnbUBXWoOPAU4KPOudOBReDPoy2pczWGCS4C/iXqWjqRmSWAF+M/BB4FjJvZq9d6T0eEdWP89Qrgc865L0ddTzdodMFdC5wfcSmd6JnARY3x2H/GL8zzj9GW1Jkan/Bxzj0KfAU4M9qKOtaDwIMrerK+hA9vWd0LgZudc7ujLqRDPR+41zm3xzlXAb4MnLXWGyIPay1L2jwz22RmM43fjwLnAb+ItKgO5Jz7b865Y5xz2/Bdcd91zq35qbUfmdl4Y1InjS7d3wK0kc4qnHOPAA+Y2fKmC+cCmgR7cJegLvC13A883czGGhl4Ln6+1kF1wqbRy8uS3t4YiwX4C+fcVdGV1LG2AJc3ZlkOAF90zum2JDlUm4Gv+J8VxIF/cs59M9qSOtpbgc81unh/Dbwu4no6UuOD33nAH0RdS6dyzt1gZl8CbgaqwE9ZZyWzyG/dEhERkbVF3g0uIiIia1NYi4iIdDiFtYiISIdTWIuIiHQ4hbWIiEiHU1iLhMzM3MpFWcwsbmZ7NrobWGOHrPShHLNid63bzOzbZnbkRq6937neYWZ/0vj9u8zs+Wsce5qZXbDi64vMTCt/iWyQwlokfIvAkxoL2YC/B/WhCOo4xzl3KrAT+IuVL5i34Z8Hzrn/6Zy7Zo1DTgP2hrVz7l+dc+/d6HVE+p3CWqQ9rsLvAgb7re5kZkkzu7LR6v2xmZ3aeD7VaAXfYWafAGzFe17d2Nv8FjP7WGOhnGZ9HzjBzLaZ2V1m9hn8ymXHmtmfmtmNjVr27pduZv/dzO42s+uAE1c8/2kze3nj92eY2fXm91v/SWOnqncBr2zU+Uoz+49m9uHG8dvM7LuNa33HzI5bcc4PNs716+Xzi/QzhbVIe/wzcHFjg/lT2XdnuXcCP220ev8C+Ezj+bcD1znnTsGv270cZicDrwSe2djUpQa8agO1vAi4vfH7xwN/17jGiY2vz8S3iJ9qZmeb2VPxy7aehm8ln7H/CRuren0Bv7/6k/FrHy8C/xP4gnPuNOfcF/Z724eAyxt/7s8BH1zx2hbgWY1a1RKXvtcJy42K9Dzn3G2NLWAvwbeyV3oW8O8bx3230aKewu+f/LLG8183s1zj+HOBpwI3NpYKHcVvmbqea82sBtwG/CUwA+xyzv248fpvNR4/bXw9gQ/vSeArzrklADP711XOfSLwsHPuxka9c41j16rnGct/PuCzwN+seO1K51wduNPMNjfxZxPpaQprkfb5V/wets8FUodxHsO3SP/bBt93jnMus/ckflOYxf3O+9fOuY/tczGztx1inYejtLKECK4v0lHUDS7SPv8AvNM5d/t+z/+ARje2mT0XyDRapt8Hfrfx/AuBROP47wAvN7MjGq8lzWxrC+r7FvD6xt7ymNnRjWt8H3iJmY02dum6cJX33gVsMbMzGu+dNLM4MI9vma/menz3Ovg//w9a8GcQ6UlqWYu0iXPuQfYdl132DuAfzOw2YAn4D43n3wl83szuwAfb/Y3z3Glmfwl8uzGDuwK8Bdh1mPV9uzEe/qNG9/UC8Grn3M1m9gXgVnx3+42rvLdsZq8EPtSY9V7Aj1tfC/x5Y0e9v97vbW8FPmVmfwrsQbtYiRyUdt0SERHpcOoGFxER6XAKaxERkQ6nsBYREelwCmsREZEOp7AWERHpcAprERGRDqewFhER6XAKaxERkQ73/wFGLHhFl6Z/MgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xvals = []\n",
    "yvals = []\n",
    "cvals = []\n",
    "cvals_old = []\n",
    "for x in np.arange(3, 8, 0.5):\n",
    "    for y in np.arange(3, 8, 0.5):\n",
    "        xvals.append(x)\n",
    "        yvals.append(y)\n",
    "        \n",
    "        cvals.append(sum((y_pred == x) & (y_test_old == y)))\n",
    "        cvals_old.append(sum((y_test_old == x) & (y_test == y)))\n",
    "xvals = np.array(xvals)\n",
    "yvals = np.array(yvals)\n",
    "cvals = np.array(cvals) / 2\n",
    "cvals_old = np.array(cvals_old) / 2\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlim(2, 8)\n",
    "plt.ylim(2, 8)\n",
    "plt.scatter(xvals, yvals, c='red', alpha=0.5, s=cvals)\n",
    "plt.xlabel('Model Prediction')\n",
    "plt.ylabel('Age-based formula')\n",
    "#lgnd = plt.legend()\n",
    "#lgnd.legendHandles[0]._sizes = [30]\n",
    "plt.plot([2,8], [2,8], 'k-', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb799e3d-1f4f-4608-bd1d-e24652454ccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T15:28:28.919750Z",
     "iopub.status.busy": "2023-02-17T15:28:28.919159Z",
     "iopub.status.idle": "2023-02-17T15:28:29.562161Z",
     "shell.execute_reply": "2023-02-17T15:28:29.561641Z",
     "shell.execute_reply.started": "2023-02-17T15:28:28.919694Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'age-based formula')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAcTElEQVR4nO3de5QedZ3n8fdnQiQRkYhEhQRtQDa6ICTQsiAclstwEVhA0AO73sBLhHVGHBU3zOwq4CA4zBEvnIGJsCMoDmAGMFwE4wDiuojb4RYuwQsGocGT5hIQDZfEz/5R1dh58nR3NXY9T6fr8zrnOXnqV9VVnxR0fVNVv/qVbBMREc31F90OEBER3ZVCEBHRcCkEERENl0IQEdFwKQQREQ23UbcDjNUWW2zhnp6ebseIiNigLF269HHbM9vN2+AKQU9PD319fd2OERGxQZH00HDzcmkoIqLhUggiIhouhSAiouFSCCIiGi6FICKi4WrtNSRpBnABsCNg4EO2bx0yX8BXgUOAPwDH2b69zkwR0X1v+bvreG7tnwa8nDZFLD/jkC4mKvQsuHa9thVnHdqFJOuqO1fdZwRfBa63/RZgZ+D+lvnvBLYvP/OB82rOExFd1loEAJ5ba97yd9d1KVGh3cF2pPZO6USu2gqBpM2AvYELAWy/YHtVy2JHABe78FNghqQt68oUEd3XWgRGa4/61XlGsA0wAPyLpDskXSBpk5ZlZgEPD5l+pGxbh6T5kvok9Q0MDNSXOCKigeosBBsBuwDn2Z4H/B5Y8HJWZHuh7V7bvTNntn1COiIiXqY6C8EjwCO2byunF1EUhqH6ga2HTM8u2yJikpo2RWNqj/rVVghs/xZ4WNKcsml/4L6WxRYDH1Bhd+Bp24/VlSkium/5GYesd9CfCL2GhuuF0+1eQ53IpTrfWSxpLkX30VcADwLHA8cA2D6/7D56LnAwRffR422POKJcb2+vM+hcRMTYSFpqu7fdvFqfI7B9J9C64fOHzDfw8TozRETEyPJkcUREw6UQREQ0XApBRETDpRBERDRcCkFERMOlEERENFwKQUREw6UQREQ0XApBRETDpRBERDRcCkFERMOlEERENFwKQUREw6UQREQ0XApBRETDpRBERDRcCkFERMOlEERENFwKQUREw6UQREQ0XApBRETDpRBERDRcCkFERMOlEERENNxGda5c0grgd8BaYI3t3pb5+wDfA35dNl1h+/Q6M0U0yQFfvplfrPz9S9Pbv24Tlnxqn+4FKvUsuHa9thVnHdqFJOuaqLnq1okzgn1tz20tAkP8uJw/N0UgYvy0FgGAX6z8PQd8+ebuBCq1O9iO1N4pEzVXJ+TSUMQk1VoERmuP5qq7EBj4gaSlkuYPs8weku6S9H1JO7RbQNJ8SX2S+gYGBupLGxHRQLXeIwD2st0v6XXAEknLbd8yZP7twJtsPyvpEOAqYPvWldheCCwE6O3tdc2ZIyIapdYzAtv95Z8rgSuB3VrmP2P72fL7dcBUSVvUmSmiKbZ/3SZjao/mqq0QSNpE0qaD34EDgXtalnmDJJXfdyvzPFFXpogmWfKpfdY76E+EXkPD9cLpdu+ciZqrE2TXc6VF0rYUZwFQXIL6ju0zJJ0AYPt8SX8FnAisAVYDn7L9f0dab29vr/v6+mrJHBExWUlaOlzvzdruEdh+ENi5Tfv5Q76fC5xbV4aIiBhduo9GRDRcCkFERMOlEERENFwKQUREw6UQREQ0XApBRETDpRBERDRcCkFERMOlEERENFwKQUREw6UQREQ0XApBRETDpRBERDRcCkFERMOlEERENFyl9xFIeg3Fu4SnDba1vHs4IiI2UKMWAkkfAU4CZgN3ArsDtwL71ZosIiI6osqloZOAtwMP2d4XmAesqjNURER0TpVC8Jzt5wAkbWx7OTCn3lgREdEpVe4RPCJpBnAVsETSU8BDdYaKiIjOGbUQ2H5X+fVUSTcBmwHX15oqIiI6ZthCIGnzNs3Lyj9fBTxZS6KIiOiokc4IlgIG1GaegW1rSRQRER01bCGwvU0ng0RERHdUeY5g73btVR4ok7QC+B2wFlhju7dlvoCvAocAfwCOs3376LEjJpaeBdeu17birEO7kGRdyTU2EzVX3ap0Hz15yOd/AVcDp45hG/vanttaBErvpHhieXtgPnDeGNYbMSG0O3iM1N4pyTU2EzVXJ1TpNfRfhk5L2hr4yjht/wjgYtsGfipphqQtbT82TuuPiIhRvJxB5x4B3lpxWQM/kLRU0vw282cBD7ese1brQpLmS+qT1DcwMDDmwBERMbwq9wi+TnFAh6JwzAWqXsffy3a/pNdRPIy2/OUMVmd7IbAQoLe316MsHhERY1DlyeK+Id/XAP9q+ydVVm67v/xzpaQrgd2AoYWgH9h6yPTssi0iIjpk1EtDti8a8rmkahGQtImkTQe/AwcC97Qsthj4gAq7A0/n/kBsaIbrVdLt3ibJNTYTNVcnqLhPO8IC0mHAF4A3UZxBCLDtV4/yc9sCV5aTGwHfsX2GpBMoVnB+2X30XOBgiu6jx9vua7vCUm9vr/v6RlwkIiJaSFo6TO/NSpeGvgIcBSzzaFVjCNsPAju3aT9/yHcDH6+6zoiIGH9Veg09DNwzliIQEREbjipnBJ8FrpP0I+D5wUbbX64tVUREdEyVQnAG8CzF+4pfUW+ciIjotCqFYCvbO9aeJCIiuqLKPYLrJB1Ye5KIiOiKKoXgROB6SaslPSPpd5KeqTtYRER0xoiXhiT9BXBw1YfIIiJiwzPiGYHtP1I88BUREZNUlUtD/y7p6PIp4IiImGSqFIKPAd8FXsg9goiIyafKi2k27USQiIjojirPESDpcGDw3cU3276mvkgREdFJo14aknQWcBJwX/k5SdKZdQeLiIjOqHJGcAgwt+xBhKSLgDuAU+oMFhERnVH1ncUzhnzfrIYcERHRJVXOCM4E7pB0E8VLafYGFtSaKiIiOmbYQiBpz/KJ4iuAm4G3l7P+h+3fdiBbRER0wEhnBF8DdgVutb0LxfuFIyJikhmpELwoaSEwW9LXWmfa/kR9sSIiolNGKgSHAX8JHAQs7UyciIjotGELge3HgUsl3W/7rg5mioiIDhq1+2iKQETE5Fb1OYKIiJikUggiIhpupOcIPjXSD9r+cpUNSJoC9AH9tg9rmXcccDbQXzada/uCKuuNZrrqjn7OvuEBHl21mq1mTOfkg+Zw5LxZ3Y5Fz4Jr12tbcdahXUiyruSKKkY6I9i0/PRSvLd4Vvk5AdhlDNs4Cbh/hPmX2Z5bflIEYlhX3dHPKVcso3/Vagz0r1rNKVcs46o7+kf92Tq1O6iN1N4pyRVVDVsIbJ9m+zRgNrCL7U/b/jTFQ2ZvrLJySbOBQ4Ec4OPPdvYND7D6xbXrtK1+cS1n3/BAlxJFTA5V7hG8HnhhyPQLZVsVXwE+C/xxhGWOlnS3pEWStm63gKT5kvok9Q0MDFTcdEw2j65aPab2iKimSiG4GPiZpFMlnQrcBlw02g9JOgxYaXukh9GuBnps7wQsGW69thfa7rXdO3PmzAqRYzLaasb0MbVHRDVVniM4AzgeeKr8HG/7ixXWvSdwuKQVwKXAfpK+3bLuJ2w/X05eQHHZKaKtkw+aw/SpU9Zpmz51CicfNKdLiSImh6rdR18JPGP7q8AjkrYZ7Qdsn2J7tu0e4FjgRtvvG7qMpC2HTB7OyDeVo+GOnDeLM496G7NmTEfArBnTOfOot3W919BwvV263QsmuaIq2R55AenzFD2H5tj+D5K2Ar5re8/KG5H2AT5j+zBJpwN9theXr7w8HFgDPAmcaHv5SOvq7e11X19f1U1HRAQgaant3rbzKhSCO4F5wO2255Vtd5fX9TsuhSAiYuxGKgRVLg294KJauFzZJuMZLiIiuqtKIbhc0j8DMyR9FPgh8I16Y0VERKeM+s5i2/8o6QDgGWAO8DnbS2pPFhERHTFqISgvBd1oe4mkOcAcSVNtv1h/vIiIqFuVS0O3ABtLmgVcD7wf+GadoSIionOqFALZ/gNwFHCe7fcAO9QbKyIiOqVSIZC0B/BeYHB4wCkjLB8RERuQKoXgJOAU4Erb90raFrip3lgREdEpVXoN3UJxn2Bw+kHgE3WGioiIzqnSa2gmxVDSOwDTBttt71djroiI6JAql4YuAZYD2wCnASuA/1djpoiI6KAqheC1ti8EXrT9I9sfAnI2EBExSYx6aQgYfHDsMUmHAo8Cm9cXKSIiOqlKIfh7SZsBnwa+Drwa+JtaU0VERMdU6TV0Tfn1aWDfeuNERESnjXqPQNK2kq6W9LiklZK+Vz5LEBERk0CVm8XfAS4H3gBsBXwX+Nc6Q0VEROdUKQSvtP0t22vKz7cZ8jxBRERs2Ia9RyBpsGfQ9yUtAC6leEvZMcB1HcgWEREdMNLN4qUUB36V0x8bMs8U4w9FRMQGbthCYHubTgaJiIjuqHKP4CWSFtYVJCIiumNMhQDorSVFRER0zVgLwcpaUkRERNdUGWICAEmvtH3wWDcgaQrQB/TbPqxl3sbAxcCuwBPAMbZXjHUbMf6uuqOfs294gEdXrWarGdM5+aA5HDlvVrdj0bPg2vXaVpx1aBeSrGui5oqoosqTxe+QdB/FUNRI2lnSP41hGycB9w8z78PAU7bfDJwDfGkM642aXHVHP6dcsYz+Vasx0L9qNadcsYyr7ujvaq52B9uR2jtlouaKqKrKpaFzgIMo/sWO7buAvausXNJs4FDggmEWOQK4qPy+CNhfkoZZNjrk7BseYPWLa9dpW/3iWs6+4YEuJYqIOlW6R2D74ZamtW0XXN9XKN5u9sdh5s8CHi63sYZiYLvXti4kab6kPkl9AwMDFTcdL9ejq1aPqT0iNmxVCsHDkt4BWNJUSZ9h+Es9L5F0GLDS9tI/N6TthbZ7bffOnDnzz11djGKrGdPH1B4RG7YqheAE4OMU/3rvB+aW06PZEzhc0gqK4Sn2k/TtlmX6ga0BJG0EbEZ5CSq65+SD5jB96pR12qZPncLJB83pUqKIqNOohcD247bfa/v1tl9n+322Rz1Y2z7F9mzbPcCxwI2239ey2GLgg+X3d5fLeIx/hxhnR86bxZlHvY1ZM6YjYNaM6Zx51Nu63mtouF443e6dM1FzRVSl0Y67kr7WpvlpoM/29yptRNoH+IztwySdXv7sYknTgG8B84AngWNtPzjSunp7e93X11dlsxERUZK01Hbbh4KrPEcwDXgLxXsIAI4Gfg3sLGlf258cbQW2bwZuLr9/bkj7c8B7KmSIiIiaVCkEOwF72l4LIOk84MfAXsCyGrNFREQHVLlZ/BrgVUOmNwE2LwvD87WkioiIjqlyRvAPwJ2SbqZ4N8HewBclbQL8sMZsERHRAaMWAtsXSroO2K1s+lvbj5bfT64tWUREdETV0UefAx4DngLeLKnSEBMRETHxjXpGIOkjFAPHzQbuBHYHbgX2qzVZRER0RJUzgpOAtwMP2d6Xos//qjpDRURE51QpBM+V/f2RtLHt5UDGGoiImCSq9Bp6RNIM4CpgiaSngIfqDBUREZ1TpdfQu8qvp0q6iWJguOtrTRURER1T+VWVALZ/VFeQiIjojrG+vD4iIiaZFIKIiIZLIYiIaLgUgoiIhkshiIhouBSCiIiGSyGIiGi4FIKIiIZLIYiIaLgUgoiIhkshiIhouBSCiIiGSyGIiGi4MY0+OhaSpgG3ABuX21lk+/MtyxwHnA30l03n2r5gvLO89xu38pNfPfnS9J7bbc4lH91jvDczZj0Lrl2vbcVZh3Yhyfomarbkihh/dZ4RPA/sZ3tnYC5wsKTd2yx3me255af2IgDwk189yXu/cet4b2pM2h04RmrvpImaLbki6lHbGYFtA8+Wk1PLj+va3nBai8Bo7RERTVPrPQJJUyTdCawElti+rc1iR0u6W9IiSVsPs575kvok9Q0MDNQZOSKicWotBLbX2p4LzAZ2k7RjyyJXAz22dwKWABcNs56Ftntt986cObPOyBERjdORXkO2VwE3AQe3tD9h+/ly8gJg1/He9p7bbT6m9oiIpqmtEEiaKWlG+X06cACwvGWZLYdMHg7cP945LvnoHusd9CdCr6HhepRMhJ4mEzVbckXUQ8U93RpWLO1EcalnCkXBudz26ZJOB/psL5Z0JkUBWAM8CZxoe/mwKwV6e3vd19dXS+aIiMlK0lLbvW3n1VUI6pJCEBExdiMVgjxZHBHRcCkEERENl0IQEdFwKQQREQ2XQhAR0XApBBERDZdCEBHRcCkEERENl0IQEdFwKQQREQ2XQhAR0XApBBERDZdCEBHRcCkEERENl0IQEdFwKQQREQ2XQhAR0XApBBERDZdCEBHRcCkEERENl0IQEdFwKQQREQ2XQhAR0XApBBERDbdRXSuWNA24Bdi43M4i259vWWZj4GJgV+AJ4BjbK+rKNNH0LLh2vbYVZx3ahSTrm6jZdvr89Tzz/NqXpl+98RTuPu3gLiaK2PDVeUbwPLCf7Z2BucDBknZvWebDwFO23wycA3ypxjwTSrsD7UjtnTRRs7UWAYBnnl/LTp+/vkuJIiaH2gqBC8+Wk1PLj1sWOwK4qPy+CNhfkurKFBu21iIwWntEVFPrPQJJUyTdCawElti+rWWRWcDDALbXAE8Dr22znvmS+iT1DQwM1Bk5IqJxai0EttfangvMBnaTtOPLXM9C2722e2fOnDmuGSMimq4jvYZsrwJuAlrv6vUDWwNI2gjYjOKmccR6Xr3xlDG1R0Q1tRUCSTMlzSi/TwcOAJa3LLYY+GD5/d3AjbZb7yNMSsP1wJkIPXMmara7Tzt4vYN+eg1F/PlU13FX0k4UN4KnUBScy22fLul0oM/24rKL6beAecCTwLG2Hxxpvb29ve7r66slc0TEZCVpqe3edvNqe47A9t0UB/jW9s8N+f4c8J66MkRExOjyZHFERMOlEERENFwKQUREw6UQREQ0XG29huoiaQB46GX++BbA4+MYZ7xM1FwwcbMl19gk19hMxlxvst32idwNrhD8OST1Ddd9qpsmai6YuNmSa2ySa2yaliuXhiIiGi6FICKi4ZpWCBZ2O8AwJmoumLjZkmtskmtsGpWrUfcIIiJifU07I4iIiBYpBBERDTfpCoGkrSXdJOk+SfdKOqnNMvtIelrSneXnc+3WNc65pkn6maS7ylyntVlmY0mXSfqlpNsk9UyQXMdJGhiyvz5Sd64h254i6Q5J17SZ1/H9VTFXN/fXCknLyu2uN0yvCl8r99ndknaZILk6/jtZbneGpEWSlku6X9IeLfO7tb9GyzWu+6u20Ue7aA3wadu3S9oUWCppie37Wpb7se3DOpjreWA/289Kmgr8H0nft/3TIct8GHjK9pslHQt8CThmAuQCuMz2X9WcpZ2TgPuBV7eZ1439VSUXdG9/Aexre7iHjt4JbF9+/hNwXvlnt3NB538nAb4KXG/73ZJeAbyyZX639tdouWAc99ekOyOw/Zjt28vvv6P4ZZ3V3VTgwrPl5NTy03qn/giKdzgALAL2l6QJkKsrJM0GDgUuGGaRju+virkmsiOAi8v/7j8FZkjastuhukHSZsDewIUAtl8o36Y4VMf3V8Vc42rSFYKhyksF84Db2szeo7wc8n1JO3QozxRJdwIrgSW2W3PNAh4GsL0GeBp47QTIBXB0eWq8SNLWdWcqfQX4LPDHYeZ3ZX9VyAXd2V9QFPEfSFoqaX6b+S/ts9IjdOYfSqPlgs7/Tm4DDAD/Ul7mu0DSJi3LdGN/VckF47i/Jm0hkPQq4N+AT9p+pmX27RTjbuwMfB24qhOZbK+1PReYDewmacdObHc0FXJdDfTY3glYwp/+FV4bSYcBK20vrXtbY1ExV8f31xB72d6F4pLGxyXt3cFtj2S0XN34ndwI2AU4z/Y84PfAgg5sdzRVco3r/pqUhaC81v1vwCW2r2idb/uZwcshtq8DpkraolP5ytO8m4DWl+32A1sDSNoI2Ax4otu5bD9h+/ly8gJg1w7E2RM4XNIK4FJgP0nfblmmG/tr1Fxd2l+D2+4v/1wJXAns1rLIS/usNLts62quLv1OPgI8MuQMeBHFAXiobuyvUXON9/6adIWgvEZ8IXC/7S8Ps8wbBq8lS9qNYj/UegCRNFPSjPL7dOAAYHnLYouBD5bf3w3c6Jqf+KuSq+Wa6OEU911qZfsU27Nt9wDHUuyL97Us1vH9VSVXN/ZXud1Nyg4SlJcSDgTuaVlsMfCBsjfM7sDTth/rdq5u/E7a/i3wsKQ5ZdP+QGunko7vryq5xnt/TcZeQ3sC7weWlde9Af4WeCOA7fMpDhonSloDrAaOrfsAAmwJXCRpCsV/tMttXyPpdKDP9mKKAvYtSb8EnqQ40NStSq5PSDqcokfWk8BxHcjV1gTYX1VydWt/vR64sjw+bAR8x/b1kk6Al/7fvw44BPgl8Afg+AmSqxu/kwB/DVxS9sx5EDh+AuyvKrnGdX9liImIiIabdJeGIiJibFIIIiIaLoUgIqLhUggiIhouhSAiouFSCGKDoWJk2V9L2rycfk053VNOby/pGkm/KocyuGnwCVatOyLoveXQD68s550qqb+cd5+k/9qhv8/NknrL79cNPs8xzLJHSvqPQ6ZPl/SXHYgZDZBCEBsM2w9TjP54Vtl0FrDQ9gpJ04Bry+ntbO9K0Rd72yGruMz2XNs7AC+w7kil55TDbBwB/HP5dPqYlU84j5ntQ0YZWOxI4KVCYPtztn/4crYV0SqFIDY05wC7S/oksBfwj2X7e4Fbywe6ALB9j+1vtq6gPFhvAjzVOs/2LygeHHpNm5/7pqTzJfVJ+nk57tDg2cZiSTcC/14+Sfu/Vbzn4Q5JR5TLTZd0qYrx5a8Epg9Z94rBIQIkfUDFgHV3SfqWpHdQPKF8dnnWsl2Z5d3l8vuX21lWbnfjIes8TdLt5by3jHFfR0NMxieLYxKz/aKkk4HrgQNtv1jO2oFiIK6RHCNpL4qnqX9OMTjcOlS8eOQX5Zg47fRQjJOzHXCTpDeX7bsAO9l+UtIXKYae+FB5uednkn4IfAz4g+23StqpXV4Vo0j+T+Adth+XtHm5zsXANbYXlcsNLj8N+Cawv+2fS7oYOJFihFSAx23vIum/A58BOvaSnNhw5IwgNkTvBB4Dhh29VdKVku6RNHTQwcvKyz9vAJYBJw+Z9zeS7qUYsvyMEbZ9ue0/lmcODwKD/8peYvvJ8vuBwIJyiJObgWkUQ5zsDXwbwPbdwN1t1r8f8F2XL3AZss7hzAF+bfvn5fRF5XYGDf79l1IUsYj1pBDEBkXSXIqB8XanOHgPDvB2L0NGaLT9LooxfjZvXUc5JsvVrHvAPKe8d3A0cGH5L+12WsdkGZz+/dCYwNHl/Yi5tt9ouyMDz7UxOArqWnIFIIaRQhAbDBXXQ86jeMfEb4Cz+dM9gu8Ae5aDvQ1q93q/QXsBv2ptLO8x9PGnUU1bvUfSX0jajuJG9ANtlrkB+OsyL5Lmle23AP+tbNsR2KnNz95YbuO15XKDhex3wKZtln8A6Blyier9wI+GyR7RVgpBbEg+CvzG9pJy+p+At0r6z7ZXA4cBJ0h6UNKtFNfa/37Izx9T3my9m+LNdV8YZjunA5+S1O734zfAz4DvAyfYfq7NMl+geOXn3eXlpsHtnAe8StL95TbWe7mN7XspLk39SNJdwOBQ6pcCJ5c3hbcbsvxzFCNiflfSMoq3pp0/zN8roq2MPhpRkaRvMuSGbcRkkTOCiIiGyxlBRETD5YwgIqLhUggiIhouhSAiouFSCCIiGi6FICKi4f4/cKSbwjrPvF4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_pred, y_test_old)\n",
    "plt.xlabel('XGBR prediction')\n",
    "plt.ylabel('age-based formula')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f82a0-5ccf-441b-a47c-dd416d0554e9",
   "metadata": {},
   "source": [
    "# XGBr vs RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f34b76-6d4c-4290-b860-62a573174486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgbr model\n",
    "xgbr = xgb.XGBRegressor()\n",
    "xgbr.load_model('result/size/acc1-0.601_acc3-0.966_XGBR_10fold/model.model')\n",
    "y_xgbr = xgbr.predict(x_test)\n",
    "\n",
    "# RF model\n",
    "rfr = pickle.load(open(f'result/size/acc1-0.595_acc3-0.963_RF_10fold/gridSearch','rb'))\n",
    "y_rbr = rbr.predict(x_test_imputed).flatten()\n",
    "#y_rbr = np.round(y_rbr * 2) / 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437fc599-ddc7-4145-bae1-b0d7507974c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e42f2d45-188b-42a4-9758-3726f1222d7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:17:27.547259Z",
     "iopub.status.busy": "2023-02-19T01:17:27.546809Z",
     "iopub.status.idle": "2023-02-19T01:17:27.564173Z",
     "shell.execute_reply": "2023-02-19T01:17:27.563596Z",
     "shell.execute_reply.started": "2023-02-19T01:17:27.547210Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (27234, 5), x_test: (6808, 5)\n"
     ]
    }
   ],
   "source": [
    "dat = np.load(f'dataset/ETT_size.npz')\n",
    "x, y = dat['x'], dat['y']\n",
    "y_old  = dat['y_old']\n",
    "\n",
    "# training set의 뒤쪽 20%를 test set 으로 사용\n",
    "nsamp = len(y)\n",
    "ntest = int(nsamp * 0.2)\n",
    "ntrain = nsamp - ntest\n",
    "x_test = x[-ntest:, :]\n",
    "y_test = y[-ntest:]\n",
    "y_test_old = y_old[-ntest:]\n",
    "x_train = x[:ntrain, :]\n",
    "y_train = y[:ntrain]\n",
    "\n",
    "print(f'x_train: {(x_train).shape}, x_test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b76ab3-0236-4a58-86b4-26ddbf2458cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:20:17.943137Z",
     "iopub.status.busy": "2023-02-19T01:20:17.942559Z",
     "iopub.status.idle": "2023-02-19T01:20:17.951612Z",
     "shell.execute_reply": "2023-02-19T01:20:17.950574Z",
     "shell.execute_reply.started": "2023-02-19T01:20:17.943081Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024234768814993243"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0: age, 1: sex, 2: weight, 3: height, 4: cuffed\n",
    "np.mean(np.isnan(x[:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e8d2f6-5f6a-4676-a0d7-bc1b5e35f25d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T01:19:28.932986Z",
     "iopub.status.busy": "2023-02-19T01:19:28.932498Z",
     "iopub.status.idle": "2023-02-19T01:19:28.951030Z",
     "shell.execute_reply": "2023-02-19T01:19:28.950171Z",
     "shell.execute_reply.started": "2023-02-19T01:19:28.932933Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34042 entries, 0 to 34041\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       34042 non-null  float64\n",
      " 1   1       34042 non-null  float64\n",
      " 2   2       33603 non-null  float64\n",
      " 3   3       33217 non-null  float64\n",
      " 4   4       34042 non-null  float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(x).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452b3cb-be99-41ed-84cb-74e43acbd5c5",
   "metadata": {},
   "source": [
    "# 모델의 AGE 별 over/under estimate 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf8998-5045-4ca3-b8e7-1548bbfb95e3",
   "metadata": {},
   "source": [
    "## Age 별 사용된 tube size (age-size plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732516e5-eb74-4a0e-bdbf-0be0481c32f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T00:39:12.273601Z",
     "iopub.status.busy": "2023-02-20T00:39:12.273076Z",
     "iopub.status.idle": "2023-02-20T00:39:12.280663Z",
     "shell.execute_reply": "2023-02-20T00:39:12.279478Z",
     "shell.execute_reply.started": "2023-02-20T00:39:12.273547Z"
    },
    "tags": []
   },
   "source": [
    "### Age - size plot (uncuffed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "905d4792-da05-4184-a634-e2d7c23e3ba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T01:02:27.571751Z",
     "iopub.status.busy": "2023-02-20T01:02:27.571169Z",
     "iopub.status.idle": "2023-02-20T01:02:27.812691Z",
     "shell.execute_reply": "2023-02-20T01:02:27.812199Z",
     "shell.execute_reply.started": "2023-02-20T01:02:27.571695Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Size of uncuffed ETT used')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.xticks(np.arange(0,10))\n",
    "plt.xlim(-1, 10)\n",
    "plt.ylim(2, 8)\n",
    "plt.scatter(x_test[:,0][~cuff_mask], y_test[~cuff_mask])\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Size of uncuffed ETT used')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381a1f9a-c467-4fd6-b70c-0445b5f98cce",
   "metadata": {},
   "source": [
    "### Uncuffed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "40426e6c-f032-41e6-bcfb-33a494a4f78d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T00:40:04.405763Z",
     "iopub.status.busy": "2023-02-20T00:40:04.405156Z",
     "iopub.status.idle": "2023-02-20T00:40:05.740526Z",
     "shell.execute_reply": "2023-02-20T00:40:05.739913Z",
     "shell.execute_reply.started": "2023-02-20T00:40:04.405706Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f39037dc340>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cole's formula 값\n",
    "coles = [3.5, 4] + [math.floor((x / 4 + 4) * 2) / 2 if x >= 2 else (3.5 if x < 1 else 4) for x in range(2,10)]\n",
    "coles = np.array(coles)\n",
    "\n",
    "xvals = []\n",
    "yvals = []\n",
    "cvals = []\n",
    "cvals_old = []\n",
    "x_age = np.array([math.floor(x) for x in x_test[:,0]])\n",
    "cuff_mask = (x_test[:,4]==1)\n",
    "\n",
    "for x in np.arange(0, 10, 1):\n",
    "    for y in np.arange(3, 8, 0.5):\n",
    "        xvals.append(x)\n",
    "        yvals.append(y)\n",
    "        \n",
    "        cvals.append(sum((x_age[~cuff_mask] == x) & (y_test[~cuff_mask] == y)))\n",
    "        #cvals_old.append(sum((y_test_old == x) & (y_test == y)))\n",
    "xvals = np.array(xvals)\n",
    "yvals = np.array(yvals)\n",
    "cvals = np.array(cvals) / 2\n",
    "#cvals_old = np.array(cvals_old) / 2\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.xticks(np.arange(0,10))\n",
    "plt.xlim(-1, 10)\n",
    "plt.ylim(2, 8)\n",
    "plt.scatter(xvals, yvals, c='red', alpha=0.5, s=cvals)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Size of uncuffed ETT used')\n",
    "#lgnd = plt.legend()\n",
    "#lgnd.legendHandles[0]._sizes = [30]\n",
    "plt.plot(np.arange(0,10), coles, 'k-', alpha=0.1)\n",
    "#plt.plot([2,10], [4.5,6.5], 'k-', alpha=0.1) # 4 + age/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb6686e-22df-47af-bfab-28b788ecc5fe",
   "metadata": {},
   "source": [
    "### Cuffed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0b97b732-e10c-42eb-b28b-a6a20dd57029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T01:06:27.427031Z",
     "iopub.status.busy": "2023-02-20T01:06:27.426440Z",
     "iopub.status.idle": "2023-02-20T01:06:28.048027Z",
     "shell.execute_reply": "2023-02-20T01:06:28.047538Z",
     "shell.execute_reply.started": "2023-02-20T01:06:27.426974Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f390b68b2b0>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cole's formula 값\n",
    "coles = [3.5, 4] + [math.floor((x / 4 + 3.5) * 2) / 2 if x >= 2 else (3.0 if x < 1 else 3.5) for x in range(2,10)]\n",
    "coles = np.array(coles)\n",
    "\n",
    "xvals = []\n",
    "yvals = []\n",
    "cvals = []\n",
    "cvals_old = []\n",
    "x_age = np.array([math.floor(x) for x in x_test[:,0]])\n",
    "\n",
    "cuff_mask = (x_test[:,4]==1)\n",
    "\n",
    "for x in np.arange(0, 10, 1):\n",
    "    for y in np.arange(3, 8, 0.5):\n",
    "        xvals.append(x)\n",
    "        yvals.append(y)\n",
    "        \n",
    "        cvals.append(sum((x_age[cuff_mask] == x) & (y_test[cuff_mask] == y)))\n",
    "        #cvals_old.append(sum((y_test_old == x) & (y_test == y)))\n",
    "xvals = np.array(xvals)\n",
    "yvals = np.array(yvals)\n",
    "cvals = np.array(cvals) / 2\n",
    "#cvals_old = np.array(cvals_old) / 2\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlim(-1, 10)\n",
    "plt.ylim(2, 8)\n",
    "plt.scatter(xvals, yvals, c='red', alpha=0.5, s=cvals)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Size of cuffed ETT used')\n",
    "#lgnd = plt.legend()\n",
    "#lgnd.legendHandles[0]._sizes = [30]\n",
    "#plt.plot([2,10], [4,6], 'k-', alpha=0.1) # 3.5 + age/4\n",
    "plt.plot(np.arange(0,10), coles, 'k-', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97941a57-f370-4568-a3cf-206a332515b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_vals, cnt_vals = [], \n",
    "x_age = np.array([math.floor(x) for x in x_test[:,0]])\n",
    "age_interval = ['0-1mo','1mo-1y'] + [f'{i}-{i+1}y' for i in range(1,10)]\n",
    "\n",
    "for i in range(1,10):\n",
    "    size_vals.append(np.mean(y_test[x_age==i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ede6a-2c01-40c1-8dd7-8ee7323930d0",
   "metadata": {},
   "source": [
    "## Height - size plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dca60c37-537e-46d3-8606-dcfe04af0694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T01:09:00.548239Z",
     "iopub.status.busy": "2023-02-20T01:09:00.547698Z",
     "iopub.status.idle": "2023-02-20T01:09:00.702449Z",
     "shell.execute_reply": "2023-02-20T01:09:00.701658Z",
     "shell.execute_reply.started": "2023-02-20T01:09:00.548184Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imp = IterativeImputer().fit(x_train)\n",
    "x_train_imputed = imp.transform(x_train)\n",
    "x_test_imputed = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e587e275-b6c4-4662-ba85-3ba70091c40c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T01:14:35.695148Z",
     "iopub.status.busy": "2023-02-20T01:14:35.694566Z",
     "iopub.status.idle": "2023-02-20T01:14:35.860642Z",
     "shell.execute_reply": "2023-02-20T01:14:35.860050Z",
     "shell.execute_reply.started": "2023-02-20T01:14:35.695092Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  29.,    2.,  184.,  845., 1217., 1347., 1314., 1139.,  644.,\n",
       "          86.]),\n",
       " array([  1.  ,  16.68,  32.36,  48.04,  63.72,  79.4 ,  95.08, 110.76,\n",
       "        126.44, 142.12, 157.8 ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_height = x_test_imputed[:,3]\n",
    "x_height_ = x_height[x_height<200]\n",
    "plt.hist(x_height_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ebf530cd-c511-4ab9-a4cd-99e3f0e64078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T01:09:01.446542Z",
     "iopub.status.busy": "2023-02-20T01:09:01.446289Z",
     "iopub.status.idle": "2023-02-20T01:09:02.764565Z",
     "shell.execute_reply": "2023-02-20T01:09:02.763945Z",
     "shell.execute_reply.started": "2023-02-20T01:09:01.446510Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3905f97550>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cole's formula 값\n",
    "coles = [3.5, 4] + [math.floor((x / 4 + 4) * 2) / 2 if x >= 2 else (3.5 if x < 1 else 4) for x in range(2,10)]\n",
    "coles = np.array(coles)\n",
    "\n",
    "xvals = []\n",
    "yvals = []\n",
    "cvals = []\n",
    "cvals_old = []\n",
    "x_age = np.array([math.floor(x) for x in x_test[:,0]])\n",
    "cuff_mask = (x_test[:,4]==1)\n",
    "\n",
    "for x in np.arange(0, 160, 10):\n",
    "    for y in np.arange(3, 8, 0.5):\n",
    "        xvals.append(x)\n",
    "        yvals.append(y)\n",
    "        \n",
    "        cvals.append(sum((x_test_imputed[:,3][~cuff_mask] == x) & (y_test[~cuff_mask] == y)))\n",
    "        #cvals_old.append(sum((y_test_old == x) & (y_test == y)))\n",
    "xvals = np.array(xvals)\n",
    "yvals = np.array(yvals)\n",
    "cvals = np.array(cvals) / 2\n",
    "#cvals_old = np.array(cvals_old) / 2\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.xticks(np.arange(0,10))\n",
    "plt.xlim(-1, 10)\n",
    "plt.ylim(2, 8)\n",
    "plt.scatter(xvals, yvals, c='red', alpha=0.5, s=cvals)\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Size of uncuffed ETT used')\n",
    "#lgnd = plt.legend()\n",
    "#lgnd.legendHandles[0]._sizes = [30]\n",
    "plt.plot(np.arange(0,10), coles, 'k-', alpha=0.1)\n",
    "#plt.plot([2,10], [4.5,6.5], 'k-', alpha=0.1) # 4 + age/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886218c-d8e4-48af-8093-f7fe6423d1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "514e0461-e581-45e5-8249-95aac0d7b4b0",
   "metadata": {},
   "source": [
    "## Age-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f677c002-001d-459e-8592-2d2b8ef0cf21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T06:50:54.035447Z",
     "iopub.status.busy": "2023-02-19T06:50:54.034868Z",
     "iopub.status.idle": "2023-02-19T06:50:54.073294Z",
     "shell.execute_reply": "2023-02-19T06:50:54.072133Z",
     "shell.execute_reply.started": "2023-02-19T06:50:54.035390Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>overestimate</th>\n",
       "      <th>underestimate</th>\n",
       "      <th>bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0-1mo</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.11</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1mo-1y</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-2y</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3y</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.53</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-4y</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-5y</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.52</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5-6y</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.88</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6-7y</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.70</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7-8y</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.88</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8-9y</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.63</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9-10y</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.77</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        correct  overestimate  underestimate  bins\n",
       "0-1mo      0.47          0.42           0.11   242\n",
       "1mo-1y     0.35          0.11           0.54  1644\n",
       "1-2y       0.22          0.02           0.76  1032\n",
       "2-3y       0.41          0.06           0.53   654\n",
       "3-4y       0.09          0.02           0.90   571\n",
       "4-5y       0.44          0.04           0.52   557\n",
       "5-6y       0.11          0.01           0.88   569\n",
       "6-7y       0.26          0.04           0.70   481\n",
       "7-8y       0.10          0.02           0.88   372\n",
       "8-9y       0.32          0.05           0.63   348\n",
       "9-10y      0.20          0.03           0.77   338"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 계산\n",
    "x_age = np.array([math.floor(x) for x in x_test[:,0]])\n",
    "xvals = []\n",
    "yvals = []\n",
    "cvals = []\n",
    "p_under, p_over, p_cor = [], [], []\n",
    "po_under, po_over, po_cor = [], [], []\n",
    "age_bins = []\n",
    "age_interval = ['0-1mo','1mo-1y'] + [f'{i}-{i+1}y' for i in range(1,10)]\n",
    "\n",
    "# 신생아\n",
    "ag_mask = (x_test[:,0] < 1/12)\n",
    "p_over.append(np.mean(y_test_old[ag_mask] > y_test[ag_mask]))\n",
    "p_under.append(np.mean(y_test_old[ag_mask] < y_test[ag_mask]))\n",
    "p_cor.append(np.mean(y_test_old[ag_mask] == y_test[ag_mask]))\n",
    "age_bins.append(np.sum(ag_mask))\n",
    "\n",
    "# 영아\n",
    "ag_mask = (1/12 <= x_test[:,0]) & (x_test[:,0] < 1)\n",
    "p_over.append(np.mean(y_test_old[ag_mask] > y_test[ag_mask]))\n",
    "p_under.append(np.mean(y_test_old[ag_mask] < y_test[ag_mask]))\n",
    "p_cor.append(np.mean(y_test_old[ag_mask] == y_test[ag_mask]))\n",
    "age_bins.append(np.sum(ag_mask))\n",
    "\n",
    "# 소아 (1~10살)\n",
    "for x in np.arange(1, 10, 1):\n",
    "    ag_mask = (x_age == x)\n",
    "    p_over.append(np.mean(y_test_old[ag_mask] > y_test[ag_mask]))\n",
    "    p_under.append(np.mean(y_test_old[ag_mask] < y_test[ag_mask]))\n",
    "    p_cor.append(np.mean(y_test_old[ag_mask] == y_test[ag_mask]))\n",
    "    age_bins.append(np.sum(ag_mask))\n",
    "    \n",
    "df_est = pd.DataFrame({'correct':p_cor, 'overestimate':p_over, 'underestimate':p_under, 'bins':age_bins}, index=age_interval)\n",
    "df_est.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235cc7e7-004a-45a2-a7a9-9fc99e2980ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T06:10:37.196761Z",
     "iopub.status.busy": "2023-02-19T06:10:37.196232Z",
     "iopub.status.idle": "2023-02-19T06:10:37.215384Z",
     "shell.execute_reply": "2023-02-19T06:10:37.214067Z",
     "shell.execute_reply.started": "2023-02-19T06:10:37.196705Z"
    },
    "tags": []
   },
   "source": [
    "## XGBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5814859c-2809-4045-acac-2360ec2892d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T06:52:35.009930Z",
     "iopub.status.busy": "2023-02-19T06:52:35.009323Z",
     "iopub.status.idle": "2023-02-19T06:52:35.074284Z",
     "shell.execute_reply": "2023-02-19T06:52:35.073378Z",
     "shell.execute_reply.started": "2023-02-19T06:52:35.009873Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBR model prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:782: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>overestimate</th>\n",
       "      <th>underestimate</th>\n",
       "      <th>bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0-1mo</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.22</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1mo-1y</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-2y</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3y</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.24</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-4y</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.22</td>\n",
       "      <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-5y</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5-6y</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6-7y</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7-8y</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.33</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8-9y</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9-10y</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        correct  overestimate  underestimate  bins\n",
       "0-1mo      0.57          0.21           0.22   242\n",
       "1mo-1y     0.66          0.17           0.18  1644\n",
       "1-2y       0.63          0.19           0.18  1032\n",
       "2-3y       0.58          0.18           0.24   654\n",
       "3-4y       0.62          0.16           0.22   571\n",
       "4-5y       0.53          0.25           0.22   557\n",
       "5-6y       0.56          0.22           0.21   569\n",
       "6-7y       0.56          0.22           0.22   481\n",
       "7-8y       0.51          0.16           0.33   372\n",
       "8-9y       0.56          0.21           0.23   348\n",
       "9-10y      0.67          0.18           0.14   338"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgbr 모델 예측값\n",
    "xgbr = xgb.XGBRegressor()\n",
    "xgbr.load_model('result/size/acc1-0.601_acc3-0.966_XGBR_10fold/model.model')\n",
    "\n",
    "y_pred = xgbr.predict(x_test).flatten()\n",
    "y_pred = np.round(y_pred * 2) / 2\n",
    "\n",
    "\n",
    "# 계산\n",
    "x_age = np.array([math.floor(x) for x in x_test[:,0]])\n",
    "xvals = []\n",
    "yvals = []\n",
    "cvals = []\n",
    "age_bins = []\n",
    "p_under, p_over, p_cor = [], [], []\n",
    "po_under, po_over, po_cor = [], [], []\n",
    "age_interval = ['0-1mo','1mo-1y'] + [f'{i}-{i+1}y' for i in range(1,10)]\n",
    "\n",
    "# 신생아\n",
    "ag_mask = (x_test[:,0] < 1/12)\n",
    "p_over.append(np.mean(y_pred[ag_mask] > y_test[ag_mask]))\n",
    "p_under.append(np.mean(y_pred[ag_mask] < y_test[ag_mask]))\n",
    "p_cor.append(np.mean(y_pred[ag_mask] == y_test[ag_mask]))\n",
    "age_bins.append(np.sum(ag_mask))\n",
    "\n",
    "# 영아\n",
    "ag_mask = (1/12 <= x_test[:,0]) & (x_test[:,0] < 1)\n",
    "p_over.append(np.mean(y_pred[ag_mask] > y_test[ag_mask]))\n",
    "p_under.append(np.mean(y_pred[ag_mask] < y_test[ag_mask]))\n",
    "p_cor.append(np.mean(y_pred[ag_mask] == y_test[ag_mask]))\n",
    "age_bins.append(np.sum(ag_mask))\n",
    "\n",
    "# 소아 (1~10살)\n",
    "for x in np.arange(1, 10, 1):\n",
    "    ag_mask = (x_age == x)\n",
    "    p_over.append(np.mean(y_pred[ag_mask] > y_test[ag_mask]))\n",
    "    p_under.append(np.mean(y_pred[ag_mask] < y_test[ag_mask]))\n",
    "    p_cor.append(np.mean(y_pred[ag_mask] == y_test[ag_mask]))\n",
    "    age_bins.append(np.sum(ag_mask))\n",
    "    \n",
    "df_est = pd.DataFrame({'correct':p_cor, 'overestimate':p_over, 'underestimate':p_under, 'bins':age_bins}, index=age_interval)\n",
    "print(f'XGBR model prediction')\n",
    "df_est.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d20ad0e7-4837-4121-b0b4-a99208845f52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T06:21:22.796155Z",
     "iopub.status.busy": "2023-02-19T06:21:22.795643Z",
     "iopub.status.idle": "2023-02-19T06:21:22.963196Z",
     "shell.execute_reply": "2023-02-19T06:21:22.962504Z",
     "shell.execute_reply.started": "2023-02-19T06:21:22.796100Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3905ead9d0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(p_cor, label='correct')\n",
    "plt.plot(p_under, label='underestimate')\n",
    "plt.plot(p_over, label='overestimate')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "painstudy_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
